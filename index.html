<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-21T00:00:00Z">2025-02-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AVD2: Accident Video Diffusion for Accident Video Description <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Li, Keyuan Zhou, Tong Liu, Yu Wang, Mingqiao Zhuang, Huan-ang Gao, Bu Jin, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic accidents present complex challenges for autonomous driving, often
featuring unpredictable scenarios that hinder accurate system interpretation
and responses. Nonetheless, prevailing methodologies fall short in elucidating
the causes of accidents and proposing preventive measures due to the paucity of
training data specific to accident scenarios. In this work, we introduce AVD2
(Accident Video Diffusion for Accident Video Description), a novel framework
that enhances accident scene understanding by generating accident videos that
aligned with detailed natural language descriptions and reasoning, resulting in
the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding)
dataset. Empirical results reveal that the integration of the EMM-AU dataset
establishes state-of-the-art performance across both automated metrics and
human evaluations, markedly advancing the domains of accident analysis and
prevention. Project resources are available at https://an-answer-tree.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025, Project Page: https://an-answer-tree.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limitations of current humanoid robot control
frameworks, which primarily rely on reactive mechanisms and lack autonomous
interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel
framework that integrates language understanding, egocentric scene perception,
and motion control, enabling universal humanoid control. Humanoid-VLA begins
with language-motion pre-alignment using non-egocentric human motion datasets
paired with textual descriptions, allowing the model to learn universal motion
patterns and action semantics. We then incorporate egocentric visual context
through a parameter efficient video-conditioned fine-tuning, enabling
context-aware motion generation. Furthermore, we introduce a self-supervised
data augmentation strategy that automatically generates pseudoannotations
directly derived from motion data. This process converts raw motion sequences
into informative question-answer pairs, facilitating the effective use of
large-scale unlabeled video data. Built upon whole-body control architectures,
extensive experiments show that Humanoid-VLA achieves object interaction and
environment exploration tasks with enhanced contextual awareness, demonstrating
a more human-like capacity for adaptive and intelligent engagement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Foundation Models in Medical Image Analysis: Advances and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Vision Foundation Models (VFMs), particularly Vision
Transformers (ViT) and Segment Anything Model (SAM), has sparked significant
advances in the field of medical image analysis. These models have demonstrated
exceptional capabilities in capturing long-range dependencies and achieving
high generalization in segmentation tasks. However, adapting these large models
to medical image analysis presents several challenges, including domain
differences between medical and natural images, the need for efficient model
adaptation strategies, and the limitations of small-scale medical datasets.
This paper reviews the state-of-the-art research on the adaptation of VFMs to
medical image segmentation, focusing on the challenges of domain adaptation,
model compression, and federated learning. We discuss the latest developments
in adapter-based improvements, knowledge distillation techniques, and
multi-scale contextual feature modeling, and propose future directions to
overcome these bottlenecks. Our analysis highlights the potential of VFMs,
along with emerging methodologies such as federated learning and model
compression, to revolutionize medical image analysis and enhance clinical
applications. The goal of this work is to provide a comprehensive overview of
current approaches and suggest key areas for future research that can drive the
next wave of innovation in medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via
  Probabilistic Spiking Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Velibor Bojković, Xiaofeng Wu, Bin Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to
Artificial Neural Networks (ANNs) by mimicking biological neural principles,
establishing them as a promising approach to mitigate the increasing energy
demands of large-scale neural models. However, fully harnessing the
capabilities of SNNs remains challenging due to their discrete signal
processing and temporal dynamics. ANN-SNN conversion has emerged as a practical
approach, enabling SNNs to achieve competitive performance on complex machine
learning tasks. In this work, we identify a phenomenon in the ANN-SNN
conversion framework, termed temporal misalignment, in which random spike
rearrangement across SNN layers leads to performance improvements. Based on
this observation, we introduce biologically plausible two-phase probabilistic
(TPP) spiking neurons, further enhancing the conversion process. We demonstrate
the advantages of our proposed method both theoretically and empirically
through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet
across a variety of architectures, achieving state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatVLA: Unified Multimodal Understanding and Robot Control with
  Vision-Language-Action Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess a unified cognitive ability to perceive, comprehend, and
interact with the physical world. Why can't large language models replicate
this holistic understanding? Through a systematic analysis of existing training
paradigms in vision-language-action models (VLA), we identify two key
challenges: spurious forgetting, where robot training overwrites crucial
visual-text alignments, and task interference, where competing control and
understanding tasks degrade performance when trained jointly. To overcome these
limitations, we propose ChatVLA, a novel framework featuring Phased Alignment
Training, which incrementally integrates multimodal data after initial control
mastery, and a Mixture-of-Experts architecture to minimize task interference.
ChatVLA demonstrates competitive performance on visual question-answering
datasets and significantly surpasses state-of-the-art vision-language-action
(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a
six times higher performance on MMMU and scores 47.2% on MMStar with a more
parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates
superior performance on 25 real-world robot manipulation tasks compared to
existing VLA methods like OpenVLA. Our findings highlight the potential of our
unified framework for achieving both robust multimodal understanding and
effective robot control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RelaCtrl: Relevance-Guided Efficient Control for Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Cao, Jing Wang, Ao Ma, Jiasong Feng, Zhanjie Zhang, Xuanhua He, Shanyuan Liu, Bo Cheng, Dawei Leng, Yuhui Yin, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Diffusion Transformer plays a pivotal role in advancing text-to-image and
text-to-video generation, owing primarily to its inherent scalability. However,
existing controlled diffusion transformer methods incur significant parameter
and computational overheads and suffer from inefficient resource allocation due
to their failure to account for the varying relevance of control information
across different transformer layers. To address this, we propose the
Relevance-Guided Efficient Controllable Generation framework, RelaCtrl,
enabling efficient and resource-optimized integration of control signals into
the Diffusion Transformer. First, we evaluate the relevance of each layer in
the Diffusion Transformer to the control information by assessing the
"ControlNet Relevance Score"-i.e., the impact of skipping each control layer on
both the quality of generation and the control effectiveness during inference.
Based on the strength of the relevance, we then tailor the positioning,
parameter scale, and modeling capacity of the control layers to reduce
unnecessary parameters and redundant computations. Additionally, to further
improve efficiency, we replace the self-attention and FFN in the commonly used
copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),
enabling efficient implementation of both the token mixer and channel mixer.
Both qualitative and quantitative experimental results demonstrate that our
approach achieves superior performance with only 15% of the parameters and
computational complexity compared to PixArt-delta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://360cvgroup.github.io/RelaCtrl/ Github:
  https://github.com/360CVGroup/RelaCtrl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex
  Task Automation on PC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of MLLM-based GUI agents, compared to smartphones, the PC
scenario not only features a more complex interactive environment, but also
involves more intricate intra- and inter-app workflows. To address these
issues, we propose a hierarchical agent framework named PC-Agent. Specifically,
from the perception perspective, we devise an Active Perception Module (APM) to
overcome the inadequate abilities of current MLLMs in perceiving screenshot
content. From the decision-making perspective, to handle complex user
instructions and interdependent subtasks more effectively, we propose a
hierarchical multi-agent collaboration architecture that decomposes
decision-making processes into Instruction-Subtask-Action levels. Within this
architecture, three agents (i.e., Manager, Progress and Decision) are set up
for instruction decomposition, progress tracking and step-by-step
decision-making respectively. Additionally, a Reflection agent is adopted to
enable timely bottom-up error feedback and adjustment. We also introduce a new
benchmark PC-Eval with 25 real-world complex instructions. Empirical results on
PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task
success rate over previous state-of-the-art methods. The code is available at
https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in Sequence: Do Large Language Models Understand Sequential
  Recommendation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently emerged as promising tools for
recommendation thanks to their advanced textual understanding ability and
context-awareness. Despite the current practice of training and evaluating
LLM-based recommendation (LLM4Rec) models under a sequential recommendation
scenario, we found that whether these models understand the sequential
information inherent in users' item interaction sequences has been largely
overlooked. In this paper, we first demonstrate through a series of experiments
that existing LLM4Rec models do not fully capture sequential information both
during training and inference. Then, we propose a simple yet effective
LLM-based sequential recommender, called LLM-SRec, a method that enhances the
integration of sequential information into LLMs by distilling the user
representations extracted from a pre-trained CF-SRec model into LLMs. Our
extensive experiments show that LLM-SRec enhances LLMs' ability to understand
users' item interaction sequences, ultimately leading to improved
recommendation performance. Furthermore, unlike existing LLM4Rec models that
require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by
training only a few lightweight MLPs, highlighting its practicality in
real-world applications. Our code is available at
https://github.com/Sein-Kim/LLM-SRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reproducing NevIR: Negation in Neural Information Retrieval <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coen van den Elsen, Francien Barkhof, Thijmen Nijdam, Simon Lupart, Mohammad Alliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negation is a fundamental aspect of human communication, yet it remains a
challenge for Language Models (LMs) in Information Retrieval (IR). Despite the
heavy reliance of modern neural IR systems on LMs, little attention has been
given to their handling of negation. In this study, we reproduce and extend the
findings of NevIR, a benchmark study that revealed most IR models perform at or
below the level of random ranking when dealing with negation. We replicate
NevIR's original experiments and evaluate newly developed state-of-the-art IR
models. Our findings show that a recently emerging category - listwise Large
Language Model (LLM) rerankers - outperforms other models but still
underperforms human performance. Additionally, we leverage ExcluIR, a benchmark
dataset designed for exclusionary queries with extensive negation, to assess
the generalizability of negation understanding. Our findings suggest that
fine-tuning on one dataset does not reliably improve performance on the other,
indicating notable differences in their data distributions. Furthermore, we
observe that only cross-encoders and listwise LLM rerankers achieve reasonable
performance across both negation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, under review at SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Range Retrieval with Graph-Based Indices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalen Dobson Manohar, Taekseung Kim, Guy E. Blelloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving points based on proximity in a high-dimensional vector space is a
crucial step in information retrieval applications. The approximate nearest
neighbor search (ANNS) problem, which identifies the $k$ nearest neighbors for
a query (approximately, since exactly is hard), has been extensively studied in
recent years. However, comparatively little attention has been paid to the
related problem of finding all points within a given distance of a query, the
range retrieval problem, despite its applications in areas such as duplicate
detection, plagiarism checking, and facial recognition. In this paper, we
present a set of algorithms for range retrieval on graph-based vector indices,
which are known to achieve excellent performance on ANNS queries. Since a range
query may have anywhere from no matching results to thousands of matching
results in the database, we introduce a set of range retrieval algorithms based
on modifications of the standard graph search that adapt to terminate quickly
on queries in the former group, and to put more resources into finding results
for the latter group. Due to the lack of existing benchmarks for range
retrieval, we also undertake a comprehensive study of range characteristics of
existing embedding datasets, and select a suitable range retrieval radius for
eight existing datasets with up to 100 million points in addition to the one
existing benchmark. We test our algorithms on these datasets, and find up to
100x improvement in query throughput over a naive baseline approach, with 5-10x
improvement on average, and strong performance up to 100 million data points.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-20T00:00:00Z">2025-02-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">115</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical
  and Cultural Artifacts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Ghaboura, Ketan More, Ritesh Thawkar, Wafa Alghallabi, Omkar Thawakar, Fahad Shahbaz Khan, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding historical and cultural artifacts demands human expertise and
advanced computational techniques, yet the process remains complex and
time-intensive. While large multimodal models offer promising support, their
evaluation and improvement require a standardized benchmark. To address this,
we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning
266 distinct cultures across 10 major historical regions. Designed for
AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological
discoveries, TimeTravel provides a structured dataset and robust evaluation
framework to assess AI models' capabilities in classification, interpretation,
and historical comprehension. By integrating AI with historical research,
TimeTravel fosters AI-powered tools for historians, archaeologists,
researchers, and cultural tourists to extract valuable insights while ensuring
technology contributes meaningfully to historical discovery and cultural
heritage preservation. We evaluate contemporary AI models on TimeTravel,
highlighting their strengths and identifying areas for improvement. Our goal is
to establish AI as a reliable partner in preserving cultural heritage, ensuring
that technological advancements contribute meaningfully to historical
discovery. Our code is available at:
\url{https://github.com/mbzuai-oryx/TimeTravel}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Multimodal RAG through a Chart-based Document
  Question-Answering Generation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Jiang Zhong, Li Jin, Jingwang Huang, Jingpeng Gao, Qing Liu, Yang Bai, Jingyuan Zhang, Rui Jiang, Kaiwen Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning
capabilities by integrating external knowledge. However, existing benchmarks
primarily focus on simple image-text interactions, overlooking complex visual
formats like charts that are prevalent in real-world applications. In this
work, we introduce a novel task, Chart-based MRAG, to address this limitation.
To semi-automatically generate high-quality evaluation samples, we propose
CHARt-based document question-answering GEneration (CHARGE), a framework that
produces evaluation data through structured keypoint extraction, crossmodal
verification, and keypoint-based generation. By combining CHARGE with expert
validation, we construct Chart-MRAG Bench, a comprehensive benchmark for
chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8
domains from real-world documents. Our evaluation reveals three critical
limitations in current approaches: (1) unified multimodal embedding retrieval
methods struggles in chart-based scenarios, (2) even with ground-truth
retrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%
Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality
bias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are
released at https://github.com/Nomothings/CHARGE.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Text-Rich Image Understanding via Code-Guided Synthetic
  Multimodal Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning about images with rich text, such as charts and documents, is a
critical application of vision-language models (VLMs). However, VLMs often
struggle in these domains due to the scarcity of diverse text-rich
vision-language data. To address this challenge, we present CoSyn, a framework
that leverages the coding capabilities of text-only large language models
(LLMs) to automatically create synthetic text-rich multimodal data. Given input
text describing a target domain (e.g., "nutrition fact labels"), CoSyn prompts
an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic
images. With the underlying code as textual representations of the synthetic
images, CoSyn can generate high-quality instruction-tuning data, again relying
on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K
images and 2.7M rows of vision-language instruction-tuning data. Comprehensive
experiments on seven benchmarks demonstrate that models trained on our
synthetic data achieve state-of-the-art performance among competitive
open-source models, including Llama 3.2, and surpass proprietary models such as
GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing
data, enabling VLMs to ground information within input images, showcasing its
potential for developing multimodal agents capable of acting in real-world
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 19 figures, 9 tables, website:
  https://yueyang1996.github.io/cosyn/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Concepts Personalization from Single Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalizing generative text-to-image models has seen remarkable progress,
but extending this personalization to text-to-video models presents unique
challenges. Unlike static concepts, personalizing text-to-video models has the
potential to capture dynamic concepts, i.e., entities defined not only by their
appearance but also by their motion. In this paper, we introduce
Set-and-Sequence, a novel framework for personalizing Diffusion Transformers
(DiTs)-based generative video models with dynamic concepts. Our approach
imposes a spatio-temporal weight space within an architecture that does not
explicitly separate spatial and temporal features. This is achieved in two key
stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an
unordered set of frames from the video to learn an identity LoRA basis that
represents the appearance, free from temporal interference. In the second
stage, with the identity LoRAs frozen, we augment their coefficients with
Motion Residuals and fine-tune them on the full video sequence, capturing
motion dynamics. Our Set-and-Sequence framework results in a spatio-temporal
weight space that effectively embeds dynamic concepts into the video model's
output domain, enabling unprecedented editability and compositionality while
setting a new benchmark for personalizing dynamic concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage: https://snap-research.github.io/dynamic_concepts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Large Vision-Language Models (LVLMs) can process inputs with context
lengths up to 128k visual and text tokens, yet they struggle to generate
coherent outputs beyond 1,000 words. We find that the primary limitation is the
absence of long output examples during supervised fine-tuning (SFT). To tackle
this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158
examples, each with multiple input images, an instruction, and corresponding
outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that
maintain high-fidelity to the input images, we employ Direct Preference
Optimization (DPO) to the SFT model. Given the high cost of collecting human
feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which
breaks long outputs into segments and uses iterative corrections to form
preference pairs with the original outputs. Additionally, we develop
MMLongBench-Write, a benchmark featuring six tasks to evaluate the
long-generation capabilities of VLMs. Our 7B parameter model, trained with
LongWriter-V-22k and IterDPO, achieves impressive performance on this
benchmark, outperforming larger proprietary models like GPT-4o. Code and data:
https://github.com/THU-KEG/LongWriter-V
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Diffusability of Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, Aliaksandr Siarohin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent diffusion models have emerged as the leading approach for generating
high-quality images and videos, utilizing compressed latent representations to
reduce the computational burden of the diffusion process. While recent
advancements have primarily focused on scaling diffusion backbones and
improving autoencoder reconstruction quality, the interaction between these
components has received comparatively less attention. In this work, we perform
a spectral analysis of modern autoencoders and identify inordinate
high-frequency components in their latent spaces, which are especially
pronounced in the autoencoders with a large bottleneck channel size. We
hypothesize that this high-frequency component interferes with the
coarse-to-fine nature of the diffusion synthesis process and hinders the
generation quality. To mitigate the issue, we propose scale equivariance: a
simple regularization strategy that aligns latent and RGB spaces across
frequencies by enforcing scale equivariance in the decoder. It requires minimal
code changes and only up to 20K autoencoder fine-tuning steps, yet
significantly improves generation quality, reducing FID by 19% for image
generation on ImageNet-1K 256x256 and FVD by at least 44% for video generation
on Kinetics-700 17x256x256.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 22 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Advanced Techniques for Visual Question Answering: A
  Comprehensive Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiswarya Baby, Tintu Thankom Koshy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) has emerged as a pivotal task in the
intersection of computer vision and natural language processing, requiring
models to understand and reason about visual content in response to natural
language questions. Analyzing VQA datasets is essential for developing robust
models that can handle the complexities of multimodal reasoning. Several
approaches have been developed to examine these datasets, each offering
distinct perspectives on question diversity, answer distribution, and
visual-textual correlations. Despite significant progress, existing VQA models
face challenges related to dataset bias, limited model complexity, commonsense
reasoning gaps, rigid evaluation methods, and generalization to real world
scenarios. This paper presents a comprehensive comparative study of five
advanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling,
BLIP-2, and OFA, each employing distinct methodologies to address these
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, No figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadillah Maani, Numan Saeed, Tausifa Saleem, Zaid Farooq, Hussain Alasmawi, Werner Diehl, Ameera Mohammad, Gareth Waring, Saudabi Valappi, Leanne Bricker, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are becoming increasingly effective in the medical domain,
offering pre-trained models on large datasets that can be readily adapted for
downstream tasks. Despite progress, fetal ultrasound images remain a
challenging domain for foundation models due to their inherent complexity,
often requiring substantial additional training and facing limitations due to
the scarcity of paired multimodal data. To overcome these challenges, here we
introduce FetalCLIP, a vision-language foundation model capable of generating
universal representation of fetal ultrasound images. FetalCLIP was pre-trained
using a multimodal learning approach on a diverse dataset of 210,035 fetal
ultrasound images paired with text. This represents the largest paired dataset
of its kind used for foundation model development to date. This unique training
approach allows FetalCLIP to effectively learn the intricate anatomical
features present in fetal ultrasound images, resulting in robust
representations that can be used for a variety of downstream applications. In
extensive benchmarking across a range of key fetal ultrasound applications,
including classification, gestational age estimation, congenital heart defect
(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all
baselines while demonstrating remarkable generalizability and strong
performance even with limited labeled data. We plan to release the FetalCLIP
model publicly for the benefit of the broader scientific community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVD2: Accident Video Diffusion for Accident Video Description <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Li, Keyuan Zhou, Tong Liu, Yu Wang, Mingqiao Zhuang, Huan-ang Gao, Bu Jin, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic accidents present complex challenges for autonomous driving, often
featuring unpredictable scenarios that hinder accurate system interpretation
and responses.Nonetheless, prevailing methodologies fall short in elucidating
the causes of accidents and proposing preventive measures due to the paucity of
training data specific to accident scenarios.In this work, we introduce AVD2
(Accident Video Diffusion for Accident Video Description), a novel framework
that enhances accident scene understanding by generating accident videos that
aligned with detailed natural language descriptions and reasoning, resulting in
the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding)
dataset. Empirical results reveal that the integration of the EMM-AU dataset
establishes state-of-the-art performance across both automated metrics and
human evaluations, markedly advancing the domains of accident analysis and
prevention. Project resources are available at https://an-answer-tree.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025, Project Page: https://an-answer-tree.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Text-Driven 360-Degree Panorama Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of text-driven 360-degree panorama generation, enabling the
synthesis of 360-degree panoramic images directly from textual descriptions,
marks a transformative advancement in immersive visual content creation. This
innovation significantly simplifies the traditionally complex process of
producing such content. Recent progress in text-to-image diffusion models has
accelerated the rapid development in this emerging field. This survey presents
a comprehensive review of text-driven 360-degree panorama generation, offering
an in-depth analysis of state-of-the-art algorithms and their expanding
applications in 360-degree 3D scene generation. Furthermore, we critically
examine current limitations and propose promising directions for future
research. A curated project page with relevant resources and research papers is
available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limitations of current humanoid robot control
frameworks, which primarily rely on reactive mechanisms and lack autonomous
interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel
framework that integrates language understanding, egocentric scene perception,
and motion control, enabling universal humanoid control. Humanoid-VLA begins
with language-motion pre-alignment using non-egocentric human motion datasets
paired with textual descriptions, allowing the model to learn universal motion
patterns and action semantics. We then incorporate egocentric visual context
through a parameter efficient video-conditioned fine-tuning, enabling
context-aware motion generation. Furthermore, we introduce a self-supervised
data augmentation strategy that automatically generates pseudoannotations
directly derived from motion data. This process converts raw motion sequences
into informative question-answer pairs, facilitating the effective use of
large-scale unlabeled video data. Built upon whole-body control architectures,
extensive experiments show that Humanoid-VLA achieves object interaction and
environment exploration tasks with enhanced contextual awareness, demonstrating
a more human-like capacity for adaptive and intelligent engagement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RendBEV: Semantic Novel View Synthesis for <span class="highlight-title">Self-Supervised</span> Bird's Eye
  View Segmentation <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrique Piñeiro Monteagudo, Leonardo Taccari, Aurel Pjetri, Francesco Sambo, Samuele Salti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention
as a useful representation of the environment to tackle assisted and autonomous
driving tasks. However, most of the existing work focuses on the fully
supervised setting, training networks on large annotated datasets. In this
work, we present RendBEV, a new method for the self-supervised training of BEV
semantic segmentation networks, leveraging differentiable volumetric rendering
to receive supervision from semantic perspective views computed by a 2D
semantic segmentation model. Our method enables zero-shot BEV semantic
segmentation, and already delivers competitive results in this challenging
setting. When used as pretraining to then fine-tune on labeled BEV
ground-truth, our method significantly boosts performance in low-annotation
regimes, and sets a new state of the art when fine-tuning on all available
labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structurally Disentangled Feature Fields Distillation for 3D
  Understanding and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoel Levy, David Shavin, Itai Lang, Sagie Benaim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated the ability to leverage or distill pre-trained
2D features obtained using large pre-trained 2D models into 3D features,
enabling impressive 3D editing and understanding capabilities using only 2D
supervision. Although impressive, models assume that 3D features are captured
using a single feature field and often make a simplifying assumption that
features are view-independent. In this work, we propose instead to capture 3D
features using multiple disentangled feature fields that capture different
structural components of 3D features involving view-dependent and
view-independent components, which can be learned from 2D feature supervision
only. Subsequently, each element can be controlled in isolation, enabling
semantic and structural understanding and editing capabilities. For instance,
using a user click, one can segment 3D features corresponding to a given object
and then segment, edit, or remove their view-dependent (reflective) properties.
We evaluate our approach on the task of 3D segmentation and demonstrate a set
of novel understanding and editing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic
  Understanding, Localization, and Dense Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, Xiaohua Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SigLIP 2, a family of new multilingual vision-language encoders
that build on the success of the original SigLIP. In this second iteration, we
extend the original image-text training objective with several prior,
independently developed techniques into a unified recipe -- this includes
captioning-based pretraining, self-supervised losses (self-distillation, masked
prediction) and online data curation. With these changes, SigLIP 2 models
outperform their SigLIP counterparts at all model scales in core capabilities,
including zero-shot classification, image-text retrieval, and transfer
performance when extracting visual representations for Vision-Language Models
(VLMs). Furthermore, the new training recipe leads to significant improvements
on localization and dense prediction tasks. We also train variants which
support multiple resolutions and preserve the input's native aspect ratio.
Finally, we train on a more diverse data-mixture that includes de-biasing
techniques, leading to much better multilingual understanding and improved
fairness. To allow users to trade off inference cost with performance, we
release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),
and g (1B).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model checkpoints are available at
  https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReVision: A <span class="highlight-title">Dataset</span> and Baseline VLM for Privacy-Preserving
  Task-Oriented Visual Instruction Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Mishra, Richard Noh, Hsiang Fu, Mingda Li, Minji Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and privacy-preserving multimodal interaction is essential as AR,
VR, and modern smartphones with powerful cameras become primary interfaces for
human-computer communication. Existing powerful large vision-language models
(VLMs) enabling multimodal interaction often rely on cloud-based processing,
raising significant concerns about (1) visual privacy by transmitting sensitive
vision data to servers, and (2) their limited real-time, on-device usability.
This paper explores Visual Instruction Rewriting, a novel approach that
transforms multimodal instructions into text-only commands, allowing seamless
integration of lightweight on-device instruction rewriter VLMs (250M
parameters) with existing conversational AI systems, enhancing vision data
privacy. To achieve this, we present a dataset of over 39,000 examples across
14 domains and develop a compact VLM, pretrained on image captioning datasets
and fine-tuned for instruction rewriting. Experimental results, evaluated
through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic
parsing analysis, demonstrate that even a quantized version of the model
(<500MB storage footprint) can achieve effective instruction rewriting, thus
enabling privacy-focused, multimodal AI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image
  Generation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongji Yang, Wencheng Han, Yucheng Zhou, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and
precisely controllable framework for multi-condition image generation. The core
idea behind DC-ControlNet is to decouple control conditions, transforming
global control into a hierarchical system that integrates distinct elements,
contents, and layouts. This enables users to mix these individual conditions
with greater flexibility, leading to more efficient and accurate image
generation control. Previous ControlNet-based models rely solely on global
conditions, which affect the entire image and lack the ability of element- or
region-specific control. This limitation reduces flexibility and can cause
condition misunderstandings in multi-conditional image generation. To address
these challenges, we propose both intra-element and Inter-element Controllers
in DC-ControlNet. The Intra-Element Controller handles different types of
control signals within individual elements, accurately describing the content
and layout characteristics of the object. For interactions between elements, we
introduce the Inter-Element Controller, which accurately handles multi-element
interactions and occlusion based on user-defined relationships. Extensive
evaluations show that DC-ControlNet significantly outperforms existing
ControlNet models and Layout-to-Image generative models in terms of control
flexibility and precision in multi-condition control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing PDF Data for Improving Japanese Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonghun Baek, Akiko Aizawa, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have demonstrated strong performance in
English, but their effectiveness in Japanese remains limited due to the lack of
high-quality training data. Current Japanese LMMs often rely on translated
English datasets, restricting their ability to capture Japan-specific cultural
knowledge. To address this, we explore the potential of Japanese PDF data as a
training resource, an area that remains largely underutilized. We introduce a
fully automated pipeline that leverages pretrained models to extract image-text
pairs from PDFs through layout analysis, OCR, and vision-language pairing,
removing the need for manual annotation. Additionally, we construct instruction
data from extracted image-text pairs to enrich the training data. To evaluate
the effectiveness of PDF-derived data, we train Japanese LMMs and assess their
performance on the Japanese LMM Benchmark. Our results demonstrate substantial
improvements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.
Further analysis highlights the impact of PDF-derived data on various factors,
such as model size and language models, reinforcing its value as a multimodal
resource for Japanese LMMs. We plan to make the source code and data publicly
available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sculpting [CLS] Features for <span class="highlight-title">Pre-Train</span>ed Model-Based Class-Incremental
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Onur Yildirim, Elif Ceren Gok Yildirim, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning requires models to continually acquire knowledge
of new classes without forgetting old ones. Although pre-trained models have
demonstrated strong performance in class-incremental learning, they remain
susceptible to catastrophic forgetting when learning new concepts. Excessive
plasticity in the models breaks generalizability and causes forgetting, while
strong stability results in insufficient adaptation to new classes. This
necessitates effective adaptation with minimal modifications to preserve the
general knowledge of pre-trained models. To address this challenge, we first
introduce a new parameter-efficient fine-tuning module 'Learn and Calibrate',
or LuCA, designed to acquire knowledge through an adapter-calibrator couple,
enabling effective adaptation with well-refined feature representations.
Second, for each learning session, we deploy a sparse LuCA module on top of the
last token just before the classifier, which we refer to as 'Token-level Sparse
Calibration and Adaptation', or TOSCA. This strategic design improves the
orthogonality between the modules and significantly reduces both training and
inference complexity. By leaving the generalization capabilities of the
pre-trained models intact and adapting exclusively via the last token, our
approach achieves a harmonious balance between stability and plasticity.
Extensive experiments demonstrate TOSCA's state-of-the-art performance while
introducing ~8 times fewer parameters compared to prior methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedVAE: Efficient Automated Interpretation of Medical Images with
  Large-Scale Generalizable Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images are acquired at high resolutions with large fields of view in
order to capture fine-grained features necessary for clinical decision-making.
Consequently, training deep learning models on medical images can incur large
computational costs. In this work, we address the challenge of downsizing
medical images in order to improve downstream computational efficiency while
preserving clinically-relevant features. We introduce MedVAE, a family of six
large-scale 2D and 3D autoencoders capable of encoding medical images as
downsized latent representations and decoding latent representations back to
high-resolution images. We train MedVAE autoencoders using a novel two-stage
training approach with 1,052,730 medical images. Across diverse tasks obtained
from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent
representations in place of high-resolution images when training downstream
models can lead to efficiency benefits (up to 70x improvement in throughput)
while simultaneously preserving clinically-relevant features and (2) MedVAE can
decode latent representations back to high-resolution images with high
fidelity. Our work demonstrates that large-scale, generalizable autoencoders
can help address critical efficiency challenges in the medical domain. Our code
is available at https://github.com/StanfordMIMI/MedVAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLOv12: A Breakdown of the Key Architectural Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mujadded Al Rabbani Alif, Muhammad Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an architectural analysis of YOLOv12, a significant
advancement in single-stage, real-time object detection building upon the
strengths of its predecessors while introducing key improvements. The model
incorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and
FlashAttention-driven area-based attention, improving feature extraction,
enhanced efficiency, and robust detections. With multiple model variants,
similar to its predecessors, YOLOv12 offers scalable solutions for both
latency-sensitive and high-accuracy applications. Experimental results manifest
consistent gains in mean average precision (mAP) and inference speed, making
YOLOv12 a compelling choice for applications in autonomous systems, security,
and real-time analytics. By achieving an optimal balance between computational
efficiency and performance, YOLOv12 sets a new benchmark for real-time computer
vision, facilitating deployment across diverse hardware platforms, from edge
devices to high-performance clusters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-<span class="highlight-title">dataset</span> synergistic in supervised learning to pre-label structural
  components in point clouds from shell construction scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Rauch, Thomas Braml
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significant effort required to annotate data for new training datasets
hinders computer vision research and machine learning in the construction
industry. This work explores adapting standard datasets and the latest
transformer model architectures for point cloud semantic segmentation in the
context of shell construction sites. Unlike common approaches focused on object
segmentation of building interiors and furniture, this study addressed the
challenges of segmenting complex structural components in Architecture,
Engineering, and Construction (AEC). We establish a baseline through supervised
training and a custom validation dataset, evaluate the cross-domain inference
with large-scale indoor datasets, and utilize transfer learning to maximize
segmentation performance with minimal new data. The findings indicate that with
minimal fine-tuning, pre-trained transformer architectures offer an effective
strategy for building component segmentation. Our results are promising for
automating the annotation of new, previously unseen data when creating larger
training resources and for the segmentation of frequently recurring objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Zhang, Olaf Wysocki, Steffen Urban, Boris Jutzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has shown significant advantages in novel view
synthesis (NVS), particularly in achieving high rendering speeds and
high-quality results. However, its geometric accuracy in 3D reconstruction
remains limited due to the lack of explicit geometric constraints during
optimization. This paper introduces CDGS, a confidence-aware depth
regularization approach developed to enhance 3DGS. We leverage multi-cue
confidence maps of monocular depth estimation and sparse Structure-from-Motion
depth to adaptively adjust depth supervision during the optimization process.
Our method demonstrates improved geometric detail preservation in early
training stages and achieves competitive performance in both NVS quality and
geometric accuracy. Experiments on the publicly available Tanks and Temples
benchmark dataset show that our method achieves more stable convergence
behavior and more accurate geometric reconstruction results, with improvements
of up to 2.31 dB in PSNR for NVS and consistently lower geometric errors in
M3C2 distance metrics. Notably, our method reaches comparable F-scores to the
original 3DGS with only 50% of the training iterations. We expect this work
will facilitate the development of efficient and accurate 3D reconstruction
systems for real-world applications such as digital twin creation, heritage
preservation, or forestry applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution
  Network for Pedestrian and Heterogeneous Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Li, Stamos Katsigiannis, Tae-Kyun Kim, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction allows better decision-making in applications of
autonomous vehicles or surveillance by predicting the short-term future
movement of traffic agents. It is classified into pedestrian or heterogeneous
trajectory prediction. The former exploits the relatively consistent behavior
of pedestrians, but is limited in real-world scenarios with heterogeneous
traffic agents such as cyclists and vehicles. The latter typically relies on
extra class label information to distinguish the heterogeneous agents, but such
labels are costly to annotate and cannot be generalized to represent different
behaviors within the same class of agents. In this work, we introduce the
behavioral pseudo-labels that effectively capture the behavior distributions of
pedestrians and heterogeneous agents solely based on their motion features,
significantly improving the accuracy of trajectory prediction. To implement the
framework, we propose the Behavioral Pseudo-Label Informed Sparse Graph
Convolution Network (BP-SGCN) that learns pseudo-labels and informs to a
trajectory predictor. For optimization, we propose a cascaded training scheme,
in which we first learn the pseudo-labels in an unsupervised manner, and then
perform end-to-end fine-tuning on the labels in the direction of increasing the
trajectory prediction accuracy. Experiments show that our pseudo-labels
effectively model different behavior clusters and improve trajectory
prediction. Our proposed BP-SGCN outperforms existing methods using both
pedestrian (ETH/UCY, pedestrian-only SDD) and heterogeneous agent datasets
(SDD, Argoverse 1).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only
  VIBE MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Graf, Hendrik Möller, Sophie Starck, Matan Atad, Philipp Braun, Jonathan Stelter, Annette Peters, Lilian Krist, Stefan N. Willich, Henry Völzke, Robin Bülow, Klaus Berger, Tobias Pischon, Thoralf Niendorf, Johannes Paetzold, Dimitrios Karampinos, Daniel Rueckert, Jan Kirschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images
suitable for water and fat signal composition estimation. While the two-point
VIBE provides water-fat-separated images, the six-point VIBE allows estimation
of the effective transversal relaxation rate R2* and the proton density fat
fraction (PDFF), which are imaging markers for health and disease. Ambiguity
during signal reconstruction can lead to water-fat swaps. This shortcoming
challenges the application of VIBE-MRI for automated PDFF analyses of
large-scale clinical data and of population studies. This study develops an
automated pipeline to detect and correct water-fat swaps in
non-contrast-enhanced VIBE images. Our three-step pipeline begins with training
a segmentation network to classify volumes as "fat-like" or "water-like," using
synthetic water-fat swaps generated by merging fat and water volumes with
Perlin noise. Next, a denoising diffusion image-to-image network predicts water
volumes as signal priors for correction. Finally, we integrate this prior into
a physics-constrained model to recover accurate water and fat signals. Our
approach achieves a < 1% error rate in water-fat swap detection for a 6-point
VIBE. Notably, swaps disproportionately affect individuals in the Underweight
and Class 3 Obesity BMI categories. Our correction algorithm ensures accurate
solution selection in chemical phase MRIs, enabling reliable PDFF estimation.
This forms a solid technical foundation for automated large-scale population
imaging analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NAVIG: Natural Language-guided Analysis with Vision Language Models for
  Image Geo-localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Zhang, Runze Li, Tasnim Kabir, Jordan Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image geo-localization is the task of predicting the specific location of an
image and requires complex reasoning across visual, geographical, and cultural
contexts. While prior Vision Language Models (VLMs) have the best accuracy at
this task, there is a dearth of high-quality datasets and models for analytical
reasoning. We first create NaviClues, a high-quality dataset derived from
GeoGuessr, a popular geography game, to supply examples of expert reasoning
from language. Using this dataset, we present Navig, a comprehensive image
geo-localization framework integrating global and fine-grained image
information. By reasoning with language, Navig reduces the average distance
error by 14% compared to previous state-of-the-art models while requiring fewer
than 1000 training samples. Our dataset and code are available at
https://github.com/SparrowZheyuan18/Navig/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monocular Depth Estimation and Segmentation for Transparent Object with
  Iterative Semantic and Geometric Fusion <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangyuan Liu, Hongxuan Ma, Yuxin Guo, Yuhao Zhao, Chi Zhang, Wei Sui, Wei Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparent object perception is indispensable for numerous robotic tasks.
However, accurately segmenting and estimating the depth of transparent objects
remain challenging due to complex optical properties. Existing methods
primarily delve into only one task using extra inputs or specialized sensors,
neglecting the valuable interactions among tasks and the subsequent refinement
process, leading to suboptimal and blurry predictions. To address these issues,
we propose a monocular framework, which is the first to excel in both
segmentation and depth estimation of transparent objects, with only a
single-image input. Specifically, we devise a novel semantic and geometric
fusion module, effectively integrating the multi-scale information between
tasks. In addition, drawing inspiration from human perception of objects, we
further incorporate an iterative strategy, which progressively refines initial
features for clearer results. Experiments on two challenging synthetic and
real-world datasets demonstrate that our model surpasses state-of-the-art
monocular, stereo, and multi-view methods by a large margin of about
38.8%-46.2% with only a single RGB input. Codes and models are publicly
available at https://github.com/L-J-Yuan/MODEST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA(2025). The code is accessible through:
  https://github.com/L-J-Yuan/MODEST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Foundation Models in Medical Image Analysis: Advances and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Vision Foundation Models (VFMs), particularly Vision
Transformers (ViT) and Segment Anything Model (SAM), has sparked significant
advances in the field of medical image analysis. These models have demonstrated
exceptional capabilities in capturing long-range dependencies and achieving
high generalization in segmentation tasks. However, adapting these large models
to medical image analysis presents several challenges, including domain
differences between medical and natural images, the need for efficient model
adaptation strategies, and the limitations of small-scale medical datasets.
This paper reviews the state-of-the-art research on the adaptation of VFMs to
medical image segmentation, focusing on the challenges of domain adaptation,
model compression, and federated learning. We discuss the latest developments
in adapter-based improvements, knowledge distillation techniques, and
multi-scale contextual feature modeling, and propose future directions to
overcome these bottlenecks. Our analysis highlights the potential of VFMs,
along with emerging methodologies such as federated learning and model
compression, to revolutionize medical image analysis and enhance clinical
applications. The goal of this work is to provide a comprehensive overview of
current approaches and suggest key areas for future research that can drive the
next wave of innovation in medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Monocular Depth Estimation Robust to Reflective Surface
  Leveraged by Triplet Mining <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonhyeok Choi, Kyumin Hwang, Wei Peng, Minwoo Choi, Sunghoon Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation (SSMDE) aims to predict the dense
depth map of a monocular image, by learning depth from RGB image sequences,
eliminating the need for ground-truth depth labels. Although this approach
simplifies data acquisition compared to supervised methods, it struggles with
reflective surfaces, as they violate the assumptions of Lambertian reflectance,
leading to inaccurate training on such surfaces. To tackle this problem, we
propose a novel training strategy for an SSMDE by leveraging triplet mining to
pinpoint reflective regions at the pixel level, guided by the camera geometry
between different viewpoints. The proposed reflection-aware triplet mining loss
specifically penalizes the inappropriate photometric error minimization on the
localized reflective regions while preserving depth accuracy in non-reflective
areas. We also incorporate a reflection-aware knowledge distillation method
that enables a student model to selectively learn the pixel-level knowledge
from reflective and non-reflective regions. This results in robust depth
estimation across areas. Evaluation results on multiple datasets demonstrate
that our method effectively enhances depth quality on reflective surfaces and
outperforms state-of-the-art SSMDE baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wang, Fan Wu, Ruihui Li, Yunchuan Qin, Zhuo Tang, Kenli Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Semantic Scene Completion (SSC) provides comprehensive scene geometry and
semantics for autonomous driving perception, which is crucial for enabling
accurate and reliable decision-making. However, existing SSC methods are
limited to capturing sparse information from the current frame or naively
stacking multi-frame temporal features, thereby failing to acquire effective
scene context. These approaches ignore critical motion dynamics and struggle to
achieve temporal consistency. To address the above challenges, we propose a
novel temporal SSC method FlowScene: Learning Temporal 3D Semantic Scene
Completion via Optical Flow Guidance. By leveraging optical flow, FlowScene can
integrate motion, different viewpoints, occlusions, and other contextual cues,
thereby significantly improving the accuracy of 3D scene completion.
Specifically, our framework introduces two key components: (1) a Flow-Guided
Temporal Aggregation module that aligns and aggregates temporal features using
optical flow, capturing motion-aware context and deformable structures; and (2)
an Occlusion-Guided Voxel Refinement module that injects occlusion masks and
temporally aggregated features into 3D voxel space, adaptively refining voxel
representations for explicit geometric modeling. Experimental results
demonstrate that FlowScene achieves state-of-the-art performance on the
SemanticKITTI and SSCBench-KITTI-360 benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mobile Robotic Approach to Autonomous Surface Scanning in Legal
  Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Grube, Sarah Latus, Martin Fischer, Vidas Raudonis, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Comprehensive legal medicine documentation includes both an internal
but also an external examination of the corpse. Typically, this documentation
is conducted manually during conventional autopsy. A systematic digital
documentation would be desirable, especially for the external examination of
wounds, which is becoming more relevant for legal medicine analysis. For this
purpose, RGB surface scanning has been introduced. While a manual full surface
scan using a handheld camera is timeconsuming and operator dependent, floor or
ceiling mounted robotic systems require substantial space and a dedicated room.
Hence, we consider whether a mobile robotic system can be used for external
documentation. Methods: We develop a mobile robotic system that enables
full-body RGB-D surface scanning. Our work includes a detailed configuration
space analysis to identify the environmental parameters that need to be
considered to successfully perform a surface scan. We validate our findings
through an experimental study in the lab and demonstrate the system's
application in a legal medicine environment. Results: Our configuration space
analysis shows that a good trade-off between coverage and time is reached with
three robot base positions, leading to a coverage of 94.96 %. Experiments
validate the effectiveness of the system in accurately capturing body surface
geometry with an average surface coverage of 96.90 +- 3.16 % and 92.45 +- 1.43
% for a body phantom and actual corpses, respectively. Conclusion: This work
demonstrates the potential of a mobile robotic system to automate RGB-D surface
scanning in legal medicine, complementing the use of post-mortem CT scans for
inner documentation. Our results indicate that the proposed system can
contribute to more efficient and autonomous legal medicine documentation,
reducing the need for manual intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted and accepted for presentation at CARS 2025. This preprint
  has not undergone peer review or post-submission revisions. The final version
  of this work will appear in the official CARS 2025 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Meng, Kaiyuan Li, Chenran Huang, Chen Gao, Xinlei Chen, Yong Li, Xiaoping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have demonstrated remarkable
capabilities across a range of multimodal tasks. However, their inference
efficiency is constrained by the large number of visual tokens processed during
decoding. To address this challenge, we propose Per-Layer Per-Head Vision Token
Pruning (PLPHP), a two-level fine-grained pruning method including Layer-Level
Retention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the
Vision Token Re-attention phenomenon across decoder layers, we dynamically
adjust token retention rates layer by layer. Layers that exhibit stronger
attention to visual information preserve more vision tokens, while layers with
lower vision attention are aggressively pruned. Furthermore, PLPHP applies
pruning at the attention head level, enabling different heads within the same
layer to independently retain critical context. Experiments on multiple
benchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and
reduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of
0.46% average performance drop, while also achieving notable performance
improvements in multi-image tasks. These results highlight the effectiveness of
fine-grained token pruning and contribute to advancing the efficiency and
scalability of LVLMs. Our source code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of
  4D Radar and Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyi Xiong, Zean Zou, Qiuchi Zhao, Fengchun He, Bing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the previous state-of-the-art 4D radar-camera fusion-based 3D object
detection method, LXL utilizes the predicted image depth distribution maps and
radar 3D occupancy grids to assist the sampling-based image view
transformation. However, the depth prediction lacks accuracy and consistency,
and the concatenation-based fusion in LXL impedes the model robustness. In this
work, we propose LXLv2, where modifications are made to overcome the
limitations and improve the performance. Specifically, considering the position
error in radar measurements, we devise a one-to-many depth supervision strategy
via radar points, where the radar cross section (RCS) value is further
exploited to adjust the supervision area for object-level depth consistency.
Additionally, a channel and spatial attention-based fusion module named
CSAFusion is introduced to improve feature adaptiveness. Experimental results
on the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can
outperform LXL in detection accuracy, inference speed and robustness,
demonstrating the effectiveness of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nearshore Underwater Target Detection Meets UAV-borne Hyperspectral
  Remote Sensing: A Novel Hybrid-level Contrastive Learning Framework and
  Benchmark <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Qi, Chuanhong Zhou, Xingyue Liu, Chen Chen, Dehui Zhu, Kangcheng Bin, Ping Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UAV-borne hyperspectral remote sensing has emerged as a promising approach
for underwater target detection (UTD). However, its effectiveness is hindered
by spectral distortions in nearshore environments, which compromise the
accuracy of traditional hyperspectral UTD (HUTD) methods that rely on
bathymetric model. These distortions lead to significant uncertainty in target
and background spectra, challenging the detection process. To address this, we
propose the Hyperspectral Underwater Contrastive Learning Network (HUCLNet), a
novel framework that integrates contrastive learning with a self-paced learning
paradigm for robust HUTD in nearshore regions. HUCLNet extracts discriminative
features from distorted hyperspectral data through contrastive learning, while
the self-paced learning strategy selectively prioritizes the most informative
samples. Additionally, a reliability-guided clustering strategy enhances the
robustness of learned representations.To evaluate the method effectiveness, we
conduct a novel nearshore HUTD benchmark dataset, ATR2-HUTD, covering three
diverse scenarios with varying water types and turbidity, and target types.
Extensive experiments demonstrate that HUCLNet significantly outperforms
state-of-the-art methods. The dataset and code will be publicly available at:
https://github.com/qjh1996/HUTD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18pages,13figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor
  Top-K Vision Alignment and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukai Shi, Cidan Shi, Zhipeng Weng, Yin Tian, Xiaoyu Xian, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared and visible image fusion (IVIF) is increasingly applied in critical
fields such as video surveillance and autonomous driving systems. Significant
progress has been made in deep learning-based fusion methods. However, these
models frequently encounter out-of-distribution (OOD) scenes in real-world
applications, which severely impact their performance and reliability.
Therefore, addressing the challenge of OOD data is crucial for the safe
deployment of these models in open-world environments. Unlike existing
research, our focus is on the challenges posed by OOD data in real-world
applications and on enhancing the robustness and generalization of models. In
this paper, we propose an infrared-visible fusion framework based on Multi-View
Augmentation. For external data augmentation, Top-k Selective Vision Alignment
is employed to mitigate distribution shifts between datasets by performing
RGB-wise transformations on visible images. This strategy effectively
introduces augmented samples, enhancing the adaptability of the model to
complex real-world scenarios. Additionally, for internal data augmentation,
self-supervised learning is established using Weak-Aggressive Augmentation.
This enables the model to learn more robust and general feature representations
during the fusion process, thereby improving robustness and generalization.
Extensive experiments demonstrate that the proposed method exhibits superior
performance and robustness across various conditions and environments. Our
approach significantly enhances the reliability and stability of IVIF tasks in
practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE T-CSVT. We mainly discuss the out-of-distribution challenges in
  infrared and visible image fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Misalignment and Probabilistic Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Velibor Bojković, Xiaofeng Wu, Bin Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to
Artificial Neural Networks (ANNs) by mimicking biological neural principles,
establishing them as a promising approach to mitigate the increasing energy
demands of large-scale neural models. However, fully harnessing the
capabilities of SNNs remains challenging due to their discrete signal
processing and temporal dynamics. ANN-SNN conversion has emerged as a practical
approach, enabling SNNs to achieve competitive performance on complex machine
learning tasks. In this work, we identify a phenomenon in the ANN-SNN
conversion framework, termed temporal misalignment, in which random spike
rearrangement across SNN layers leads to performance improvements. Based on
this observation, we introduce biologically plausible two-phase probabilistic
(TPP) spiking neurons, further enhancing the conversion process. We demonstrate
the advantages of our proposed method both theoretically and empirically
through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet
across a variety of architectures, achieving state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Extra Modality Helps Segmentor Find Camouflaged Objects Well 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Fang, Chunming He, Longxiang Tang, Yuelin Zhang, Chenyang Zhu, Yuqi Shen, Chubin Chen, Guoxia Xu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged Object Segmentation (COS) remains a challenging problem due to
the subtle visual differences between camouflaged objects and backgrounds.
Owing to the exceedingly limited visual cues available from visible spectrum,
previous RGB single-modality approaches often struggle to achieve satisfactory
results, prompting the exploration of multimodal data to enhance detection
accuracy. In this work, we present UniCOS, a novel framework that effectively
leverages diverse data modalities to improve segmentation performance. UniCOS
comprises two key components: a multimodal segmentor, UniSEG, and a cross-modal
knowledge learning module, UniLearner. UniSEG employs a state space fusion
mechanism to integrate cross-modal features within a unified state space,
enhancing contextual understanding and improving robustness to integration of
heterogeneous data. Additionally, it includes a fusion-feedback mechanism that
facilitate feature extraction. UniLearner exploits multimodal data unrelated to
the COS task to improve the segmentation ability of the COS models by
generating pseudo-modal content and cross-modal semantic associations.
Extensive experiments demonstrate that UniSEG outperforms existing Multimodal
COS (MCOS) segmentors, regardless of whether real or pseudo-multimodal COS data
is available. Moreover, in scenarios where multimodal COS data is unavailable
but multimodal non-COS data is accessible, UniLearner effectively exploits
these data to enhance segmentation performance. Our code will be made publicly
available on \href{https://github.com/cnyvfang/UniCOS}{GitHub}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-image Reflectance and Transmittance Estimation from Any Flatbed
  Scanner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Rodriguez-Pardo, David Pascual-Hernandez, Javier Rodriguez-Vazquez, Jorge Lopez-Moreno, Elena Garces
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flatbed scanners have emerged as promising devices for high-resolution,
single-image material capture. However, existing approaches assume very
specific conditions, such as uniform diffuse illumination, which are only
available in certain high-end devices, hindering their scalability and cost. In
contrast, in this work, we introduce a method inspired by intrinsic image
decomposition, which accurately removes both shading and specularity,
effectively allowing captures with any flatbed scanner. Further, we extend
previous work on single-image material reflectance capture with the estimation
of opacity and transmittance, critical components of full material appearance
(SVBSDF), improving the results for any material captured with a flatbed
scanner, at a very high resolution and accuracy
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Computers & Graphics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Deblurring Networks for Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyun Choi, Heemin Yang, Janghyeok Han, Sunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose DeepDeblurRF, a novel radiance field deblurring
approach that can synthesize high-quality novel views from blurred training
views with significantly reduced training time. DeepDeblurRF leverages deep
neural network (DNN)-based deblurring modules to enjoy their deblurring
performance and computational efficiency. To effectively combine DNN-based
deblurring and radiance field construction, we propose a novel radiance field
(RF)-guided deblurring and an iterative framework that performs RF-guided
deblurring and radiance field construction in an alternating manner. Moreover,
DeepDeblurRF is compatible with various scene representations, such as voxel
grids and 3D Gaussians, expanding its applicability. We also present
BlurRF-Synth, the first large-scale synthetic dataset for training radiance
field deblurring frameworks. We conduct extensive experiments on both camera
motion blur and defocus blur, demonstrating that DeepDeblurRF achieves
state-of-the-art novel-view synthesis quality with significantly reduced
training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Resonance Improves the Detection of Low Contrast Images in
  Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siegfried Ludwig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic resonance describes the utility of noise in improving the
detectability of weak signals in certain types of systems. It has been observed
widely in natural and engineered settings, but its utility in image
classification with rate-based neural networks has not been studied
extensively. In this analysis a simple LSTM recurrent neural network is trained
for digit recognition and classification. During the test phase, image contrast
is reduced to a point where the model fails to recognize the presence of a
stimulus. Controlled noise is added to partially recover classification
performance. The results indicate the presence of stochastic resonance in
rate-based recurrent neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MSc Course Project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Daily Land Surface Temperature Reconstruction in Landsat Cross-Track
  Areas Using Deep Ensemble Learning With Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Liu, Siqin Wang, Lu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world applications rely on land surface temperature (LST) data at
high spatiotemporal resolution. In complex urban areas, LST exhibits
significant variations, fluctuating dramatically within and across city blocks.
Landsat provides high spatial resolution data at 100 meters but is limited by
long revisit time, with cloud cover further disrupting data collection. Here,
we propose DELAG, a deep ensemble learning method that integrates annual
temperature cycles and Gaussian processes, to reconstruct Landsat LST in
complex urban areas. Leveraging the cross-track characteristics and
dual-satellite operation of Landsat since 2021, we further enhance data
availability to 4 scenes every 16 days. We select New York City, London and
Hong Kong from three different continents as study areas. Experiments show that
DELAG successfully reconstructed LST in the three cities under clear-sky (RMSE
= 0.73-0.96 K) and heavily-cloudy (RMSE = 0.84-1.62 K) situations, superior to
existing methods. Additionally, DELAG can quantify uncertainty that enhances
LST reconstruction reliability. We further tested the reconstructed LST to
estimate near-surface air temperature, achieving results (RMSE = 1.48-2.11 K)
comparable to those derived from clear-sky LST (RMSE = 1.63-2.02 K). The
results demonstrate the successful reconstruction through DELAG and highlight
the broader applications of LST reconstruction for estimating accurate air
temperature. Our study thus provides a novel and practical method for Landsat
LST reconstruction, particularly suited for complex urban areas within Landsat
cross-track areas, taking one step toward addressing complex climate events at
high spatiotemporal resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatVLA: Unified Multimodal Understanding and Robot Control with
  Vision-Language-Action Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess a unified cognitive ability to perceive, comprehend, and
interact with the physical world. Why can't large language models replicate
this holistic understanding? Through a systematic analysis of existing training
paradigms in vision-language-action models (VLA), we identify two key
challenges: spurious forgetting, where robot training overwrites crucial
visual-text alignments, and task interference, where competing control and
understanding tasks degrade performance when trained jointly. To overcome these
limitations, we propose ChatVLA, a novel framework featuring Phased Alignment
Training, which incrementally integrates multimodal data after initial control
mastery, and a Mixture-of-Experts architecture to minimize task interference.
ChatVLA demonstrates competitive performance on visual question-answering
datasets and significantly surpasses state-of-the-art vision-language-action
(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a
six times higher performance on MMMU and scores 47.2% on MMStar with a more
parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates
superior performance on 25 real-world robot manipulation tasks compared to
existing VLA methods like OpenVLA. Our findings highlight the potential of our
unified framework for achieving both robust multimodal understanding and
effective robot control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Role of the <span class="highlight-title">Pretrain</span>ing and the Adaptation data sizes for low-resource
  real-time MRI video segmentation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masoud Thajudeen Tholan, Vinayaka Hegde, Chetan Sharma, Prasanta Kumar Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time Magnetic Resonance Imaging (rtMRI) is frequently used in speech
production studies as it provides a complete view of the vocal tract during
articulation. This study investigates the effectiveness of rtMRI in analyzing
vocal tract movements by employing the SegNet and UNet models for Air-Tissue
Boundary (ATB)segmentation tasks. We conducted pretraining of a few base models
using increasing numbers of subjects and videos, to assess performance on two
datasets. First, consisting of unseen subjects with unseen videos from the same
data source, achieving 0.33% and 0.91% (Pixel-wise Classification Accuracy
(PCA) and Dice Coefficient respectively) better than its matched condition.
Second, comprising unseen videos from a new data source, where we obtained an
accuracy of 99.63% and 98.09% (PCA and Dice Coefficient respectively) of its
matched condition performance. Here, matched condition performance refers to
the performance of a model trained only on the test subjects which was set as a
benchmark for the other models. Our findings highlight the significance of
fine-tuning and adapting models with limited data. Notably, we demonstrated
that effective model adaptation can be achieved with as few as 15 rtMRI frames
from any new dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Precise Geolocation Inference Capabilities of Vision Language
  Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neel Jay, Hieu Minh Nguyen, Trung Dung Hoang, Jacob Haimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of Vision-Language Models (VLMs) raises important questions
about privacy in an era where visual information is increasingly available.
While foundation VLMs demonstrate broad knowledge and learned capabilities, we
specifically investigate their ability to infer geographic location from
previously unseen image data. This paper introduces a benchmark dataset
collected from Google Street View that represents its global distribution of
coverage. Foundation models are evaluated on single-image geolocation
inference, with many achieving median distance errors of <300 km. We further
evaluate VLM "agents" with access to supplemental tools, observing up to a
30.6% decrease in distance error. Our findings establish that modern foundation
VLMs can act as powerful image geolocation tools, without being specifically
trained for this task. When coupled with increasing accessibility of these
models, our findings have greater implications for online privacy. We discuss
these risks, as well as future work in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 Workshop DATASAFE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedFuncta: Modality-Agnostic Representations Based on Efficient Neural
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Friedrich, Florentin Bieder, Phlippe C. Cattin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in medical image analysis with deep learning almost
exclusively focuses on grid- or voxel-based data representations. We challenge
this common choice by introducing MedFuncta, a modality-agnostic continuous
data representation based on neural fields. We demonstrate how to scale neural
fields from single instances to large datasets by exploiting redundancy in
medical signals and by applying an efficient meta-learning approach with a
context reduction scheme. We further address the spectral bias in commonly used
SIREN activations, by introducing an $\omega_0$-schedule, improving
reconstruction quality and convergence speed. We validate our proposed approach
on a large variety of medical signals of different dimensions and modalities
(1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, Colon
Histopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfully
demonstrate that we can solve relevant downstream tasks on these
representations. We additionally release a large-scale dataset of > 550k
annotated neural fields to promote research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and Dataset: https://github.com/pfriedri/medfuncta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, Jiaming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PhotoDoodle, a novel image editing framework designed to
facilitate photo doodling by enabling artists to overlay decorative elements
onto photographs. Photo doodling is challenging because the inserted elements
must appear seamlessly integrated with the background, requiring realistic
blending, perspective alignment, and contextual coherence. Additionally, the
background must be preserved without distortion, and the artist's unique style
must be captured efficiently from limited training data. These requirements are
not addressed by previous methods that primarily focus on global style transfer
or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage
training strategy. Initially, we train a general-purpose image editing model,
OmniEditor, using large-scale data. Subsequently, we fine-tune this model with
EditLoRA using a small, artist-curated dataset of before-and-after image pairs
to capture distinct editing styles and techniques. To enhance consistency in
the generated results, we introduce a positional encoding reuse mechanism.
Additionally, we release a PhotoDoodle dataset featuring six high-quality
styles. Extensive experiments demonstrate the advanced performance and
robustness of our method in customized image editing, opening new possibilities
for artistic creation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RelaCtrl: Relevance-Guided Efficient Control for Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Cao, Jing Wang, Ao Ma, Jiasong Feng, Zhanjie Zhang, Xuanhua He, Shanyuan Liu, Bo Cheng, Dawei Leng, Yuhui Yin, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Diffusion Transformer plays a pivotal role in advancing text-to-image and
text-to-video generation, owing primarily to its inherent scalability. However,
existing controlled diffusion transformer methods incur significant parameter
and computational overheads and suffer from inefficient resource allocation due
to their failure to account for the varying relevance of control information
across different transformer layers. To address this, we propose the
Relevance-Guided Efficient Controllable Generation framework, RelaCtrl,
enabling efficient and resource-optimized integration of control signals into
the Diffusion Transformer. First, we evaluate the relevance of each layer in
the Diffusion Transformer to the control information by assessing the
"ControlNet Relevance Score"-i.e., the impact of skipping each control layer on
both the quality of generation and the control effectiveness during inference.
Based on the strength of the relevance, we then tailor the positioning,
parameter scale, and modeling capacity of the control layers to reduce
unnecessary parameters and redundant computations. Additionally, to further
improve efficiency, we replace the self-attention and FFN in the commonly used
copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),
enabling efficient implementation of both the token mixer and channel mixer.
Both qualitative and quantitative experimental results demonstrate that our
approach achieves superior performance with only 15% of the parameters and
computational complexity compared to PixArt-delta. More examples are available
at https://relactrl.github.io/RelaCtrl/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Similarity Paradigm Through Textual Regularization Without Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning has emerged as a promising method for adapting pre-trained
visual-language models (VLMs) to a range of downstream tasks. While optimizing
the context can be effective for improving performance on specific tasks, it
can often lead to poor generalization performance on unseen classes or datasets
sampled from different distributions. It may be attributed to the fact that
textual prompts tend to overfit downstream data distributions, leading to the
forgetting of generalized knowledge derived from hand-crafted prompts. In this
paper, we propose a novel method called Similarity Paradigm with Textual
Regularization (SPTR) for prompt learning without forgetting. SPTR is a
two-pronged design based on hand-crafted prompts that is an inseparable
framework. 1) To avoid forgetting general textual knowledge, we introduce the
optimal transport as a textual regularization to finely ensure approximation
with hand-crafted features and tuning textual features. 2) In order to
continuously unleash the general ability of multiple hand-crafted prompts, we
propose a similarity paradigm for natural alignment score and adversarial
alignment score to improve model robustness for generalization. Both modules
share a common objective in addressing generalization issues, aiming to
maximize the generalization capability derived from multiple hand-crafted
prompts. Four representative tasks (i.e., non-generalization few-shot learning,
base-to-novel generalization, cross-dataset generalization, domain
generalization) across 11 datasets demonstrate that SPTR outperforms existing
prompt learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual
  Try-on guided by Tri-zone Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghao Luo, Yujie Liang, Xu Peng, Xiaobin Hu, Boyuan Jiang, Chengming Xu, Taisong Jin, Chengjie Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable progress in image-based virtual try-on systems, generating
realistic and robust fitting images for cross-category virtual try-on remains a
challenging task. The primary difficulty arises from the absence of human-like
reasoning, which involves addressing size mismatches between garments and
models while recognizing and leveraging the distinct functionalities of various
regions within the model images. To address this issue, we draw inspiration
from human cognitive processes and disentangle the complex reasoning required
for cross-category try-on into a structured framework. This framework
systematically decomposes the model image into three distinct regions: try-on,
reconstruction, and imagination zones. Each zone plays a specific role in
accommodating the garment and facilitating realistic synthesis. To endow the
model with robust reasoning capabilities for cross-category scenarios, we
propose an iterative data constructor. This constructor encompasses diverse
scenarios, including intra-category try-on, any-to-dress transformations
(replacing any garment category with a dress), and dress-to-any transformations
(replacing a dress with another garment category). Utilizing the generated
dataset, we introduce a tri-zone priors generator that intelligently predicts
the try-on, reconstruction, and imagination zones by analyzing how the input
garment is expected to align with the model image. Guided by these tri-zone
priors, our proposed method, CrossVTON, achieves state-of-the-art performance,
surpassing existing baselines in both qualitative and quantitative evaluations.
Notably, it demonstrates superior capability in handling cross-category virtual
try-on, meeting the complex demands of real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy
  Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model inversion attacks pose a significant privacy risk by attempting to
reconstruct private training data from trained models. Most of the existing
methods either depend on gradient estimation or require white-box access to
model parameters, which limits their applicability in practical scenarios. In
this paper, we propose PPO-MI, a novel reinforcement learning-based framework
for black-box model inversion attacks. Our approach formulates the inversion
task as a Markov Decision Process, where an agent navigates the latent space of
a generative model to reconstruct private training samples using only model
predictions. By employing Proximal Policy Optimization (PPO) with a
momentum-based state transition mechanism, along with a reward function
balancing prediction accuracy and exploration, PPO-MI ensures efficient latent
space exploration and high query efficiency. We conduct extensive experiments
illustrates that PPO-MI outperforms the existing methods while require less
attack knowledge, and it is robust across various model architectures and
datasets. These results underline its effectiveness and generalizability in
practical black-box scenarios, raising important considerations for the privacy
vulnerabilities of deployed machine learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, submitting to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topology-Aware Wavelet Mamba for Airway Structure Segmentation in
  Postoperative Recurrent Nasopharyngeal Carcinoma CT Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haishan Huang, Pengchen Liang, Naier Lin, Luxi Wang, Bin Pu, Jianguo Chen, Qing Chang, Xia Shen, Guo Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nasopharyngeal carcinoma (NPC) patients often undergo radiotherapy and
chemotherapy, which can lead to postoperative complications such as limited
mouth opening and joint stiffness, particularly in recurrent cases that require
re-surgery. These complications can affect airway function, making accurate
postoperative airway risk assessment essential for managing patient care.
Accurate segmentation of airway-related structures in postoperative CT scans is
crucial for assessing these risks. This study introduces TopoWMamba
(Topology-aware Wavelet Mamba), a novel segmentation model specifically
designed to address the challenges of postoperative airway risk evaluation in
recurrent NPC patients. TopoWMamba combines wavelet-based multi-scale feature
extraction, state-space sequence modeling, and topology-aware modules to
segment airway-related structures in CT scans robustly. By leveraging the
Wavelet-based Mamba Block (WMB) for hierarchical frequency decomposition and
the Snake Conv VSS (SCVSS) module to preserve anatomical continuity, TopoWMamba
effectively captures both fine-grained boundaries and global structural
context, crucial for accurate segmentation in complex postoperative scenarios.
Through extensive testing on the NPCSegCT dataset, TopoWMamba achieves an
average Dice score of 88.02%, outperforming existing models such as UNet,
Attention UNet, and SwinUNet. Additionally, TopoWMamba is tested on the SegRap
2023 Challenge dataset, where it shows a significant improvement in trachea
segmentation with a Dice score of 95.26%. The proposed model provides a strong
foundation for automated segmentation, enabling more accurate postoperative
airway risk evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weed Detection using Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santosh Kumar Tripathi, Shivendra Pratap Singh, Devansh Sharma, Harshavardhan U Patekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we use convolutional neural networks (CNNs) for weed detection
in agricultural land. We specifically investigate the application of two CNN
layer types, Conv2d and dilated Conv2d, for weed detection in crop fields. The
suggested method extracts features from the input photos using pre-trained
models, which are subsequently adjusted for weed detection. The findings of the
experiment, which used a sizable collection of dataset consisting of 15336
segments, being 3249 of soil, 7376 of soybean, 3520 grass and 1191 of broadleaf
weeds. show that the suggested approach can accurately and successfully detect
weeds at an accuracy of 94%. This study has significant ramifications for
lowering the usage of toxic herbicides and increasing the effectiveness of weed
management in agriculture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triply Laplacian Scale Mixture Modeling for Seismic Data Noise
  Suppression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirui Pan, Zhiyuan Zha, Shigang Wang, Yue Li, Zipei Fan, Gang Yan, Binh T. Nguyen, Bihan Wen, Ce Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsity-based tensor recovery methods have shown great potential in
suppressing seismic data noise. These methods exploit tensor sparsity measures
capturing the low-dimensional structures inherent in seismic data tensors to
remove noise by applying sparsity constraints through soft-thresholding or
hard-thresholding operators. However, in these methods, considering that real
seismic data are non-stationary and affected by noise, the variances of tensor
coefficients are unknown and may be difficult to accurately estimate from the
degraded seismic data, leading to undesirable noise suppression performance. In
this paper, we propose a novel triply Laplacian scale mixture (TLSM) approach
for seismic data noise suppression, which significantly improves the estimation
accuracy of both the sparse tensor coefficients and hidden scalar parameters.
To make the optimization problem manageable, an alternating direction method of
multipliers (ADMM) algorithm is employed to solve the proposed TLSM-based
seismic data noise suppression problem. Extensive experimental results on
synthetic and field seismic data demonstrate that the proposed TLSM algorithm
outperforms many state-of-the-art seismic data noise suppression methods in
both quantitative and qualitative evaluations while providing exceptional
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegAnyPET: Universal <span class="highlight-title">Prompt</span>able Segmentation from Positron Emission
  Tomography Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Le Xue, Wenbo Zhang, Lanlan Li, Yuchen Liu, Chen Jiang, Yuan Cheng, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positron Emission Tomography (PET) imaging plays a crucial role in modern
medical diagnostics by revealing the metabolic processes within a patient's
body, which is essential for quantification of therapy response and monitoring
treatment progress. However, the segmentation of PET images presents unique
challenges due to their lower contrast and less distinct boundaries compared to
other structural medical modalities. Recent developments in segmentation
foundation models have shown superior versatility across diverse natural image
segmentation tasks. Despite the efforts of medical adaptations, these works
primarily focus on structural medical images with detailed physiological
structural information and exhibit poor generalization ability when adapted to
molecular PET imaging. In this paper, we collect and construct PETS-5k, the
largest PET segmentation dataset to date, comprising 5,731 three-dimensional
whole-body PET images and encompassing over 1.3M 2D images. Based on the
established dataset, we develop SegAnyPET, a modality-specific 3D foundation
model for universal promptable segmentation from PET images. To issue the
challenge of discrepant annotation quality of PET images, we adopt a cross
prompting confident learning (CPCL) strategy with an uncertainty-guided
self-rectification process to robustly learn segmentation from high-quality
labeled data and low-quality noisy labeled data. Experimental results
demonstrate that SegAnyPET can correctly segment seen and unseen targets using
only one or a few prompt points, outperforming state-of-the-art foundation
models and task-specific fully supervised models with higher accuracy and
strong generalization ability for universal segmentation. As the first
foundation model for PET images, we believe that SegAnyPET will advance the
applications to various downstream tasks for molecular imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive
  Gradient Modulation Mechanism <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Liang, Wenjie Wei, Ammar Belatreche, Honglin Cao, Zijian Zhou, Shuai Wang, Malu Zhang, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of
SNNs, while also adopting the reduced storage burden of binarization
techniques. These distinct advantages grant BSNNs lightweight and
energy-efficient characteristics, rendering them ideal for deployment on
resource-constrained edge devices. However, due to the binary synaptic weights
and non-differentiable spike function, effectively training BSNNs remains an
open question. In this paper, we conduct an in-depth analysis of the challenge
for BSNN learning, namely the frequent weight sign flipping problem. To
mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism
(AGMM), which is designed to reduce the frequency of weight sign flipping by
adaptively adjusting the gradients during the learning process. The proposed
AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy,
effectively narrowing the gap between BSNNs and their full-precision
equivalents. We validate AGMM on both static and neuromorphic datasets, and
results indicate that it achieves state-of-the-art results among BSNNs. This
work substantially reduces storage demands and enhances SNNs' inherent energy
efficiency, making them highly feasible for resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, AAAI conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Collaborative Jade Recognition System for Mobile Devices Based on
  Lightweight and Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wang, Wenjia Li, Pengyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption and development of mobile devices, vision-based
recognition applications have become a hot topic in research. Jade, as an
important cultural heritage and artistic item, has significant applications in
fields such as jewelry identification and cultural relic preservation. However,
existing jade recognition systems still face challenges in mobile
implementation, such as limited computing resources, real-time requirements,
and accuracy issues. To address these challenges, this paper proposes a jade
recognition system based on size model collaboration, aiming to achieve
efficient and accurate jade identification using mobile devices such as
smartphones.First, we design a size model based on multi-scale image
processing, extracting key visual information by analyzing jade's dimensions,
shapes, and surface textures. Then, a collaborative multi-model classification
framework is built by combining deep learning and traditional computer vision
algorithms. This framework can effectively select and adjust models based on
different jade characteristics, providing high accuracy results across various
environments and devices.Experimental results show that the proposed system can
provide high recognition accuracy and fast processing time on mobile devices,
while consuming relatively low computational resources. The system not only
holds great application potential but also provides new ideas and technical
support for the intelligent development of jade identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textured 3D Regenerative Morphing with 3D Diffusion Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Yushi Lan, Honghua Chen, Xingang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textured 3D morphing creates smooth and plausible interpolation sequences
between two 3D objects, focusing on transitions in both shape and texture. This
is important for creative applications like visual effects in filmmaking.
Previous methods rely on establishing point-to-point correspondences and
determining smooth deformation trajectories, which inherently restrict them to
shape-only morphing on untextured, topologically aligned datasets. This
restriction leads to labor-intensive preprocessing and poor generalization. To
overcome these challenges, we propose a method for 3D regenerative morphing
using a 3D diffusion prior. Unlike previous methods that depend on explicit
correspondences and deformations, our method eliminates the additional need for
obtaining correspondence and uses the 3D diffusion prior to generate morphing.
Specifically, we introduce a 3D diffusion model and interpolate the source and
target information at three levels: initial noise, model parameters, and
condition features. We then explore an Attention Fusion strategy to generate
more smooth morphing sequences. To further improve the plausibility of semantic
interpolation and the generated 3D surfaces, we propose two strategies: (a)
Token Reordering, where we match approximate tokens based on semantic analysis
to guide implicit correspondences in the denoising process of the diffusion
model, and (b) Low-Frequency Enhancement, where we enhance low-frequency
signals in the tokens to improve the quality of generated surfaces.
Experimental results show that our method achieves superior smoothness and
plausibility in 3D morphing across diverse cross-category object pairs,
offering a novel regenerative method for 3D morphing with textured
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODVerse33: Is the New YOLO Version Always Better? A Multi Domain
  benchmark from YOLO v5 to v11 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyou Jiang, Yang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  You Look Only Once (YOLO) models have been widely used for building real-time
object detectors across various domains. With the increasing frequency of new
YOLO versions being released, key questions arise. Are the newer versions
always better than their previous versions? What are the core innovations in
each YOLO version and how do these changes translate into real-world
performance gains? In this paper, we summarize the key innovations from YOLOv1
to YOLOv11, introduce a comprehensive benchmark called ODverse33, which
includes 33 datasets spanning 11 diverse domains (Autonomous driving,
Agricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife,
Retail, Microscopic, and Security), and explore the practical impact of model
improvements in real-world, multi-domain applications through extensive
experimental results. We hope this study can provide some guidance to the
extensive users of object detection models and give some references for future
real-time object detector development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex
  Task Automation on PC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of MLLM-based GUI agents, compared to smartphones, the PC
scenario not only features a more complex interactive environment, but also
involves more intricate intra- and inter-app workflows. To address these
issues, we propose a hierarchical agent framework named PC-Agent. Specifically,
from the perception perspective, we devise an Active Perception Module (APM) to
overcome the inadequate abilities of current MLLMs in perceiving screenshot
content. From the decision-making perspective, to handle complex user
instructions and interdependent subtasks more effectively, we propose a
hierarchical multi-agent collaboration architecture that decomposes
decision-making processes into Instruction-Subtask-Action levels. Within this
architecture, three agents (i.e., Manager, Progress and Decision) are set up
for instruction decomposition, progress tracking and step-by-step
decision-making respectively. Additionally, a Reflection agent is adopted to
enable timely bottom-up error feedback and adjustment. We also introduce a new
benchmark PC-Eval with 25 real-world complex instructions. Empirical results on
PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task
success rate over previous state-of-the-art methods. The code will be publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from
  Monocular Camera Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Zheng, Henry Williams, Bruce A MacDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation is a rudimentary task in robotic perception.
Recently, with the development of more accurate and robust neural network
models and different types of datasets, monocular depth estimation has
significantly improved performance and efficiency. However, most of the
research in this area focuses on very concentrated domains. In particular, most
of the benchmarks in outdoor scenarios belong to urban environments for the
improvement of autonomous driving devices, and these benchmarks have a massive
disparity with the orchard/vineyard environment, which is hardly helpful for
research in the primary industry. Therefore, we propose OrchardDepth, which
fills the gap in the estimation of the metric depth of the monocular camera in
the orchard/vineyard environment. In addition, we present a new retraining
method to improve the training result by monitoring the consistent
regularization between dense depth maps and sparse points. Our method improves
the RMSE of depth estimation in the orchard environment from 1.5337 to 0.6738,
proving our method's validation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, Australasian Conference on Robotics and
  Automation, ACRA, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-EvRep: Learning an LLM-Compatible Event Representation Using a
  <span class="highlight-title">Self-Supervised</span> Framework <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyou Yu, Qiang Qu, Qian Zhang, Nan Zhang, Xiaoming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in event-based recognition have demonstrated significant
promise, yet most existing approaches rely on extensive training, limiting
their adaptability for efficient processing of event-driven visual content.
Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot
capabilities across diverse domains, but their application to event-based
visual recognition remains largely unexplored. To bridge this gap, we propose
\textbf{LLM-EvGen}, an event representation generator that produces
LLM-compatible event representations \textbf{LLM-EvRep}, thereby enhancing the
performance of LLMs on event recognition tasks. The generator is trained using
a self-supervised framework, aligning the generated representations with
semantic consistency and structural fidelity. Comprehensive experiments were
conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results
demonstrate that our method, \textbf{LLM-EvRep}, outperforms the event-to-video
method, E2VID, by 15.93\%, 0.82\%, and 50.21\%, respectively, in recognition
tasks when evaluated using GPT-4o.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures,Companion Proceedings of the ACM Web Conference
  2025 (WWW Companion '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Money Recognition for the Visually Impaired: A Case Study on Sri Lankan
  Banknotes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshaan Bandara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currency note recognition is a critical accessibility need for blind
individuals, as identifying banknotes accurately can impact their independence
and security in financial transactions. Several traditional and technological
initiatives have been taken to date. Nevertheless, these approaches are less
user-friendly and have made it more challenging for blind people to identify
banknotes. This research proposes a user-friendly stand-alone system for the
identification of Sri Lankan currency notes. A custom-created dataset of images
of Sri Lankan currency notes was used to fine-tune an EfficientDet model. The
currency note recognition model achieved 0.9847 AP on the validation dataset
and performs exceptionally well in real-world scenarios. The high accuracy and
the intuitive interface have enabled blind individuals to quickly and
accurately identify currency denominations, ultimately encouraging
accessibility and independence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EyeBench: A Call for More Rigorous Evaluation of Retinal Image
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhui Zhu, Xuanzhao Dong, Xin Li, Yujian Xiong, Xiwen Chen, Peijie Qiu, Vamsi Krishna Vasa, Zhangsihao Yang, Yi Su, Oana Dumitrascu, Yalin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, generative models have achieved significant success in
enhancement fundus images.However, the evaluation of these models still
presents a considerable challenge. A comprehensive evaluation benchmark for
fundus image enhancement is indispensable for three main reasons: 1) The
existing denoising metrics (e.g., PSNR, SSIM) are hardly to extend to
downstream real-world clinical research (e.g., Vessel morphology consistency).
2) There is a lack of comprehensive evaluation for both paired and unpaired
enhancement methods, along with the need for expert protocols to accurately
assess clinical value. 3) An ideal evaluation system should provide insights to
inform future developments of fundus image enhancement. To this end, we propose
a novel comprehensive benchmark, EyeBench, to provide insights that align
enhancement models with clinical needs, offering a foundation for future work
to improve the clinical relevance and applicability of generative models for
fundus image enhancement. EyeBench has three appealing properties: 1)
multi-dimensional clinical alignment downstream evaluation: In addition to
evaluating the enhancement task, we provide several clinically significant
downstream tasks for fundus images, including vessel segmentation, DR grading,
denoising generalization, and lesion segmentation. 2) Medical expert-guided
evaluation design: We introduce a novel dataset that promote comprehensive and
fair comparisons between paired and unpaired methods and includes a manual
evaluation protocol by medical experts. 3) Valuable insights: Our benchmark
study provides a comprehensive and rigorous evaluation of existing methods
across different downstream tasks, assisting medical experts in making informed
choices. Additionally, we offer further analysis of the challenges faced by
existing methods. The code is available at
\url{https://github.com/Retinal-Research/EyeBench}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and
  Texture Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Yang, Taizhang Shang, Weixuan Sun, Xibin Song, Ziang Chen, Senbo Wang, Shenzhou Chen, Weizhe Liu, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report presents a comprehensive framework for generating high-quality 3D
shapes and textures from diverse input prompts, including single images,
multi-view images, and text descriptions. The framework consists of 3D shape
generation and texture generation. (1). The 3D shape generation pipeline
employs a Variational Autoencoder (VAE) to encode implicit 3D geometries into a
latent space and a diffusion network to generate latents conditioned on input
prompts, with modifications to enhance model capacity. An alternative
Artist-Created Mesh (AM) generation approach is also explored, yielding
promising results for simpler geometries. (2). Texture generation involves a
multi-stage process starting with frontal images generation followed by
multi-view images generation, RGB-to-PBR texture conversion, and
high-resolution multi-view texture refinement. A consistency scheduler is
plugged into every stage, to enforce pixel-wise consistency among multi-view
textures during inference, ensuring seamless integration.
  The pipeline demonstrates effective handling of diverse input formats,
leveraging advanced neural architectures and novel methodologies to produce
high-quality 3D content. This report details the system architecture,
experimental results, and potential future directions to improve and expand the
framework. The source code and pretrained weights are released at:
\url{https://github.com/Tencent/Tencent-XR-3DGen}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tencent XR 3D Gen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedong Shen, Xinran Zhang, Yifan Duan, Shiqi Zhang, Heng Li, Yilong Wu, Jianmin Ji, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and realistic 3D scene reconstruction enables the lifelike creation
of autonomous driving simulation environments. With advancements in 3D Gaussian
Splatting (3DGS), previous studies have applied it to reconstruct complex
dynamic driving scenes. These methods typically require expensive LiDAR sensors
and pre-annotated datasets of dynamic objects. To address these challenges, we
propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with
Occupancy Grids (OGs) generated from surround-view camera images using
Occupancy Prediction Network (ONet). Our method leverages the semantic
information in OGs to separate dynamic vehicles from static street background,
converting these grids into two distinct sets of initial point clouds for
reconstructing both static and dynamic objects. Additionally, we estimate the
trajectories and poses of dynamic objects through a learning-based approach,
eliminating the need for complex manual annotations. Experiments on Waymo Open
dataset demonstrate that OG-Gaussian is on par with the current
state-of-the-art in terms of reconstruction quality and rendering speed,
achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while
significantly reducing computational costs and economic overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Parameter and Compute Efficient Diffusion <span class="highlight-title">Transformer</span>s using
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vignesh Sundaresha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) with billions of model parameters form the
backbone of popular image and video generation models like DALL.E,
Stable-Diffusion and SORA. Though these models are necessary in many
low-latency applications like Augmented/Virtual Reality, they cannot be
deployed on resource-constrained Edge devices (like Apple Vision Pro or Meta
Ray-Ban glasses) due to their huge computational complexity. To overcome this,
we turn to knowledge distillation and perform a thorough design-space
exploration to achieve the best DiT for a given parameter size. In particular,
we provide principles for how to choose design knobs such as depth, width,
attention heads and distillation setup for a DiT. During the process, a
three-way trade-off emerges between model performance, size and speed that is
crucial for Edge implementation of diffusion. We also propose two distillation
approaches - Teaching Assistant (TA) method and Multi-In-One (MI1) method - to
perform feature distillation in the DiT context. Unlike existing solutions, we
demonstrate and benchmark the efficacy of our approaches on practical Edge
devices such as NVIDIA Jetson Orin Nano.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Huang, Ronghao Xu, Xiaoqian Zhou, Yangbo Wei, Suhua Wang, Xiaoxin Sun, Han Li, Qingsong Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D landmark detection is a critical task in medical image analysis, and
accurately detecting anatomical landmarks is essential for subsequent medical
imaging tasks. However, mainstream deep learning methods in this field struggle
to simultaneously capture fine-grained local features and model global spatial
relationships, while maintaining a balance between accuracy and computational
efficiency. Local feature extraction requires capturing fine-grained anatomical
details, while global modeling requires understanding the spatial relationships
within complex anatomical structures. The high-dimensional nature of 3D volume
further exacerbates these challenges, as landmarks are sparsely distributed,
leading to significant computational costs. Therefore, achieving efficient and
precise 3D landmark detection remains a pressing challenge in medical image
analysis.
  In this work, We propose a \textbf{H}ybrid \textbf{3}D \textbf{DE}tection
\textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature
extraction with a lightweight attention mechanism designed to efficiently
capture global dependencies in 3D volumetric data. This mechanism employs a
hierarchical routing strategy to reduce computational cost while maintaining
global context modeling. To our knowledge, H3DE-Net is the first 3D landmark
detection model that integrates such a lightweight attention mechanism with
CNNs. Additionally, integrating multi-scale feature fusion further enhances
detection accuracy and robustness. Experimental results on a public CT dataset
demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance,
significantly improving accuracy and robustness, particularly in scenarios with
missing landmarks or complex anatomical variations. We aready open-source our
project, including code, data and model weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengxu Li, Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free unsupervised domain adaptation (SFUDA) has gained significant
attention as an alternative to traditional unsupervised domain adaptation
(UDA), which relies on the constant availability of labeled source data.
However, SFUDA approaches come with inherent limitations that are frequently
overlooked. These challenges include performance degradation when the unlabeled
target data fails to meet critical assumptions, such as having a closed-set
label distribution identical to that of the source domain, or when sufficient
unlabeled target data is unavailable-a common situation in real-world
applications. To address these issues, we propose an asymmetric co-training
(ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a
more practical alternative to SFUDA, as gathering a few labeled target
instances is more feasible than acquiring large volumes of unlabeled target
data in many real-world contexts. Our ACT method begins by employing a
weak-strong augmentation to enhance data diversity. Then we use a two-step
optimization process to train the target model. In the first step, we optimize
the label smoothing cross-entropy loss, the entropy of the class-conditional
distribution, and the reverse-entropy loss to bolster the model's
discriminative ability while mitigating overfitting. The second step focuses on
reducing redundancy in the output space by minimizing classifier determinacy
disparity. Extensive experiments across four benchmarks demonstrate the
superiority of our ACT approach, which outperforms state-of-the-art SFUDA
methods and transfer learning techniques. Our findings suggest that adapting a
source pre-trained model using only a small amount of labeled target data
offers a practical and dependable solution. The code is available at
https://github.com/gengxuli/ACT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial and Frequency Domain Adaptive Fusion Network for Image
  Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Gao, Depeng Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image deblurring aims to reconstruct a latent sharp image from its
corresponding blurred one. Although existing methods have achieved good
performance, most of them operate exclusively in either the spatial domain or
the frequency domain, rarely exploring solutions that fuse both domains. In
this paper, we propose a spatial-frequency domain adaptive fusion network
(SFAFNet) to address this limitation. Specifically, we design a gated
spatial-frequency domain feature fusion block (GSFFBlock), which consists of
three key components: a spatial domain information module, a frequency domain
information dynamic generation module (FDGM), and a gated fusion module (GFM).
The spatial domain information module employs the NAFBlock to integrate local
information. Meanwhile, in the FDGM, we design a learnable low-pass filter that
dynamically decomposes features into separate frequency subbands, capturing the
image-wide receptive field and enabling the adaptive exploration of global
contextual information. Additionally, to facilitate information flow and the
learning of complementary representations. In the GFM, we present a gating
mechanism (GATE) to re-weight spatial and frequency domain features, which are
then fused through the cross-attention mechanism (CAM). Experimental results
demonstrate that our SFAFNet performs favorably compared to state-of-the-art
approaches on commonly used benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Text and Vision: A Multi-View Text-Vision Registration Approach
  for Cross-Modal Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Shang, Zhenyu Li, Pengjie Xu, Jinwei Qiao, Gang Chen, Zihan Ruan, Weijun Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robots necessitate advanced natural language understanding
capabilities to accurately identify locations and perform tasks such as package
delivery. However, traditional visual place recognition (VPR) methods rely
solely on single-view visual information and cannot interpret human language
descriptions. To overcome this challenge, we bridge text and vision by
proposing a multiview (360{\deg} views of the surroundings) text-vision
registration approach called Text4VPR for place recognition task, which is the
first method that exclusively utilizes textual descriptions to match a database
of images. Text4VPR employs the frozen T5 language model to extract global
textual embeddings. Additionally, it utilizes the Sinkhorn algorithm with
temperature coefficient to assign local tokens to their respective clusters,
thereby aggregating visual descriptors from images. During the training stage,
Text4VPR emphasizes the alignment between individual text-image pairs for
precise textual description. In the inference stage, Text4VPR uses the Cascaded
Cross-Attention Cosine Alignment (CCCA) to address the internal mismatch
between text and image groups. Subsequently, Text4VPR performs precisely place
match based on the descriptions of text-image groups. On Street360Loc, the
first text to image VPR dataset we created, Text4VPR builds a robust baseline,
achieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92%
within a 5-meter radius on the test set, which indicates that localization from
textual descriptions to images is not only feasible but also holds significant
potential for further advancement, as shown in Figure 1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michihiro Yasunaga, Luke Zettlemoyer, Marjan Ghazvininejad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models play an essential role in training vision-language models
(VLMs) by assessing output quality to enable aligning with human preferences.
Despite their importance, the research community lacks comprehensive open
benchmarks for evaluating multimodal reward models in VLMs. To address this
gap, we introduce Multimodal RewardBench, an expert-annotated benchmark
covering six domains: general correctness, preference, knowledge, reasoning,
safety, and visual question-answering. Our dataset comprises 5,211 annotated
(prompt, chosen response, rejected response) triplets collected from various
VLMs. In evaluating a range of VLM judges, we find that even the top-performing
models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall
accuracy. Notably, most models struggle in the reasoning and safety domains.
These findings suggest that Multimodal RewardBench offers a challenging testbed
for advancing reward model development across multiple domains. We release the
benchmark at https://github.com/facebookresearch/multimodal_rewardbench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset available at
  https://github.com/facebookresearch/multimodal_rewardbench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stereo Image Coding for Machines with Joint Visual Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengchao Jin, Jianjun Lei, Bo Peng, Zhaoqing Pan, Nam Ling, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D image coding for machines (ICM) has achieved great success in coding
efficiency, while less effort has been devoted to stereo image fields. To
promote the efficiency of stereo image compression (SIC) and intelligent
analysis, the stereo image coding for machines (SICM) is formulated and
explored in this paper. More specifically, a machine vision-oriented stereo
feature compression network (MVSFC-Net) is proposed for SICM, where the stereo
visual features are effectively extracted, compressed, and transmitted for 3D
visual task. To efficiently compress stereo visual features in MVSFC-Net, a
stereo multi-scale feature compression (SMFC) module is designed to gradually
transform sparse stereo multi-scale features into compact joint visual
representations by removing spatial, inter-view, and cross-scale redundancies
simultaneously. Experimental results show that the proposed MVSFC-Net obtains
superior compression efficiency as well as 3D visual task performance, when
compared with the existing ICM anchors recommended by MPEG and the
state-of-the-art SIC method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian SegNet for Semantic Segmentation with Improved Interpretation
  of Microstructural Evolution During Irradiation of Materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marjolein Oostrom, Alex Hagen, Nicole LaHaye, Karl Pazdernik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the relationship between the evolution of microstructures of
irradiated LiAlO2 pellets and tritium diffusion, retention and release could
improve predictions of tritium-producing burnable absorber rod performance.
Given expert-labeled segmented images of irradiated and unirradiated pellets,
we trained Deep Convolutional Neural Networks to segment images into defect,
grain, and boundary classes. Qualitative microstructural information was
calculated from these segmented images to facilitate the comparison of
unirradiated and irradiated pellets. We tested modifications to improve the
sensitivity of the model, including incorporating meta-data into the model and
utilizing uncertainty quantification. The predicted segmentation was similar to
the expert-labeled segmentation for most methods of microstructural
qualification, including pixel proportion, defect area, and defect density.
Overall, the high performance metrics for the best models for both irradiated
and unirradiated images shows that utilizing neural network models is a viable
alternative to expert-labeled images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio
  Disentanglement for Talking Head Synthesis <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxing Liu, Zhilei Liu, Chongke Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head synthesis is to synthesize a lip-synchronized talking head video
using audio. Recently, the capability of NeRF to enhance the realism and
texture details of synthesized talking heads has attracted the attention of
researchers. However, most current NeRF methods based on audio are exclusively
concerned with the rendering of frontal faces. These methods are unable to
generate clear talking heads in novel views. Another prevalent challenge in
current 3D talking head synthesis is the difficulty in aligning acoustic and
visual spaces, which often results in suboptimal lip-syncing of the generated
talking heads. To address these issues, we propose Neural Radiance Field with
3D Prior Aided Audio Disentanglement for Talking Head Synthesis
(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information
to synthesize clear talking heads with free views. Additionally, we propose a
3D Prior Aided Audio Disentanglement module, which is designed to disentangle
the audio into two distinct categories: features related to 3D awarded speech
movements and features related to speaking style. Moreover, to reposition the
generated frames that are distant from the speaker's motion space in the real
space, we have devised a local-global Standardized Space. This method
normalizes the irregular positions in the generated frames from both global and
local semantic perspectives. Through comprehensive qualitative and quantitative
experiments, it has been demonstrated that our NeRF-3DTalker outperforms
state-of-the-art in synthesizing realistic talking head videos, exhibiting
superior image quality and lip synchronization. Project page:
https://nerf-3dtalker.github.io/NeRF-3Dtalker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning based infrared small object segmentation: Challenges and
  future directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengeng Yang, Hongshan Yu, Jianjun Zhang, Qiang Tang, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared sensing is a core method for supporting unmanned systems, such as
autonomous vehicles and drones. Recently, infrared sensors have been widely
deployed on mobile and stationary platforms for detection and classification of
objects from long distances and in wide field of views. Given its success in
the vision image analysis domain, deep learning has also been applied for
object recognition in infrared images. However, techniques that have proven
successful in visible light perception face new challenges in the infrared
domain. These challenges include extremely low signal-to-noise ratios in
infrared images, very small and blurred objects of interest, and limited
availability of labeled/unlabeled training data due to the specialized nature
of infrared sensors. Numerous methods have been proposed in the literature for
the detection and classification of small objects in infrared images achieving
varied levels of success. There is a need for a survey paper that critically
analyzes existing techniques in this domain, identifies unsolved challenges and
provides future research directions. This paper fills the gap and offers a
concise and insightful review of deep learning-based methods. It also
identifies the challenges faced by existing infrared object segmentation
methods and provides a structured review of existing infrared perception
methods from the perspective of these challenges and highlights the motivations
behind the various approaches. Finally, this review suggests promising future
directions based on recent advancements within this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a submitted version of a paper accepted by Information
  Fusion. If you want a better reading experience, please refer to the final
  published version of Information Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemiHMER: Semi-supervised Handwritten Mathematical Expression
  Recognition using pseudo-labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehua Chen, Haoyang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study semi-supervised Handwritten Mathematical Expression
Recognition (HMER) via exploring both labeled data and extra unlabeled data. We
propose a novel consistency regularization framework, termed SemiHMER, which
introduces dual-branch semi-supervised learning. Specifically, we enforce
consistency between the two networks for the same input image. The
pseudo-label, generated by one perturbed recognition network, is utilized to
supervise the other network using the standard cross-entropy loss. The SemiHMER
consistency encourages high similarity between the predictions of the two
perturbed networks for the same input image and expands the training data by
leveraging unlabeled data with pseudo-labels. We further introduce a
weak-to-strong strategy by applying different levels of augmentation to each
branch, effectively expanding the training data and enhancing the quality of
network training. Additionally, we propose a novel module, the Global Dynamic
Counting Module (GDCM), to enhance the performance of the HMER decoder by
alleviating recognition inaccuracies in long-distance formula recognition and
reducing the occurrence of repeated characters. The experimental results
demonstrate that our work achieves significant performance improvements, with
an average accuracy increase of 5.47% on CROHME14, 4.87% on CROHME16, and 5.25%
on CROHME19, compared to our baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji Ali, Andrew Bond, Tolga Birdal, Duygu Ceylan, Levent Karacan, Erkut Erdem, Aykut Erdem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose $\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled
$\textbf{Vid}$eo representation based upon $\textbf{Style}$GAN and
Neural-$\textbf{ODE}$s. Effective traversal of the latent space learned by
Generative Adversarial Networks (GANs) has been the basis for recent
breakthroughs in image editing. However, the applicability of such advancements
to the video domain has been hindered by the difficulty of representing and
controlling videos in the latent space of GANs. In particular, videos are
composed of content (i.e., appearance) and complex motion components that
require a special mechanism to disentangle and control. To achieve this,
VidStyleODE encodes the video content in a pre-trained StyleGAN $\mathcal{W}_+$
space and benefits from a latent ODE component to summarize the spatiotemporal
dynamics of the input video. Our novel continuous video generation process then
combines the two to generate high-quality and temporally consistent videos with
varying frame rates. We show that our proposed method enables a variety of
applications on real videos: text-guided appearance manipulation, motion
manipulation, image animation, and video interpolation and extrapolation.
Project website: https://cyberiada.github.io/VidStyleODE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://cyberiada.github.io/VidStyleODE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitai Kang, Haifeng Huang, Yuzhang Shang, Mubarak Shah, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D Large Language Models (3DLLMs) have highlighted
their potential in building general-purpose agents in the 3D real world, yet
challenges remain due to the lack of high-quality robust instruction-following
data, leading to limited discriminative power and generalization of 3DLLMs. In
this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale
instruction-following data generated by our novel data engine, Robust
Instruction Generation (RIG) engine. RIG generates two key instruction data: 1)
the Adversarial Instruction-following data, which features mixed negative and
positive samples to enhance the model's discriminative understanding. 2) the
Diverse Instruction-following data, which contains various instruction styles
to enhance model's generalization. As a result, we construct 1 million
instruction-following data, consisting of 344K Adversarial samples, 508K
Diverse samples, and 165K benchmark training set samples. To better handle
these complex instructions, Robin3D first incorporates Relation-Augmented
Projector to enhance spatial understanding, and then strengthens the object
referring and grounding ability through ID-Feature Bonding. Robin3D
consistently outperforms previous methods across five widely-used 3D multimodal
learning benchmarks, without the need for task-specific fine-tuning. Notably,
we achieve a 7.8\% improvement in the grounding task (Multi3DRefer) and a 6.9\%
improvement in the captioning task (Scan2Cap).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Attribution for Text-to-Image Models by Unlearning Synthesized
  Images <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09408v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09408v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng-Yu Wang, Aaron Hertzmann, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of data attribution for text-to-image models is to identify the
training images that most influence the generation of a new image. Influence is
defined such that, for a given output, if a model is retrained from scratch
without the most influential images, the model would fail to reproduce the same
output. Unfortunately, directly searching for these influential images is
computationally infeasible, since it would require repeatedly retraining models
from scratch. In our work, we propose an efficient data attribution method by
simulating unlearning the synthesized image. We achieve this by increasing the
training loss on the output image, without catastrophic forgetting of other,
unrelated concepts. We then identify training images with significant loss
deviations after the unlearning process and label these as influential. We
evaluate our method with a computationally intensive but "gold-standard"
retraining from scratch and demonstrate our method's advantages over previous
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera ready version. Project page:
  https://peterwang512.github.io/AttributeByUnlearning Code:
  https://github.com/PeterWang512/AttributeByUnlearning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sketch2CAD: 3D CAD Model Reconstruction from 2D Sketch using Visual
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D reconstruction methods typically generate outputs in the form of
voxels, point clouds, or meshes. However, each of these formats has inherent
limitations, such as rough surfaces and distorted structures. Additionally,
these data types are not ideal for further manual editing and post-processing.
In this paper, we present a novel 3D reconstruction method designed to overcome
these disadvantages by reconstructing CAD-compatible models. We trained a
visual transformer to predict a "scene descriptor" from a single 2D wire-frame
image. This descriptor includes essential information, such as object types and
parameters like position, rotation, and size. Using the predicted parameters, a
3D scene can be reconstructed with 3D modeling software that has programmable
interfaces, such as Rhino Grasshopper, to build highly editable 3D models in
the form of B-rep. To evaluate our proposed model, we created two datasets: one
consisting of simple scenes and another with more complex scenes. The test
results indicate the model's capability to accurately reconstruct simple scenes
while highlighting its difficulties with more complex ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding Why Label Smoothing Degrades Selective
  Classification and How to Fix It <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14715v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14715v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Olivier Laurent, Gianni Franchi, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label smoothing (LS) is a popular regularisation method for training neural
networks as it is effective in improving test accuracy and is simple to
implement. ``Hard'' one-hot labels are ``smoothed'' by uniformly distributing
probability mass to other classes, reducing overfitting. Prior work has
suggested that in some cases LS can degrade selective classification (SC) --
where the aim is to reject misclassifications using a model's uncertainty. In
this work, we first demonstrate empirically across an extended range of
large-scale tasks and architectures that LS consistently degrades SC. We then
address a gap in existing knowledge, providing an explanation for this
behaviour by analysing logit-level gradients: LS degrades the uncertainty rank
ordering of correct vs incorrect predictions by suppressing the max logit more
when a prediction is likely to be correct, and less when it is likely to be
wrong. This elucidates previously reported experimental results where strong
classifiers underperform in SC. We then demonstrate the empirical effectiveness
of post-hoc logit normalisation for recovering lost SC performance caused by
LS. Furthermore, linking back to our gradient analysis, we again provide an
explanation for why such normalisation is effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Chen, Xinbin Yuan, Jiabao Wang, Ruiqi Wu, Xiang Li, Qibin Hou, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim at providing the object detection community with an efficient and
performant object detector, termed YOLO-MS. The core design is based on a
series of investigations on how multi-branch features of the basic block and
convolutions with different kernel sizes affect the detection performance of
objects at different scales. The outcome is a new strategy that can
significantly enhance multi-scale feature representations of real-time object
detectors. To verify the effectiveness of our work, we train our YOLO-MS on the
MS COCO dataset from scratch without relying on any other large-scale datasets,
like ImageNet or pre-trained weights. Without bells and whistles, our YOLO-MS
outperforms the recent state-of-the-art real-time object detectors, including
YOLO-v7, RTMDet, and YOLO-v8. Taking the XS version of YOLO-MS as an example,
it can achieve an AP score of 42+% on MS COCO, which is about 2% higher than
RTMDet with the same model size. Furthermore, our work can also serve as a
plug-and-play module for other YOLO models. Typically, our method significantly
advances the APs, APl, and AP of YOLOv8-N from 18%+, 52%+, and 37%+ to 20%+,
55%+, and 40%+, respectively, with even fewer parameters and MACs. Code and
trained models are publicly available at
https://github.com/FishAndWasabi/YOLO-MS. We also provide the Jittor version at
https://github.com/NK-JittorCV/nk-yolo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learned Image Transmission with Hierarchical Variational Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16340v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16340v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Zhang, Hanlei Li, Yunlong Cai, Qiyu Hu, Guanding Yu, Runmin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an innovative hierarchical joint source-channel
coding (HJSCC) framework for image transmission, utilizing a hierarchical
variational autoencoder (VAE). Our approach leverages a combination of
bottom-up and top-down paths at the transmitter to autoregressively generate
multiple hierarchical representations of the original image. These
representations are then directly mapped to channel symbols for transmission by
the JSCC encoder. We extend this framework to scenarios with a feedback link,
modeling transmission over a noisy channel as a probabilistic sampling process
and deriving a novel generative formulation for JSCC with feedback. Compared
with existing approaches, our proposed HJSCC provides enhanced adaptability by
dynamically adjusting transmission bandwidth, encoding these representations
into varying amounts of channel symbols. Extensive experiments on images of
varying resolutions demonstrate that our proposed model outperforms existing
baselines in rate-distortion performance and maintains robustness against
channel noise. The source code will be made available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PFDiff: Training-Free Acceleration of Diffusion Models Combining Past
  and Future Scores <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Wang, Yuren Cai, Lijiang Li, Wei Peng, Songzhi Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Probabilistic Models (DPMs) have shown remarkable potential in
image generation, but their sampling efficiency is hindered by the need for
numerous denoising steps. Most existing solutions accelerate the sampling
process by proposing fast ODE solvers. However, the inevitable discretization
errors of the ODE solvers are significantly magnified when the number of
function evaluations (NFE) is fewer. In this work, we propose PFDiff, a novel
training-free and orthogonal timestep-skipping strategy, which enables existing
fast ODE solvers to operate with fewer NFE. Specifically, PFDiff initially
utilizes score replacement from past time steps to predict a ``springboard".
Subsequently, it employs this ``springboard" along with foresight updates
inspired by Nesterov momentum to rapidly update current intermediate states.
This approach effectively reduces unnecessary NFE while correcting for
discretization errors inherent in first-order ODE solvers. Experimental results
demonstrate that PFDiff exhibits flexible applicability across various
pre-trained DPMs, particularly excelling in conditional DPMs and surpassing
previous state-of-the-art training-free methods. For instance, using DDIM as a
baseline, we achieved 16.46 FID (4 NFE) compared to 138.81 FID with DDIM on
ImageNet 64x64 with classifier guidance, and 13.06 FID (10 NFE) on Stable
Diffusion with 7.5 guidance scale. Code is available at
\url{https://github.com/onefly123/PFDiff}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-Image Rectified Flow as Plug-and-Play Priors <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03293v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03293v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Yang, Cheng Chen, Xulei Yang, Fayao Liu, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale diffusion models have achieved remarkable performance in
generative tasks. Beyond their initial training applications, these models have
proven their ability to function as versatile plug-and-play priors. For
instance, 2D diffusion models can serve as loss functions to optimize 3D
implicit models. Rectified flow, a novel class of generative models, enforces a
linear progression from the source to the target distribution and has
demonstrated superior performance across various domains. Compared to
diffusion-based methods, rectified flow approaches surpass in terms of
generation quality and efficiency, requiring fewer inference steps. In this
work, we present theoretical and experimental evidence demonstrating that
rectified flow based methods offer similar functionalities to diffusion models
- they can also serve as effective priors. Besides the generative capabilities
of diffusion priors, motivated by the unique time-symmetry properties of
rectified flow models, a variant of our method can additionally perform image
inversion. Experimentally, our rectified flow-based priors outperform their
diffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our
method also displays competitive performance in image inversion and editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Camera Ready. Code:
  https://github.com/yangxiaofeng/rectified_flow_prior</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayar Lotfy Mostafa, Anna Alperovich, Tommaso Giannantonio, Bjorn Barz, Xiaohan Zhang, Felix Holm, Nassir Navab, Felix Boehm, Carolin Schwamborn, Thomas K. Hoffmann, Patrick J. Schuler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting the boundary between tumor and healthy tissue during surgical
cancer resection poses a significant challenge. In recent years, Hyperspectral
Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising
solution. However, due to the extensive information contained within the
spectral domain, most ML approaches primarily classify individual HSI
(super-)pixels, or tiles, without taking into account their spatial context. In
this paper, we propose an improved methodology that leverages the spatial
context of tiles for more robust and smoother segmentation. To address the
irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate
context information across neighboring regions. The features for each tile
within the graph are extracted using a Convolutional Neural Network (CNN),
which is trained simultaneously with the subsequent GNN. Moreover, we
incorporate local image quality metrics into the loss function to enhance the
training procedure's robustness against low-quality regions in the training
images. We demonstrate the superiority of our proposed method using a clinical
ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the
limited dataset, the GNN-based model significantly outperforms context-agnostic
approaches, accurately distinguishing between healthy and tumor tissues, even
in images from previously unseen patients. Furthermore, we show that our
carefully designed loss function, accounting for local image quality, results
in additional improvements. Our findings demonstrate that context-aware GNN
algorithms can robustly find tumor demarcations on HSI images, ultimately
contributing to better surgery success and patient outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, The German Conference on Pattern Recognition
  (GCPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00486v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00486v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dapeng Feng, Zhiqiang Chen, Yizhen Yin, Shipeng Zhong, Yuhua Qi, Hongbo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with
photorealistic scene reconstruction emerging as a key challenge. To address
this, we introduce Computational Alignment for Real-Time Gaussian Splatting
SLAM (CaRtGS), a novel method enhancing the efficiency and quality of
photorealistic scene reconstruction in real-time environments. Leveraging 3D
Gaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and
processing speed, which is crucial for scene photorealistic reconstruction. Our
approach tackles computational misalignment in Gaussian Splatting SLAM
(GS-SLAM) through an adaptive strategy that enhances optimization iterations,
addresses long-tail optimization, and refines densification. Experiments on
Replica, TUM-RGBD, and VECtor datasets demonstrate CaRtGS's effectiveness in
achieving high-fidelity rendering with fewer Gaussian primitives. This work
propels SLAM towards real-time, photorealistic dense rendering, significantly
advancing photorealistic scene representation. For the benefit of the research
community, we release the code and accompanying videos on our project website:
https://dapengfeng.github.io/cartgs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12788v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12788v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bochao Zou, Zizheng Guo, Jiansheng Chen, Junbao Zhuo, Weiran Huang, Huimin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote photoplethysmography (rPPG) is a non-contact method for detecting
physiological signals based on facial videos, holding high potential in various
applications. Due to the periodicity nature of rPPG signals, the long-range
dependency capturing capacity of the transformer was assumed to be advantageous
for such signals. However, existing methods have not conclusively demonstrated
the superior performance of transformers over traditional convolutional neural
networks. This may be attributed to the quadratic scaling exhibited by
transformer with sequence length, resulting in coarse-grained feature
extraction, which in turn affects robustness and generalization. To address
that, this paper proposes a periodic sparse attention mechanism based on
temporal attention sparsity induced by periodicity. A pre-attention stage is
introduced before the conventional attention mechanism. This stage learns
periodic patterns to filter out a large number of irrelevant attention
computations, thus enabling fine-grained feature extraction. Moreover, to
address the issue of fine-grained features being more susceptible to noise
interference, a fusion stem is proposed to effectively guide self-attention
towards rPPG features. It can be easily integrated into existing methods to
enhance their performance. Extensive experiments show that the proposed method
achieves state-of-the-art performance in both intra-dataset and cross-dataset
evaluations. The codes are available at
https://github.com/zizheng-guo/RhythmFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open-Source Tool for Mapping War Destruction at Scale in Ukraine
  using Sentinel-1 Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Dietrich, Torben Peters, Vivien Sainte Fare Garnot, Valerie Sticher, Thao Ton-That Whelan, Konrad Schindler, Jan Dirk Wegner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to detailed war impact assessments is crucial for humanitarian
organizations to assist affected populations effectively. However, maintaining
a comprehensive understanding of the situation on the ground is challenging,
especially in widespread and prolonged conflicts. Here we present a scalable
method for estimating building damage resulting from armed conflicts. By
training a machine learning model on Synthetic Aperture Radar image time
series, we generate probabilistic damage estimates at the building level,
leveraging existing damage assessments and open building footprints. To allow
large-scale inference and ensure accessibility, we tie our method to run on
Google Earth Engine. Users can adjust confidence intervals to suit their needs,
enabling rapid and flexible assessments of war-related damage across large
areas. We provide two publicly accessible dashboards: a Ukraine Damage Explorer
to dynamically view our precomputed estimates, and a Rapid Damage Mapping Tool
to run our method and generate custom maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Feature Engineering Techniques for Designing Efficient Motor
  Imagery-Based BCI-Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Saim Gardezi, Soyiba Jawed, Mahnoor Khan, Muneeba Bukhari, Rizwan Ahmed Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A multitude of individuals across the globe grapple with motor disabilities.
Neural prosthetics utilizing Brain-Computer Interface (BCI) technology exhibit
promise for improving motor rehabilitation outcomes. The intricate nature of
EEG data poses a significant hurdle for current BCI systems. Recently, a
qualitative repository of EEG signals tied to both upper and lower limb
execution of motor and motor imagery tasks has been unveiled. Despite this, the
productivity of the Machine Learning (ML) Models that were trained on this
dataset was alarmingly deficient, and the evaluation framework seemed
insufficient. To enhance outcomes, robust feature engineering (signal
processing) methodologies are implemented. A collection of time domain,
frequency domain, and wavelet-derived features was obtained from 16-channel EEG
signals, and the Maximum Relevance Minimum Redundancy (MRMR) approach was
employed to identify the four most significant features. For classification K
Nearest Neighbors (KNN), Support Vector Machine (SVM), Decision Tree (DT), and
Na\"ive Bayes (NB) models were implemented with these selected features,
evaluating their effectiveness through metrics such as testing accuracy,
precision, recall, and F1 Score. By leveraging SVM with a Gaussian Kernel, a
remarkable maximum testing accuracy of 92.50% for motor activities and 95.48%
for imagery activities is achieved. These results are notably more dependable
and gratifying compared to the previous study, where the peak accuracy was
recorded at 74.36%. This research work provides an in-depth analysis of the MI
Limb EEG dataset and it will help in designing and developing simple,
cost-effective and reliable BCI systems for neuro-rehabilitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06490v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06490v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Hsi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance,
security, and airspace management has created an urgent demand for precise,
scalable, and efficient UAV detection. However, existing datasets often suffer
from limited scale diversity and inaccurate annotations, hindering robust model
development. This paper introduces UAVDB, a high-resolution UAV detection
dataset constructed using Patch Intensity Convergence (PIC). This novel
technique automatically generates high-fidelity bounding box annotations from
UAV trajectory data~\cite{li2020reconstruction}, eliminating the need for
manual labeling. UAVDB features single-class annotations with a fixed-camera
setup and consists of RGB frames capturing UAVs across various scales, from
large-scale UAVs to near-single-pixel representations, along with challenging
backgrounds that pose difficulties for modern detectors. We first validate the
accuracy and efficiency of PIC-generated bounding boxes by comparing
Intersection over Union (IoU) performance and runtime against alternative
annotation methods, demonstrating that PIC achieves higher annotation accuracy
while being more efficient. Subsequently, we benchmark UAVDB using
state-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable
resource for advancing long-range and high-resolution UAV detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DaBiT: Depth and Blur informed <span class="highlight-title">Transformer</span> for Video Focal Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01230v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01230v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Crispian Morris, Nantheera Anantrasirichai, Fan Zhang, David Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world scenarios, recorded videos suffer from accidental focus
blur, and while video deblurring methods exist, most specifically target motion
blur or spatial-invariant blur. This paper introduces a framework optimized for
the as yet unattempted task of video focal deblurring (refocusing). The
proposed method employs novel map-guided transformers, in addition to image
propagation, to effectively leverage the continuous spatial variance of focal
blur and restore the footage. We also introduce a flow re-focusing module
designed to efficiently align relevant features between blurry and sharp
domains. Additionally, we propose a novel technique for generating synthetic
focal blur data, broadening the model's learning capabilities and robustness to
include a wider array of content. We have made a new benchmark dataset,
DAVIS-Blur, available. This dataset, a modified extension of the popular DAVIS
video segmentation set, provides realistic focal blur degradations as well as
the corresponding blur maps. Comprehensive experiments demonstrate the
superiority of our approach. We achieve state-of-the-art results with an
average PSNR performance over 1.9dB greater than comparable existing video
restoration methods. Our source code and the developed databases will be made
available at https://github.com/crispianm/DaBiT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSCA: A Digital Subtraction Angiography Sequence <span class="highlight-title">Dataset</span> and
  Spatio-Temporal Model for Cerebral Artery Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiong Zhang, Qihang Xie, Lei Mou, Dan Zhang, Da Chen, Caifeng Shan, Yitian Zhao, Ruisheng Su, Mengguo Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cerebrovascular diseases (CVDs) remain a leading cause of global disability
and mortality. Digital Subtraction Angiography (DSA) sequences, recognized as
the gold standard for diagnosing CVDs, can clearly visualize the dynamic flow
and reveal pathological conditions within the cerebrovasculature. Therefore,
precise segmentation of cerebral arteries (CAs) and classification between
their main trunks and branches are crucial for physicians to accurately
quantify diseases. However, achieving accurate CA segmentation in DSA sequences
remains a challenging task due to small vessels with low contrast, and
ambiguity between vessels and residual skull structures. Moreover, the lack of
publicly available datasets limits exploration in the field. In this paper, we
introduce a DSA Sequence-based Cerebral Artery segmentation dataset (DSCA), the
publicly accessible dataset designed specifically for pixel-level semantic
segmentation of CAs. Additionally, we propose DSANet, a spatio-temporal network
for CA segmentation in DSA sequences. Unlike existing DSA segmentation methods
that focus only on a single frame, the proposed DSANet introduces a separate
temporal encoding branch to capture dynamic vessel details across multiple
frames. To enhance small vessel segmentation and improve vessel connectivity,
we design a novel TemporalFormer module to capture global context and
correlations among sequential frames. Furthermore, we develop a Spatio-Temporal
Fusion (STF) module to effectively integrate spatial and temporal features from
the encoder. Extensive experiments demonstrate that DSANet outperforms other
state-of-the-art methods in CA segmentation, achieving a Dice of 0.9033.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published by TMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01401v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01401v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding (3DVG) is challenging because of the requirement of
understanding on visual information, language and spatial relationships. While
supervised approaches have achieved superior performance, they are constrained
by the scarcity and high cost of 3D vision-language datasets. On the other
hand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for
training data. However, these methods incur prohibitive time and token costs
during inference. To address the challenges, we introduce a novel training-free
symbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual
Grounder, that offers significantly reduced inference costs compared to
previous agent-based methods while maintaining comparable performance. EaSe
uses LLM generated codes to compute on spatial relationships. EaSe also
implements an automatic pipeline to evaluate and optimize the quality of these
codes and integrate VLMs to assist in the grounding process. Experimental
results demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%
Acc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,
it substantially reduces the inference time and cost, offering a balanced
trade-off between performance and efficiency. Codes are available at
https://github.com/OpenRobotLab/EaSe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Texture and Noise Dual Adaptation for Infrared Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Yafei Dong, Shinichiro Omachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts have explored leveraging visible light images to enrich
texture details in infrared (IR) super-resolution. However, this direct
adaptation approach often becomes a double-edged sword, as it improves texture
at the cost of introducing noise and blurring artifacts. To address these
challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN),
an innovative framework specifically engineered for robust IR super-resolution
model adaptation. DASRGAN operates on the synergy of two key components: 1)
Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and
2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer.
Specifically, TOA uniquely integrates a specialized discriminator,
incorporating a prior extraction branch, and employs a Sobel-guided adversarial
loss to align texture distributions effectively. Concurrently, NOA utilizes a
noise adversarial loss to distinctly separate the generative and Gaussian noise
pattern distributions during adversarial training. Our extensive experiments
confirm DASRGAN's superiority. Comparative analyses against leading methods
across multiple benchmarks and upsampling factors reveal that DASRGAN sets new
state-of-the-art performance standards. Code are available at
\url{https://github.com/yongsongH/DASRGAN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infrared Image Super-Resolution: Systematic <span class="highlight-title">Review</span>, and Future Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12322v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12322v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Shinichiro Omachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Super-Resolution (SR) is essential for a wide range of computer vision
and image processing tasks. Investigating infrared (IR) image (or thermal
images) super-resolution is a continuing concern within the development of deep
learning. This survey aims to provide a comprehensive perspective of IR image
super-resolution, including its applications, hardware imaging system dilemmas,
and taxonomy of image processing methodologies. In addition, the datasets and
evaluation metrics in IR image super-resolution tasks are also discussed.
Furthermore, the deficiencies in current technologies and possible promising
directions for the community to explore are highlighted. To cope with the rapid
development in this field, we intend to regularly update the relevant excellent
work at \url{https://github.com/yongsongH/Infrared_Image_SR_Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the Pattern Recognition for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infrared Small Target Detection in Satellite Videos: A New <span class="highlight-title">Dataset</span> and A
  Novel Recurrent Feature Refinement Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12448v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12448v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Ying, Li Liu, Zaipin Lin, Yangsi Shi, Yingqian Wang, Ruojing Li, Xu Cao, Boyang Li, Shilin Zhou, Wei An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-frame infrared small target (MIRST) detection in satellite videos is a
long-standing, fundamental yet challenging task for decades, and the challenges
can be summarized as: First, extremely small target size, highly complex
clutters & noises, various satellite motions result in limited feature
representation, high false alarms, and difficult motion analyses. Second, the
lack of large-scale public available MIRST dataset in satellite videos greatly
hinders the algorithm development. To address the aforementioned challenges, in
this paper, we first build a large-scale dataset for MIRST detection in
satellite videos (namely IRSatVideo-LEO), and then develop a recurrent feature
refinement (RFR) framework as the baseline method. Specifically, IRSatVideo-LEO
is a semi-simulated dataset with synthesized satellite motion, target
appearance, trajectory and intensity, which can provide a standard toolbox for
satellite video generation and a reliable evaluation platform to facilitate the
algorithm development. For baseline method, RFR is proposed to be equipped with
existing powerful CNN-based methods for long-term temporal dependency
exploitation and integrated motion compensation & MIRST detection.
Specifically, a pyramid deformable alignment (PDA) module and a
temporal-spatial-frequency modulation (TSFM) module are proposed to achieve
effective and efficient feature alignment, propagation, aggregation and
refinement. Extensive experiments have been conducted to demonstrate the
effectiveness and superiority of our scheme. The comparative results show that
ResUNet equipped with RFR outperforms the state-of-the-art MIRST detection
methods. Dataset and code are released at https://github.com/XinyiYing/RFR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedXpertQA: Benchmarking Expert-Level Medical Reasoning and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visible-Thermal Tiny Object Detection: A Benchmark <span class="highlight-title">Dataset</span> and Baselines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Ying, Chao Xiao, Ruojing Li, Xu He, Boyang Li, Xu Cao, Zhaoxu Li, Yingqian Wang, Mingyuan Hu, Qingyu Xu, Zaiping Lin, Miao Li, Shilin Zhou, Wei An, Weidong Sheng, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small object detection (SOD) has been a longstanding yet challenging task for
decades, with numerous datasets and algorithms being developed. However, they
mainly focus on either visible or thermal modality, while visible-thermal
(RGBT) bimodality is rarely explored. Although some RGBT datasets have been
developed recently, the insufficient quantity, limited category, misaligned
images and large target size cannot provide an impartial benchmark to evaluate
multi-category visible-thermal small object detection (RGBT SOD) algorithms. In
this paper, we build the first large-scale benchmark with high diversity for
RGBT SOD (namely RGBT-Tiny), including 115 paired sequences, 93K frames and
1.2M manual annotations. RGBT-Tiny contains abundant targets (7 categories) and
high-diversity scenes (8 types that cover different illumination and density
variations). Note that, over 81% of targets are smaller than 16x16, and we
provide paired bounding box annotations with tracking ID to offer an extremely
challenging benchmark with wide-range applications, such as RGBT fusion,
detection and tracking. In addition, we propose a scale adaptive fitness
(SAFit) measure that exhibits high robustness on both small and large targets.
The proposed SAFit can provide reasonable performance evaluation and promote
detection performance. Based on the proposed RGBT-Tiny dataset and SAFit
measure, extensive evaluations have been conducted, including 23 recent
state-of-the-art algorithms that cover four different types (i.e., visible
generic detection, visible SOD, thermal SOD and RGBT object detection). Project
is available at https://github.com/XinyiYing/RGBT-Tiny.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with
  Attention Mamba Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15740v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15740v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Shang, Zhenyu Li, Pengjie Xu, Jinwei Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Place Recognition (VLVPR) enhances robot localization
performance by incorporating natural language descriptions from images. By
utilizing language information, VLVPR directs robot place matching, overcoming
the constraint of solely depending on vision. The essence of multimodal fusion
lies in mining the complementary information between different modalities.
However, general fusion methods rely on traditional neural architectures and
are not well equipped to capture the dynamics of cross modal interactions,
especially in the presence of complex intra modal and inter modal correlations.
To this end, this paper proposes a novel coarse to fine and end to end
connected cross modal place recognition framework, called MambaPlace. In the
coarse localization stage, the text description and 3D point cloud are encoded
by the pretrained T5 and instance encoder, respectively. They are then
processed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for
data enhancement and alignment. In the subsequent fine localization stage, the
features of the text description and 3D point cloud are cross modally fused and
further enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we
predict the positional offset from the fused text point cloud features,
achieving the most accurate localization. Extensive experiments show that
MambaPlace achieves improved localization accuracy on the KITTI360Pose dataset
compared to the state of the art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defining and Evaluating Visual Language Models' Basic Spatial Abilities:
  A Perspective from Psychometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Theory of Multiple Intelligences underscores the hierarchical nature of
cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer
a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual
Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial
Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13
mainstream VLMs through nine validated psychometric experiments reveals
significant gaps versus humans (average score 24.95 vs. 68.38), with three key
findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,
weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller
models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading
(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought
(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from
architectural constraints. Identified barriers include weak geometry encoding
and missing dynamic simulation. By linking psychometric BSAs to VLM
capabilities, we provide a diagnostic toolkit for spatial intelligence
evaluation, methodological foundations for embodied AI development, and a
cognitive science-informed roadmap for achieving human-like spatial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surface Vision Mamba: Leveraging Bidirectional State Space Model for
  Efficient Spherical Manifold Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14679v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14679v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhao He, Weihao Zheng, Leilei Zhao, Ying Wang, Dalin Zhu, Dan Wu, Bin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based methods have demonstrated exceptional performance in
modelling long-range dependencies on spherical cortical surfaces, surpassing
traditional Geometric Deep Learning (GDL) models. However, their extensive
inference time and high memory demands pose challenges for application to large
datasets with limited computing resources. Inspired by the state space model in
computer vision, we introduce the attention-free Vision Mamba (Vim) to
spherical surfaces, presenting a domain-agnostic architecture for analyzing
data on spherical manifolds. Our method achieves surface patching by
representing spherical data as a sequence of triangular patches derived from a
subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on
multiple neurodevelopmental phenotype regression tasks using cortical surface
metrics from neonatal brains. Experimental results demonstrate that SiM
outperforms both attention- and GDL-based methods, delivering 4.8 times faster
inference and achieving 91.7% lower memory consumption compared to the Surface
Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity
analysis further underscores the potential of SiM to identify subtle cognitive
developmental patterns. The code is available at
https://github.com/Rongzhao-He/surface-vision-mamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AuroraCap: Efficient, Performant Video Detailed Captioning and a New
  Benchmark <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video detailed captioning is a key task which aims to generate comprehensive
and coherent textual descriptions of video content, benefiting both video
understanding and generation. In this paper, we propose AuroraCap, a video
captioner based on a large multimodal model. We follow the simplest
architecture design without additional parameters for temporal modeling. To
address the overhead caused by lengthy video sequences, we implement the token
merging strategy, reducing the number of input visual tokens. Surprisingly, we
found that this strategy results in little performance loss. AuroraCap shows
superior performance on various video and image captioning benchmarks, for
example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and
Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include
simple descriptions, consisting of a few dozen words, which limits research in
this field. Therefore, we develop VDC, a video detailed captioning benchmark
with over one thousand carefully annotated structured captions. In addition, we
propose a new LLM-assisted metric VDCscore for bettering evaluation, which
adopts a divide-and-conquer strategy to transform long caption evaluation into
multiple short question-answer pairs. With the help of human Elo ranking, our
experiments show that this benchmark better correlates with human judgments of
video detailed captioning quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code, docs, weight, benchmark and training
  data are all avaliable at https://rese1f.github.io/aurora-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring How Generative MLLMs Perceive More Than CLIP with the Same
  Vision Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siting Li, Pang Wei Koh, Simon Shaolei Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown that CLIP models struggle with visual reasoning
tasks that require grounding compositionality, understanding spatial
relationships, or capturing fine-grained details. One natural hypothesis is
that the CLIP vision encoder does not embed essential information for these
tasks. However, we find that this is not always the case: The encoder gathers
query-relevant visual information, while CLIP fails to extract it. In
particular, we show that another branch of Vision-Language Models (VLMs),
Generative Multimodal Large Language Models (MLLMs), achieve significantly
higher accuracy than CLIP in many of these tasks using the same vision encoder
and weights, indicating that these Generative MLLMs perceive more -- as they
extract and utilize visual information more effectively. We conduct a series of
controlled experiments and reveal that their success is attributed to multiple
key design choices, including patch tokens, position embeddings, and
prompt-based weighting. On the other hand, enhancing the training data alone or
applying a stronger text encoder does not suffice to solve the task, and
additional text tokens offer little benefit. Interestingly, we find that
fine-grained visual reasoning is not exclusive to generative models trained by
an autoregressive loss: When converted into CLIP-like encoders by contrastive
finetuning, these MLLMs still outperform CLIP under the same cosine
similarity-based evaluation protocol. Our study highlights the importance of
VLM architectural choices and suggests directions for improving the performance
of CLIP-like contrastive VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial
  Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07742v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07742v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Sun, Jianhao Li, Xueqian Zhang, Zhongdao Wang, Bailan Feng, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies point cloud perception within outdoor environments.
Existing methods face limitations in recognizing objects located at a distance
or occluded, due to the sparse nature of outdoor point clouds. In this work, we
observe a significant mitigation of this problem by accumulating multiple
temporally consecutive point cloud sweeps, resulting in a remarkable
improvement in perception accuracy. However, the computation cost also
increases, hindering previous approaches from utilizing a large number of point
cloud sweeps. To tackle this challenge, we find that a considerable portion of
points in the accumulated point cloud is redundant, and discarding these points
has minimal impact on perception accuracy. We introduce a simple yet effective
Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a
learned end-to-end sampling. The GSP layer is decoupled from other network
components and thus can be seamlessly integrated into existing point cloud
network architectures. Without incurring additional computational overhead, we
increase the number of point cloud sweeps from 10, a common practice, to as
many as 40. Consequently, there is a significant enhancement in perception
performance. For instance, in nuScenes 3D object detection and BEV map
segmentation tasks, our pruning strategy improves several 3D perception
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepFracture: A Generative Approach for Predicting Brittle Fractures
  with Neural Discrete Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Huang, Takashi Kanai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of brittle fracture animation, generating realistic destruction
animations using physics-based simulation methods is computationally expensive.
While techniques based on Voronoi diagrams or pre-fractured patterns are
effective for real-time applications, they fail to incorporate collision
conditions when determining fractured shapes during runtime. This paper
introduces a novel learning-based approach for predicting fractured shapes
based on collision dynamics at runtime. Our approach seamlessly integrates
realistic brittle fracture animations with rigid body simulations, utilising
boundary element method (BEM) brittle fracture simulations to generate training
data. To integrate collision scenarios and fractured shapes into a deep
learning framework, we introduce generative geometric segmentation, distinct
from both instance and semantic segmentation, to represent 3D fragment shapes.
We propose an eight-dimensional latent code to address the challenge of
optimising multiple discrete fracture pattern targets that share similar
continuous collision latent codes. This code will follow a discrete normal
distribution corresponding to a specific fracture pattern within our latent
impulse representation design. This adaptation enables the prediction of
fractured shapes using neural discrete representation learning. Our
experimental results show that our approach generates considerably more
detailed brittle fractures than existing techniques, while the computational
time is typically reduced compared to traditional simulation methods at
comparable resolutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of an article published in the Computer Graphics
  Forum. The final authenticated version is available at
  (https://doi.org/10.1111/cgf.70002). Please also check the project page:
  https://nikoloside.github.io/deepfracture/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Memorization in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their capacity to generate novel and high-quality samples, diffusion
models have attracted significant research interest in recent years. Notably,
the typical training objective of diffusion models, i.e., denoising score
matching, has a closed-form optimal solution that can only generate training
data replicating samples. This indicates that a memorization behavior is
theoretically expected, which contradicts the common generalization ability of
state-of-the-art diffusion models, and thus calls for a deeper understanding.
Looking into this, we first observe that memorization behaviors tend to occur
on smaller-sized datasets, which motivates our definition of effective model
memorization (EMM), a metric measuring the maximum size of training data at
which a learned diffusion model approximates its theoretical optimum. Then, we
quantify the impact of the influential factors on these memorization behaviors
in terms of EMM, focusing primarily on data distribution, model configuration,
and training procedure. Besides comprehensive empirical results identifying the
influential factors, we surprisingly find that conditioning training data on
uninformative random labels can significantly trigger the memorization in
diffusion models. Our study holds practical significance for diffusion model
users and offers clues to theoretical research in deep generative models. Code
is available at https://github.com/sail-sg/DiffMemorize.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpinQuant: LLM quantization with learned rotations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16406v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16406v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) techniques applied to weights, activations,
and the KV cache greatly reduce memory usage, latency, and power consumption of
Large Language Models (LLMs), but may lead to large quantization errors when
outliers are present. Rotating activation or weight matrices helps remove
outliers and benefits quantization. In this work, we identify a collection of
applicable rotation parameterizations that lead to identical outputs in
full-precision Transformer architectures while enhancing quantization accuracy.
In addition, we find that some random rotations lead to much better
quantization than others, with an up to 13 points difference in downstream
zero-shot reasoning performance. As a result, we propose SpinQuant, a novel
approach that incorporates learned rotation matrices for optimal quantized
network accuracy. With 4-bit quantization of weight, activation, and KV-cache,
SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full
precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by
19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also
outperforms concurrent work QuaRot, which applies random rotations to remove
outliers. In particular, for LLaMA-3 8B models that are hard to quantize,
SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.
Code is available at https://github.com/facebookresearch/SpinQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Anomaly Detection for Lane Rendering Using <span class="highlight-title">Transformer</span> with
  <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing and Customized Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04398v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04398v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Dong, Xingmin Lu, Ruohan Li, Wei Song, Bart van Arem, Haneen Farah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning navigation services using digital maps provide great
convenience to drivers. Nevertheless, the presence of anomalies in lane
rendering map images occasionally introduces potential hazards, as such
anomalies can be misleading to human drivers and consequently contribute to
unsafe driving conditions. In response to this concern and to accurately and
effectively detect the anomalies, this paper transforms lane rendering image
anomaly detection into a classification problem and proposes a four-phase
pipeline consisting of data pre-processing, self-supervised pre-training with
the masked image modeling (MiM) method, customized fine-tuning using
cross-entropy based loss with label smoothing, and post-processing to tackle it
leveraging state-of-the-art deep learning techniques, especially those
involving Transformer models. Various experiments verify the effectiveness of
the proposed pipeline. Results indicate that the proposed pipeline exhibits
superior performance in lane rendering image anomaly detection, and notably,
the self-supervised pre-training with MiM can greatly enhance the detection
accuracy while significantly reducing the total training time. For instance,
employing the Swin Transformer with Uniform Masking as self-supervised
pretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an
improved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin
Transformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an
AUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the
original 280. In conclusion, the proposed pipeline, with its incorporation of
self-supervised pre-training using MiM and other advanced deep learning
techniques, emerges as a robust solution for enhancing the accuracy and
efficiency of lane rendering image anomaly detection in digital navigation
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 figures, accepted by the 103rd Transportation Research
  Board (TRB) Annual Meeting, under review by Transportation Research Record:
  Journal of the Transportation Research Board</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMSci: A <span class="highlight-title">Dataset</span> for Graduate-Level Multi-Discipline Multimodal
  Scientific Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04903v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04903v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, Linda Ruth Petzold, Stephen D. Wilson, Woosang Lim, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific figure interpretation is a crucial capability for AI-driven
scientific assistants built on advanced Large Vision Language Models. However,
current datasets and benchmarks primarily focus on simple charts or other
relatively straightforward figures from limited science domains. To address
this gap, we present a comprehensive dataset compiled from peer-reviewed Nature
Communications articles covering 72 scientific fields, encompassing complex
visualizations such as schematic diagrams, microscopic images, and experimental
data which require graduate-level expertise to interpret. We evaluated 19
proprietary and open-source models on two benchmark tasks, figure captioning
and multiple-choice, and conducted human expert annotation. Our analysis
revealed significant task challenges and performance gaps among models. Beyond
serving as a benchmark, this dataset serves as a valuable resource for
large-scale training. Fine-tuning Qwen2-VL-7B with our task-specific data
achieved better performance than GPT-4o and even human experts in
multiple-choice evaluations. Furthermore, continuous pre-training on our
interleaved article and figure data substantially enhanced the model's
downstream task performance in materials science. We have released our dataset
to support further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data are available at https://github.com/Leezekun/MMSci</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Egocentric Video-Language Models Truly Understand Hand-Object
  Interactions? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17719v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17719v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshen Xu, Ziheng Wang, Yang Du, Zhinan Song, Sipeng Zheng, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric video-language pretraining is a crucial step in advancing the
understanding of hand-object interactions in first-person scenarios. Despite
successes on existing testbeds, we find that current EgoVLMs can be easily
misled by simple modifications, such as changing the verbs or nouns in
interaction descriptions, with models struggling to distinguish between these
changes. This raises the question: Do EgoVLMs truly understand hand-object
interactions? To address this question, we introduce a benchmark called
EgoHOIBench, revealing the performance limitation of current egocentric models
when confronted with such challenges. We attribute this performance gap to
insufficient fine-grained supervision and the greater difficulty EgoVLMs
experience in recognizing verbs compared to nouns. To tackle these issues, we
propose a novel asymmetric contrastive objective named EgoNCE++. For the
video-to-text objective, we enhance text supervision by generating negative
captions using large language models or leveraging pretrained vocabulary for
HOI-related word substitutions. For the text-to-video objective, we focus on
preserving an object-centric feature space that clusters video representations
based on shared nouns. Extensive experiments demonstrate that EgoNCE++
significantly enhances EgoHOI understanding, leading to improved performance
across various EgoVLMs in tasks such as multi-instance retrieval, action
recognition, and temporal understanding. Our code is available at
https://github.com/xuboshen/EgoNCEpp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Code: https://github.com/xuboshen/EgoNCEpp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose Prior Learner: Unsupervised Categorical Prior Learning for Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wang, Shuangpeng Han, Mengmi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A prior represents a set of beliefs or assumptions about a system, aiding
inference and decision-making. In this paper, we introduce the challenge of
unsupervised categorical prior learning in pose estimation, where AI models
learn a general pose prior for an object category from images in a
self-supervised manner. Although priors are effective in estimating pose,
acquiring them can be difficult. We propose a novel method, named Pose Prior
Learner (PPL), to learn a general pose prior for any object category. PPL uses
a hierarchical memory to store compositional parts of prototypical poses, from
which we distill a general pose prior. This prior improves pose estimation
accuracy through template transformation and image reconstruction. PPL learns
meaningful pose priors without any additional human annotations or
interventions, outperforming competitive baselines on both human and animal
pose estimation datasets. Notably, our experimental results reveal the
effectiveness of PPL using learned prototypical poses for pose estimation on
occluded images. Through iterative inference, PPL leverages the pose prior to
refine estimated poses, regressing them to any prototypical poses stored in
memory. Our code, model, and data will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for
  Zero-Shot Customized Video Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        D. She, Mushui Liu, Jingxuan Pang, Jin Wang, Zhen Yang, Wanggui He, Guanghao Zhang, Yi Wang, Qihan Huang, Haobin Tang, Yunlong Yu, Siming Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized generation has achieved significant progress in image synthesis,
yet personalized video generation remains challenging due to temporal
inconsistencies and quality degradation. In this paper, we introduce
CustomVideoX, an innovative framework leveraging the video diffusion
transformer for personalized video generation from a reference image.
CustomVideoX capitalizes on pre-trained video networks by exclusively training
the LoRA parameters to extract reference features, ensuring both efficiency and
adaptability. To facilitate seamless interaction between the reference image
and video content, we propose 3D Reference Attention, which enables direct and
simultaneous engagement of reference image features with all video frames
across spatial and temporal dimensions. To mitigate the excessive influence of
reference image features and textual guidance on generated video content during
inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,
dynamically modulating reference bias over different time steps. Additionally,
we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly
activated regions of key entity tokens with reference feature injection by
adjusting attention bias. To thoroughly evaluate personalized video generation,
we establish a new benchmark, VideoBench, comprising over 50 objects and 100
prompts for extensive assessment. Experimental results show that CustomVideoX
significantly outperforms existing methods in terms of video consistency and
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Section 4 in CustomVideoX Entity Region-Aware Enhancement has
  description errors. The compared methods data of Table I lacks other metrics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansheng Chen, Bokui Shen, Yulin Liu, Ruoxi Shi, Linqi Zhou, Connor Z. Lin, Jiayuan Gu, Hao Su, Gordon Wetzstein, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image diffusion models have significantly advanced open-domain 3D
object generation. However, most existing models rely on 2D network
architectures that lack inherent 3D biases, resulting in compromised geometric
consistency. To address this challenge, we introduce 3D-Adapter, a plug-in
module designed to infuse 3D geometry awareness into pretrained image diffusion
models. Central to our approach is the idea of 3D feedback augmentation: for
each denoising step in the sampling loop, 3D-Adapter decodes intermediate
multi-view features into a coherent 3D representation, then re-encodes the
rendered RGBD views to augment the pretrained base model through feature
addition. We study two variants of 3D-Adapter: a fast feed-forward version
based on Gaussian splatting and a versatile training-free version utilizing
neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter
not only greatly enhances the geometry quality of text-to-multi-view models
such as Instant3D and Zero123++, but also enables high-quality 3D generation
using the plain text-to-image Stable Diffusion. Furthermore, we showcase the
broad application potential of 3D-Adapter by presenting high quality results in
text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lakonik.github.io/3d-adapter/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Adversarial Robustness of Vision-Language Models through
  Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Ji, Yue Liu, Zhicheng Zhang, Zhao Zhang, Yuting Zhao, Xiaoshuai Hao, Gang Zhou, Xingwei Zhang, Xiaolong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) play a crucial role in the advancement of
Artificial General Intelligence (AGI). As AGI rapidly evolves, addressing
security concerns has emerged as one of the most significant challenges for
VLMs. In this paper, we present extensive experiments that expose the
vulnerabilities of conventional adaptation methods for VLMs, highlighting
significant security risks. Moreover, as VLMs grow in size, the application of
traditional adversarial adaptation techniques incurs substantial computational
costs. To address these issues, we propose a parameter-efficient adversarial
adaptation method called \textbf{\textit{AdvLoRA}} based on Low-Rank
Adaptation. We investigate and reveal the inherent low-rank properties involved
in adversarial adaptation for VLMs. Different from LoRA, we enhance the
efficiency and robustness of adversarial adaptation by introducing a novel
reparameterization method that leverages parameter clustering and alignment.
Additionally, we propose an adaptive parameter update strategy to further
bolster robustness. These innovations enable our AdvLoRA to mitigate issues
related to model security and resource wastage. Extensive experiments confirm
the effectiveness and efficiency of AdvLoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rendering dynamic 3D human from monocular videos is crucial for various
applications such as virtual reality and digital entertainment. Most methods
assume the people is in an unobstructed scene, while various objects may cause
the occlusion of body parts in real-life scenarios. Previous method utilizing
NeRF for surface rendering to recover the occluded areas, but it requiring more
than one day to train and several seconds to render, failing to meet the
requirements of real-time interactive applications. To address these issues, we
propose OccGaussian based on 3D Gaussian Splatting, which can be trained within
6 minutes and produces high-quality human renderings up to 160 FPS with
occluded input. OccGaussian initializes 3D Gaussian distributions in the
canonical space, and we perform occlusion feature query at occluded regions,
the aggregated pixel-align feature is extracted to compensate for the missing
information. Then we use Gaussian Feature MLP to further process the feature
along with the occlusion-aware loss functions to better perceive the occluded
area. Extensive experiments both in simulated and real-world occlusions,
demonstrate that our method achieves comparable or even superior performance
compared to the state-of-the-art method. And we improving training and
inference speeds by 250x and 800x, respectively. Our code will be available for
research purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have decided to withdraw this paper because the results require
  further verification or additional experimental data. We plan to resubmit an
  updated version once the necessary work is completed</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Text Embeddings and Text Similarity Explanation: A Primer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juri Opitz, Lucas Möller, Andrianos Michail, Simon Clematide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings and text embedding models are a backbone of many AI and NLP
systems, particularly those involving search. However, interpretability
challenges persist, especially in explaining obtained similarity scores, which
is crucial for applications requiring transparency. In this paper, we give a
structured overview of interpretability methods specializing in explaining
those similarity scores, an emerging research area. We study the methods'
individual ideas and techniques, evaluating their potential for improving
interpretability of text embeddings and explaining predicted similarities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Model Architectures in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu, Fengran Mo, Zhiqi Huang, Crystina Zhang, Puxuan Yu, Bei Wang, Jimmy Lin, Vivek Srikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey examines the evolution of model architectures in information
retrieval (IR), focusing on two key aspects: backbone models for feature
extraction and end-to-end system architectures for relevance estimation. The
review intentionally separates architectural considerations from training
methodologies to provide a focused analysis of structural innovations in IR
systems.We trace the development from traditional term-based methods to modern
neural approaches, particularly highlighting the impact of transformer-based
models and subsequent large language models (LLMs). We conclude by discussing
emerging challenges and future directions, including architectural
optimizations for performance and scalability, handling of multimodal,
multilingual data, and adaptation to novel application domains beyond
traditional search paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Agent Perspective on Modern Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haya Nachimovsky, Moshe Tennenholtz, Oren Kurland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has introduced a new era in
information retrieval (IR), where queries and documents that were once assumed
to be generated exclusively by humans can now also be created by automated
agents. These agents can formulate queries, generate documents, and perform
ranking. This shift challenges some long-standing IR paradigms and calls for a
reassessment of both theoretical frameworks and practical methodologies. We
advocate for a multi-agent perspective to better capture the complex
interactions between query agents, document agents, and ranker agents. Through
empirical exploration of various multi-agent retrieval settings, we reveal the
significant impact of these interactions on system performance. Our findings
underscore the need to revisit classical IR paradigms and develop new
frameworks for more effective modeling and evaluation of modern retrieval
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAGER-LLM: Enhancing Large Language Models as Recommenders through
  Exogenous Behavior-Semantic Integration <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Hong, Yan Xia, Zehan Wang, Jieming Zhu, Ye Wang, Sihang Cai, Xiaoda Yang, Quanyu Dai, Zhenhua Dong, Zhimeng Zhang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly leveraged as foundational
backbones in the development of advanced recommender systems, offering enhanced
capabilities through their extensive knowledge and reasoning. Existing
llm-based recommender systems (RSs) often face challenges due to the
significant differences between the linguistic semantics of pre-trained LLMs
and the collaborative semantics essential for RSs. These systems use
pre-trained linguistic semantics but learn collaborative semantics from scratch
via the llm-Backbone. However, LLMs are not designed for recommendations,
leading to inefficient collaborative learning, weak result correlations, and
poor integration of traditional RS features. To address these challenges, we
propose EAGER-LLM, a decoder-only llm-based generative recommendation framework
that integrates endogenous and exogenous behavioral and semantic information in
a non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich
item indices that integrates indexing sequences for exogenous signals, enabling
efficient link-wide processing; 2)non-invasive multiscale alignment
reconstruction tasks guide the model toward a deeper understanding of both
collaborative and semantic signals; 3)an annealing adapter designed to finely
balance the model's recommendation performance with its comprehension
capabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing
on three public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, accpeted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Knowledge Generation to Knowledge Verification: Examining the
  BioMedical Generative Capabilities of Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeen Hamed, Byung Suk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generative capabilities of LLM models present opportunities in
accelerating tasks and concerns with the authenticity of the knowledge it
produces. To address the concerns, we present a computational approach that
systematically evaluates the factual accuracy of biomedical knowledge that an
LLM model has been prompted to generate. Our approach encompasses two
processes: the generation of disease-centric associations and the verification
of them using the semantic knowledge of the biomedical ontologies. Using
ChatGPT as the select LLM model, we designed a set of prompt-engineering
processes to generate linkages between diseases, drugs, symptoms, and genes to
establish grounds for assessments. Experimental results demonstrate high
accuracy in identifying disease terms (88%-97%), drug names (90%-91%), and
genetic information (88%-98%). The symptom term identification accuracy was
notably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO
ontologies accordingly. The verification of associations reveals literature
coverage rates of (89%-91%) among disease-drug and disease-gene associations.
The low identification accuracy for symptom terms also contributed to the
verification of symptom-related associations (49%-62%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, In Review with a Cell Press Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructAgent: Building User Controllable Recommender via LLM Agent <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommender systems usually take the user-platform paradigm,
where users are directly exposed under the control of the platform's
recommendation algorithms. However, the defect of recommendation algorithms may
put users in very vulnerable positions under this paradigm. First, many
sophisticated models are often designed with commercial objectives in mind,
focusing on the platform's benefits, which may hinder their ability to protect
and capture users' true interests. Second, these models are typically optimized
using data from all users, which may overlook individual user's preferences.
Due to these shortcomings, users may experience several disadvantages under the
traditional user-platform direct exposure paradigm, such as lack of control
over the recommender system, potential manipulation by the platform, echo
chamber effects, or lack of personalization for less active users due to the
dominance of active users during collaborative learning. Therefore, there is an
urgent need to develop a new paradigm to protect user interests and alleviate
these issues. Recently, some researchers have introduced LLM agents to simulate
user behaviors, these approaches primarily aim to optimize platform-side
performance, leaving core issues in recommender systems unresolved. To address
these limitations, we propose a new user-agent-platform paradigm, where agent
serves as the protective shield between user and recommender system that
enables indirect exposure. To this end, we first construct four recommendation
datasets, denoted as $\dataset$, along with user instructions for each record.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW2025@HCRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Record Web Page Information Extraction From News Websites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kustenkov, Maksim Varlamov, Alexander Yatskov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focused on the problem of extracting information from web
pages containing many records, a task of growing importance in the era of
massive web data. Recently, the development of neural network methods has
improved the quality of information extraction from web pages. Nevertheless,
most of the research and datasets are aimed at studying detailed pages. This
has left multi-record "list pages" relatively understudied, despite their
widespread presence and practical significance.
  To address this gap, we created a large-scale, open-access dataset
specifically designed for list pages. This is the first dataset for this task
in the Russian language. Our dataset contains 13,120 web pages with news lists,
significantly exceeding existing datasets in both scale and complexity. Our
dataset contains attributes of various types, including optional and
multi-valued, providing a realistic representation of real-world list pages.
These features make our dataset a valuable resource for studying information
extraction from pages containing many records.
  Furthermore, we proposed our own multi-stage information extraction methods.
In this work, we explore and demonstrate several strategies for applying
MarkupLM to the specific challenges of multi-record web pages. Our experiments
validate the advantages of our methods.
  By releasing our dataset to the public, we aim to advance the field of
information extraction from multi-record pages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unstructured Evidence Attribution for Long Context Query Focused
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Zain Muhammad Mujahid, Lu Wang, Isabelle Augenstein, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are capable of generating coherent summaries
from very long contexts given a user query. Extracting and properly citing
evidence spans could help improve the transparency and reliability of these
summaries. At the same time, LLMs suffer from positional biases in terms of
which information they understand and attend to, which could affect evidence
citation. Whereas previous work has focused on evidence citation with
predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we
propose the task of long-context query focused summarization with unstructured
evidence citation. We show how existing systems struggle to generate and
properly cite unstructured evidence from their context, and that evidence tends
to be "lost-in-the-middle". To help mitigate this, we create the Summaries with
Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated
using a novel domain-agnostic pipeline which can be used as supervision to
adapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4
datasets with varying document types and lengths that LLMs adapted with SUnsET
data generate more relevant and factually consistent evidence than their base
models, extract evidence from more diverse locations in their context, and can
generate more relevant and consistent summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 21 figures; 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Process Reward Model for Generalizable Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have significantly advanced mathematical
reasoning, Process Reward Models (PRMs) have been developed to evaluate the
logical validity of reasoning steps. However, PRMs still struggle with
out-of-distribution (OOD) challenges. This paper identifies key OOD issues,
including step OOD, caused by differences in reasoning patterns across model
types and sizes, and question OOD, which arises from dataset shifts between
training data and real-world problems. To address these issues, we introduce
Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework
designed to tackle these OOD issues. By utilizing a two-stage
retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar
questions and steps as a warmup, enhancing PRM's ability to evaluate target
steps and improving generalization and reasoning consistency across different
models and problem types. Our extensive experiments demonstrate that
RetrievalPRM outperforms existing baselines across multiple real-world
datasets. Our open-source contributions include a retrieval-enhanced dataset, a
tuning framework for PRM training, and the RetrievalPRM model, establishing a
new standard for PRM performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Collaborative Jade Recognition System for Mobile Devices Based on
  Lightweight and Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wang, Wenjia Li, Pengyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread adoption and development of mobile devices, vision-based
recognition applications have become a hot topic in research. Jade, as an
important cultural heritage and artistic item, has significant applications in
fields such as jewelry identification and cultural relic preservation. However,
existing jade recognition systems still face challenges in mobile
implementation, such as limited computing resources, real-time requirements,
and accuracy issues. To address these challenges, this paper proposes a jade
recognition system based on size model collaboration, aiming to achieve
efficient and accurate jade identification using mobile devices such as
smartphones.First, we design a size model based on multi-scale image
processing, extracting key visual information by analyzing jade's dimensions,
shapes, and surface textures. Then, a collaborative multi-model classification
framework is built by combining deep learning and traditional computer vision
algorithms. This framework can effectively select and adjust models based on
different jade characteristics, providing high accuracy results across various
environments and devices.Experimental results show that the proposed system can
provide high recognition accuracy and fast processing time on mobile devices,
while consuming relatively low computational resources. The system not only
holds great application potential but also provides new ideas and technical
support for the intelligent development of jade identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient AI in Practice: Training and Deployment of Efficient LLMs for
  Industry Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kayhan Behdin, Yun Dai, Ata Fatahibaarzi, Aman Gupta, Qingquan Song, Shao Tang, Hejian Sang, Gregory Dexter, Sirou Zhu, Siyu Zhu, Tejas Dharamsi, Maziar Sanjabi, Vignesh Kothapalli, Hamed Firooz, Zhoutong Fu, Yihan Cao, Pin-Lun Hsu, Fedor Borisyuk, Zhipeng Wang, Rahul Mazumder, Natesh Pillai, Luke Simon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable performance across
a wide range of industrial applications, from search and recommendations to
generative tasks. Although scaling laws indicate that larger models generally
yield better generalization and performance, their substantial computational
requirements often render them impractical for many real-world scenarios at
scale. In this paper, we present methods and insights for training small
language models (SLMs) that deliver high performance and efficiency in
deployment. We focus on two key techniques: (1) knowledge distillation and (2)
model compression via quantization and pruning. These approaches enable SLMs to
retain much of the quality of their larger counterparts while significantly
reducing training, serving costs, and latency. We detail the impact of these
techniques on a variety of use cases at a large professional social network
platform and share deployment lessons - including hardware optimization
strategies that enhance speed and throughput for both predictive and
reasoning-based applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Evaluation of Sakana's AI Scientist for Autonomous Research: Wishful
  Thinking or an Emerging Reality Towards 'Artificial General Research
  Intelligence' (AGRI)? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joeran Beel, Min-Yen Kan, Moritz Baumgart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major step toward Artificial General Intelligence (AGI) and Super
Intelligence is AI's ability to autonomously conduct research - what we term
Artificial General Research Intelligence (AGRI). If machines could generate
hypotheses, conduct experiments, and write research papers without human
intervention, it would transform science. Recently, Sakana.ai introduced the AI
Scientist, a system claiming to automate the research lifecycle, generating
both excitement and skepticism.
  We evaluated the AI Scientist and found it a milestone in AI-driven research.
While it streamlines some aspects, it falls short of expectations. Literature
reviews are weak, nearly half the experiments failed, and manuscripts sometimes
contain hallucinated results. Most notably, users must provide an experimental
pipeline, limiting the AI Scientist's autonomy in research design and
execution.
  Despite its limitations, the AI Scientist advances research automation. Many
reviewers or instructors who assess work superficially may not recognize its
output as AI-generated. The system produces research papers with minimal human
effort and low cost. Our analysis suggests a paper costs a few USD with a few
hours of human involvement, making it significantly faster than human
researchers. Compared to AI capabilities from a few years ago, this marks
progress toward AGRI.
  The rise of AI-driven research systems requires urgent discussion within
Information Retrieval (IR) and broader scientific communities. Enhancing
literature retrieval, citation validation, and evaluation benchmarks could
improve AI-generated research reliability. We propose concrete steps, including
AGRI-specific benchmarks, refined peer review, and standardized attribution
frameworks. Whether AGRI becomes a stepping stone to AGI depends on how the
academic and AI communities shape its development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: On the Importance of Data Quality for Unit Test Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei Zhang, Xing Hu, Shan Gao, Xin Xia, David Lo, Shanping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unit testing is crucial for software development and maintenance. Effective
unit testing ensures and improves software quality, but writing unit tests is
time-consuming and labor-intensive. Recent studies have proposed deep learning
(DL) techniques or large language models (LLMs) to automate unit test
generation. These models are usually trained or fine-tuned on large-scale
datasets. Despite growing awareness of the importance of data quality, there
has been limited research on the quality of datasets used for test generation.
To bridge this gap, we systematically examine the impact of noise on the
performance of learning-based test generation models. We first apply the open
card sorting method to analyze the most popular and largest test generation
dataset, Methods2Test, to categorize eight distinct types of noise. Further, we
conduct detailed interviews with 17 domain experts to validate and assess the
importance, reasonableness, and correctness of the noise taxonomy. Then, we
propose CleanTest, an automated noise-cleaning framework designed to improve
the quality of test generation datasets. CleanTest comprises three filters: a
rule-based syntax filter, a rule-based relevance filter, and a model-based
coverage filter. To evaluate its effectiveness, we apply CleanTest on two
widely-used test generation datasets, i.e., Methods2Test and Atlas. Our
findings indicate that 43.52% and 29.65% of datasets contain noise,
highlighting its prevalence. Finally, we conduct comparative experiments using
four LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess
the impact of noise on test generation performance. The results show that
filtering noise positively influences the test generation ability of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with
  Test-Driven Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11589v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11589v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Gong, Yanghui Wu, Linxi Liang, Yanlin Wang, Jiachi Chen, Mingwei Liu, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic code search, retrieving code that matches a given natural language
query, is an important task to improve productivity in software engineering.
Existing code search datasets face limitations: they rely on human annotators
who assess code primarily through semantic understanding rather than functional
verification, leading to potential inaccuracies and scalability issues.
Additionally, current evaluation metrics often overlook the multi-choice nature
of code search. This paper introduces CoSQA+, pairing high-quality queries from
CoSQA with multiple suitable codes. We develop an automated pipeline featuring
multiple model-based candidate selections and the novel test-driven agent
annotation system. Among a single Large Language Model (LLM) annotator and
Python expert annotators (without test-based verification), agents leverage
test-based verification and achieve the highest accuracy of 96.4%. Through
extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.
Models trained on CoSQA+ exhibit improved performance. We provide the code and
data at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TALKPLAY: Multimodal Music Recommendation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungheon Doh, Keunwoo Choi, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TalkPlay, a multimodal music recommendation system that
reformulates the recommendation task as large language model token generation.
TalkPlay represents music through an expanded token vocabulary that encodes
multiple modalities - audio, lyrics, metadata, semantic tags, and playlist
co-occurrence. Using these rich representations, the model learns to generate
recommendations through next-token prediction on music recommendation
conversations, that requires learning the associations natural language query
and response, as well as music items. In other words, the formulation
transforms music recommendation into a natural language understanding task,
where the model's ability to predict conversation tokens directly optimizes
query-item relevance. Our approach eliminates traditional
recommendation-dialogue pipeline complexity, enabling end-to-end learning of
query-aware music recommendations. In the experiment, TalkPlay is successfully
trained and outperforms baseline methods in various aspects, demonstrating
strong context understanding as a conversational music recommender.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine writing with large language models often relies on
retrieval-augmented generation. However, these approaches remain confined
within the boundaries of the model's predefined scope, limiting the generation
of content with rich information. Specifically, vanilla-retrieved information
tends to lack depth, novelty, and suffers from redundancy, which negatively
impacts the quality of generated articles, leading to shallow, unoriginal, and
repetitive outputs. To address these issues, we propose OmniThink, a
slow-thinking machine writing framework that emulates the human-like process of
iterative expansion and reflection. The core idea behind OmniThink is to
simulate the cognitive behavior of learners as they slowly deepen their
knowledge of the topics. Experimental results demonstrate that OmniThink
improves the knowledge density of generated articles without compromising
metrics such as coherence and depth. Human evaluations and expert feedback
further highlight the potential of OmniThink to address real-world challenges
in the generation of long-form articles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/zjunlp/OmniThink</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CKnowEdit: A New Chinese Knowledge Editing <span class="highlight-title">Dataset</span> for Linguistics,
  Facts, and Logic Error Correction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhan Fang, Tianhe Lu, Yunzhi Yao, Ziyan Jiang, Xin Xu, Ningyu Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese, as a linguistic system rich in depth and complexity, is
characterized by distinctive elements such as ancient poetry, proverbs, idioms,
and other cultural constructs. However, current Large Language Models (LLMs)
face limitations in these specialized domains, highlighting the need for the
development of comprehensive datasets that can assess, continuously update, and
progressively improve these culturally-grounded linguistic competencies through
targeted training optimizations. To address this gap, we introduce CKnowEdit,
the first-ever Chinese knowledge editing dataset designed to correct
linguistic, factual, and logical errors in LLMs. We collect seven types of
knowledge from a wide range of sources, including classical texts, idioms, and
content from Baidu Tieba Ruozhiba, taking into account the unique polyphony,
antithesis, and logical structures inherent in the Chinese language. By
analyzing this dataset, we highlight the challenges current LLMs face in
mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge
editing techniques reveals opportunities to advance the correction of Chinese
knowledge. Code and dataset are available at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; project website is available at
  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Sentence Embeddings from <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained transformer models shine in many natural language processing
tasks and therefore are expected to bear the representation of the input
sentence or text meaning. These sentence-level embeddings are also important in
retrieval-augmented generation. But do commonly used plain averaging or prompt
templates sufficiently capture and represent the underlying meaning? After
providing a comprehensive review of existing sentence embedding extraction and
refinement methods, we thoroughly test different combinations and our original
extensions of the most promising ones on pretrained models. Namely, given 110 M
parameters, BERT's hidden representations from multiple layers, and many
tokens, we try diverse ways to extract optimal sentence embeddings. We test
various token aggregation and representation post-processing techniques. We
also test multiple ways of using a general Wikitext dataset to complement
BERT's sentence embeddings. All methods are tested on eight Semantic Textual
Similarity (STS), six short text clustering, and twelve classification tasks.
We also evaluate our representation-shaping techniques on other static models,
including random token representations. Proposed representation extraction
methods improve the performance on STS and clustering tasks for all models
considered. Very high improvements for static token-based models, especially
random embeddings for STS tasks, almost reach the performance of BERT-derived
representations. Our work shows that the representation-shaping techniques
significantly improve sentence embeddings extracted from BERT-based and simple
baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Postprint update</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-aware Fine-tuning of Sequential Recommendation Systems via
  Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Fangxin Wang, Ruocheng Guo, Yueqing Liang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Sequential Recommendation Systems (SRecsys), traditional training
approaches that rely on Cross-Entropy (CE) loss often prioritize accuracy but
fail to align well with user satisfaction metrics. CE loss focuses on
maximizing the confidence of the ground truth item, which is challenging to
achieve universally across all users and sessions. It also overlooks the
practical acceptability of ranking the ground truth item within the top-$K$
positions, a common metric in SRecsys. To address this limitation, we propose
\textbf{CPFT}, a novel fine-tuning framework that integrates Conformal
Prediction (CP)-based losses with CE loss to optimize accuracy alongside
confidence that better aligns with widely used top-$K$ metrics. CPFT embeds CP
principles into the training loop using differentiable proxy losses and
computationally efficient calibration strategies, enabling the generation of
high-confidence prediction sets. These sets focus on items with high relevance
while maintaining robust coverage guarantees. Extensive experiments on five
real-world datasets and four distinct sequential models demonstrate that CPFT
improves precision metrics and confidence calibration. Our results highlight
the importance of confidence-aware fine-tuning in delivering accurate,
trustworthy recommendations that enhance user satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual and Auditory Aesthetic Preferences Across Cultures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harin Lee, Eline Van Geert, Elif Celen, Raja Marjieh, Pol van Rijn, Minsu Park, Nori Jacoby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on how humans perceive aesthetics in shapes, colours, and music has
predominantly focused on Western populations, limiting our understanding of how
cultural environments shape aesthetic preferences. We present a large-scale
cross-cultural study examining aesthetic preferences across five distinct
modalities extensively explored in the literature: shape, curvature, colour,
musical harmony and melody. Our investigation gathers 401,403 preference
judgements from 4,835 participants across 10 countries, systematically sampling
two-dimensional parameter spaces for each modality. The findings reveal both
universal patterns and cultural variations. Preferences for shape and curvature
cross-culturally demonstrate a consistent preference for symmetrical forms.
While colour preferences are categorically consistent, relational preferences
vary across cultures. Musical harmony shows strong agreement in interval
relationships despite differing regions of preference within the broad
frequency spectrum, while melody shows the highest cross-cultural variation.
These results suggest that aesthetic preferences emerge from an interplay
between shared perceptual mechanisms and cultural learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission to CogSci 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-EvRep: Learning an LLM-Compatible Event Representation Using a
  <span class="highlight-title">Self-Supervised</span> Framework <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyou Yu, Qiang Qu, Qian Zhang, Nan Zhang, Xiaoming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in event-based recognition have demonstrated significant
promise, yet most existing approaches rely on extensive training, limiting
their adaptability for efficient processing of event-driven visual content.
Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot
capabilities across diverse domains, but their application to event-based
visual recognition remains largely unexplored. To bridge this gap, we propose
\textbf{LLM-EvGen}, an event representation generator that produces
LLM-compatible event representations \textbf{LLM-EvRep}, thereby enhancing the
performance of LLMs on event recognition tasks. The generator is trained using
a self-supervised framework, aligning the generated representations with
semantic consistency and structural fidelity. Comprehensive experiments were
conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results
demonstrate that our method, \textbf{LLM-EvRep}, outperforms the event-to-video
method, E2VID, by 15.93\%, 0.82\%, and 50.21\%, respectively, in recognition
tasks when evaluated using GPT-4o.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures,Companion Proceedings of the ACM Web Conference
  2025 (WWW Companion '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio
  Disentanglement for Talking Head Synthesis <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxing Liu, Zhilei Liu, Chongke Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head synthesis is to synthesize a lip-synchronized talking head video
using audio. Recently, the capability of NeRF to enhance the realism and
texture details of synthesized talking heads has attracted the attention of
researchers. However, most current NeRF methods based on audio are exclusively
concerned with the rendering of frontal faces. These methods are unable to
generate clear talking heads in novel views. Another prevalent challenge in
current 3D talking head synthesis is the difficulty in aligning acoustic and
visual spaces, which often results in suboptimal lip-syncing of the generated
talking heads. To address these issues, we propose Neural Radiance Field with
3D Prior Aided Audio Disentanglement for Talking Head Synthesis
(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information
to synthesize clear talking heads with free views. Additionally, we propose a
3D Prior Aided Audio Disentanglement module, which is designed to disentangle
the audio into two distinct categories: features related to 3D awarded speech
movements and features related to speaking style. Moreover, to reposition the
generated frames that are distant from the speaker's motion space in the real
space, we have devised a local-global Standardized Space. This method
normalizes the irregular positions in the generated frames from both global and
local semantic perspectives. Through comprehensive qualitative and quantitative
experiments, it has been demonstrated that our NeRF-3DTalker outperforms
state-of-the-art in synthesizing realistic talking head videos, exhibiting
superior image quality and lip synchronization. Project page:
https://nerf-3dtalker.github.io/NeRF-3Dtalker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Code to Canvas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06616v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06616v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard O. Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web-based dynamic geometry software CindyJS is a versatile tool to create
interactive applications for mathematics and other topics. In this workshop, we
will look at a code package that makes the creation of animations in CindyJS
easier and more streamlined. Animations, which can then be embedded into
presentations or be used in (lecture) videos. The focus lies on the creation of
the animations themselves and some of the technical and artistic fundamentals
to do so.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A workshop paper for the Bridges 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Audio-Visual Segmentation Models Truly Segment Sounding Objects? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, Yapeng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike traditional visual segmentation, audio-visual segmentation (AVS)
requires the model not only to identify and segment objects but also to
determine whether they are sound sources. Recent AVS approaches, leveraging
transformer architectures and powerful foundation models like SAM, have
achieved impressive performance on standard benchmarks. Yet, an important
question remains: Do these models genuinely integrate audio-visual cues to
segment sounding objects? In this paper, we systematically investigate this
issue in the context of robust AVS. Our study reveals a fundamental bias in
current methods: they tend to generate segmentation masks based predominantly
on visual salience, irrespective of the audio context. This bias results in
unreliable predictions when sounds are absent or irrelevant. To address this
challenge, we introduce AVSBench-Robust, a comprehensive benchmark
incorporating diverse negative audio scenarios including silence, ambient
noise, and off-screen sounds. We also propose a simple yet effective approach
combining balanced training with negative samples and classifier-guided
similarity learning. Our extensive experiments show that state-of-theart AVS
methods consistently fail under negative audio conditions, demonstrating the
prevalence of visual bias. In contrast, our approach achieves remarkable
improvements in both standard metrics and robustness measures, maintaining
near-perfect false positive rates while preserving highquality segmentation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-19T00:00:00Z">2025-02-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">131</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting
  Objects <span class="chip">3DV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhas Gopal, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Separable 3D reconstruction of multiple objects from multi-view RGB images --
resulting in two different 3D shapes for the two objects with a clear
separation between them -- remains a sparsely researched problem. It is
challenging due to severe mutual occlusions and ambiguities along the objects'
interaction boundaries. This paper investigates the setting and introduces a
new neuro-implicit method that can reconstruct the geometry and appearance of
two objects undergoing close interactions while disjoining both in 3D, avoiding
surface inter-penetrations and enabling novel-view synthesis of the observed
scene. The framework is end-to-end trainable and supervised using a novel
alpha-blending regularisation that ensures that the two geometries are well
separated even under extreme occlusions. Our reconstruction method is
markerless and can be applied to rigid as well as articulated objects. We
introduce a new dataset consisting of close interactions between a human and an
object and also evaluate on two scenes of humans performing martial arts. The
experiments confirm the effectiveness of our framework and substantial
improvements using 3D and novel view synthesis metrics compared to several
existing approaches applicable in our setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 20 figures and 6 tables; International Conference on 3D
  Vision (3DV) 2025; Project page:
  https://vcai.mpi-inf.mpg.de/projects/separable-recon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexTok: Resampling Images into 1D Token Sequences of Flexible Length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oğuzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, Afshin Dehghan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image tokenization has enabled major advances in autoregressive image
generation by providing compressed, discrete representations that are more
efficient to process than raw pixels. While traditional approaches use 2D grid
tokenization, recent methods like TiTok have shown that 1D tokenization can
achieve high generation quality by eliminating grid redundancies. However,
these methods typically use a fixed number of tokens and thus cannot adapt to
an image's inherent complexity. We introduce FlexTok, a tokenizer that projects
2D images into variable-length, ordered 1D token sequences. For example, a
256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,
hierarchically and semantically compressing its information. By training a
rectified flow model as the decoder and using nested dropout, FlexTok produces
plausible reconstructions regardless of the chosen token sequence length. We
evaluate our approach in an autoregressive generation setting using a simple
GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to
128 tokens, outperforming TiTok and matching state-of-the-art methods with far
fewer tokens. We further extend the model to support to text-conditioned image
generation and examine how FlexTok relates to traditional 2D tokenization. A
key finding is that FlexTok enables next-token prediction to describe images in
a coarse-to-fine "visual vocabulary", and that the number of tokens to generate
depends on the complexity of the generation task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://flextok.epfl.ch/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Training-Free Framework for Precise Mobile Manipulation of Small
  Everyday Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Gupta, Rishik Sathua, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many everyday mobile manipulation tasks require precise interaction with
small objects, such as grasping a knob to open a cabinet or pressing a light
switch. In this paper, we develop Servoing with Vision Models (SVM), a
closed-loop training-free framework that enables a mobile manipulator to tackle
such precise tasks involving the manipulation of small objects. SVM employs an
RGB-D wrist camera and uses visual servoing for control. Our novelty lies in
the use of state-of-the-art vision models to reliably compute 3D targets from
the wrist image for diverse tasks and under occlusion due to the end-effector.
To mitigate occlusion artifacts, we employ vision models to out-paint the
end-effector thereby significantly enhancing target localization. We
demonstrate that aided by out-painting methods, open-vocabulary object
detectors can serve as a drop-in module to identify semantic targets (e.g.
knobs) and point tracking methods can reliably track interaction sites
indicated by user clicks. This training-free method obtains an 85% zero-shot
success rate on manipulating unseen objects in novel environments in the real
world, outperforming an open-loop control method and an imitation learning
baseline trained on 1000+ demonstrations by an absolute success rate of 50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://arjung128.github.io/svm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IP-Composer: Semantic Composition of Visual Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Dorfman, Dana Cohen-Bar, Rinon Gal, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content creators often draw inspiration from multiple visual sources,
combining distinct elements to craft new compositions. Modern computational
approaches now aim to emulate this fundamental creative process. Although
recent diffusion models excel at text-guided compositional synthesis, text as a
medium often lacks precise control over visual details. Image-based composition
approaches can capture more nuanced features, but existing methods are
typically limited in the range of concepts they can capture, and require
expensive training procedures or specialized data. We present IP-Composer, a
novel training-free approach for compositional image generation that leverages
multiple image references simultaneously, while using natural language to
describe the concept to be extracted from each image. Our method builds on
IP-Adapter, which synthesizes novel images conditioned on an input image's CLIP
embedding. We extend this approach to multiple visual inputs by crafting
composite embeddings, stitched from the projections of multiple input images
onto concept-specific CLIP-subspaces identified through text. Through
comprehensive evaluation, we show that our approach enables more precise
control over a larger range of visual concept compositions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://ip-composer.github.io/IP-Composer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU-Friendly Laplacian Texture Blending 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bartlomiej Wronski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Texture and material blending is one of the leading methods for adding
variety to rendered virtual worlds, creating composite materials, and
generating procedural content. When done naively, it can introduce either
visible seams or contrast loss, leading to an unnatural look not representative
of blended textures. Earlier work proposed addressing this problem through
careful manual parameter tuning, lengthy per-texture statistics precomputation,
look-up tables, or training deep neural networks. In this work, we propose an
alternative approach based on insights from image processing and Laplacian
pyramid blending. Our approach does not require any precomputation or increased
memory usage (other than the presence of a regular, non-Laplacian, texture
mipmap chain), does not produce ghosting, preserves sharp local features, and
can run in real time on the GPU at the cost of a few additional lower mipmap
texture taps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures, Journal of Computer Graphics Techniques (JCGT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning
  with Large Vision and Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Huang, Shuaihang Yuan, Yu Hao, Congcong Wen, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large-scale vision and language model that has been pretrained on massive
data encodes visual and linguistic prior, which makes it easier to generate
images and language that are more natural and realistic. Despite this, there is
still a significant domain gap between the modalities of vision and language,
especially when training data is scarce in few-shot settings, where only very
limited data are available for training. In order to mitigate this issue, a
multi-modal meta-learning framework has been proposed to bridge the gap between
two frozen pretrained large vision and language models by introducing a tunable
prompt connecting these two large models. For few-shot image captioning, the
existing multi-model meta-learning framework utilizes a one-step prompting
scheme to accumulate the visual features of input images to guide the language
model, which struggles to generate accurate image descriptions with only a few
training samples. Instead, we propose a chain-of-thought (CoT) meta-learning
scheme as a multi-step image captioning procedure to better imitate how humans
describe images. In addition, we further propose to learn different
meta-parameters of the model corresponding to each CoT step in distinct
subspaces to avoid interference. We evaluated our method on three commonly used
image captioning datasets, i.e., MSCOCO, Flickr8k, and Flickr30k, under
few-shot settings. The results of our experiments indicate that our
chain-of-thought subspace meta-learning strategy is superior to the baselines
in terms of performance across different datasets measured by different
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image compositing is all you need for data augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Jia Ning Shermaine, Michalis Lazarou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the impact of various data augmentation techniques on
the performance of object detection models. Specifically, we explore classical
augmentation methods, image compositing, and advanced generative models such as
Stable Diffusion XL and ControlNet. The objective of this work is to enhance
model robustness and improve detection accuracy, particularly when working with
limited annotated data. Using YOLOv8, we fine-tune the model on a custom
dataset consisting of commercial and military aircraft, applying different
augmentation strategies. Our experiments show that image compositing offers the
highest improvement in detection performance, as measured by precision, recall,
and mean Average Precision (mAP@0.50). Other methods, including Stable
Diffusion XL and ControlNet, also demonstrate significant gains, highlighting
the potential of advanced data augmentation techniques for object detection
tasks. The results underline the importance of dataset diversity and
augmentation in achieving better generalization and performance in real-world
applications. Future work will explore the integration of semi-supervised
learning methods and further optimizations to enhance model performance across
larger and more complex datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continually Learning Structured Visual Representations via Network
  Refinement with Rerelation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeki Doruk Erden, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current machine learning paradigm relies on continuous representations like
neural networks, which iteratively adjust parameters to approximate outcomes
rather than directly learning the structure of problem. This spreads
information across the network, causing issues like information loss and
incomprehensibility Building on prior work in environment dynamics modeling, we
propose a method that learns visual space in a structured, continual manner.
Our approach refines networks to capture the core structure of objects while
representing significant subvariants in structure efficiently. We demonstrate
this with 2D shape detection, showing incremental learning on MNIST without
overwriting knowledge and creating compact, comprehensible representations.
These results offer a promising step toward a transparent, continually learning
alternative to traditional neural networks for visual processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetrical Visual Contrastive Optimization: Aligning Vision-Language
  Models with Minimal Contrastive Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that Large Vision-Language Models (VLMs) tend to
neglect image content and over-rely on language-model priors, resulting in
errors in visually grounded tasks and hallucinations. We hypothesize that this
issue arises because existing VLMs are not explicitly trained to generate texts
that are accurately grounded in fine-grained image details. To enhance visual
feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive
Optimization), a novel finetuning objective that steers the model toward
capturing important visual details and aligning them with corresponding text
tokens. To further facilitate this detailed alignment, we introduce MVC, a
paired image-text dataset built by automatically filtering and augmenting
visual counterfactual data to challenge the model with hard contrastive cases
involving Minimal Visual Contrasts. Experiments show that our method
consistently improves VLM performance across diverse benchmarks covering
various abilities and domains, achieving up to a 22% reduction in
hallucinations, and significant gains in vision-centric and general tasks.
Notably, these improvements become increasingly pronounced in benchmarks with
higher visual dependency. In short, S-VCO offers a significant enhancement of
VLM's visually-dependent task performance while retaining or even improving the
model's general abilities. We opensource our code at https://s-vco.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://s-vco.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qwen2.5-VL Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language
series, which demonstrates significant advancements in both foundational
capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap
forward in understanding and interacting with the world through enhanced visual
recognition, precise object localization, robust document parsing, and
long-video comprehension. A standout feature of Qwen2.5-VL is its ability to
localize objects using bounding boxes or points accurately. It provides robust
structured data extraction from invoices, forms, and tables, as well as
detailed analysis of charts, diagrams, and layouts. To handle complex inputs,
Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding,
enabling it to process images of varying sizes and videos of extended durations
(up to hours) with second-level event localization. This allows the model to
natively perceive spatial scales and temporal dynamics without relying on
traditional normalization techniques. By training a native dynamic-resolution
Vision Transformer (ViT) from scratch and incorporating Window Attention, we
reduce computational overhead while maintaining native resolution. As a result,
Qwen2.5-VL excels not only in static image and document understanding but also
as an interactive visual agent capable of reasoning, tool usage, and task
execution in real-world scenarios such as operating computers and mobile
devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases
from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model
matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly
excelling in document and diagram understanding. Additionally, Qwen2.5-VL
maintains robust linguistic performance, preserving the core language
competencies of the Qwen2.5 LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GroundCap: A Visually Grounded Image Captioning <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel A. P. Oliveira, Lourenço Teodoro, David Martins de Matos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current image captioning systems lack the ability to link descriptive text to
specific visual elements, making their outputs difficult to verify. While
recent approaches offer some grounding capabilities, they cannot track object
identities across multiple references or ground both actions and objects
simultaneously. We propose a novel ID-based grounding system that enables
consistent object reference tracking and action-object linking, and present
GroundCap, a dataset containing 52,016 images from 77 movies, with 344
human-annotated and 52,016 automatically generated captions. Each caption is
grounded on detected objects (132 classes) and actions (51 classes) using a tag
system that maintains object identity while linking actions to the
corresponding objects. Our approach features persistent object IDs for
reference tracking, explicit action-object linking, and segmentation of
background elements through K-means clustering. We propose gMETEOR, a metric
combining caption quality with grounding accuracy, and establish baseline
performance by fine-tuning Pixtral-12B. Human evaluation demonstrates our
approach's effectiveness in producing verifiable descriptions with coherent
object references.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Qin, Ao Sun, Yuze Hong, Benyou Wang, Ruimao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating unfamiliar environments presents significant challenges for
household robots, requiring the ability to recognize and reason about novel
decoration and layout. Existing reinforcement learning methods cannot be
directly transferred to new environments, as they typically rely on extensive
mapping and exploration, leading to time-consuming and inefficient. To address
these challenges, we try to transfer the logical knowledge and the
generalization ability of pre-trained foundation models to zero-shot
navigation. By integrating a large vision-language model with a diffusion
network, our approach named \mname ~constructs a visual predictor that
continuously predicts the agent's potential observations in the next step which
can assist robots generate robust actions. Furthermore, to adapt the temporal
property of navigation, we introduce temporal historical information to ensure
that the predicted image is aligned with the navigation scene. We then
carefully designed an information fusion framework that embeds the predicted
future frames as guidance into goal-reaching policy to solve downstream image
navigation tasks. This approach enhances navigation control and generalization
across both simulated and real-world environments. Through extensive
experimentation, we demonstrate the robustness and versatility of our method,
showcasing its potential to improve the efficiency and effectiveness of robotic
navigation in diverse settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view Video-Pose <span class="highlight-title">Pretrain</span>ing for Operating Room Surgical Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Hamoud, Vinkle Srivastav, Muhammad Abdullah Jamal, Didier Mutter, Omid Mohareri, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the workflow of surgical procedures in complex operating rooms
requires a deep understanding of the interactions between clinicians and their
environment. Surgical activity recognition (SAR) is a key computer vision task
that detects activities or phases from multi-view camera recordings. Existing
SAR models often fail to account for fine-grained clinician movements and
multi-view knowledge, or they require calibrated multi-view camera setups and
advanced point-cloud processing to obtain better results. In this work, we
propose a novel calibration-free multi-view multi-modal pretraining framework
called Multiview Pretraining for Video-Pose Surgical Activity Recognition
PreViPS, which aligns 2D pose and vision embeddings across camera views. Our
model follows CLIP-style dual-encoder architecture: one encoder processes
visual features, while the other encodes human pose embeddings. To handle the
continuous 2D human pose coordinates, we introduce a tokenized discrete
representation to convert the continuous 2D pose coordinates into discrete pose
embeddings, thereby enabling efficient integration within the dual-encoder
framework. To bridge the gap between these two modalities, we propose several
pretraining objectives using cross- and in-modality geometric constraints
within the embedding space and incorporating masked pose token prediction
strategy to enhance representation learning. Extensive experiments and ablation
studies demonstrate improvements over the strong baselines, while
data-efficiency experiments on two distinct operating room datasets further
highlight the effectiveness of our approach. We highlight the benefits of our
approach for surgical activity recognition in both multi-view and single-view
settings, showcasing its practical applicability in complex surgical
environments. Code will be made available at:
https://github.com/CAMMA-public/PreViPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEX: Memory-efficient Approach to Referring Multi-Object Tracking <span class="chip">ATC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huu-Thien Tran, Phuoc-Sang Pham, Thai-Son Tran, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Multi-Object Tracking (RMOT) is a relatively new concept that has
rapidly gained traction as a promising research direction at the intersection
of computer vision and natural language processing. Unlike traditional
multi-object tracking, RMOT identifies and tracks objects and incorporates
textual descriptions for object class names, making the approach more
intuitive. Various techniques have been proposed to address this challenging
problem; however, most require the training of the entire network due to their
end-to-end nature. Among these methods, iKUN has emerged as a particularly
promising solution. Therefore, we further explore its pipeline and enhance its
performance. In this paper, we introduce a practical module dubbed
Memory-Efficient Cross-modality -- MEX. This memory-efficient technique can be
directly applied to off-the-shelf trackers like iKUN, resulting in significant
architectural improvements. Our method proves effective during inference on a
single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI
dataset, which offers diverse autonomous driving scenes with relevant language
expressions, is particularly useful for studying this problem. Empirically, our
method demonstrates effectiveness and efficiency regarding HOTA tracking
scores, substantially improving memory allocation and processing speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 2024 International Conference on Advanced
  Technologies for Communications (ATC), Signal Processing Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSVCOD:A Large-Scale Multi-Scene <span class="highlight-title">Dataset</span> for Video Camouflage Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyong Gao, Yu'ang Feng, Qishan Wang, Lingyi Hong, Xinyu Zhou, Liu Fei, Yan Wang, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Camouflaged Object Detection (VCOD) is a challenging task which aims to
identify objects that seamlessly concealed within the background in videos. The
dynamic properties of video enable detection of camouflaged objects through
motion cues or varied perspectives. Previous VCOD datasets primarily contain
animal objects, limiting the scope of research to wildlife scenarios. However,
the applications of VCOD extend beyond wildlife and have significant
implications in security, art, and medical fields. Addressing this problem, we
construct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve
high-quality annotations, we design a semi-automatic iterative annotation
pipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD
is the largest VCOD dataset to date, introducing multiple object categories
including human, animal, medical, and vehicle objects for the first time, while
also expanding background diversity across various environments. This expanded
scope increases the practical applicability of the VCOD task in camouflaged
object detection. Alongside this dataset, we introduce a one-steam video
camouflage object detection model that performs both feature extraction and
information fusion without additional motion feature fusion modules. Our
framework achieves state-of-the-art results on the existing VCOD animal dataset
and the proposed MSVCOD. The dataset and code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicGeo: Training-Free Text-Guided Geometric Diagram Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxiao Wang, Ting Zhang, Heng Yu, Jingdong Wang, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric diagrams are critical in conveying mathematical and scientific
concepts, yet traditional diagram generation methods are often manual and
resource-intensive. While text-to-image generation has made strides in
photorealistic imagery, creating accurate geometric diagrams remains a
challenge due to the need for precise spatial relationships and the scarcity of
geometry-specific datasets. This paper presents MagicGeo, a training-free
framework for generating geometric diagrams from textual descriptions. MagicGeo
formulates the diagram generation process as a coordinate optimization problem,
ensuring geometric correctness through a formal language solver, and then
employs coordinate-aware generation. The framework leverages the strong
language translation capability of large language models, while formal
mathematical solving ensures geometric correctness. We further introduce
MagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and
demonstrate that MagicGeo outperforms current methods in both qualitative and
quantitative evaluations. This work provides a scalable, accurate solution for
automated diagram generation, with significant implications for educational and
academic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Video Semantic Communication via Multimodal Semantic Fusion
  with Large Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yin, Li Qiao, Yu Ma, Shuo Sun, Kan Li, Zhen Gao, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in traditional syntactic communications
based on Shannon's theory, these methods struggle to meet the requirements of
6G immersive communications, especially under challenging transmission
conditions. With the development of generative artificial intelligence (GenAI),
progress has been made in reconstructing videos using high-level semantic
information. In this paper, we propose a scalable generative video semantic
communication framework that extracts and transmits semantic information to
achieve high-quality video reconstruction. Specifically, at the transmitter,
description and other condition signals (e.g., first frame, sketches, etc.) are
extracted from the source video, functioning as text and structural semantics,
respectively. At the receiver, the diffusion-based GenAI large models are
utilized to fuse the semantics of the multiple modalities for reconstructing
the video. Simulation results demonstrate that, at an ultra-low channel
bandwidth ratio (CBR), our scheme effectively captures semantic information to
reconstruct videos aligned with human perception under different
signal-to-noise ratios. Notably, the proposed ``First Frame+Desc." scheme
consistently achieves CLIP score exceeding 0.92 at CBR = 0.0057 for SNR > 0 dB.
This demonstrates its robust performance even under low SNR conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Age Estimation: A New Multi-Modal Benchmark <span class="highlight-title">Dataset</span> and
  Community Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Dionelis, Nicolas Longépé, Alessandra Feliciotti, Mattia Marconcini, Devis Peressutti, Nika Oman Kadunc, JaeWan Park, Hagai Raja Sinulingga, Steve Andreas Immanuel, Ba Tran, Caroline Arnold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the construction year of buildings is of great importance for
sustainability. Sustainable buildings minimize energy consumption and are a key
part of responsible and sustainable urban planning and development to
effectively combat climate change. By using Artificial Intelligence (AI) and
recently proposed Transformer models, we are able to estimate the construction
epoch of buildings from a multi-modal dataset. In this paper, we introduce a
new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD),
containing top-view Very High Resolution (VHR) images, Earth Observation (EO)
multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and
street-view images in many different cities in Europe, co-localized with
respect to the building under study and labelled with the construction epoch.
We assess EO generalization performance on new/ previously unseen cities that
have been held-out from training and appear only during inference. In this
work, we present the community-based data challenge we organized based on MyCD.
The ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we
present the Top-4 performing models, and the main evaluation results. During
inference, the performance of the models using both all three input modalities
and only the two top-view modalities, i.e. without the street-view images, is
examined. The evaluation results show that the models are effective and can
achieve good performance on this difficult real-world task of estimating the
age of buildings, even on previously unseen cities, as well as even using only
the two top-view modalities (i.e. VHR and Sentinel-2) during inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGFI-Net: A Multi-Grained Feature Integration Network for Enhanced
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation plays a crucial role in various clinical
applications. A major challenge in medical image segmentation is achieving
accurate delineation of regions of interest in the presence of noise, low
contrast, or complex anatomical structures. Existing segmentation models often
neglect the integration of multi-grained information and fail to preserve edge
details, which are critical for precise segmentation. To address these
challenges, we propose a novel image semantic segmentation model called the
Multi-Grained Feature Integration Network (MGFI-Net). Our MGFI-Net is designed
with two dedicated modules to tackle these issues. First, to enhance
segmentation accuracy, we introduce a Multi-Grained Feature Extraction Module,
which leverages hierarchical relationships between different feature scales to
selectively focus on the most relevant information. Second, to preserve edge
details, we incorporate an Edge Enhancement Module that effectively retains and
integrates boundary information to refine segmentation results. Extensive
experiments demonstrate that MGFI-Net not only outperforms state-of-the-art
methods in terms of segmentation accuracy but also achieves superior time
efficiency, establishing it as a leading solution for real-time medical image
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Gaussian Splatting aided Localization for Large and Complex
  Indoor-Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Ress, Jonas Meyer, Wei Zhang, David Skuddis, Uwe Soergel, Norbert Haala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of visual localization has been researched for several decades and
has meanwhile found many practical applications. Despite the strong progress in
this field, there are still challenging situations in which established methods
fail. We present an approach to significantly improve the accuracy and
reliability of established visual localization methods by adding rendered
images. In detail, we first use a modern visual SLAM approach that provides a
3D Gaussian Splatting (3DGS) based map to create reference data. We demonstrate
that enriching reference data with images rendered from 3DGS at randomly
sampled poses significantly improves the performance of both geometry-based
visual localization and Scene Coordinate Regression (SCR) methods. Through
comprehensive evaluation in a large industrial environment, we analyze the
performance impact of incorporating these additional rendered views.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Correctness to Comprehension: AI Agents for Personalized Error
  Diagnosis in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Fan Zhang, Hang Li, Dingjie Song, Lichao Sun, Tianlong Xu, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as GPT-4, have demonstrated impressive
mathematical reasoning capabilities, achieving near-perfect performance on
benchmarks like GSM8K. However, their application in personalized education
remains limited due to an overemphasis on correctness over error diagnosis and
feedback generation. Current models fail to provide meaningful insights into
the causes of student mistakes, limiting their utility in educational contexts.
To address these challenges, we present three key contributions. First, we
introduce \textbf{MathCCS} (Mathematical Classification and Constructive
Suggestions), a multi-modal benchmark designed for systematic error analysis
and tailored feedback. MathCCS includes real-world problems, expert-annotated
error categories, and longitudinal student data. Evaluations of
state-of-the-art models, including \textit{Qwen2-VL}, \textit{LLaVA-OV},
\textit{Claude-3.5-Sonnet} and \textit{GPT-4o}, reveal that none achieved
classification accuracy above 30\% or generated high-quality suggestions
(average scores below 4/10), highlighting a significant gap from human-level
performance. Second, we develop a sequential error analysis framework that
leverages historical data to track trends and improve diagnostic precision.
Finally, we propose a multi-agent collaborative framework that combines a Time
Series Agent for historical analysis and an MLLM Agent for real-time
refinement, enhancing error classification and feedback generation. Together,
these contributions provide a robust platform for advancing personalized
education, bridging the gap between current AI capabilities and the demands of
real-world teaching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Overall Real-Time Mechanism for Classification and Quality Evaluation
  of Rice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanke Xia, Ruxin Peng, Haoqi Chu, Xinlei Zhu, Zhiyu Yang, Yaojun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rice is one of the most widely cultivated crops globally and has been
developed into numerous varieties. The quality of rice during cultivation is
primarily determined by its cultivar and characteristics. Traditionally, rice
classification and quality assessment rely on manual visual inspection, a
process that is both time-consuming and prone to errors. However, with
advancements in machine vision technology, automating rice classification and
quality evaluation based on its cultivar and characteristics has become
increasingly feasible, enhancing both accuracy and efficiency. This study
proposes a real-time evaluation mechanism for comprehensive rice grain
assessment, integrating a one-stage object detection approach, a deep
convolutional neural network, and traditional machine learning techniques. The
proposed framework enables rice variety identification, grain completeness
grading, and grain chalkiness evaluation. The rice grain dataset used in this
study comprises approximately 20,000 images from six widely cultivated rice
varieties in China. Experimental results demonstrate that the proposed
mechanism achieves a mean average precision (mAP) of 99.14% in the object
detection task and an accuracy of 97.89% in the classification task.
Furthermore, the framework attains an average accuracy of 97.56% in grain
completeness grading within the same rice variety, contributing to an effective
quality evaluation system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geolocation with Real Human Gameplay Data: A Large-Scale <span class="highlight-title">Dataset</span> and
  Human-Like Reasoning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Song, Jingpu Yang, Yuan Huang, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych, Xiuying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geolocation, the task of identifying an image's location, requires complex
reasoning and is crucial for navigation, monitoring, and cultural preservation.
However, current methods often produce coarse, imprecise, and non-interpretable
localization. A major challenge lies in the quality and scale of existing
geolocation datasets. These datasets are typically small-scale and
automatically constructed, leading to noisy data and inconsistent task
difficulty, with images that either reveal answers too easily or lack
sufficient clues for reliable inference. To address these challenges, we
introduce a comprehensive geolocation framework with three key components:
GeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval,
an evaluation metric, collectively designed to address critical challenges and
drive advancements in geolocation research. At the core of this framework is
GeoComp (Geolocation Competition Dataset), a large-scale dataset collected from
a geolocation game platform involving 740K users over two years. It comprises
25 million entries of metadata and 3 million geo-tagged locations spanning much
of the globe, with each location annotated thousands to tens of thousands of
times by human users. The dataset offers diverse difficulty levels for detailed
analysis and highlights key gaps in current models. Building on this dataset,
we propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning
framework designed to enhance the reasoning capabilities of Large Vision Models
(LVMs) in geolocation tasks. GeoCoT improves performance by integrating
contextual and spatial cues through a multi-step process that mimics human
geolocation reasoning. Finally, using the GeoEval metric, we demonstrate that
GeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing
interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Access dataset: https://huggingface.co/datasets/ShirohAO/tuxun</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware
  Graph <span class="highlight-title">Transformer</span> for Video Captioning <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caihua Liu, Xu Li, Wenjing Xue, Wei Tang, Xia Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video captioning methods merely provide shallow or simplistic
representations of object behaviors, resulting in superficial and ambiguous
descriptions. However, object behavior is dynamic and complex. To
comprehensively capture the essence of object behavior, we propose a dynamic
action semantic-aware graph transformer. Firstly, a multi-scale temporal
modeling module is designed to flexibly learn long and short-term latent action
features. It not only acquires latent action features across time scales, but
also considers local latent action details, enhancing the coherence and
sensitiveness of latent action representations. Secondly, a visual-action
semantic aware module is proposed to adaptively capture semantic
representations related to object behavior, enhancing the richness and
accurateness of action representations. By harnessing the collaborative efforts
of these two modules,we can acquire rich behavior representations to generate
human-like natural descriptions. Finally, this rich behavior representations
and object representations are used to construct a temporal objects-action
graph, which is fed into the graph transformer to model the complex temporal
dependencies between objects and actions. To avoid adding complexity in the
inference phase, the behavioral knowledge of the objects will be distilled into
a simple network through knowledge distillation. The experimental results on
MSVD and MSR-VTT datasets demonstrate that the proposed method achieves
significant performance improvements across multiple metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, published ICASSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking of Different YOLO Models for CAPTCHAs Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikołaj Wysocki, Henryk Gierszal, Piotr Tyczka, Sophia Karagiorgou, George Pantelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides an analysis and comparison of the YOLOv5, YOLOv8 and
YOLOv10 models for webpage CAPTCHAs detection using the datasets collected from
the web and darknet as well as synthetized data of webpages. The study examines
the nano (n), small (s), and medium (m) variants of YOLO architectures and use
metrics such as Precision, Recall, F1 score, mAP@50 and inference speed to
determine the real-life utility. Additionally, the possibility of tuning the
trained model to detect new CAPTCHA patterns efficiently was examined as it is
a crucial part of real-life applications. The image slicing method was proposed
as a way to improve the metrics of detection on oversized input images which
can be a common scenario in webpages analysis. Models in version nano achieved
the best results in terms of speed, while more complexed architectures scored
better in terms of other metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CARE: Confidence-Aware Regression Estimation of building density
  fine-tuning EO Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Dionelis, Jente Bosmans, Nicolas Longépé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing accurate confidence quantification and assessment is important for
deep neural networks to predict their failures, improve their performance and
enhance their capabilities in real-world applications, for their practical
deployment in real life. For pixel-wise regression tasks, confidence
quantification and assessment has not been well addressed in the literature, in
contrast to classification tasks like semantic segmentation. The softmax output
layer is not used in deep neural networks that solve pixel-wise regression
problems. In this paper, to address these problems, we develop, train and
evaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our
model CARE computes and assigns confidence to regression output results. We
focus on solving regression problems as downstream tasks of an AI Foundation
Model for Earth Observation (EO). We evaluate the proposed model CARE and
experimental results on data from the Copernicus Sentinel-2 satellite
constellation for estimating the density of buildings show that the proposed
method can be successfully applied to regression problems. We also show that
our approach outperforms other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, Submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-Based Video Frame Interpolation With Cross-Modal Asymmetric
  Bidirectional Motion Fields <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewoo Kim, Yujeong Chae, Hyun-Kurl Jang, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Frame Interpolation (VFI) aims to generate intermediate video frames
between consecutive input frames. Since the event cameras are bio-inspired
sensors that only encode brightness changes with a micro-second temporal
resolution, several works utilized the event camera to enhance the performance
of VFI. However, existing methods estimate bidirectional inter-frame motion
fields with only events or approximations, which can not consider the complex
motion in real-world scenarios. In this paper, we propose a novel event-based
VFI framework with cross-modal asymmetric bidirectional motion field
estimation. In detail, our EIF-BiOFNet utilizes each valuable characteristic of
the events and images for direct estimation of inter-frame motion fields
without any approximation methods. Moreover, we develop an interactive
attention-based frame synthesis network to efficiently leverage the
complementary warping-based and synthesis-based features. Finally, we build a
large-scale event-based VFI dataset, ERF-X170FPS, with a high frame rate,
extreme motion, and dynamic textures to overcome the limitations of previous
event-based VFI datasets. Extensive experimental results validate that our
method shows significant performance improvement over the state-of-the-art VFI
methods on various datasets. Our project pages are available at:
https://github.com/intelpro/CBMNet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR2023(Highlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Classification with KAN-Integrated <span class="highlight-title">Transformer</span>s and
  Dilated Neighborhood Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omid Nejati Manzari, Hojat Asgariandehkordi, Taha Koleilat, Yiming Xiao, Hassan Rivaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional networks, transformers, hybrid models, and Mamba-based
architectures have demonstrated strong performance across various medical image
classification tasks. However, these methods were primarily designed to
classify clean images using labeled data. In contrast, real-world clinical data
often involve image corruptions that are unique to multi-center studies and
stem from variations in imaging equipment across manufacturers. In this paper,
we introduce the Medical Vision Transformer (MedViTV2), a novel architecture
incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer
architecture for the first time, aiming for generalized medical image
classification. We have developed an efficient KAN block to reduce
computational load while enhancing the accuracy of the original MedViT.
Additionally, to counteract the fragility of our MedViT when scaled up, we
propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the
efficient fused dot-product attention kernel capable of capturing global
context and expanding receptive fields to scale the model effectively and
addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is
introduced to stack our Local Feature Perception and Global Feature Perception
blocks in an efficient manner, which balances local and global feature
perceptions to boost performance. Extensive experiments on 17 medical image
classification datasets and 12 corrupted medical image datasets demonstrate
that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments
with reduced computational complexity. MedViTV2 is 44\% more computationally
efficient than the previous version and significantly enhances accuracy,
achieving improvements of 4.6\% on MedMNIST, 5.8\% on NonMNIST, and 13.4\% on
the MedMNIST-C benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Mutual Cross-Modal Attention for Context-Aware Human
  Affordance Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human affordance learning investigates contextually relevant novel pose
prediction such that the estimated pose represents a valid human action within
the scene. While the task is fundamental to machine perception and automated
interactive navigation agents, the exponentially large number of probable pose
and action variations make the problem challenging and non-trivial. However,
the existing datasets and methods for human affordance prediction in 2D scenes
are significantly limited in the literature. In this paper, we propose a novel
cross-attention mechanism to encode the scene context for affordance prediction
by mutually attending spatial feature maps from two different modalities. The
proposed method is disentangled among individual subtasks to efficiently reduce
the problem complexity. First, we sample a probable location for a person
within the scene using a variational autoencoder (VAE) conditioned on the
global scene context encoding. Next, we predict a potential pose template from
a set of existing human pose candidates using a classifier on the local context
encoding around the predicted location. In the subsequent steps, we use two
VAEs to sample the scale and deformation parameters for the predicted pose
template by conditioning on the local context and template class. Our
experiments show significant improvements over the previous baseline of human
affordance injection into complex 2D scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space
  Models for Remote Physiological Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Wu, Yiping Xie, Bo Zhao, Jiguang He, Fei Luo, Ning Deng, Zitong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a
non-invasive solution for health monitoring. However, traditional
single-modality approaches (RGB or Radio Frequency (RF)) face challenges in
balancing robustness and accuracy due to lighting variations, motion artifacts,
and skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF
fusion framework that leverages the complementary strengths of both modalities.
It introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic
changes in RF signals using timing differences between frames, enhancing the
extraction of local and global features. Additionally, CardiacMamba employs a
Bidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier
Transform (CFFT) to effectively capture and refine the frequency domain
characteristics of RGB and RF signals, ultimately improving heart rate
estimation accuracy and periodicity detection. Extensive experiments on the
EquiPleth dataset demonstrate state-of-the-art performance, achieving marked
improvements in accuracy and robustness. CardiacMamba significantly mitigates
skin tone bias, reducing performance disparities across demographic groups, and
maintains resilience under missing-modality scenarios. By addressing critical
challenges in fairness, adaptability, and precision, the framework advances
rPPG technology toward reliable real-world deployment in healthcare. The codes
are available at: https://github.com/WuZheng42/CardiacMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaVCa: LLM-assisted Visual Cortex Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Matsuyama, Shinji Nishimoto, Yu Takagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the property of neural populations (or voxels) in the human
brain can advance our comprehension of human perceptual and cognitive
processing capabilities and contribute to developing brain-inspired computer
models. Recent encoding models using deep neural networks (DNNs) have
successfully predicted voxel-wise activity. However, interpreting the
properties that explain voxel responses remains challenging because of the
black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex
Captioning (LaVCa), a data-driven approach that uses large language models
(LLMs) to generate natural-language captions for images to which voxels are
selective. By applying LaVCa for image-evoked brain activity, we demonstrate
that LaVCa generates captions that describe voxel selectivity more accurately
than the previously proposed method. Furthermore, the captions generated by
LaVCa quantitatively capture more detailed properties than the existing method
at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed
analysis of the voxel-specific properties generated by LaVCa reveals
fine-grained functional differentiation within regions of interest (ROIs) in
the visual cortex and voxels that simultaneously represent multiple distinct
concepts. These findings offer profound insights into human visual
representations by assigning detailed captions throughout the visual cortex
while highlighting the potential of LLM-based methods in understanding brain
representations. Please check out our webpage at
https://sites.google.com/view/lavca-llm/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Robust Non-Transferable Learning: A <span class="highlight-title">Survey</span> and Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Hong, Yongli Xiang, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decades, researchers have primarily focused on improving the
generalization abilities of models, with limited attention given to regulating
such generalization. However, the ability of models to generalize to unintended
data (e.g., harmful or unauthorized data) can be exploited by malicious
adversaries in unforeseen ways, potentially resulting in violations of model
ethics. Non-transferable learning (NTL), a task aimed at reshaping the
generalization abilities of deep learning models, was proposed to address these
challenges. While numerous methods have been proposed in this field, a
comprehensive review of existing progress and a thorough analysis of current
limitations remain lacking. In this paper, we bridge this gap by presenting the
first comprehensive survey on NTL and introducing NTLBench, the first benchmark
to evaluate NTL performance and robustness within a unified framework.
Specifically, we first introduce the task settings, general framework, and
criteria of NTL, followed by a summary of NTL approaches. Furthermore, we
emphasize the often-overlooked issue of robustness against various attacks that
can destroy the non-transferable mechanism established by NTL. Experiments
conducted via NTLBench verify the limitations of existing NTL methods in
robustness. Finally, we discuss the practical applications of NTL, along with
its future directions and associated challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D
  Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dai, Steven Wang, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient evaluation of three-dimensional (3D) medical images is crucial for
diagnostic and therapeutic practices in healthcare. Recent years have seen a
substantial uptake in applying deep learning and computer vision to analyse and
interpret medical images. Traditional approaches, such as convolutional neural
networks (CNNs) and vision transformers (ViTs), face significant computational
challenges, prompting the need for architectural advancements. Recent efforts
have led to the introduction of novel architectures like the ``Mamba'' model as
alternative solutions to traditional CNNs or ViTs. The Mamba model excels in
the linear processing of one-dimensional data with low computational demands.
However, Mamba's potential for 3D medical image analysis remains underexplored
and could face significant computational challenges as the dimension increases.
This manuscript presents MobileViM, a streamlined architecture for efficient
segmentation of 3D medical images. In the MobileViM network, we invent a new
dimension-independent mechanism and a dual-direction traversing approach to
incorporate with a vision-Mamba-based framework. MobileViM also features a
cross-scale bridging technique to improve efficiency and accuracy across
various medical imaging modalities. With these enhancements, MobileViM achieves
segmentation speeds exceeding 90 frames per second (FPS) on a single graphics
processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster
than the state-of-the-art deep learning models for processing 3D images with
the same computational resources. In addition, experimental evaluations
demonstrate that MobileViM delivers superior performance, with Dice similarity
scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,
ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses
existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is accessible through:
  https://github.com/anthonyweidai/MobileViM_3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Collision-Free Success Rate For Object Goal Visual Navigation
  Via Two-Stage Training With Collision Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Lian, Feitian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The object goal visual navigation is the task of navigating to a specific
target object using egocentric visual observations. Recent end-to-end
navigation models based on deep reinforcement learning have achieved remarkable
performance in finding and reaching target objects. However, the collision
problem of these models during navigation remains unresolved, since the
collision is typically neglected when evaluating the success. Although
incorporating a negative reward for collision during training appears
straightforward, it results in a more conservative policy, thereby limiting the
agent's ability to reach targets. In addition, many of these models utilize
only RGB observations, further increasing the difficulty of collision avoidance
without depth information. To address these limitations, a new concept --
collision-free success is introduced to evaluate the ability of navigation
models to find a collision-free path towards the target object. A two-stage
training method with collision prediction is proposed to improve the
collision-free success rate of the existing navigation models using RGB
observations. In the first training stage, the collision prediction module
supervises the agent's collision states during exploration to learn to predict
the possible collision. In the second stage, leveraging the trained collision
prediction, the agent learns to navigate to the target without collision. The
experimental results in the AI2-THOR environment demonstrate that the proposed
method greatly improves the collision-free success rate of different navigation
models and outperforms other comparable collision-avoidance methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferring Textual Preferences to Vision-Language Understanding
  through Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-An Li, Tzu-Han Lin, Yun-Nung Chen, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) perform outstandingly across various
multimodal tasks. However, their ability to evaluate generated content remains
limited, and training vision-language reward models (VLRMs) with preference
data is computationally expensive. This paper explores a training-free
alternative by merging text-based reward models (RMs) with LVLMs to create
VLRMs. Our approach shows that integrating these models leads to improved
performance over LVLMs' scoring and text-based RMs, offering an efficient
method for incorporating textual preferences into LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2.5D U-Net with Depth Reduction for 3D CryoET Object Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Uchida, Takaaki Fukui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron tomography (cryoET) is a crucial technique for unveiling the
structure of protein complexes. Automatically analyzing tomograms captured by
cryoET is an essential step toward understanding cellular structures. In this
paper, we introduce the 4th place solution from the CZII - CryoET Object
Identification competition, which was organized to advance the development of
automated tomogram analysis techniques. Our solution adopted a heatmap-based
keypoint detection approach, utilizing an ensemble of two different types of
2.5D U-Net models with depth reduction. Despite its highly unified and simple
architecture, our method achieved 4th place, demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Chest X-ray Classification through Knowledge Injection in
  Cross-Modality Learning <span class="chip">ICASSP'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yan, Bingqing Yue, Qiaxuan Li, Man Huang, Jingyu Chen, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence in medical imaging has shown
tremendous potential, yet the relationship between pre-trained knowledge and
performance in cross-modality learning remains unclear. This study investigates
how explicitly injecting medical knowledge into the learning process affects
the performance of cross-modality classification, focusing on Chest X-ray (CXR)
images. We introduce a novel Set Theory-based knowledge injection framework
that generates captions for CXR images with controllable knowledge granularity.
Using this framework, we fine-tune CLIP model on captions with varying levels
of medical information. We evaluate the model's performance through zero-shot
classification on the CheXpert dataset, a benchmark for CXR classification. Our
results demonstrate that injecting fine-grained medical knowledge substantially
improves classification accuracy, achieving 72.5\% compared to 49.9\% when
using human-generated captions. This highlights the crucial role of
domain-specific knowledge in medical cross-modality learning. Furthermore, we
explore the influence of knowledge density and the use of domain-specific Large
Language Models (LLMs) for caption generation, finding that denser knowledge
and specialized LLMs contribute to enhanced performance. This research advances
medical image analysis by demonstrating the effectiveness of knowledge
injection for improving automated CXR classification, paving the way for more
accurate and reliable diagnostic tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised classification of bird vocalizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simen Hexeberg, Mandar Chitre, Matthias Hoffmann-Kuhnt, Bing Wen Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Changes in bird populations can indicate broader changes in ecosystems,
making birds one of the most important animal groups to monitor. Combining
machine learning and passive acoustics enables continuous monitoring over
extended periods without direct human involvement. However, most existing
techniques require extensive expert-labeled datasets for training and cannot
easily detect time-overlapping calls in busy soundscapes. We propose a
semi-supervised acoustic bird detector designed to allow both the detection of
time-overlapping calls (when separated in frequency) and the use of few labeled
training samples. The classifier is trained and evaluated on a combination of
community-recorded open-source data and long-duration soundscape recordings
from Singapore. It achieves a mean F0.5 score of 0.701 across 315 classes from
110 bird species on a hold-out test set, with an average of 11 labeled training
samples per class. It outperforms the state-of-the-art BirdNET classifier on a
test set of 103 bird species despite significantly fewer labeled training
samples. The detector is further tested on 144 microphone-hours of continuous
soundscape data. The rich soundscape in Singapore makes suppression of false
positives a challenge on raw, continuous data streams. Nevertheless, we
demonstrate that achieving high precision in such environments with minimal
labeled training data is possible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust
  Multi-Teacher Knowledge Distillation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, Yuantao Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has achieved significant success in the field of remote sensing
image change detection (CD), yet two major challenges remain: the scarcity of
sub-meter, all-inclusive open-source CD datasets, and the difficulty of
achieving consistent and satisfactory detection results across images with
varying change areas. To address these issues, we introduce the JL1-CD dataset,
which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5
to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation
(MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD
datasets demonstrate that the MTKD framework significantly improves the
performance of CD models with various network architectures and parameter
sizes, achieving new state-of-the-art results. The code is available at
https://github.com/circleLZY/MTKD-CD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures. Submitted to IEEE Transactions on Geoscience and
  Remote Sensing (TGRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaizeEar-SAM: Zero-Shot Maize Ear Phenotyping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Zaremehrjerdi, Lisa Coffey, Talukder Jubery, Huyu Liu, Jon Turkus, Kyle Linders, James C. Schnable, Patrick S. Schnable, Baskar Ganapathysubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying the variation in yield component traits of maize (Zea mays L.),
which together determine the overall productivity of this globally important
crop, plays a critical role in plant genetics research, plant breeding, and the
development of improved farming practices. Grain yield per acre is calculated
by multiplying the number of plants per acre, ears per plant, number of kernels
per ear, and the average kernel weight. The number of kernels per ear is
determined by the number of kernel rows per ear multiplied by the number of
kernels per row. Traditional manual methods for measuring these two traits are
time-consuming, limiting large-scale data collection. Recent automation efforts
using image processing and deep learning encounter challenges such as high
annotation costs and uncertain generalizability.
  We tackle these issues by exploring Large Vision Models for zero-shot,
annotation-free maize kernel segmentation. By using an open-source large vision
model, the Segment Anything Model (SAM), we segment individual kernels in RGB
images of maize ears and apply a graph-based algorithm to calculate the number
of kernels per row. Our approach successfully identifies the number of kernels
per row across a wide range of maize ears, showing the potential of zero-shot
learning with foundation vision models combined with image processing
techniques to improve automation and reduce subjectivity in agronomic data
collection. All our code is open-sourced to make these affordable phenotyping
methods accessible to everyone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SNN-Driven Multimodal Human Action Recognition via Event Camera and
  Skeleton Data Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naichuan Zheng, Hailun Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal human action recognition based on RGB and skeleton data fusion,
while effective, is constrained by significant limitations such as high
computational complexity, excessive memory consumption, and substantial energy
demands, particularly when implemented with Artificial Neural Networks (ANN).
These limitations restrict its applicability in resource-constrained scenarios.
To address these challenges, we propose a novel Spiking Neural Network
(SNN)-driven framework for multimodal human action recognition, utilizing event
camera and skeleton data. Our framework is centered on two key innovations: (1)
a novel multimodal SNN architecture that employs distinct backbone networks for
each modality-an SNN-based Mamba for event camera data and a Spiking Graph
Convolutional Network (SGN) for skeleton data-combined with a spiking semantic
extraction module to capture deep semantic representations; and (2) a
pioneering SNN-based discretized information bottleneck mechanism for modality
fusion, which effectively balances the preservation of modality-specific
semantics with efficient information compression. To validate our approach, we
propose a novel method for constructing a multimodal dataset that integrates
event camera and skeleton data, enabling comprehensive evaluation. Extensive
experiments demonstrate that our method achieves superior performance in both
recognition accuracy and energy efficiency, offering a promising solution for
practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the Test-Time Scaling, the integration of External Slow-Thinking
with the Verify mechanism has been demonstrated to enhance multi-round
reasoning in large language models (LLMs). However, in the multimodal (MM)
domain, there is still a lack of a strong MM-Verifier. In this paper, we
introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through
longer inference and more robust verification. First, we propose a two-step MM
verification data synthesis method, which combines a simulation-based tree
search with verification and uses rejection sampling to generate high-quality
Chain-of-Thought (COT) data. This data is then used to fine-tune the
verification model, MM-Verifier. Additionally, we present a more efficient
method for synthesizing MMCOT data, bridging the gap between text-based and
multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner.
Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and
MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness
and scalability, with performance improving as data size increases. Finally,
our approach achieves strong performance when combining MM-Reasoner and
MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o
(63.8) with 12 rollouts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoVer: Motion Verification for Motion Graphics Animations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaju Ma, Maneesh Agrawala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large vision-language models can generate motion graphics animations
from text prompts, they regularly fail to include all of spatio-temporal
properties described in the prompt. We introduce MoVer, a motion verification
DSL based on first-order logic that can check spatio-temporal properties of a
motion graphics animation. We identify a general set of such properties that
people commonly use to describe animations (e.g., the direction and timing of
motions, the relative positioning of objects, etc.). We implement these
properties as predicates in MoVer and provide an execution engine that can
apply a MoVer program to any input SVG-based motion graphics animation. We then
demonstrate how MoVer can be used in an LLM-based synthesis and verification
pipeline for iteratively refining motion graphics animations. Given a text
prompt, our pipeline synthesizes a motion graphics animation and a
corresponding MoVer program. Executing the verification program on the
animation yields a report of the predicates that failed and the report can be
automatically fed back to LLM to iteratively correct the animation. To evaluate
our pipeline, we build a synthetic dataset of 5600 text prompts paired with
ground truth MoVer verification programs. We find that while our LLM-based
pipeline is able to automatically generate a correct motion graphics animation
for 58.8% of the test prompts without any iteration, this number raises to
93.6% with up to 50 correction iterations. Project website:
https://mover-dsl.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pretrain</span>ed Image-Text Models are Secretly Video Captioners <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Zhang, Yiren Jian, Zhongyu Ouyang, Soroush Vosoughi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing video captioning models is computationally expensive. The dynamic
nature of video also complicates the design of multimodal models that can
effectively caption these sequences. However, we find that by using minimal
computational resources and without complex modifications to address video
dynamics, an image-based model can be repurposed to outperform several
specialised video captioning systems. Our adapted model demonstrates top tier
performance on major benchmarks, ranking 2nd on MSRVTT and MSVD, and 3rd on
VATEX. We transform it into a competitive video captioner by post training a
typical image captioning model BLIP2 with only 6,000 video text pairs and
simply concatenating frames (significantly fewer data than other methods),
which use 2.5 to 144 million pairs. From a resource optimization perspective,
this video captioning study focuses on three fundamental factors: optimizing
model scale, maximizing data efficiency, and incorporating reinforcement
learning. This extensive study demonstrates that a lightweight, image based
adaptation strategy can rival state-of-the-art video captioning systems,
offering a practical solution for low-resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2025 Annual Conference of the Nations of the Americas
  Chapter of the Association for Computational Linguistics (NAACL 2025). The
  first two authors contributed equally and were listed in random order</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed Signals: A Diverse Point Cloud <span class="highlight-title">Dataset</span> for Heterogeneous LiDAR V2X
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katie Z Luo, Minh-Quan Dao, Zhenzhen Liu, Mark Campbell, Wei-Lun Chao, Kilian Q. Weinberger, Ezio Malis, Vincent Fremont, Bharath Hariharan, Mao Shan, Stewart Worrall, Julie Stephany Berrio Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-everything (V2X) collaborative perception has emerged as a
promising solution to address the limitations of single-vehicle perception
systems. However, existing V2X datasets are limited in scope, diversity, and
quality. To address these gaps, we present Mixed Signals, a comprehensive V2X
dataset featuring 45.1k point clouds and 240.6k bounding boxes collected from
three connected autonomous vehicles (CAVs) equipped with two different types of
LiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides
precisely aligned point clouds and bounding box annotations across 10 classes,
ensuring reliable data for perception training. We provide detailed statistical
analysis on the quality of our dataset and extensively benchmark existing V2X
methods on it. Mixed Signals V2X Dataset is one of the highest quality,
large-scale datasets publicly available for V2X perception research. Details on
the website https://mixedsignalsdataset.cs.cornell.edu/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual
  Question Answering in Pituitary Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarakol Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) in visual question answering (VQA) offer a
unique opportunity to enhance intra-operative decision-making, promote
intuitive interactions, and significantly advancing surgical education.
However, the development of VLMs for surgical VQA is challenging due to limited
datasets and the risk of overfitting and catastrophic forgetting during full
fine-tuning of pretrained weights. While parameter-efficient techniques like
Low-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MoRA) address
adaptation challenges, their uniform parameter distribution overlooks the
feature hierarchy in deep networks, where earlier layers, that learn general
features, require more parameters than later ones. This work introduces
PitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank
adaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting
GPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames
from 25 procedural videos with 745,972 question-answer sentence pairs, covering
key surgical elements such as phase and step recognition, context
understanding, tool detection, localization, and interactions recognition.
Vector-MoLoRA incorporates the principles of LoRA and MoRA to develop a
matrix-low-rank adaptation strategy that employs vector ranking to allocate
more parameters to earlier layers, gradually reducing them in the later layers.
Our approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets,
effectively mitigates catastrophic forgetting while significantly enhancing
performance over recent baselines. Furthermore, our risk-coverage analysis
highlights its enhanced reliability and trustworthiness in handling uncertain
predictions. Our source code and dataset is available
at~\url{https://github.com/HRL-Mike/PitVQA-Plus}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Adaptation via Side Graph Convolution for Temporally and Spatially
  Efficient Fine-tuning of 3D Point Cloud <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiko Furuya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloud
Transformers has emerged as a promising technique for 3D point cloud analysis.
While existing PEFT methods attempt to minimize the number of tunable
parameters, they still suffer from high temporal and spatial computational
costs during fine-tuning. This paper proposes a novel PEFT algorithm for 3D
point cloud Transformers, called Side Token Adaptation on a neighborhood Graph
(STAG), to achieve superior temporal and spatial efficiency. STAG employs a
graph convolutional side network that operates in parallel with a frozen
backbone Transformer to adapt tokens to downstream tasks. STAG's side network
realizes high efficiency through three key components: connection with the
backbone that enables reduced gradient computation, parameter sharing
framework, and efficient graph convolution. Furthermore, we present Point Cloud
Classification 13 (PCC13), a new benchmark comprising diverse publicly
available 3D point cloud datasets, enabling comprehensive evaluation of PEFT
methods. Extensive experiments using multiple pre-trained models and PCC13
demonstrates the effectiveness of STAG. Specifically, STAG maintains
classification accuracy comparable to existing methods while reducing tunable
parameters to only 0.43M and achieving significant reductions in both
computational time and memory consumption for fine-tuning. Code and benchmark
will be available at: https://github.com/takahikof/STAG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModSkill: Physical Character Skill Modularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Huang, Zhiyang Dou, Lingjie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion is highly diverse and dynamic, posing challenges for imitation
learning algorithms that aim to generalize motor skills for controlling
simulated characters. Previous methods typically rely on a universal full-body
controller for tracking reference motion (tracking-based model) or a unified
full-body skill embedding space (skill embedding). However, these approaches
often struggle to generalize and scale to larger motion datasets. In this work,
we introduce a novel skill learning framework, ModSkill, that decouples complex
full-body skills into compositional, modular skills for independent body parts.
Our framework features a skill modularization attention layer that processes
policy observations into modular skill embeddings that guide low-level
controllers for each body part. We also propose an Active Skill Learning
approach with Generative Adaptive Sampling, using large motion generation
models to adaptively enhance policy learning in challenging tracking scenarios.
Our results show that this modularized skill learning framework, enhanced by
generative sampling, outperforms existing methods in precise full-body motion
tracking and enables reusable skill embeddings for diverse goal-driven tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GlossGau: Efficient Inverse Rendering for Glossy Surface with
  Anisotropic Spherical Gaussian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Du, Runfa Blark Li, Chen Du, Truong Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reconstruction of 3D objects from calibrated photographs represents a
fundamental yet intricate challenge in the domains of computer graphics and
vision. Although neural reconstruction approaches based on Neural Radiance
Fields (NeRF) have shown remarkable capabilities, their processing costs remain
substantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely
improves the training efficiency and facilitates to generate realistic
rendering in real-time. However, due to the limited ability of Spherical
Harmonics (SH) to represent high-frequency information, 3D-GS falls short in
reconstructing glossy objects. Researchers have turned to enhance the specular
expressiveness of 3D-GS through inverse rendering. Yet these methods often
struggle to maintain the training and rendering efficiency, undermining the
benefits of Gaussian Splatting techniques. In this paper, we introduce
GlossGau, an efficient inverse rendering framework that reconstructs scenes
with glossy surfaces while maintaining training and rendering speeds comparable
to vanilla 3D-GS. Specifically, we explicitly model the surface normals,
Bidirectional Reflectance Distribution Function (BRDF) parameters, as well as
incident lights and use Anisotropic Spherical Gaussian (ASG) to approximate the
per-Gaussian Normal Distribution Function under the microfacet model. We
utilize 2D Gaussian Splatting (2D-GS) as foundational primitives and apply
regularization to significantly alleviate the normal estimation challenge
encountered in related works. Experiments demonstrate that GlossGau achieves
competitive or superior reconstruction on datasets with glossy surfaces.
Compared with previous GS-based works that address the specular surface, our
optimization time is considerably less.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular <span class="highlight-title">Prompt</span> Learning Improves Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhan Huang, Tejaswini Pedapati, Pin-Yu Chen, Jianxi Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained vision-language models are able to interpret visual concepts and
language semantics. Prompt learning, a method of constructing prompts for text
encoders or image encoders, elicits the potentials of pre-trained models and
readily adapts them to new scenarios. Compared to fine-tuning, prompt learning
enables the model to achieve comparable or better performance using fewer
trainable parameters. Besides, prompt learning freezes the pre-trained model
and avoids the catastrophic forgetting issue in the fine-tuning. Continuous
prompts inserted into the input of every transformer layer (i.e. deep prompts)
can improve the performances of pre-trained models on downstream tasks. For
i-th transformer layer, the inserted prompts replace previously inserted
prompts in the $(i-1)$-th layer. Although the self-attention mechanism
contextualizes newly inserted prompts for the current layer and embeddings from
the previous layer's output, removing all inserted prompts from the previous
layer inevitably loses information contained in the continuous prompts. In this
work, we propose Modular Prompt Learning (MPL) that is designed to promote the
preservation of information contained in the inserted prompts. We evaluate the
proposed method on base-to-new generalization and cross-dataset tasks. On
average of 11 datasets, our method achieves 0.7% performance gain on the
base-to-new generalization task compared to the state-of-the-art method. The
largest improvement on the individual dataset is 10.7% (EuroSAT dataset).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Acoustics, Speech, and Signal
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-centric Binding in Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rim Assouel, Pietro Astolfi, Florian Bordes, Michal Drozdzal, Adriana Romero-Soriano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in vision language models (VLM) have been driven by
contrastive models such as CLIP, which learn to associate visual information
with their corresponding text descriptions. However, these models have
limitations in understanding complex compositional scenes involving multiple
objects and their spatial relationships. To address these challenges, we
propose a novel approach that diverges from commonly used strategies, which
rely on the design of hard-negative augmentations. Instead, our work focuses on
integrating inductive biases into pre-trained CLIP-like models to improve their
compositional understanding without using any additional hard-negatives. To
that end, we introduce a binding module that connects a scene graph, derived
from a text description, with a slot-structured image representation,
facilitating a structured similarity assessment between the two modalities. We
also leverage relationships as text-conditioned visual constraints, thereby
capturing the intricate interactions between objects and their contextual
relationships more effectively. Our resulting model not only enhances the
performance of CLIP-based models in multi-object compositional understanding
but also paves the way towards more accurate and sample-efficient image-text
matching of complex scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud Geometry Scalable Coding Using a Resolution and
  Quality-conditioned Latents Probability Estimator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Mari, André F. R. Guarda, Nuno M. M. Rodrigues, Simone Milani, Fernando Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current age, users consume multimedia content in very heterogeneous
scenarios in terms of network, hardware, and display capabilities. A naive
solution to this problem is to encode multiple independent streams, each
covering a different possible requirement for the clients, with an obvious
negative impact in both storage and computational requirements. These drawbacks
can be avoided by using codecs that enable scalability, i.e., the ability to
generate a progressive bitstream, containing a base layer followed by multiple
enhancement layers, that allow decoding the same bitstream serving multiple
reconstructions and visualization specifications. While scalable coding is a
well-known and addressed feature in conventional image and video codecs, this
paper focuses on a new and very different problem, notably the development of
scalable coding solutions for deep learning-based Point Cloud (PC) coding. The
peculiarities of this 3D representation make it hard to implement flexible
solutions that do not compromise the other functionalities of the codec. This
paper proposes a joint quality and resolution scalability scheme, named
Scalable Resolution and Quality Hyperprior (SRQH), that, contrary to previous
solutions, can model the relationship between latents obtained with models
trained for different RD tradeoffs and/or at different resolutions.
Experimental results obtained by integrating SRQH in the emerging JPEG Pleno
learning-based PC coding standard show that SRQH allows decoding the PC at
different qualities and resolutions with a single bitstream while incurring
only in a limited RD penalty and increment in complexity w.r.t. non-scalable
JPEG PCC that would require one bitstream per coding configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE and currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Visual Servoing of Tendon-driven Continuum Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rana Danesh, Farrokh Janabi-Sharifi, Farhad Aghili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Hybrid Visual Servoing (HVS) approach for
controlling tendon-driven continuum robots (TDCRs). The HVS system combines
Image-Based Visual Servoing (IBVS) with Deep Learning-Based Visual Servoing
(DLBVS) to overcome the limitations of each method and improve overall
performance. IBVS offers higher accuracy and faster convergence in feature-rich
environments, while DLBVS enhances robustness against disturbances and offers a
larger workspace. By enabling smooth transitions between IBVS and DLBVS, the
proposed HVS ensures effective control in dynamic, unstructured environments.
The effectiveness of this approach is validated through simulations and
real-world experiments, demonstrating that HVS achieves reduced iteration time,
faster convergence, lower final error, and smoother performance compared to
DLBVS alone, while maintaining DLBVS's robustness in challenging conditions
such as occlusions, lighting changes, actuator noise, and physical impacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romina Aalishah, Mozhgan Navardi, Tinoosh Mohsenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (AI) has gained significant attention in
recent years, revolutionizing various applications across industries. Among
these, advanced vision models for image super-resolution are in high demand,
particularly for deployment on edge devices where real-time processing is
crucial. However, deploying such models on edge devices is challenging due to
limited computing power and memory. In this paper, we present MambaLiteSR, a
novel lightweight image Super-Resolution (SR) model that utilizes the
architecture of Vision Mamba. It integrates State Space Blocks and a
reconstruction module for efficient feature extraction. To optimize efficiency
without affecting performance, MambaLiteSR employs knowledge distillation to
transfer key insights from a larger Mamba-based teacher model to a smaller
student model via hyperparameter tuning. Through mathematical analysis of model
parameters and their impact on PSNR, we identify key factors and adjust them
accordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms
state-of-the-art edge SR methods by reducing power consumption while
maintaining competitive PSNR and SSIM scores across benchmark datasets. It also
reduces power usage during training via low-rank approximation. Moreover,
MambaLiteSR reduces parameters with minimal performance loss, enabling
efficient deployment of generative AI models on resource-constrained devices.
Deployment on the embedded NVIDIA Jetson Orin Nano confirms the superior
balance of MambaLiteSR size, latency, and efficiency. Experiments show that
MambaLiteSR achieves performance comparable to both the baseline and other edge
models while using 15% fewer parameters. It also improves power consumption by
up to 58% compared to state-of-the-art SR edge models, all while maintaining
low energy use during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Special Session: Generative AI on Edge, 26th International Symposium
  on Quality Electronic Design (ISQED'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression in EO: Are VLMs Up to the Challenge? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizhe Xue, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) data encompass a vast range of remotely sensed
information, featuring multi-sensor and multi-temporal, playing an
indispensable role in understanding our planet's dynamics. Recently, Vision
Language Models (VLMs) have achieved remarkable success in perception and
reasoning tasks, bringing new insights and opportunities to the EO field.
However, the potential for EO applications, especially for scientific
regression related applications remains largely unexplored. This paper bridges
that gap by systematically examining the challenges and opportunities of
adapting VLMs for EO regression tasks. The discussion first contrasts the
distinctive properties of EO data with conventional computer vision datasets,
then identifies four core obstacles in applying VLMs to EO regression: 1) the
absence of dedicated benchmarks, 2) the discrete-versus-continuous
representation mismatch, 3) cumulative error accumulation, and 4) the
suboptimal nature of text-centric training objectives for numerical tasks.
Next, a series of methodological insights and potential subtle pitfalls are
explored. Lastly, we offer some promising future directions for designing
robust, domain-aware solutions. Our findings highlight the promise of VLMs for
scientific regression in EO, setting the stage for more precise and
interpretable modeling of critical environmental processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image
  Diffusion Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daewon Chae, June Suk Choi, Jinkyu Kim, Kimin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning text-to-image diffusion models to maximize rewards has proven
effective for enhancing model performance. However, reward fine-tuning methods
often suffer from slow convergence due to online sample generation. Therefore,
obtaining diverse samples with strong reward signals is crucial for improving
sample efficiency and overall performance. In this work, we introduce DiffExp,
a simple yet effective exploration strategy for reward fine-tuning of
text-to-image models. Our approach employs two key strategies: (a) dynamically
adjusting the scale of classifier-free guidance to enhance sample diversity,
and (b) randomly weighting phrases of the text prompt to exploit high-quality
reward signals. We demonstrate that these strategies significantly enhance
exploration during online sample generation, improving the sample efficiency of
recent reward fine-tuning methods, such as DDPO and AlignProp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Racing <span class="highlight-title">Dataset</span> and Baseline Model for Track Detection in Autonomous
  Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Ghosh, Yi-Huan Chen, Ching-Hsiang Huang, Abu Shafin Mohammad Mahdee Jameel, Chien Chou Ho, Aly El Gamal, Samuel Labi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant challenge in racing-related research is the lack of publicly
available datasets containing raw images with corresponding annotations for the
downstream task. In this paper, we introduce RoRaTrack, a novel dataset that
contains annotated multi-camera image data from racing scenarios for track
detection. The data is collected on a Dallara AV-21 at a racing circuit in
Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack
addresses common problems such as blurriness due to high speed, color inversion
from the camera, and absence of lane markings on the track. Consequently, we
propose RaceGAN, a baseline model based on a Generative Adversarial Network
(GAN) that effectively addresses these challenges. The proposed model
demonstrates superior performance compared to current state-of-the-art machine
learning models in track detection. The dataset and code for this work are
available at github.com/RaceGAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shansong Wang, Mojtaba Safari, Qiang Li, Chih-Wei Chang, Richard LJ Qiu, Justin Roper, David S. Yu, Xiaofeng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models (VFMs) are pre-trained on extensive image datasets
to learn general representations for diverse types of data. These models can
subsequently be fine-tuned for specific downstream tasks, significantly
boosting performance across a broad range of applications. However, existing
vision foundation models that claim to be applicable to various radiology tasks
are mostly pre-trained on 3D computed tomography (CT), which benefits from the
availability of extensive 3D CT databases. Significant differences between CT
and magnetic resonance imaging (MRI) in imaging principles, signal
characteristics, and data distribution may hinder their practical performance
and versatility in MRI-specific applications. Here, we propose Triad, a vision
foundation model for 3D MRI. Triad adopts a widely used autoencoder
architecture to learn robust representations from 131,170 3D MRI volumes and
uses organ-independent imaging descriptions to constrain the semantic
distribution of the visual modality. The above pre-training dataset is called
Triad-131K, which is currently the largest 3D MRI pre-training dataset. We
evaluate Triad across three tasks, namely, organ/tumor segmentation,
organ/cancer classification, and medical image registration, in two data
modalities (within-domain and out-of-domain) settings using 25 downstream
datasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad
improves segmentation performance by 6.88% compared to nnUNet-Scratch across 17
datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in
classification tasks across five datasets. SwinUNETR-Triad improves by 4.00%
compared to SwinUNETR-Scratch in registration tasks across two datasets. Our
study demonstrates that pre-training can maximize performance when the data
modalities and organs of upstream and downstream tasks are consistent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhao, Zeyu Zhang, Yi Xu, Yi Yao, Yan Huang, Wenxin Zhang, Zirui Song, Xiuying Chen, Yang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian detection in intelligent transportation systems has made
significant progress but faces two critical challenges: (1) insufficient fusion
of complementary information between visible and infrared spectra, particularly
in complex scenarios, and (2) sensitivity to illumination changes, such as
low-light or overexposed conditions, leading to degraded performance. To
address these issues, we propose PedDet, an adaptive spectral optimization
complementarity framework specifically enhanced and optimized for multispectral
pedestrian detection. PedDet introduces the Multi-scale Spectral Feature
Perception Module (MSFPM) to adaptively fuse visible and infrared features,
enhancing robustness and flexibility in feature extraction. Additionally, the
Illumination Robustness Feature Decoupling Module (IRFDM) improves detection
stability under varying lighting by decoupling pedestrian and background
features. We further design a contrastive alignment to enhance intermodal
feature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that
PedDet achieves state-of-the-art performance, improving the mAP by 6.6% with
superior detection accuracy even in low-light conditions, marking a significant
step forward for road safety. Code will be available at
https://github.com/AIGeeksGroup/PedDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Fang, Thomas Pöllabauer, Tristan Wirth, Sarah Berkei, Volker Knauthe, Arjan Kuijper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industrial applications requiring real-time feedback, such as quality
control and robotic manipulation, the demand for high-speed and accurate pose
estimation remains critical. Despite advances improving speed and accuracy in
pose estimation, finding a balance between computational efficiency and
accuracy poses significant challenges in dynamic environments. Most current
algorithms lack scalability in estimation time, especially for diverse
datasets, and the state-of-the-art (SOTA) methods are often too slow. This
study focuses on developing a fast and scalable set of pose estimators based on
GDRNPP to meet or exceed current benchmarks in accuracy and robustness,
particularly addressing the efficiency-accuracy trade-off essential in
real-time scenarios. We propose the AMIS algorithm to tailor the utilized model
according to an application-specific trade-off between inference time and
accuracy. We further show the effectiveness of the AMIS-based model choice on
four prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Cognition and Explainability of Multimodal Foundation Models
  with Self-Synthesized Data <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Shi, Quanzheng Li, Jin Sun, Xiang Li, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have shown impressive capabilities in a wide
range of visual tasks. However, they often struggle with fine-grained visual
reasoning, failing to identify domain-specific objectives and provide
justifiable explanations for their predictions. To address this, we propose a
novel visual rejection sampling framework to improve the cognition and
explainability of LMMs using self-synthesized data. Specifically, visual
fine-tuning requires images, queries, and target answers. Our approach begins
by synthesizing interpretable answers that include human-verifiable visual
features. These features are based on expert-defined concepts, carefully
selected based on their alignment with the image content. After each round of
fine-tuning, we apply a reward model-free filtering mechanism to select the
highest-quality interpretable answers for the next round of tuning. This
iterative process of data synthesis and fine-tuning progressively improves the
model's ability to generate accurate and reasonable explanations. Experimental
results demonstrate the effectiveness of our method in improving both the
accuracy and explainability of specialized visual classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Code: https://github.com/sycny/SelfSynthX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Activation with Knowledge Distillation for Energy-Efficient
  Spiking NN Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orestis Konstantaropoulos, Theodoris Mallios, Maria Papadopouli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While foundation AI models excel at tasks like classification and
decision-making, their high energy consumption makes them unsuitable for
energy-constrained applications. Inspired by the brain's efficiency, spiking
neural networks (SNNs) have emerged as a viable alternative due to their
event-driven nature and compatibility with neuromorphic chips. This work
introduces a novel system that combines knowledge distillation and ensemble
learning to bridge the performance gap between artificial neural networks
(ANNs) and SNNs. A foundation AI model acts as a teacher network, guiding
smaller student SNNs organized into an ensemble, called Spiking Neural Ensemble
(SNE). SNE enables the disentanglement of the teacher's knowledge, allowing
each student to specialize in predicting a distinct aspect of it, while
processing the same input. The core innovation of SNE is the adaptive
activation of a subset of SNN models of an ensemble, leveraging
knowledge-distillation, enhanced with an informed-partitioning
(disentanglement) of the teacher's feature space. By dynamically activating
only a subset of these student SNNs, the system balances accuracy and energy
efficiency, achieving substantial energy savings with minimal accuracy loss.
Moreover, SNE is significantly more efficient than the teacher network,
reducing computational requirements by up to 20x with only a 2% drop in
accuracy on the CIFAR-10 dataset. This disentanglement procedure achieves an
accuracy improvement of up to 2.4% on the CIFAR-10 dataset compared to other
partitioning schemes. Finally, we comparatively analyze SNE performance under
noisy conditions, demonstrating enhanced robustness compared to its ANN
teacher. In summary, SNE offers a promising new direction for
energy-constrained applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAR: Scale-wise Text-conditioned AutoRegressive image generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10797v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10797v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Biye Li, Huaian Chen, Yi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce STAR, a text-to-image model that employs a scale-wise
auto-regressive paradigm. Unlike VAR, which is constrained to class-conditioned
synthesis for images up to 256$\times$256, STAR enables text-driven image
generation up to 1024$\times$1024 through three key designs. First, we
introduce a pre-trained text encoder to extract and adopt representations for
textual constraints, enhancing details and generalizability. Second, given the
inherent structural correlation across different scales, we leverage 2D Rotary
Positional Encoding (RoPE) and tweak it into a normalized version, ensuring
consistent interpretation of relative positions across token maps and
stabilizing the training process. Third, we observe that simultaneously
sampling all tokens within a single scale can disrupt inter-token
relationships, leading to structural instability, particularly in
high-resolution generation. To address this, we propose a novel stable sampling
method that incorporates causal relationships into the sampling process,
ensuring both rich details and stable structures. Compared to previous
diffusion models and auto-regressive models, STAR surpasses existing benchmarks
in fidelity, text-image consistency, and aesthetic quality, requiring just
2.21s for 1024$\times$1024 images on A100. This highlights the potential of
auto-regressive methods in high-quality image synthesis, offering new
directions for the text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular
  Micro-video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchen Fu, Xuri Ge, Kaiwen Zheng, Ioannis Arapakis, Xin Xin, Joemon M. Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold
significant commercial value. The rise of high-quality AI-generated content has
spurred interest in AI-driven micro-video creation. However, despite the
advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek
in text generation and reasoning, their potential to assist the creation of
popular micro-videos remains largely unexplored.
  In this paper, we conduct an empirical study on LLM-assisted popular
micro-video generation (LLMPopcorn). Specifically, we investigate the following
research questions: (i) How can LLMs be effectively utilized to assist popular
micro-video generation? (ii) To what extent can prompt-based enhancements
optimize the LLM-generated content for higher popularity? (iii) How well do
various LLMs and video generators perform in the popular micro-video generation
task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3
enable micro-video generation to achieve popularity comparable to human-created
content. Prompt enhancements further boost popularity, and benchmarking
highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and
HunyuanVideo lead in video generation. This pioneering work advances
AI-assisted micro-video creation, uncovering new research opportunities. We
will release the code and datasets to support future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spherical Dense Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timon Winter, Stanislav Frolov, Brian Bernhard Moser, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image (T2I) have improved synthesis results,
but challenges remain in layout control and generating omnidirectional
panoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address
these issues, but so far no unified approach exists. Trivial approaches, like
prompting a DT2I model to generate panoramas can not generate proper spherical
distortions and seamless transitions at the borders. Our work shows that
spherical dense text-to-image (SDT2I) can be achieved by integrating
training-free DT2I approaches into finetuned panorama models. Specifically, we
propose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating
MultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no
benchmark for SDT2I exists, we further construct Dense-Synthetic-View
(DSynView), a new synthetic dataset containing spherical layouts to evaluate
our models. Our results show that MSTD outperforms MPF across image quality as
well as prompt- and layout adherence. MultiPanFusion generates more diverse
images but struggles to synthesize flawless foreground objects. We propose
bootstrap-coupling and turning off equirectangular perspective-projection
attention in the foreground as an improvement of MPF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud
  Serialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Li, Weiwei Sun, Shrisudhan Govindarajan, Shaobo Xia, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to large-scale point cloud surface reconstruction
by developing an efficient framework that converts an irregular point cloud
into a signed distance field (SDF). Our backbone builds upon recent
transformer-based architectures (i.e., PointTransformerV3), that serializes the
point cloud into a locality-preserving sequence of tokens. We efficiently
predict the SDF value at a point by aggregating nearby tokens, where fast
approximate neighbors can be retrieved thanks to the serialization. We
serialize the point cloud at different levels/scales, and non-linearly
aggregate a feature to predict the SDF value. We show that aggregating across
multiple scales is critical to overcome the approximations introduced by the
serialization (i.e. false negatives in the neighborhood). Our frameworks sets
the new state-of-the-art in terms of accuracy and efficiency (better or similar
performance with half the latency of the best prior method, coupled with a
simpler implementation), particularly on outdoor datasets where sparse-grid
methods have shown limited performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: see https://theialab.github.io/noksr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fusing Point Cloud and Visual Representations for Imitation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atalay Donat, Xiaogang Jia, Xi Huang, Aleksandar Taranovic, Denis Blessing, Ge Li, Hongyi Zhou, Hanyi Zhang, Rudolf Lioutikov, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning for manipulation requires using policies that have access to rich
sensory information such as point clouds or RGB images. Point clouds
efficiently capture geometric structures, making them essential for
manipulation tasks in imitation learning. In contrast, RGB images provide rich
texture and semantic information that can be crucial for certain tasks.
Existing approaches for fusing both modalities assign 2D image features to
point clouds. However, such approaches often lose global contextual information
from the original images. In this work, we propose FPV-Net, a novel imitation
learning method that effectively combines the strengths of both point cloud and
RGB modalities. Our method conditions the point-cloud encoder on global and
local image tokens using adaptive layer norm conditioning, leveraging the
beneficial properties of both modalities. Through extensive experiments on the
challenging RoboCasa benchmark, we demonstrate the limitations of relying on
either modality alone and show that our method achieves state-of-the-art
performance across all tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Efficient Limited-Angle CT Using Deep Priors and Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilmari Vahteristo, Zhi-Song Liu, Andreas Rupp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing an image from its Radon transform is a fundamental computed
tomography (CT) task arising in applications such as X-ray scans. In many
practical scenarios, a full 180-degree scan is not feasible, or there is a
desire to reduce radiation exposure. In these limited-angle settings, the
problem becomes ill-posed, and methods designed for full-view data often leave
significant artifacts. We propose a very low-data approach to reconstruct the
original image from its Radon transform under severe angle limitations. Because
the inverse problem is ill-posed, we combine multiple regularization methods,
including Total Variation, a sinogram filter, Deep Image Prior, and a
patch-level autoencoder. We use a differentiable implementation of the Radon
transform, which allows us to use gradient-based techniques to solve the
inverse problem. Our method is evaluated on a dataset from the Helsinki
Tomography Challenge 2022, where the goal is to reconstruct a binary disk from
its limited-angle sinogram. We only use a total of 12 data points--eight for
learning a prior and four for hyperparameter selection--and achieve results
comparable to the best synthetic data-driven approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 reference pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with
  360$^\circ$ Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongki Jung, Jaehoon Choi, Yonghan Lee, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel 3D reconstruction pipeline for 360$^\circ$ cameras for 3D
mapping and rendering of indoor environments. Traditional Structure-from-Motion
(SfM) methods may not work well in large-scale indoor scenes due to the
prevalence of textureless and repetitive regions. To overcome these challenges,
our approach (IM360) leverages the wide field of view of omnidirectional images
and integrates the spherical camera model into every core component of the SfM
pipeline. In order to develop a comprehensive 3D reconstruction solution, we
integrate a neural implicit surface reconstruction technique to generate
high-quality surfaces from sparse input data. Additionally, we utilize a
mesh-based neural rendering approach to refine texture maps and accurately
capture view-dependent properties by combining diffuse and specular components.
We evaluate our pipeline on large-scale indoor scenes from the Matterport3D and
Stanford2D3D datasets. In practice, IM360 demonstrate superior performance in
terms of textured mesh reconstruction over SOTA. We observe accuracy
improvements in terms of camera localization and registration as well as
rendering high frequency details.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Quality 3D Creation from A Single Image Using Subject-Specific
  Knowledge Prior <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Huang, Ting Zhang, Yuhui Yuan, Dong Chen, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the critical bottleneck in robotics caused by the
scarcity of diverse 3D data by presenting a novel two-stage approach for
generating high-quality 3D models from a single image. This method is motivated
by the need to efficiently expand 3D asset creation, particularly for robotics
datasets, where the variety of object types is currently limited compared to
general image datasets. Unlike previous methods that primarily rely on general
diffusion priors, which often struggle to align with the reference image, our
approach leverages subject-specific prior knowledge. By incorporating
subject-specific priors in both geometry and texture, we ensure precise
alignment between the generated 3D content and the reference object.
Specifically, we introduce a shading mode-aware prior into the NeRF
optimization process, enhancing the geometry and refining texture in the coarse
outputs to achieve superior quality. Extensive experiments demonstrate that our
method significantly outperforms prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA2025, Project Page:
  https://nnanhuang.github.io/projects/customize-it-3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Carefully Blending Adversarial Training, Purification, and Aggregation
  Improves Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06081v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06081v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Ballarin, Alessio Ansuini, Luca Bortolussi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel adversarial defence mechanism for image
classification - CARSO - blending the paradigms of adversarial training and
adversarial purification in a synergistic robustness-enhancing way. The method
builds upon an adversarially-trained classifier, and learns to map its internal
representation associated with a potentially perturbed input onto a
distribution of tentative clean reconstructions. Multiple samples from such
distribution are classified by the same adversarially-trained model, and a
carefully chosen aggregation of its outputs finally constitutes the robust
prediction of interest. Experimental evaluation by a well-established benchmark
of strong adaptive attacks, across different image datasets, shows that CARSO
is able to defend itself against adaptive end-to-end white-box attacks devised
for stochastic defences. Paying a modest clean accuracy toll, our method
improves by a significant margin the state-of-the-art for Cifar-10, Cifar-100,
and TinyImageNet-200 $\ell_\infty$ robust classification accuracy against
AutoAttack. Code, and instructions to obtain pre-trained models are available
at: https://github.com/emaballarin/CARSO .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 1 figure, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining the Impact of Training on Vision Models via Activation
  Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahcène Boubekki, Samuel G. Fadel, Sebastian Mair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in the field of explainable artificial intelligence (XAI)
for vision models investigate the information extracted by their feature
encoder. We contribute to this effort and propose Neuro-Activated Vision
Explanations (NAVE), which extracts the information captured by the encoder by
clustering the feature activations of the frozen network to be explained. The
method does not aim to explain the model's prediction but to answer questions
such as which parts of the image are processed similarly or which information
is kept in deeper layers. Experimentally, we leverage NAVE to show that the
training dataset and the level of supervision affect which concepts are
captured. In addition, our method reveals the impact of registers on vision
transformers (ViT) and the information saturation caused by the watermark
Clever Hans effect in the training set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Instance-based Navigation Toward User-Specific Objects in
  Realistic Environments <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Barsellotti, Roberto Bigazzi, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last years, the research interest in visual navigation towards objects
in indoor environments has grown significantly. This growth can be attributed
to the recent availability of large navigation datasets in photo-realistic
simulated environments, like Gibson and Matterport3D. However, the navigation
tasks supported by these datasets are often restricted to the objects present
in the environment at acquisition time. Also, they fail to account for the
realistic scenario in which the target object is a user-specific instance that
can be easily confused with similar objects and may be found in multiple
locations within the environment. To address these limitations, we propose a
new task denominated Personalized Instance-based Navigation (PIN), in which an
embodied agent is tasked with locating and reaching a specific personal object
by distinguishing it among multiple instances of the same category. The task is
accompanied by PInNED, a dedicated new dataset composed of photo-realistic
scenes augmented with additional 3D objects. In each episode, the target object
is presented to the agent using two modalities: a set of visual reference
images on a neutral background and manually annotated textual descriptions.
Through comprehensive evaluations and analyses, we showcase the challenges of
the PIN task as well as the performance and shortcomings of currently available
methods designed for object-driven navigation, considering modular and
end-to-end agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track. Project page:
  https://aimagelab.github.io/pin/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EC-DIT: Scaling Diffusion <span class="highlight-title">Transformer</span>s with Adaptive Expert-Choice
  Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02098v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02098v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, Nan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion transformers have been widely adopted for text-to-image synthesis.
While scaling these models up to billions of parameters shows promise, the
effectiveness of scaling beyond current sizes remains underexplored and
challenging. By explicitly exploiting the computational heterogeneity of image
generations, we develop a new family of Mixture-of-Experts (MoE) models
(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns
to adaptively optimize the compute allocated to understand the input texts and
generate the respective image patches, enabling heterogeneous computation
aligned with varying text-image complexities. This heterogeneity provides an
efficient way of scaling EC-DIT up to 97 billion parameters and achieving
significant improvements in training convergence, text-to-image alignment, and
overall generation quality over dense models and conventional MoE models.
Through extensive ablations, we show that EC-DIT demonstrates superior
scalability and adaptive compute allocation by recognizing varying textual
importance through end-to-end training. Notably, in text-to-image alignment
evaluation, our largest models achieve a state-of-the-art GenEval score of
71.68% and still maintain competitive inference speed with intuitive
interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaSSC: Enhancing 3D Semantic Scene Completion for Autonomous Driving
  through Meta-Learning and Long-sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Qu, Zixuan Xu, Zilin Huang, Zihao Sheng, Tiantian Chen, Sikai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic scene completion (SSC) is essential for achieving comprehensive
perception in autonomous driving systems. However, existing SSC methods often
overlook the high deployment costs in real-world applications. Traditional
architectures, such as 3D Convolutional Neural Networks (3D CNNs) and
self-attention mechanisms, face challenges in efficiently capturing long-range
dependencies within 3D voxel grids, limiting their effectiveness. To address
these issues, we introduce MetaSSC, a novel meta-learning-based framework for
SSC that leverages deformable convolution, large-kernel attention, and the
Mamba (D-LKA-M) model. Our approach begins with a voxel-based semantic
segmentation (SS) pretraining task, aimed at exploring the semantics and
geometry of incomplete regions while acquiring transferable meta-knowledge.
Using simulated cooperative perception datasets, we supervise the perception
training of a single vehicle using aggregated sensor data from multiple nearby
connected autonomous vehicles (CAVs), generating richer and more comprehensive
labels. This meta-knowledge is then adapted to the target domain through a
dual-phase training strategy that does not add extra model parameters, enabling
efficient deployment. To further enhance the model's capability in capturing
long-sequence relationships within 3D voxel grids, we integrate Mamba blocks
with deformable convolution and large-kernel attention into the backbone
network. Extensive experiments demonstrate that MetaSSC achieves
state-of-the-art performance, significantly outperforming competing models
while also reducing deployment costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Emotion Recognition using Audio-Video <span class="highlight-title">Transformer</span> Fusion with
  Cross Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Dhanith P R, Shravan Venkatraman, Vigya Sharma, Santhosh Malarvannan, Modigari Narendra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding emotions is a fundamental aspect of human communication.
Integrating audio and video signals offers a more comprehensive understanding
of emotional states compared to traditional methods that rely on a single data
source, such as speech or facial expressions. Despite its potential, multimodal
emotion recognition faces significant challenges, particularly in
synchronization, feature extraction, and fusion of diverse data sources. To
address these issues, this paper introduces a novel transformer-based model
named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA
model employs a transformer fusion approach to effectively capture and
synchronize interlinked features from both audio and video inputs, thereby
resolving synchronization problems. Additionally, the Cross Attention mechanism
within AVT-CA selectively extracts and emphasizes critical features while
discarding irrelevant ones from both modalities, addressing feature extraction
and fusion challenges. Extensive experimental analysis conducted on the
CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the
proposed model. The results underscore the importance of AVT-CA in developing
precise and reliable multimodal emotion recognition systems for practical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 Pages, 9 Tables, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularization by Neural Style Transfer for MRI Field-Transfer
  Reconstruction with Limited Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10968v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10968v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyao Shen, Yancheng Zhu, Mengyu Li, Ryan McNaughton, Hernan Jara, Sean B. Andersson, Chad W. Farris, Stephan Anderson, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in MRI reconstruction have demonstrated remarkable success
through deep learning-based models. However, most existing methods rely heavily
on large-scale, task-specific datasets, making reconstruction in data-limited
settings a critical yet underexplored challenge. While regularization by
denoising (RED) leverages denoisers as priors for reconstruction, we propose
Regularization by Neural Style Transfer (RNST), a novel framework that
integrates a neural style transfer (NST) engine with a denoiser to enable
magnetic field-transfer reconstruction. RNST generates high-field-quality
images from low-field inputs without requiring paired training data, leveraging
style priors to address limited-data settings. Our experiment results
demonstrate RNST's ability to reconstruct high-quality images across diverse
anatomical planes (axial, coronal, sagittal) and noise levels, achieving
superior clarity, contrast, and structural fidelity compared to lower-field
references. Crucially, RNST maintains robustness even when style and content
images lack exact alignment, broadening its applicability in clinical
environments where precise reference matches are unavailable. By combining the
strengths of NST and denoising, RNST offers a scalable, data-efficient solution
for MRI field-transfer reconstruction, demonstrating significant potential for
resource-limited settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 9 figures, 3 tables, 1 algorithm chart</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Sizhe Wei, Xiaoming Huo, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have made significant advancements in recent years. However,
their performance often deteriorates when trained or fine-tuned on imbalanced
datasets. This degradation is largely due to the disproportionate
representation of majority and minority data in image-text pairs. In this
paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address
this challenge. Rather than directly minimizing the KL divergence between the
predicted and ground-truth distributions, PoGDiff replaces the ground-truth
distribution with a Product of Gaussians (PoG), which is constructed by
combining the original ground-truth targets with the predicted distribution
conditioned on a neighboring text embedding. Experiments on real-world datasets
demonstrate that our method effectively addresses the imbalance problem in
diffusion models, improving both generation accuracy and quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Dataset</span> Distillation via Diffusion-Driven Patch Selection for
  Improved Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhao Zhong, Shuoyang Sun, Xulin Gu, Zhaoyang Xu, Yaowei Wang, Jianlong Wu, Bin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation offers an efficient way to reduce memory and
computational costs by optimizing a smaller dataset with performance comparable
to the full-scale original. However, for large datasets and complex deep
networks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space
limits performance, reducing its practicality. Recent approaches employ
pre-trained diffusion models to generate informative images directly, avoiding
pixel-level optimization and achieving notable results. However, these methods
often face challenges due to distribution shifts between pre-trained models and
target datasets, along with the need for multiple distillation steps across
varying settings. To address these issues, we propose a novel framework
orthogonal to existing diffusion-based distillation methods, leveraging
diffusion models for selection rather than generation. Our method starts by
predicting noise generated by the diffusion model based on input images and
text prompts (with or without label text), then calculates the corresponding
loss for each pair. With the loss differences, we identify distinctive regions
of the original images. Additionally, we perform intra-class clustering and
ranking on selected patches to maintain diversity constraints. This streamlined
framework enables a single-step distillation process, and extensive experiments
demonstrate that our approach outperforms state-of-the-art methods across
various metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are generative models fair? A study of racial bias in dermatological
  image generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel López-Pérez, Søren Hauberg, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Racial bias in medicine, such as in dermatology, presents significant ethical
and clinical challenges. This is likely to happen because there is a
significant underrepresentation of darker skin tones in training datasets for
machine learning models. While efforts to address bias in dermatology have
focused on improving dataset diversity and mitigating disparities in
discriminative models, the impact of racial bias on generative models remains
underexplored. Generative models, such as Variational Autoencoders (VAEs), are
increasingly used in healthcare applications, yet their fairness across diverse
skin tones is currently not well understood. In this study, we evaluate the
fairness of generative models in clinical dermatology with respect to racial
bias. For this purpose, we first train a VAE with a perceptual loss to generate
and reconstruct high-quality skin images across different skin tones. We
utilize the Fitzpatrick17k dataset to examine how racial bias influences the
representation and performance of these models. Our findings indicate that VAE
performance is, as expected, influenced by representation, i.e. increased skin
tone representation comes with increased performance on the given skin tone.
However, we also observe, even independently of representation, that the VAE
performs better for lighter skin tones. Additionally, the uncertainty estimates
produced by the VAE are ineffective in assessing the model's fairness. These
results highlight the need for more representative dermatological datasets, but
also a need for better understanding the sources of bias in such model, as well
as improved uncertainty quantification mechanisms to detect and address racial
bias in generative models for trustworthy healthcare technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffGuard: Text-Based Safety Checker for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massine El Khader, Elias Al Bouzidi, Abdellah Oumida, Mohammed Sbaihi, Eliott Binard, Jean-Philippe Poli, Wassila Ouerdane, Boussad Addad, Katarzyna Kapusta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Diffusion Models have enabled the generation of images
from text, with powerful closed-source models like DALL-E and Midjourney
leading the way. However, open-source alternatives, such as StabilityAI's
Stable Diffusion, offer comparable capabilities. These open-source models,
hosted on Hugging Face, come equipped with ethical filter protections designed
to prevent the generation of explicit images. This paper reveals first their
limitations and then presents a novel text-based safety filter that outperforms
existing solutions. Our research is driven by the critical need to address the
misuse of AI-generated content, especially in the context of information
warfare. DiffGuard enhances filtering efficacy, achieving a performance that
surpasses the best existing filters by over 14%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChineseSimpleVQA -- "See the World, Discover Knowledge": A Chinese
  Factuality Evaluation for Large Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of factual accuracy in large vision language models (LVLMs)
has lagged behind their rapid development, making it challenging to fully
reflect these models' knowledge capacity and reliability. In this paper, we
introduce the first factuality-based visual question-answering benchmark in
Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of
LVLMs across 8 major topics and 56 subtopics. The key features of this
benchmark include a focus on the Chinese language, diverse knowledge types, a
multi-hop question construction, high-quality data, static consistency, and
easy-to-evaluate through short answers. Moreover, we contribute a rigorous data
construction pipeline and decouple the visual factuality into two parts: seeing
the world (i.e., object recognition) and discovering knowledge. This decoupling
allows us to analyze the capability boundaries and execution mechanisms of
LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source
models, revealing critical performance gaps within this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Audio-Visual Adversarial Vulnerability from Temporal and
  Modality Perspectives <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeliang Zhang, Susan Liang, Daiki Shimada, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While audio-visual learning equips models with a richer understanding of the
real world by leveraging multiple sensory modalities, this integration also
introduces new vulnerabilities to adversarial attacks.
  In this paper, we present a comprehensive study of the adversarial robustness
of audio-visual models, considering both temporal and modality-specific
vulnerabilities. We propose two powerful adversarial attacks: 1) a temporal
invariance attack that exploits the inherent temporal redundancy across
consecutive time segments and 2) a modality misalignment attack that introduces
incongruence between the audio and visual modalities. These attacks are
designed to thoroughly assess the robustness of audio-visual models against
diverse threats. Furthermore, to defend against such attacks, we introduce a
novel audio-visual adversarial training framework. This framework addresses key
challenges in vanilla adversarial training by incorporating efficient
adversarial perturbation crafting tailored to multi-modal data and an
adversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds
dataset demonstrate that our proposed temporal and modality-based attacks in
degrading model performance can achieve state-of-the-art performance, while our
adversarial training defense largely improves the adversarial robustness as
well as the adversarial training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RSNet: A Light Framework for The Detection of Multi-scale Remote Sensing
  Targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23073v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23073v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Chen, Chengcheng Chen, Fei Wang, Yuhu Shi, Weiming Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in synthetic aperture radar (SAR) ship detection using
deep learning have significantly improved accuracy and speed, yet effectively
detecting small objects in complex backgrounds with fewer parameters remains a
challenge. This letter introduces RSNet, a lightweight framework constructed to
enhance ship detection in SAR imagery. To ensure accuracy with fewer
parameters, we proposed Waveletpool-ContextGuided (WCG) as its backbone,
guiding global context understanding through multi-scale wavelet features for
effective detection in complex scenes. Additionally, Waveletpool-StarFusion
(WSF) is introduced as the neck, employing a residual wavelet element-wise
multiplication structure to achieve higher dimensional nonlinear features
without increasing network width. The Lightweight-Shared (LS) module is
designed as detect components to achieve efficient detection through
lightweight shared convolutional structure and multi-format compatibility.
Experiments on the SAR Ship Detection Dataset (SSDD) and High-Resolution SAR
Image Dataset (HRSID) demonstrate that RSNet achieves a strong balance between
lightweight design and detection performance, surpassing many state-of-the-art
detectors, reaching 72.5\% and 67.6\% in \textbf{\(\mathbf{mAP_{.50:.95}}\)
}respectively with 1.49M parameters. Our code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fake News Video Explanation Generation: <span class="highlight-title">Dataset</span>, Model, and
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Chen, Zhong Qian, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing methods have addressed fake news video detection as a
classification problem, it is not clear why certain news content is identified
as fake. Without proper explanation, end users may not be able to understand
the potential meaning of fake news. Therefore, we propose a novel task, Fake
News Video Explanation (FNVE), to generate natural language explanations that
reveal the falseness of news videos. To this end, we first developed ONVE and
VTSE, two new datasets to explain fake news video posts. Then, we propose a
Multimodal Relation Graph Transformer (MRGT) model to benchmark ONVE and VTSE.
MRGT introduces a multimodal relation graph to comprehensively represent
multimodal relations and then introduces a BART-based decoder to explain
generations. The experimental results show that the proposed MRGT outperforms
the strong baselines. In addition, the human evaluation on the annotated ONVE
and VTSE also achieves high scores in terms of adequacy rating.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based
  Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Stathoulopoulos, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in robotics are driving real-world autonomy for long-term and
large-scale missions, where loop closures via place recognition are vital for
mitigating pose estimation drift. However, achieving real-time performance
remains challenging for resource-constrained mobile robots and multi-robot
systems due to the computational burden of high-density sampling, which
increases the complexity of comparing and verifying query samples against a
growing map database. Conventional methods often retain redundant information
or miss critical data by relying on fixed sampling intervals or operating in
3-D space instead of the descriptor feature space. To address these challenges,
we introduce the concept of sample space and propose a novel keyframe sampling
approach for LiDAR-based place recognition. Our method minimizes redundancy
while preserving essential information in the hyper-dimensional descriptor
space, supporting both learning-based and handcrafted descriptors. The proposed
approach incorporates a sliding window optimization strategy to ensure
efficient keyframe selection and real-time performance, enabling seamless
integration into robotic pipelines. In sum, our approach demonstrates robust
performance across diverse datasets, with the ability to adapt seamlessly from
indoor to outdoor scenarios without parameter tuning, reducing loop closure
detection times and memory requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 17 figures, 6 tables. Revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Freda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  pySLAM is an open-source Python framework for Visual SLAM, supporting
monocular, stereo, and RGB-D cameras. It provides a flexible interface for
integrating both classical and modern local features, making it adaptable to
various SLAM tasks. The framework includes different loop closure methods, a
volumetric reconstruction pipeline, and support for depth prediction models.
Additionally, it offers a suite of tools for visual odometry and SLAM
applications. Designed for both beginners and experienced researchers, pySLAM
encourages community contributions, fostering collaborative development in the
field of Visual SLAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal
  Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Bongratz, Jan Fecht, Anne-Marie Rickmann, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the cortex from longitudinal magnetic resonance imaging (MRI)
is indispensable for analyzing morphological alterations in the human brain.
Despite the recent advancement of cortical surface reconstruction with deep
learning, challenges arising from longitudinal data are still persistent.
Especially the lack of strong spatiotemporal point correspondence between
highly convoluted brain surfaces hinders downstream analyses, as local
morphology is not directly comparable if the anatomical location is not matched
precisely. To address this issue, we present V2C-Long, the first dedicated deep
learning-based cortex reconstruction method for longitudinal MRI. V2C-Long
exhibits strong inherent spatiotemporal correspondence across subjects and
visits, thereby reducing the need for surface-based post-processing. We
establish this correspondence directly during the reconstruction via the
composition of two deep template-deformation networks and innovative
aggregation of within-subject templates in mesh space. We validate V2C-Long on
two large neuroimaging studies, focusing on surface accuracy, consistency,
generalization, test-retest reliability, and sensitivity. The results reveal a
substantial improvement in longitudinal consistency and accuracy compared to
existing methods. In addition, we demonstrate stronger evidence for
longitudinal cortical atrophy in Alzheimer's disease than longitudinal
FreeSurfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Imaging Neuroscience</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Building Point Cloud Cleaning, Plane Detection and
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Abouelaziz, Youssef Mourchid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework to address the challenges involved in
building point cloud cleaning, plane detection, and semantic segmentation, with
the ultimate goal of enhancing building modeling. We focus in the cleaning
stage on removing outliers from the acquired point cloud data by employing an
adaptive threshold technique based on z-score measure. Following the cleaning
process, we perform plane detection using the robust RANSAC paradigm. The goal
is to carry out multiple plane segmentations, and to classify segments into
distinct categories, such as floors, ceilings, and walls. The resulting
segments can generate accurate and detailed point clouds representing the
building's architectural elements. Moreover, we address the problem of semantic
segmentation, which plays a vital role in the identification and classification
of different components within the building, such as walls, windows, doors,
roofs, and objects. Inspired by the PointNet architecture, we propose a deep
learning architecture for efficient semantic segmentation in buildings. The
results demonstrate the effectiveness of the proposed framework in handling
building modeling tasks, paving the way for improved accuracy and efficiency in
the field of building modelization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FuzzRisk: Online Collision Risk Estimation for Autonomous Vehicles based
  on Depth-Aware Object Detection via Fuzzy Inference <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Hsuan-Cheng Liao, Yingjie Xu, Chih-Hong Cheng, Hasan Esen, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel monitoring framework that infers the level of
collision risk for autonomous vehicles (AVs) based on their object detection
performance. The framework takes two sets of predictions from different
algorithms and associates their inconsistencies with the collision risk via
fuzzy inference. The first set of predictions is obtained by retrieving
safety-critical 2.5D objects from a depth map, and the second set comes from
the ordinary AV's 3D object detector. We experimentally validate that, based on
Intersection-over-Union (IoU) and a depth discrepancy measure, the
inconsistencies between the two sets of predictions strongly correlate to the
error of the 3D object detector against ground truths. This correlation allows
us to construct a fuzzy inference system and map the inconsistency measures to
an AV collision risk indicator. In particular, we optimize the fuzzy inference
system towards an existing offline metric that matches AV collision rates well.
Lastly, we validate our monitor's capability to produce relevant risk estimates
with the large-scale nuScenes dataset and demonstrate that it can safeguard an
AV in closed-loop simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025, 7 pages (IEEE double column format), 5
  figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Diffusion <span class="highlight-title">Transformer</span>s with Token-wise Feature Caching <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05317v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05317v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion transformers have shown significant effectiveness in both image and
video synthesis at the expense of huge computation costs. To address this
problem, feature caching methods have been introduced to accelerate diffusion
transformers by caching the features in previous timesteps and reusing them in
the following timesteps. However, previous caching methods ignore that
different tokens exhibit different sensitivities to feature caching, and
feature caching on some tokens may lead to 10$\times$ more destruction to the
overall generation quality compared with other tokens. In this paper, we
introduce token-wise feature caching, allowing us to adaptively select the most
suitable tokens for caching, and further enable us to apply different caching
ratios to neural layers in different types and depths. Extensive experiments on
PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image
and video generation with no requirements for training. For instance,
2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and
PixArt-$\alpha$ with almost no drop in generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ToCa is honored to be accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MonoForce: Learnable Image-conditioned Physics Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Agishev, Karel Zimmermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel model for the prediction of robot trajectories on rough
offroad terrain from the onboard camera images. This model enforces the laws of
classical mechanics through a physics-aware neural symbolic layer while
preserving the ability to learn from large-scale data as it is end-to-end
differentiable. The proposed hybrid model integrates a black-box component that
predicts robot-terrain interaction forces with a neural-symbolic layer. This
layer includes a differentiable physics engine that computes the robot's
trajectory by querying these forces at the points of contact with the terrain.
As the proposed architecture comprises substantial geometrical and physics
priors, the resulting model can also be seen as a learnable physics engine
conditioned on real images that delivers $10^4$ trajectories per second. We
argue and empirically demonstrate that this architecture reduces the
sim-to-real gap and mitigates out-of-distribution sensitivity. The
differentiability, in conjunction with the rapid simulation speed, makes the
model well-suited for various applications including model predictive control,
trajectory shooting, supervised and reinforcement learning or SLAM. The codes
and data are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ctu-vras/monoforce</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNION: Unsupervised 3D Object Detection using Object Appearance-based
  Pseudo-Classes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15688v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15688v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Lentsch, Holger Caesar, Dariu M. Gavrila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised 3D object detection methods have emerged to leverage vast
amounts of data without requiring manual labels for training. Recent approaches
rely on dynamic objects for learning to detect mobile objects but penalize the
detections of static instances during training. Multiple rounds of
self-training are used to add detected static instances to the set of training
targets; this procedure to improve performance is computationally expensive. To
address this, we propose the method UNION. We use spatial clustering and
self-supervised scene flow to obtain a set of static and dynamic object
proposals from LiDAR. Subsequently, object proposals' visual appearances are
encoded to distinguish static objects in the foreground and background by
selecting static instances that are visually similar to dynamic objects. As a
result, static and dynamic mobile objects are obtained together, and existing
detectors can be trained with a single training. In addition, we extend 3D
object discovery to detection by using object appearance-based cluster labels
as pseudo-class labels for training object classification. We conduct extensive
experiments on the nuScenes dataset and increase the state-of-the-art
performance for unsupervised 3D object discovery, i.e. UNION more than doubles
the average precision to 39.5. The code is available at
github.com/TedLentsch/UNION.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Robot 3D Vision-Language Model with Fast Rendering and
  <span class="highlight-title">Pre-Train</span>ing Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangcheng Liu, Yong-Jin Liu, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network models have achieved remarkable progress in 3D scene
understanding while trained in the closed-set setting and with full labels.
However, the major bottleneck is that these models do not have the capacity to
recognize any unseen novel classes beyond the training categories in diverse
real-world applications. Therefore, we are in urgent need of a framework that
can simultaneously be applicable to both 3D point cloud segmentation and
detection, particularly in the circumstances where the labels are rather
scarce. This work presents a generalized and straightforward framework for
dealing with 3D scene understanding when the labeled scenes are quite limited.
To extract knowledge for novel categories from the pre-trained vision-language
models, we propose a hierarchical feature-aligned pre-training and knowledge
distillation strategy to extract and distill meaningful information from
large-scale vision-language models, which helps benefit the open-vocabulary
scene understanding tasks. To encourage latent instance discrimination and to
guarantee efficiency, we propose the unsupervised region-level semantic
contrastive learning scheme for point clouds, using confident predictions of
the neural network to discriminate the intermediate feature embeddings at
multiple stages. In the limited reconstruction case, our proposed approach,
termed WS3D++, ranks 1st on the large-scale ScanNet benchmark on both the task
of semantic segmentation and instance segmentation. Extensive experiments with
both indoor and outdoor scenes demonstrated the effectiveness of our approach
in both data-efficient learning and open-world few-shot learning. The code is
made publicly available at:
https://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  Manuscript Info: 17 Pages, 13 Figures, and 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D
  Medical Image Segmentation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boqian Wu, Qiao Xiao, Shiwei Liu, Lu Yin, Mykola Pechenizkiy, Decebal Constantin Mocanu, Maurice Van Keulen, Elena Mocanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have evolved as the leading approach in 3D medical image
segmentation due to their outstanding performance. However, the ever-increasing
model size and computation cost of deep neural networks have become the primary
barrier to deploying them on real-world resource-limited hardware. In pursuit
of improving performance and efficiency, we propose a 3D medical image
segmentation model, named Efficient to Efficient Network (E2ENet),
incorporating two parametrically and computationally efficient designs. i.
Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse
informative multi-scale features while reducing redundancy. ii. Restricted
depth-shift in 3D convolution: it leverages the 3D spatial information while
keeping the model and computational complexity as 2D-based methods. We conduct
extensive experiments on BTCV, AMOS-CT and Brain Tumor Segmentation Challenge,
demonstrating that E2ENet consistently achieves a superior trade-off between
accuracy and efficiency than prior arts across various resource constraints.
E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while
saving over 68\% parameter count and 29\% FLOPs in the inference phase,
compared with the previous best-performing method. Our code has been made
available at: https://github.com/boqian333/E2ENet-Medical.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-View Graph Consistency Learning for Invariant Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Chen, Hua Mao, Wai Lok Woo, Chuanbin Liu, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning is fundamental for analyzing graph-structured
data. Exploring invariant graph representations remains a challenge for most
existing graph representation learning methods. In this paper, we propose a
cross-view graph consistency learning (CGCL) method that learns invariant graph
representations for link prediction. First, two complementary augmented views
are derived from an incomplete graph structure through a coupled graph
structure augmentation scheme. This augmentation scheme mitigates the potential
information loss that is commonly associated with various data augmentation
techniques involving raw graph data, such as edge perturbation, node removal,
and attribute masking. Second, we propose a CGCL model that can learn invariant
graph representations. A cross-view training scheme is proposed to train the
proposed CGCL model. This scheme attempts to maximize the consistency
information between one augmented view and the graph structure reconstructed
from the other augmented view. Furthermore, we offer a comprehensive
theoretical CGCL analysis. This paper empirically and experimentally
demonstrates the effectiveness of the proposed CGCL method, achieving
competitive results on graph datasets in comparisons with several
state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting Neurons in Deep Vision Networks with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Bai, Rahul A. Iyer, Tuomas Oikarinen, Akshay Kulkarni, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Describe-and-Dissect (DnD), a novel method to
describe the roles of hidden neurons in vision networks. DnD utilizes recent
advancements in multimodal deep learning to produce complex natural language
descriptions, without the need for labeled training data or a predefined set of
concepts to choose from. Additionally, DnD is training-free, meaning we don't
train any new models and can easily leverage more capable general purpose
models in the future. We have conducted extensive qualitative and quantitative
analysis to show that DnD outperforms prior work by providing higher quality
neuron descriptions. Specifically, our method on average provides the highest
quality labels and is more than 2$\times$ as likely to be selected as the best
explanation for a neuron than the best baseline. Finally, we present a use case
providing critical insights into land cover prediction models for
sustainability applications. Our code and data are available at
https://github.com/Trustworthy-ML-Lab/Describe-and-Dissect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PILOT: A <span class="highlight-title">Pre-Train</span>ed Model-Based Continual Learning Toolbox <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While traditional machine learning can effectively tackle a wide range of
problems, it primarily operates within a closed-world setting, which presents
limitations when dealing with streaming data. As a solution, incremental
learning emerges to address real-world scenarios involving new data's arrival.
Recently, pre-training has made significant advancements and garnered the
attention of numerous researchers. The strong performance of these pre-trained
models (PTMs) presents a promising avenue for developing continual learning
algorithms that can effectively adapt to real-world scenarios. Consequently,
exploring the utilization of PTMs in incremental learning has become essential.
This paper introduces a pre-trained model-based continual learning toolbox
known as PILOT. On the one hand, PILOT implements some state-of-the-art
class-incremental learning algorithms based on pre-trained models, such as L2P,
DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical
class-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within the
context of pre-trained models to evaluate their effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SCIENCE CHINA Information Sciences. Code is available at
  https://github.com/sun-hailong/LAMDA-PILOT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Hard and Soft Shadow Removal via Dual-Branch Separation Network
  and Vision <span class="highlight-title">Transformer</span> <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajia Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image shadow removal is a crucial task in computer vision. In real-world
scenes, shadows alter image color and brightness, posing challenges for
perception and texture recognition. Traditional and deep learning methods often
overlook the distinct needs for handling hard and soft shadows, thereby lacking
detailed processing to specifically address each type of shadow in images.We
propose a dual-path model that processes these shadows separately using
specially designed loss functions to accomplish the hard and soft shadow
removal. The model classifies shadow types and processes them through
appropriate paths to produce shadow-free outputs, integrating a Vision
Transformer with UNet++ for enhanced edge detail and feature fusion. Our model
outperforms state-of-the-art methods and achieves 2.905 RMSE value on the ISTD
dataset, which demonstrates greater effectiveness than typical single-path
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, IEEE International Conference on Machine
  Learning and Cybernetics (ICMLC) 2024; Currently under review at IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoising as Adaptation: Noise-Space Domain Adaptation for Image
  Restoration <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Liao, Zongsheng Yue, Zhouxia Wang, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although learning-based image restoration methods have made significant
progress, they still struggle with limited generalization to real-world
scenarios due to the substantial domain gap caused by training on synthetic
data. Existing methods address this issue by improving data synthesis
pipelines, estimating degradation kernels, employing deep internal learning,
and performing domain adaptation and regularization. Previous domain adaptation
methods have sought to bridge the domain gap by learning domain-invariant
knowledge in either feature or pixel space. However, these techniques often
struggle to extend to low-level vision tasks within a stable and compact
framework. In this paper, we show that it is possible to perform domain
adaptation via the noise space using diffusion models. In particular, by
leveraging the unique property of how auxiliary conditional inputs influence
the multi-step denoising process, we derive a meaningful diffusion loss that
guides the restoration model in progressively aligning both restored synthetic
and real-world outputs with a target clean distribution. We refer to this
method as denoising as adaptation. To prevent shortcuts during joint training,
we present crucial strategies such as channel-shuffling layer and
residual-swapping contrastive learning in the diffusion model. They implicitly
blur the boundaries between conditioned synthetic and real data and prevent the
reliance of the model on easily distinguishable features. Experimental results
on three classical image restoration tasks, namely denoising, deblurring, and
deraining, demonstrate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025. Project Page:
  https://kangliao929.github.io/projects/noise-da/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BFA: Best-Feature-Aware Fusion for Multi-View Fine-grained Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Lan, Weixin Mao, Haosheng Li, Le Wang, Tiancai Wang, Haoqiang Fan, Osamu Yoshie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, multi-view cameras are typically employed for
fine-grained manipulation tasks. Existing approaches (e.g., ACT) tend to treat
multi-view features equally and directly concatenate them for policy learning.
However, it will introduce redundant visual information and bring higher
computational costs, leading to ineffective manipulation. For a fine-grained
manipulation task, it tends to involve multiple stages while the most
contributed view for different stages is varied over time. In this paper, we
propose a plug-and-play best-feature-aware (BFA) fusion strategy for multi-view
manipulation tasks, which is adaptable to various policies. Built upon the
visual backbone of the policy network, we design a lightweight network to
predict the importance score of each view. Based on the predicted importance
scores, the reweighted multi-view features are subsequently fused and input
into the end-to-end policy network, enabling seamless integration. Notably, our
method demonstrates outstanding performance in fine-grained manipulations.
Experimental results show that our approach outperforms multiple baselines by
22-46% success rate on different tasks. Our work provides new insights and
inspiration for tackling key challenges in fine-grained manipulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utility Engineering: Analyzing and Controlling Emergent Value Systems in
  AIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AIs rapidly advance and become more agentic, the risk they pose is
governed not only by their capabilities but increasingly by their propensities,
including goals and values. Tracking the emergence of goals and values has
proven a longstanding problem, and despite much interest over the years it
remains unclear whether current AIs have meaningful values. We propose a
solution to this problem, leveraging the framework of utility functions to
study the internal coherence of AI preferences. Surprisingly, we find that
independently-sampled preferences in current LLMs exhibit high degrees of
structural coherence, and moreover that this emerges with scale. These findings
suggest that value systems emerge in LLMs in a meaningful sense, a finding with
broad implications. To study these emergent value systems, we propose utility
engineering as a research agenda, comprising both the analysis and control of
AI utilities. We uncover problematic and often shocking values in LLM
assistants despite existing control measures. These include cases where AIs
value themselves over humans and are anti-aligned with specific individuals. To
constrain these emergent value systems, we propose methods of utility control.
As a case study, we show how aligning utilities with a citizen assembly reduces
political biases and generalizes to new scenarios. Whether we like it or not,
value systems have already emerged in AIs, and much work remains to fully
understand and control these emergent representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.emergent-values.ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MVAM: Multi-View Attention Method for Fine-grained Image-Text Matching <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqing Cui, Rui Cheng, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing two-stream models, such as CLIP, encode images and text through
independent representations, showing good performance while ensuring retrieval
speed, have attracted attention from industry and academia. However, the single
representation often struggles to capture complex content fully. Such models
may ignore fine-grained information during matching, resulting in suboptimal
retrieval results. To overcome this limitation and enhance the performance of
two-stream models, we propose a Multi-view Attention Method (MVAM) for
image-text matching. This approach leverages diverse attention heads with
unique view codes to learn multiple representations for images and text, which
are then concatenated for matching. We also incorporate a diversity objective
to explicitly encourage attention heads to focus on distinct aspects of the
input data, capturing complementary fine-grained details. This diversity
enables the model to represent image-text pairs from multiple perspectives,
ensuring a more comprehensive understanding and alignment of critical content.
Our method allows models to encode images and text from different perspectives
and focus on more critical details, leading to better matching performance. Our
experiments on MSCOCO and Flickr30K demonstrate enhancements over existing
models, and further case studies reveal that different attention heads can
focus on distinct content, achieving more comprehensive representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Many Heads Are Better Than One: Improved Scientific Idea Generation by A
  LLM-Based Multi-Agent System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Su, Renqi Chen, Shixiang Tang, Zhenfei Yin, Xinzhe Zheng, Jinzhe Li, Biqing Qi, Qi Wu, Hui Li, Wanli Ouyang, Philip Torr, Bowen Zhou, Nanqing Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of scientific progress requires innovative tools that
can accelerate knowledge discovery. Although recent AI methods, particularly
large language models (LLMs), have shown promise in tasks such as hypothesis
generation and experimental design, they fall short of replicating the
collaborative nature of real-world scientific practices, where diverse experts
work together in teams to tackle complex problems. To address the limitations,
we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),
designed to mimic the teamwork inherent in scientific research. VirSci
organizes a team of agents to collaboratively generate, evaluate, and refine
research ideas. Through comprehensive experiments, we demonstrate that this
multi-agent approach outperforms the state-of-the-art method in producing novel
scientific ideas. We further investigate the collaboration mechanisms that
contribute to its tendency to produce ideas with higher novelty, offering
valuable insights to guide future research and illuminating pathways toward
building a robust system for autonomous scientific discovery. The code is
available at https://github.com/open-sciencelab/Virtual-Scientists.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Localized Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has been a celebrated method
for training vision encoders to generate image/text representations
facilitating various applications. Recently, CLIP has been widely adopted as
the vision backbone of multimodal large language models (MLLMs) to connect
image inputs for language interactions. The success of CLIP as a
vision-language foundation model relies on aligning web-crawled noisy text
annotations at image levels. Nevertheless, such criteria may become
insufficient for downstream tasks in need of fine-grained vision
representations, especially when region-level understanding is demanding for
MLLMs. In this paper, we improve the localization capability of CLIP with
several advances. We propose a pre-training method called Contrastive Localized
Language-Image Pre-training (CLOC) by complementing CLIP with region-text
contrastive loss and modules. We formulate a new concept, promptable
embeddings, of which the encoder produces image embeddings easy to transform
into region representations given spatial hints. To support large-scale
pre-training, we design a visually-enriched and spatially-localized captioning
framework to effectively generate region-text pseudo-labels at scale. By
scaling up to billions of annotated images, CLOC enables high-quality regional
embeddings for image region recognition and retrieval tasks, and can be a
drop-in replacement of CLIP to enhance MLLMs, especially on referring and
grounding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Hallucinations in Large Vision-Language Models via
  Summary-Guided Decoding <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13321v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13321v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Min, Minbeom Kim, Kang-il Lee, Dongryeol Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in
generating detailed and coherent responses from visual inputs. However, they
are prone to generate hallucinations due to an over-reliance on language
priors. To address this issue, we investigate the language priors in LVLMs and
make two key observations: (1) Even when predicting the tokens associated with
image-related part-of-speech (POS), models increasingly rely on linguistic
priors as the token sequences grow, thereby amplifying hallucinations. (2)
Methods that directly calibrate LVLM's output distribution to mitigate language
priors can lead to a degradation in text quality or even exacerbate
hallucinations. Based on these findings, we propose a novel method,
Summary-Guided Decoding (SumGD). This method naturally encourages the model to
focus more on image information by reducing the text context through summaries,
while controlling only the image-related POS tokens to maintain text quality.
Through experiments, we demonstrate that SumGD achieves state-of-the-art
performance on object hallucination benchmarks. Furthermore, in terms of the
trade-off between precision and recall, SumGD achieves Pareto optimality among
the existing methods. Lastly, we observe that although existing methods
struggle to balance the reduction of object hallucinations with maintaining
text quality, SumGD demonstrates robustness in handling this challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 (Findings); Renamed SGD to SumGD in Summary-Guided
  Decoding to prevent confusion with Stochastic Gradient Descent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE
  Solvers <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In applications of diffusion models, controllable generation is of practical
significance, but is also challenging. Current methods for controllable
generation primarily focus on modifying the score function of diffusion models,
while Mean Reverting (MR) Diffusion directly modifies the structure of the
stochastic differential equation (SDE), making the incorporation of image
conditions simpler and more natural. However, current training-free fast
samplers are not directly applicable to MR Diffusion. And thus MR Diffusion
requires hundreds of NFEs (number of function evaluations) to obtain
high-quality samples. In this paper, we propose a new algorithm named MRS (MR
Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time
SDE and the probability flow ordinary differential equation (PF-ODE) associated
with MR Diffusion, and derive semi-analytical solutions. The solutions consist
of an analytical function and an integral parameterized by a neural network.
Based on this solution, we can generate high-quality samples in fewer steps.
Our approach does not require training and supports all mainstream
parameterizations, including noise prediction, data prediction and velocity
prediction. Extensive experiments demonstrate that MR Sampler maintains high
sampling quality with a speedup of 10 to 20 times across ten different image
restoration tasks. Our algorithm accelerates the sampling procedure of MR
Diffusion, making it more practical in controllable generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMValuator: Similarity-based Data Valuation for Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10701v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10701v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Yang, Wenglong Deng, Benlin Liu, Yangsibo Huang, James Zou, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data valuation plays a crucial role in machine learning. Existing data
valuation methods have primarily focused on discriminative models, neglecting
generative models that have recently gained considerable attention. A very few
existing attempts of data valuation method designed for deep generative models
either concentrates on specific models or lacks robustness in their outcomes.
Moreover, efficiency still reveals vulnerable shortcomings. To bridge the gaps,
we formulate the data valuation problem in generative models from a
similarity-matching perspective. Specifically, we introduce Generative Model
Valuator (GMValuator), the first training-free and model-agnostic approach to
provide data valuation for generation tasks. It empowers efficient data
valuation through our innovatively similarity matching module, calibrates
biased contribution by incorporating image quality assessment, and attributes
credits to all training samples based on their contributions to the generated
samples. Additionally, we introduce four evaluation criteria for assessing data
valuation methods in generative models, aligning with principles of
plausibility and truthfulness. GMValuator is extensively evaluated on various
datasets and generative architectures to demonstrate its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiview Equivariance Improves 3D Correspondence Understanding with
  Minimal Feature Finetuning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang You, Yixin Li, Congyue Deng, Yue Wang, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models, particularly the ViT family, have revolutionized
image understanding by providing rich semantic features. However, despite their
success in 2D comprehension, their abilities on grasping 3D spatial
relationships are still unclear. In this work, we evaluate and enhance the 3D
awareness of ViT-based models. We begin by systematically assessing their
ability to learn 3D equivariant features, specifically examining the
consistency of semantic embeddings across different viewpoints. Our findings
indicate that improved 3D equivariance leads to better performance on various
downstream tasks, including pose estimation, tracking, and semantic transfer.
Building on this insight, we propose a simple yet effective finetuning strategy
based on 3D correspondences, which significantly enhances the 3D correspondence
understanding of existing vision models. Remarkably, finetuning on a single
object for one iteration results in substantial gains. Our code is available at
https://github.com/qq456cvb/3DCorrEnhance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ME-CPT: Multi-Task Enhanced Cross-Temporal Point <span class="highlight-title">Transformer</span> for Urban
  3D Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14004v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14004v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luqi Zhang, Haiping Wang, Chong Liu, Zhen Dong, Bisheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The point clouds collected by the Airborne Laser Scanning (ALS) system
provide accurate 3D information of urban land covers. By utilizing
multi-temporal ALS point clouds, semantic changes in urban area can be
captured, demonstrating significant potential in urban planning, emergency
management, and infrastructure maintenance. Existing 3D change detection
methods struggle to efficiently extract multi-class semantic information and
change features, still facing the following challenges: (1) the difficulty of
accurately modeling cross-temporal point clouds spatial relationships for
effective change feature extraction; (2) class imbalance of change samples
which hinders distinguishability of semantic features; (3) the lack of
real-world datasets for 3D semantic change detection. To resolve these
challenges, we propose the Multi-task Enhanced Cross-temporal Point Transformer
(ME-CPT) network. ME-CPT establishes spatiotemporal correspondences between
point cloud across different epochs and employs attention mechanisms to jointly
extract semantic change features, facilitating information exchange and change
comparison. Additionally, we incorporate a semantic segmentation task and
through the multi-task training strategy, further enhance the
distinguishability of semantic features, reducing the impact of class imbalance
in change types. Moreover, we release a 22.5 $km^2$ 3D semantic change
detection dataset, offering diverse scenes for comprehensive evaluation.
Experiments on multiple datasets show that the proposed MT-CPT achieves
superior performance compared to existing state-of-the-art methods. The source
code and dataset will be released upon acceptance at
https://github.com/zhangluqi0209/ME-CPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural
  Vision from Slow Brain Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, Huiguang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing human dynamic vision from brain activity is a challenging task
with great scientific significance. Although prior video reconstruction methods
have made substantial progress, they still suffer from several limitations,
including: (1) difficulty in simultaneously reconciling semantic (e.g.
categorical descriptions), structure (e.g. size and color), and consistent
motion information (e.g. order of frames); (2) low temporal resolution of fMRI,
which poses a challenge in decoding multiple frames of video dynamics from a
single fMRI frame; (3) reliance on video generation models, which introduces
ambiguity regarding whether the dynamics observed in the reconstructed videos
are genuinely derived from fMRI data or are hallucinations from generative
model. To overcome these limitations, we propose a two-stage model named
Mind-Animator. During the fMRI-to-feature stage, we decouple semantic,
structure, and motion features from fMRI. Specifically, we employ
fMRI-vision-language tri-modal contrastive learning to decode semantic feature
from fMRI and design a sparse causal attention mechanism for decoding
multi-frame video motion features through a next-frame-prediction task. In the
feature-to-video stage, these features are integrated into videos using an
inflated Stable Diffusion, effectively eliminating external video data
interference. Extensive experiments on multiple video-fMRI datasets demonstrate
that our model achieves state-of-the-art performance. Comprehensive
visualization analyses further elucidate the interpretability of our model from
a neurobiological perspective. Project page:
https://mind-animator-design.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolyhedronNet: Representation Learning for Polyhedra with
  Surface-attributed Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhou Yu, Genpei Zhang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ubiquitous geometric objects can be precisely and efficiently represented as
polyhedra. The transformation of a polyhedron into a vector, known as polyhedra
representation learning, is crucial for manipulating these shapes with
mathematical and statistical tools for tasks like classification, clustering,
and generation. Recent years have witnessed significant strides in this domain,
yet most efforts focus on the vertex sequence of a polyhedron, neglecting the
complex surface modeling crucial in real-world polyhedral objects. This study
proposes \textbf{PolyhedronNet}, a general framework tailored for learning
representations of 3D polyhedral objects. We propose the concept of the
surface-attributed graph to seamlessly model the vertices, edges, faces, and
their geometric interrelationships within a polyhedron. To effectively learn
the representation of the entire surface-attributed graph, we first propose to
break it down into local rigid representations to effectively learn each local
region's relative positions against the remaining regions without geometric
information loss. Subsequently, we propose PolyhedronGNN to hierarchically
aggregate the local rigid representation via intra-face and inter-face
geometric message passing modules, to obtain a global representation that
minimizes information loss while maintaining rotation and translation
invariance. Our experimental evaluations on four distinct datasets,
encompassing both classification and retrieval tasks, substantiate
PolyhedronNet's efficacy in capturing comprehensive and informative
representations of 3D polyhedral objects. Code and data are available at
{https://github.com/dyu62/3D_polyhedron}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedIAnomaly: A comparative study of anomaly detection in medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04518v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04518v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Cai, Weiwen Zhang, Hao Chen, Kwang-Ting Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) aims at detecting abnormal samples that deviate from
the expected normal patterns. Generally, it can be trained merely on normal
data, without a requirement for abnormal samples, and thereby plays an
important role in rare disease recognition and health screening in the medical
domain. Despite the emergence of numerous methods for medical AD, the lack of a
fair and comprehensive evaluation causes ambiguous conclusions and hinders the
development of this field. To address this problem, this paper builds a
benchmark with unified comparison. Seven medical datasets with five image
modalities, including chest X-rays, brain MRIs, retinal fundus images,
dermatoscopic images, and histopathology images, are curated for extensive
evaluation. Thirty typical AD methods, including reconstruction and
self-supervised learning-based methods, are involved in comparison of
image-level anomaly classification and pixel-level anomaly segmentation.
Furthermore, for the first time, we systematically investigate the effect of
key components in existing methods, revealing unresolved challenges and
potential future directions. The datasets and code are available at
https://github.com/caiyu6666/MedIAnomaly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Medical Image Analysis, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression under ultra-low bitrates remains challenging for both
conventional learned image compression (LIC) and generative vector-quantized
(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy
quantization, while generative VQ modeling gives poor fidelity due to the
mismatch between learned generative priors and specific inputs. In this work,
we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream
framework that utilizes both generative VQ-modeling and diffusion models, as
well as conventional LIC, to achieve both high fidelity and high perceptual
quality. Different from previous hybrid methods that directly use pre-trained
LIC models to generate low-quality fidelity-preserving information from heavily
quantized latent, we use diffusion models to extract high-quality complimentary
fidelity information from the ground-truth input, which can enhance the system
performance in several aspects: improving indices map prediction, enhancing the
fidelity-preserving output of the LIC stream, and refining conditioned image
reconstruction with VQ-latent correction. In addition, our diffusion model is
based on a dense representative vector (DRV), which is lightweight with very
simple sampling schedulers. Extensive experiments demonstrate that our
HDCompression outperforms the previous conventional LIC, generative
VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative
visualization, providing balanced robust compression performance at ultra-low
bitrates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Explicit Representation for Ultra-Realistic Head Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Cai, Yuting Xiao, Xuan Wang, Jiafei Li, Yudong Guo, Yanbo Fan, Shenghua Gao, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach to creating ultra-realistic head avatars and
rendering them in real-time (>30fps at $2048 \times 1334$ resolution). First,
we propose a hybrid explicit representation that combines the advantages of two
primitive-based efficient rendering techniques. UV-mapped 3D mesh is utilized
to capture sharp and rich textures on smooth surfaces, while 3D Gaussian
Splatting is employed to represent complex geometric structures. In the
pipeline of modeling an avatar, after tracking parametric models based on
captured multi-view RGB videos, our goal is to simultaneously optimize the
texture and opacity map of mesh, as well as a set of 3D Gaussian splats
localized and rigged onto the mesh facets. Specifically, we perform
$\alpha$-blending on the color and opacity values based on the merged and
re-ordered z-buffer from the rasterization results of mesh and 3DGS. This
process involves the mesh and 3DGS adaptively fitting the captured visual
information to outline a high-fidelity digital avatar. To avoid artifacts
caused by Gaussian splats crossing the mesh facets, we design a stable hybrid
depth sorting strategy. Experiments illustrate that our modeled results exceed
those of state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Unlearning for Image-to-Image Generative Models via
  $\varepsilon$-Constrained Optimization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohua Feng, Yuyuan Li, Chaochao Chen, Li Zhang, Longfei Li, Jun Zhou, Xiaolin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative models have made significant advancements in recent years,
they also raise concerns such as privacy breaches and biases. Machine
unlearning has emerged as a viable solution, aiming to remove specific training
data, e.g., containing private information and bias, from models. In this
paper, we study the machine unlearning problem in Image-to-Image (I2I)
generative models. Previous studies mainly treat it as a single objective
optimization problem, offering a solitary solution, thereby neglecting the
varied user expectations towards the trade-off between complete unlearning and
model utility. To address this issue, we propose a controllable unlearning
framework that uses a control coefficient $\varepsilon$ to control the
trade-off. We reformulate the I2I generative model unlearning problem into a
$\varepsilon$-constrained optimization problem and solve it with a
gradient-based method to find optimal solutions for unlearning boundaries.
These boundaries define the valid range for the control coefficient. Within
this range, every yielded solution is theoretically guaranteed with Pareto
optimality. We also analyze the convergence rate of our framework under various
control functions. Extensive experiments on two benchmark datasets across three
mainstream I2I models demonstrate the effectiveness of our controllable
unlearning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizable Humanoid Manipulation with 3D Diffusion Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Ze, Zixuan Chen, Wenhao Wang, Tianyi Chen, Xialin He, Ying Yuan, Xue Bin Peng, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots capable of autonomous operation in diverse environments have
long been a goal for roboticists. However, autonomous manipulation by humanoid
robots has largely been restricted to one specific scene, primarily due to the
difficulty of acquiring generalizable skills and the expensiveness of
in-the-wild humanoid robot data. In this work, we build a real-world robotic
system to address this challenging problem. Our system is mainly an integration
of 1) a whole-upper-body robotic teleoperation system to acquire human-like
robot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart
and a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning
algorithm for humanoid robots to learn from noisy human data. We run more than
2000 episodes of policy rollouts on the real robot for rigorous policy
evaluation. Empowered by this system, we show that using only data collected in
one single scene and with only onboard computing, a full-sized humanoid robot
can autonomously perform skills in diverse real-world scenarios. Videos are
available at
\href{https://humanoid-manipulation.github.io}{humanoid-manipulation.github.io}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://humanoid-manipulation.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMITE: Segment Me In TimE <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Alimohammadi, Sauradip Nag, Saeid Asgari Taghanaki, Andrea Tagliasacchi, Ghassan Hamarneh, Ali Mahdavi Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting an object in a video presents significant challenges. Each pixel
must be accurately labelled, and these labels must remain consistent across
frames. The difficulty increases when the segmentation is with arbitrary
granularity, meaning the number of segments can vary arbitrarily, and masks are
defined based on only one or a few sample images. In this paper, we address
this issue by employing a pre-trained text to image diffusion model
supplemented with an additional tracking mechanism. We demonstrate that our
approach can effectively manage various segmentation scenarios and outperforms
state-of-the-art alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; Project page is at https://segment-me-in-time.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Template-Based Visual Program Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For users with limited computational resources, visual programming or
prompting large language models (LLMs) to generate executable code for visual
tasks, like visual question answering (VQA), remains largely inaccessible. Even
with techniques such as distillation, adapting visual programming to smaller
models or specific datasets is still quite challenging due to high annotation
costs. We propose a low-cost visual program distillation method that can be
used for models with fewer than 1 billion parameters and requires no
human-generated program annotations. We achieve this through synthetic data
augmentation based on decoupling programs into higher-level skills, called
templates, and their corresponding arguments. Experimental results show that,
with a relatively small amount of question/answer data, small language models
can generate high-quality visual programs with the added benefit of much faster
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hands-on STEM Learning Experiences using Digital Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaia Fior, Carlo Fonda, Enrique Canessa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The facilitation of STEM education can be enhanced by the provision of
opportunities for learners to gain a better understanding of science through
the utilization of tangible and visual examples. The objective of this work is
to present an account of our experiences and activities carried out in Italian
schools with this novel approach. The selection of projects and experiences
discussed --in which students develop a range of core competencies such as
collaboration, creativity, critical thinking, experimentation, prototyping,
communication and problem-solving; include tangible complex 3D printed
structures, large micro-controller board replicas and the visualization of wind
dynamics and tiny invisible elementary particles among others. These hands-on
experiences demonstrate the benefits on the use of digital fabrication
technologies implemented within a FabLab for STEM learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear STEM Education Journal (2025) 9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Ran He, Caifeng Shan, Rongrong Ji, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Long-VITA, a simple yet effective large multi-modal model for
long-context visual-language understanding tasks. It is adept at concurrently
processing and analyzing modalities of image, video, and text over 4K frames or
1M tokens while delivering advanced performances on short-context multi-modal
tasks. We propose an effective multi-modal training schema that starts with
large language models and proceeds through vision-language alignment, general
knowledge learning, and two sequential stages of long-sequence fine-tuning. We
further implement context-parallelism distributed inference and logits-masked
language modeling head to scale Long-VITA to infinitely long inputs of images
and texts during model inference. Regarding training data, Long-VITA is built
on a mix of 17M samples from public datasets only and demonstrates the
state-of-the-art performance on various multi-modal benchmarks, compared
against recent cutting-edge models with internal data. Long-VITA is fully
reproducible and supports both NPU and GPU platforms for training and testing.
By leveraging our inference designs, Long-VITA models achieve a remarkable 2x
prefill speedup and 4x context length extension in single node with 8 GPUs. We
hope Long-VITA can serve as a competitive baseline and offer valuable insights
for the open-source community in advancing long-context multi-modal
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/VITA-MLLM/Long-VITA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain age identification from diffusion MRI synergistically predicts
  neurodegenerative disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Gao, Michael E. Kim, Karthik Ramadass, Praitayini Kanakaraj, Aravind R. Krishnan, Adam M. Saunders, Nancy R. Newlin, Ho Hin Lee, Qi Yang, Warren D. Taylor, Brian D. Boyd, Lori L. Beason-Held, Susan M. Resnick, Lisa L. Barnes, David A. Bennett, Katherine D. Van Schaik, Derek B. Archer, Timothy J. Hohman, Angela L. Jefferson, Ivana Išgum, Daniel Moyer, Yuankai Huo, Kurt G. Schilling, Lianrui Zuo, Shunxing Bao, Nazirah Mohd Khairi, Zhiyuan Li, Christos Davatzikos, Bennett A. Landman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimated brain age from magnetic resonance image (MRI) and its deviation
from chronological age can provide early insights into potential
neurodegenerative diseases, supporting early detection and implementation of
prevention strategies. Diffusion MRI (dMRI) presents an opportunity to build an
earlier biomarker for neurodegenerative disease prediction because it captures
subtle microstructural changes that precede more perceptible macrostructural
changes. However, the coexistence of macro- and micro-structural information in
dMRI raises the question of whether current dMRI-based brain age estimation
models are leveraging the intended microstructural information or if they
inadvertently rely on the macrostructural information. To develop a
microstructure-specific brain age, we propose a method for brain age
identification from dMRI that mitigates the model's use of macrostructural
information by non-rigidly registering all images to a standard template.
Imaging data from 13,398 participants across 12 datasets were used for the
training and evaluation. We compare our brain age models, trained with and
without macrostructural information mitigated, with an architecturally similar
T1-weighted (T1w) MRI-based brain age model and two recent, popular, openly
available T1w MRI-based brain age models that primarily use macrostructural
information. We observe difference between our dMRI-based brain age and T1w
MRI-based brain age across stages of neurodegeneration, with dMRI-based brain
age being older than T1w MRI-based brain age in participants transitioning from
cognitively normal (CN) to mild cognitive impairment (MCI), but younger in
participants already diagnosed with Alzheimer's disease (AD). Furthermore,
dMRI-based brain age may offer advantages over T1w MRI-based brain age in
predicting the transition from CN to MCI up to five years before diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOVA3: Learning to Visual Question Answering, Asking and Assessment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Hengyuan Zhao, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering, asking, and assessment are three innate human traits
crucial for understanding the world and acquiring knowledge. By enhancing these
capabilities, humans can more effectively utilize data, leading to better
comprehension and learning outcomes. Current Multimodal Large Language Models
(MLLMs) primarily focus on question answering, often neglecting the full
potential of questioning and assessment skills. Inspired by the human learning
mechanism, we introduce LOVA3, an innovative framework named "Learning tO
Visual question Answering, Asking and Assessment," designed to equip MLLMs with
these additional capabilities. Our approach involves the creation of two
supplementary training tasks GenQA and EvalQA, aiming at fostering the skills
of asking and assessing questions in the context of images. To develop the
questioning ability, we compile a comprehensive set of multimodal foundational
tasks. For assessment, we introduce a new benchmark called EvalQABench,
comprising 64,000 training samples (split evenly between positive and negative
samples) and 5,000 validation and testing samples. We posit that enhancing
MLLMs with the capabilities to answer, ask, and assess questions will enhance
their multimodal comprehension, ultimately improving overall performance. To
validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate
them on a range of multimodal datasets and benchmarks. Our results demonstrate
consistent performance gains, underscoring the critical role of these
additional tasks in fostering comprehensive intelligence in MLLMs. The code is
available at https://github.com/showlab/LOVA3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. The code is available at
  https://github.com/showlab/LOVA3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoRRECT: A Deep Unfolding Framework for Motion-Corrected Quantitative
  R2* Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojian Xu, Weijie Gan, Satya V. V. N. Kothapalli, Dmitriy A. Yablonskiy, Ulugbek S. Kamilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative MRI (qMRI) refers to a class of MRI methods for quantifying the
spatial distribution of biological tissue parameters. Traditional qMRI methods
usually deal separately with artifacts arising from accelerated data
acquisition, involuntary physical motion, and magnetic-field inhomogeneities,
leading to suboptimal end-to-end performance. This paper presents CoRRECT, a
unified deep unfolding (DU) framework for qMRI consisting of a model-based
end-to-end neural network, a method for motion-artifact reduction, and a
self-supervised learning scheme. The network is trained to produce R2* maps
whose k-space data matches the real data by also accounting for motion and
field inhomogeneities. When deployed, CoRRECT only uses the k-space data
without any pre-computed parameters for motion or inhomogeneity correction. Our
results on experimentally collected multi-Gradient-Recalled Echo (mGRE) MRI
data show that CoRRECT recovers motion and inhomogeneity artifact-free R2* maps
in highly accelerated acquisition settings. This work opens the door to DU
methods that can integrate physical measurement models, biophysical signal
models, and learned prior models for high-quality qMRI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Concept Erasure Using Task Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Pham, Kelly O. Marshall, Chinmay Hegde, Niv Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of text-to-image models, a variety of techniques have
been suggested to prevent undesirable image generations. Yet, these methods
often only protect against specific user prompts and have been shown to allow
unsafe generations with other inputs. Here we focus on unconditionally erasing
a concept from a text-to-image model rather than conditioning the erasure on
the user's prompt. We first show that compared to input-dependent erasure
methods, concept erasure that uses Task Vectors (TV) is more robust to
unexpected user inputs, not seen during training. However, TV-based erasure can
also affect the core performance of the edited model, particularly when the
required edit strength is unknown. To this end, we propose a method called
Diverse Inversion, which we use to estimate the required strength of the TV
edit. Diverse Inversion finds within the model input space a large set of word
embeddings, each of which induces the generation of the target concept. We find
that encouraging diversity in the set makes our estimation more robust to
unexpected prompts. Finally, we show that Diverse Inversion enables us to apply
a TV edit only to a subset of the model weights, enhancing the erasure
capabilities while better maintaining the core functionality of the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ View-Invariant Policy Learning via Zero-Shot Novel View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Tian, Blake Wulfe, Kyle Sargent, Katherine Liu, Sergey Zakharov, Vitor Guizilini, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale visuomotor policy learning is a promising approach toward
developing generalizable manipulation systems. Yet, policies that can be
deployed on diverse embodiments, environments, and observational modalities
remain elusive. In this work, we investigate how knowledge from large-scale
visual data of the world may be used to address one axis of variation for
generalizable manipulation: observational viewpoint. Specifically, we study
single-image novel view synthesis models, which learn 3D-aware scene-level
priors by rendering images of the same scene from alternate camera viewpoints
given a single input image. For practical application to diverse robotic data,
these models must operate zero-shot, performing view synthesis on unseen tasks
and environments. We empirically analyze view synthesis models within a simple
data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to
understand their capabilities for learning viewpoint-invariant policies from
single-viewpoint demonstration data. Upon evaluating the robustness of policies
trained with our method to out-of-distribution camera viewpoints, we find that
they outperform baselines in both simulated and real-world manipulation tasks.
Videos and additional visualizations are available at
https://s-tian.github.io/projects/vista.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinpei Guo, Zheng Chen, Wenbo Li, Yong Guo, Yulun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable success in image restoration
tasks. However, their multi-step denoising process introduces significant
computational overhead, limiting their practical deployment. Furthermore,
existing methods struggle to effectively remove severe JPEG artifact,
especially in highly compressed images. To address these challenges, we propose
CODiff, a compression-aware one-step diffusion model for JPEG artifact removal.
The core of CODiff is the compression-aware visual embedder (CaVE), which
extracts and leverages JPEG compression priors to guide the diffusion model. We
propose a dual learning strategy that combines explicit and implicit learning.
Specifically, explicit learning enforces a quality prediction objective to
differentiate low-quality images with different compression levels. Implicit
learning employs a reconstruction objective that enhances the model's
generalization. This dual learning allows for a deeper and more comprehensive
understanding of JPEG compression. Experimental results demonstrate that CODiff
surpasses recent leading methods in both quantitative and visual quality
metrics. The code and models will be released at
https://github.com/jp-guo/CODiff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VaLID: Verification as Late Integration of Detections for LiDAR-Camera
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanshika Vats, Marzia Binta Nizam, James Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle object detection benefits from both LiDAR and camera data, with LiDAR
offering superior performance in many scenarios. Fusion of these modalities
further enhances accuracy, but existing methods often introduce complexity or
dataset-specific dependencies. In our study, we propose a model-adaptive
late-fusion method, VaLID, which validates whether each predicted bounding box
is acceptable or not. Our method verifies the higher-performing, yet overly
optimistic LiDAR model detections using camera detections that are obtained
from either specially trained, general, or open-vocabulary models. VaLID uses a
lightweight neural verification network trained with a high recall bias to
reduce the false predictions made by the LiDAR detector, while still preserving
the true ones. Evaluating with multiple combinations of LiDAR and camera
detectors on the KITTI dataset, we reduce false positives by an average of
63.9%, thus outperforming the individual detectors on 3D average precision
(3DAP). Our approach is model-adaptive and demonstrates state-of-the-art
competitive performance even when using generic camera detectors that were not
trained specifically for this dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLARE: Feed-forward Geometry, Appearance and Camera Estimation from
  Uncalibrated Sparse Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FLARE, a feed-forward model designed to infer high-quality camera
poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8
inputs), which is a challenging yet practical setting in real-world
applications. Our solution features a cascaded learning paradigm with camera
pose serving as the critical bridge, recognizing its essential role in mapping
3D structures onto 2D image planes. Concretely, FLARE starts with camera pose
estimation, whose results condition the subsequent learning of geometric
structure and appearance, optimized through the objectives of geometry
reconstruction and novel-view synthesis. Utilizing large-scale public datasets
for training, our method delivers state-of-the-art performance in the tasks of
pose estimation, geometry reconstruction, and novel view synthesis, while
maintaining the inference efficiency (i.e., less than 0.5 seconds). The project
page and code can be found at: https://zhanghe3z.github.io/FLARE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Website: https://zhanghe3z.github.io/FLARE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional diffusion model with spatial attention and latent embedding
  for medical image segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behzad Hejrati, Soumyanil Banerjee, Carri Glide-Hurst, Ming Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been used extensively for high quality image and video
generation tasks. In this paper, we propose a novel conditional diffusion model
with spatial attention and latent embedding (cDAL) for medical image
segmentation. In cDAL, a convolutional neural network (CNN) based discriminator
is used at every time-step of the diffusion process to distinguish between the
generated labels and the real ones. A spatial attention map is computed based
on the features learned by the discriminator to help cDAL generate more
accurate segmentation of discriminative regions in an input image.
Additionally, we incorporated a random latent embedding into each layer of our
model to significantly reduce the number of training and sampling time-steps,
thereby making it much faster than other diffusion models for image
segmentation. We applied cDAL on 3 publicly available medical image
segmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed
significant qualitative and quantitative improvements with higher Dice scores
and mIoU over the state-of-the-art algorithms. The source code is publicly
available at https://github.com/Hejrati/cDAL/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 3 tables, Accepted in MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Sensor Gaps via Attention Gated Tuning for Hyperspectral Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12865v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12865v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizhe Xue, Haokui Zhang, Haizhao Jing, Lijie Tao, Zongwen Bai, Ying Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-hungry HSI classification methods require high-quality labeled HSIs,
which are often costly to obtain. This characteristic limits the performance
potential of data-driven methods when dealing with limited annotated samples.
Bridging the domain gap between data acquired from different sensors allows us
to utilize abundant labeled data across sensors to break this bottleneck. In
this paper, we propose a novel Attention-Gated Tuning (AGT) strategy and a
triplet-structured transformer model, Tri-Former, to address this issue. The
AGT strategy serves as a bridge, allowing us to leverage existing labeled HSI
datasets, even RGB datasets to enhance the performance on new HSI datasets with
limited samples. Instead of inserting additional parameters inside the basic
model, we train a lightweight auxiliary branch that takes intermediate features
as input from the basic model and makes predictions. The proposed AGT resolves
conflicts between heterogeneous and even cross-modal data by suppressing the
disturbing information and enhances the useful information through a soft gate.
Additionally, we introduce Tri-Former, a triplet-structured transformer with a
spectral-spatial separation design that enhances parameter utilization and
computational efficiency, enabling easier and flexible fine-tuning. Comparison
experiments conducted on three representative HSI datasets captured by
different sensors demonstrate the proposed Tri-Former achieves better
performance compared to several state-of-the-art methods. Homologous,
heterologous and cross-modal tuning experiments verified the effectiveness of
the proposed AGT. Code has been released at:
\href{https://github.com/Cecilia-xue/AGT}{https://github.com/Cecilia-xue/AGT}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Mutual Cross-Modal Attention for Context-Aware Human
  Affordance Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human affordance learning investigates contextually relevant novel pose
prediction such that the estimated pose represents a valid human action within
the scene. While the task is fundamental to machine perception and automated
interactive navigation agents, the exponentially large number of probable pose
and action variations make the problem challenging and non-trivial. However,
the existing datasets and methods for human affordance prediction in 2D scenes
are significantly limited in the literature. In this paper, we propose a novel
cross-attention mechanism to encode the scene context for affordance prediction
by mutually attending spatial feature maps from two different modalities. The
proposed method is disentangled among individual subtasks to efficiently reduce
the problem complexity. First, we sample a probable location for a person
within the scene using a variational autoencoder (VAE) conditioned on the
global scene context encoding. Next, we predict a potential pose template from
a set of existing human pose candidates using a classifier on the local context
encoding around the predicted location. In the subsequent steps, we use two
VAEs to sample the scale and deformation parameters for the predicted pose
template by conditioning on the local context and template class. Our
experiments show significant improvements over the previous baseline of human
affordance injection into complex 2D scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated Sensing and Communication for 6G Holographic Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haijun Zhang, Ziyang Zhang, Xiangnan Liu, Wei Li, Haojin Li, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of 6G networks, offering ultra-high bandwidth and ultra-low
latency, coupled with the enhancement of terminal device resolutions,
holographic communication is gradually becoming a reality. Holographic digital
twin (HDT) is considered one of key applications of holographic communication,
capable of creating virtual replicas for real-time mapping and prediction of
physical entity states, and performing three-dimensional reproduction of
spatial information. In this context, integrated sensing and communication
(ISAC) is expected to be a crucial pathway for providing data sources to HDT.
This paper proposes a four-layer architecture assisted by ISAC for HDT,
integrating emerging paradigms and key technologies to achieve low-cost,
high-precision environmental data collection for constructing HDT.
Specifically, to enhance sensing resolution, we explore super-resolution
techniques from the perspectives of parameter estimation and point cloud
construction. Additionally, we focus on multi-point collaborative sensing for
constructing HDT, and provide a comprehensive review of four key techniques:
node selection, multi-band collaboration, cooperative beamforming, and data
fusion. Finally, we highlight several interesting research directions to guide
and inspire future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ d-Sketch: Improving Visual Fidelity of Sketch-to-Image Translation with
  <span class="highlight-title">Pretrain</span>ed Latent Diffusion Models without Retraining <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural guidance in an image-to-image translation allows intricate control
over the shapes of synthesized images. Generating high-quality realistic images
from user-specified rough hand-drawn sketches is one such task that aims to
impose a structural constraint on the conditional generation process. While the
premise is intriguing for numerous use cases of content creation and academic
research, the problem becomes fundamentally challenging due to substantial
ambiguities in freehand sketches. Furthermore, balancing the trade-off between
shape consistency and realistic generation contributes to additional complexity
in the process. Existing approaches based on Generative Adversarial Networks
(GANs) generally utilize conditional GANs or GAN inversions, often requiring
application-specific data and optimization objectives. The recent introduction
of Denoising Diffusion Probabilistic Models (DDPMs) achieves a generational
leap for low-level visual attributes in general image synthesis. However,
directly retraining a large-scale diffusion model on a domain-specific subtask
is often extremely difficult due to demanding computation costs and
insufficient data. In this paper, we introduce a technique for sketch-to-image
translation by exploiting the feature generalization capabilities of a
large-scale diffusion model without retraining. In particular, we use a
learnable lightweight mapping network to achieve latent feature translation
from source to target domain. Experimental results demonstrate that the
proposed method outperforms the existing techniques in qualitative and
quantitative benchmarks, allowing high-resolution realistic image synthesis
from rough hand-drawn sketches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression under ultra-low bitrates remains challenging for both
conventional learned image compression (LIC) and generative vector-quantized
(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy
quantization, while generative VQ modeling gives poor fidelity due to the
mismatch between learned generative priors and specific inputs. In this work,
we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream
framework that utilizes both generative VQ-modeling and diffusion models, as
well as conventional LIC, to achieve both high fidelity and high perceptual
quality. Different from previous hybrid methods that directly use pre-trained
LIC models to generate low-quality fidelity-preserving information from heavily
quantized latent, we use diffusion models to extract high-quality complimentary
fidelity information from the ground-truth input, which can enhance the system
performance in several aspects: improving indices map prediction, enhancing the
fidelity-preserving output of the LIC stream, and refining conditioned image
reconstruction with VQ-latent correction. In addition, our diffusion model is
based on a dense representative vector (DRV), which is lightweight with very
simple sampling schedulers. Extensive experiments demonstrate that our
HDCompression outperforms the previous conventional LIC, generative
VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative
visualization, providing balanced robust compression performance at ultra-low
bitrates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Code to Canvas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard O. Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web-based dynamic geometry software CindyJS is a versatile tool to create
interactive applications for mathematics and other topics. In this workshop, we
will look at a code package that makes the creation of animations in CindyJS
easier and more streamlined. Animations, which can then be embedded into
presentations or be used in (lecture) videos. The focus lies on the creation of
the animations themselves and some of the technical and artistic fundamentals
to do so.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A workshop paper for the Bridges 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fake News Video Explanation Generation: <span class="highlight-title">Dataset</span>, Model, and
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Chen, Zhong Qian, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing methods have addressed fake news video detection as a
classification problem, it is not clear why certain news content is identified
as fake. Without proper explanation, end users may not be able to understand
the potential meaning of fake news. Therefore, we propose a novel task, Fake
News Video Explanation (FNVE), to generate natural language explanations that
reveal the falseness of news videos. To this end, we first developed ONVE and
VTSE, two new datasets to explain fake news video posts. Then, we propose a
Multimodal Relation Graph Transformer (MRGT) model to benchmark ONVE and VTSE.
MRGT introduces a multimodal relation graph to comprehensively represent
multimodal relations and then introduces a BART-based decoder to explain
generations. The experimental results show that the proposed MRGT outperforms
the strong baselines. In addition, the human evaluation on the annotated ONVE
and VTSE also achieves high scores in terms of adequacy rating.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Emotion Recognition using Audio-Video <span class="highlight-title">Transformer</span> Fusion with
  Cross Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Dhanith P R, Shravan Venkatraman, Vigya Sharma, Santhosh Malarvannan, Modigari Narendra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding emotions is a fundamental aspect of human communication.
Integrating audio and video signals offers a more comprehensive understanding
of emotional states compared to traditional methods that rely on a single data
source, such as speech or facial expressions. Despite its potential, multimodal
emotion recognition faces significant challenges, particularly in
synchronization, feature extraction, and fusion of diverse data sources. To
address these issues, this paper introduces a novel transformer-based model
named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA
model employs a transformer fusion approach to effectively capture and
synchronize interlinked features from both audio and video inputs, thereby
resolving synchronization problems. Additionally, the Cross Attention mechanism
within AVT-CA selectively extracts and emphasizes critical features while
discarding irrelevant ones from both modalities, addressing feature extraction
and fusion challenges. Extensive experimental analysis conducted on the
CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the
proposed model. The results underscore the importance of AVT-CA in developing
precise and reliable multimodal emotion recognition systems for practical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 Pages, 9 Tables, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Data Provenance Gap Across Text, Speech and Video <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Naana Obeng-Marnu, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Da Yin, Kun Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas, Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm, Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker, Jad Kabbara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in AI is driven largely by the scale and quality of training data.
Despite this, there is a deficit of empirical analysis examining the attributes
of well-established datasets beyond text. In this work we conduct the largest
and first-of-its-kind longitudinal audit across modalities--popular text,
speech, and video datasets--from their detailed sourcing trends and use
restrictions to their geographical and linguistic representation. Our manual
analysis covers nearly 4000 public datasets between 1990-2024, spanning 608
languages, 798 sources, 659 organizations, and 67 countries. We find that
multimodal machine learning applications have overwhelmingly turned to
web-crawled, synthetic, and social media platforms, such as YouTube, for their
training sets, eclipsing all other sources since 2019. Secondly, tracing the
chain of dataset derivations we find that while less than 33% of datasets are
restrictively licensed, over 80% of the source content in widely-used text,
speech, and video datasets, carry non-commercial restrictions. Finally, counter
to the rising number of languages and geographies represented in public AI
training datasets, our audit demonstrates measures of relative geographical and
multilingual representation have failed to significantly improve their coverage
since 2013. We believe the breadth of our audit enables us to empirically
examine trends in data sourcing, restrictions, and Western-centricity at an
ecosystem-level, and that visibility into these questions are essential to
progress in responsible AI. As a contribution to ongoing improvements in
dataset transparency and responsible use, we release our entire multimodal
audit, allowing practitioners to trace data provenance across text, speech, and
video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. 10 pages, 5 figures (main paper)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">34</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Research Portfolio For Semantic Impact 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander V. Belikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation metrics are widely used to assess academic impact but suffer from
social biases, including institutional prestige and journal visibility. Here we
introduce rXiv Semantic Impact (XSI), a novel framework that predicts research
impact by analyzing how scientific semantic graphs evolve in underlying fabric
of science. Rather than counting citations, XSI tracks the evolution of
research concepts in the academic knowledge graph (KG). Starting with a
construction of a comprehensive KG from 324K biomedical publications
(2003-2025), we demonstrate that XSI can predict a paper's future semantic
impact (SI) with remarkable accuracy ($R^2$ = 0.69) three years in advance. We
leverage these predictions to develop an optimization framework for research
portfolio selection that systematically outperforms random allocation. We
propose SI as a complementary metric to citations and present XSI as a tool to
guide funding and publishing decisions, enhancing research impact while
mitigating risk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Sequence: Do Large Language Models Understand Sequential
  Recommendation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently emerged as promising tools for
recommendation thanks to their advanced textual understanding ability and
context-awareness. Despite the current practice of training and evaluating
LLM-based recommendation (LLM4Rec) models under a sequential recommendation
scenario, we found that whether these models understand the sequential
information inherent in users' item interaction sequences has been largely
overlooked. In this paper, we first demonstrate through a series of experiments
that existing LLM4Rec models do not fully capture sequential information both
during training and inference. Then, we propose a simple yet effective
LLM-based sequential recommender, called LLM-SRec, a method that enhances the
integration of sequential information into LLMs by distilling the user
representations extracted from a pre-trained CF-SRec model into LLMs. Our
extensive experiments show that LLM-SRec enhances LLMs' ability to understand
users' item interaction sequences, ultimately leading to improved
recommendation performance. Furthermore, unlike existing LLM4Rec models that
require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by
training only a few lightweight MLPs, highlighting its practicality in
real-world applications. Our code is available at
https://github.com/Sein-Kim/LLM-SRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judging the Judges: A Collection of LLM-Generated Relevance Judgements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using Large Language Models (LLMs) for relevance assessments offers promising
opportunities to improve Information Retrieval (IR), Natural Language
Processing (NLP), and related fields. Indeed, LLMs hold the promise of allowing
IR experimenters to build evaluation collections with a fraction of the manual
human labor currently required. This could help with fresh topics on which
there is still limited knowledge and could mitigate the challenges of
evaluating ranking systems in low-resource scenarios, where it is challenging
to find human annotators. Given the fast-paced recent developments in the
domain, many questions concerning LLMs as assessors are yet to be answered.
Among the aspects that require further investigation, we can list the impact of
various components in a relevance judgment generation pipeline, such as the
prompt used or the LLM chosen.
  This paper benchmarks and reports on the results of a large-scale automatic
relevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where
different relevance assessment approaches were proposed. In detail, we release
and benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track
relevance judgments produced by eight international teams who participated in
the challenge. Given their diverse nature, these automatically generated
relevance judgments can help the community not only investigate systematic
biases caused by LLMs but also explore the effectiveness of ensemble models,
analyze the trade-offs between different models and human assessors, and
advance methodologies for improving automated evaluation techniques. The
released resource is available at the following link:
https://llm4eval.github.io/LLMJudge-benchmark/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSCon: Toward Conversational Product Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Product Search (CPS) is confined to simulated conversations
due to the lack of real-world CPS datasets that reflect human-like language.
Additionally, current conversational datasets are limited to support
cross-market and multi-lingual usage. In this paper, we introduce a new CPS
data collection protocol and present PSCon, a novel CPS dataset designed to
assist product search via human-like conversations. The dataset is constructed
using a coached human-to-human data collection protocol and supports two
languages and dual markets. Also, the dataset enables thorough exploration of
six subtasks of CPS: user intent detection, keyword extraction, system action
prediction, question selection, item ranking, and response generation.
Furthermore, we also offer an analysis of the dataset and propose a benchmark
model on the proposed CPS dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM-Based Recommendations Through Personalized Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Liu, Xueshuo Yan, Dongsheng Li, Guangping Zhang, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current recommendation systems powered by large language models (LLMs) often
underutilize their reasoning capabilities due to a lack of explicit logical
structuring. To address this limitation, we introduce CoT-Rec, a framework that
integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by
incorporating two crucial processes: user preference analysis and item
perception evaluation. CoT-Rec operates in two key phases: (1) personalized
data extraction, where user preferences and item perceptions are identified,
and (2) personalized data application, where this information is leveraged to
refine recommendations. Our experimental analysis demonstrates that CoT-Rec
improves recommendation accuracy by making better use of LLMs' reasoning
potential. The implementation is publicly available at
https://anonymous.4open.science/r/CoT-Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based
  User Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based user agents have emerged as a powerful tool
for improving recommender systems by simulating user interactions. However,
existing methods struggle with cross-domain scenarios due to inefficient memory
structures, leading to irrelevant information retention and failure to account
for social influence factors such as popularity. To address these limitations,
we introduce AgentCF++, a novel framework featuring a dual-layer memory
architecture and a two-step fusion mechanism to filter domain-specific
preferences effectively. Additionally, we propose interest groups with shared
memory, allowing the model to capture the impact of popularity trends on users
with similar interests. Through extensive experiments on multiple cross-domain
datasets, AgentCF++ demonstrates superior performance over baseline models,
highlighting its effectiveness in refining user behavior simulation for
recommender systems. Our code is available at
https://anonymous.4open.science/r/AgentCF-plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Popularity Bias in Collaborative Filtering through Fair
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Liu, Dongsheng Li, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often suffer from popularity bias, where frequently
interacted items are overrepresented in recommendations. This bias stems from
propensity factors influencing training data, leading to imbalanced exposure.
In this paper, we introduce a Fair Sampling (FS) approach to address this issue
by ensuring that both users and items are selected with equal probability as
positive and negative instances. Unlike traditional inverse propensity score
(IPS) methods, FS does not require propensity estimation, eliminating errors
associated with inaccurate calculations. Our theoretical analysis demonstrates
that FS effectively neutralizes the influence of propensity factors, achieving
unbiased learning. Experimental results validate that FS outperforms
state-of-the-art methods in both point-wise and pair-wise recommendation tasks,
enhancing recommendation fairness without sacrificing accuracy. The
implementation is available at https://anonymous.4open.science/r/Fair-Sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Place Updates of a Graph Index for Streaming Approximate Nearest
  Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haike Xu, Magdalen Dobson Manohar, Philip A. Bernstein, Badrish Chandramouli, Richard Wen, Harsha Vardhan Simhadri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indices for approximate nearest neighbor search (ANNS) are a basic component
for information retrieval and widely used in database, search, recommendation
and RAG systems. In these scenarios, documents or other objects are inserted
into and deleted from the working set at a high rate, requiring a stream of
updates to the vector index. Algorithms based on proximity graph indices are
the most efficient indices for ANNS, winning many benchmark competitions.
However, it is challenging to update such graph index at a high rate, while
supporting stable recall after many updates. Since the graph is singly-linked,
deletions are hard because there is no fast way to find in-neighbors of a
deleted vertex. Therefore, to update the graph, state-of-the-art algorithms
such as FreshDiskANN accumulate deletions in a batch and periodically
consolidate, removing edges to deleted vertices and modifying the graph to
ensure recall stability. In this paper, we present IP-DiskANN
(InPlaceUpdate-DiskANN), the first algorithm to avoid batch consolidation by
efficiently processing each insertion and deletion in-place. Our experiments
using standard benchmarks show that IP-DiskANN has stable recall over various
lengthy update patterns in both high-recall and low-recall regimes. Further,
its query throughput and update speed are better than using the batch
consolidation algorithm and HNSW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Large Recommendation Models: Emerging Trends in LLMs for
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Wei Guo, Luankang Zhang, Jin Yao Chin, Yufei Ye, Huifeng Guo, Yong Liu, Defu Lian, Ruiming Tang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of information overload, recommendation systems play a pivotal
role in filtering data and delivering personalized content. Recent advancements
in feature interaction and user behavior modeling have significantly enhanced
the recall and ranking processes of these systems. With the rise of large
language models (LLMs), new opportunities have emerged to further improve
recommendation systems. This tutorial explores two primary approaches for
integrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning
capabilities of general LLMs, and generative large recommendation models, which
focus on scaling and sophistication. While the former has been extensively
covered in existing literature, the latter remains underexplored. This tutorial
aims to fill this gap by providing a comprehensive overview of generative large
recommendation models, including their recent advancements, challenges, and
potential research directions. Key topics include data quality, scaling laws,
user behavior mining, and efficiency in training and inference. By engaging
with this tutorial, participants will gain insights into the latest
developments and future opportunities in the field, aiding both academic
research and practical applications. The timely nature of this exploration
supports the rapid evolution of recommendation systems, offering valuable
guidance for researchers and practitioners alike.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for the tutorial track at WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Graph Embeddings for Session-based Recommendation with Item
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Peintner, Marta Moscati, Emilia Parada-Cabaleiro, Markus Schedl, Eva Zangerle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In session-based recommender systems, predictions are based on the user's
preceding behavior in the session. State-of-the-art sequential recommendation
algorithms either use graph neural networks to model sessions in a graph or
leverage the similarity of sessions by exploiting item features. In this paper,
we combine these two approaches and propose a novel method, Graph Convolutional
Network Extension (GCNext), which incorporates item features directly into the
graph representation via graph convolutional networks. GCNext creates a
feature-rich item co-occurrence graph and learns the corresponding item
embeddings in an unsupervised manner. We show on three datasets that
integrating GCNext into sequential recommendation algorithms significantly
boosts the performance of nearest-neighbor methods as well as neural network
models. Our flexible extension is easy to incorporate in state-of-the-art
methods and increases the MRR@20 by up to 12.79%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrustRAG: An Information Assistant with Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixing Fan, Qiang Yan, Wenshan Wang, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \Ac{RAG} has emerged as a crucial technique for enhancing large models with
real-time and domain-specific knowledge. While numerous improvements and
open-source tools have been proposed to refine the \ac{RAG} framework for
accuracy, relatively little attention has been given to improving the
trustworthiness of generated results. To address this gap, we introduce
TrustRAG, a novel framework that enhances \ac{RAG} from three perspectives:
indexing, retrieval, and generation. Specifically, in the indexing stage, we
propose a semantic-enhanced chunking strategy that incorporates hierarchical
indexing to supplement each chunk with contextual information, ensuring
semantic completeness. In the retrieval stage, we introduce a utility-based
filtering mechanism to identify high-quality information, supporting answer
generation while reducing input length. In the generation stage, we propose
fine-grained citation enhancement, which detects opinion-bearing sentences in
responses and infers citation relationships at the sentence-level, thereby
improving citation accuracy. We open-source the TrustRAG framework and provide
a demonstration studio designed for excerpt-based question answering tasks
\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we
aim to help researchers: 1) systematically enhancing the trustworthiness of
\ac{RAG} systems and (2) developing their own \ac{RAG} systems with more
reliable outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeerQA: A Scientific Question Answering <span class="highlight-title">Dataset</span> from Peer <span class="highlight-title">Review</span>s <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Baumgärtner, Ted Briscoe, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PeerQA, a real-world, scientific, document-level Question
Answering (QA) dataset. PeerQA questions have been sourced from peer reviews,
which contain questions that reviewers raised while thoroughly examining the
scientific article. Answers have been annotated by the original authors of each
paper. The dataset contains 579 QA pairs from 208 academic articles, with a
majority from ML and NLP, as well as a subset of other scientific communities
like Geoscience and Public Health. PeerQA supports three critical tasks for
developing practical QA systems: Evidence retrieval, unanswerable question
classification, and answer generation. We provide a detailed analysis of the
collected dataset and conduct experiments establishing baseline systems for all
three tasks. Our experiments and analyses reveal the need for
decontextualization in document-level retrieval, where we find that even simple
decontextualization approaches consistently improve retrieval performance
across architectures. On answer generation, PeerQA serves as a challenging
benchmark for long-context modeling, as the papers have an average size of 12k
tokens. Our code and data is available at https://github.com/UKPLab/peerqa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMTEB: Massive Multilingual Text Embedding Benchmark <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings are typically evaluated on a limited set of tasks, which are
constrained by language, domain, and task diversity. To address these
limitations and provide a more comprehensive evaluation, we introduce the
Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,
community-driven expansion of MTEB, covering over 500 quality-controlled
evaluation tasks across 250+ languages. MMTEB includes a diverse set of
challenging, novel tasks such as instruction following, long-document
retrieval, and code retrieval, representing the largest multilingual collection
of evaluation tasks for embedding models to date. Using this collection, we
develop several highly multilingual benchmarks, which we use to evaluate a
representative set of models. We find that while large language models (LLMs)
with billions of parameters can achieve state-of-the-art performance on certain
language subsets and task categories, the best-performing publicly available
model is multilingual-e5-large-instruct with only 560 million parameters. To
facilitate accessibility and reduce computational cost, we introduce a novel
downsampling method based on inter-task correlation, ensuring a diverse
selection while preserving relative model rankings. Furthermore, we optimize
tasks such as retrieval by sampling hard negatives, creating smaller but
effective splits. These optimizations allow us to introduce benchmarks that
drastically reduce computational demands. For instance, our newly introduced
zero-shot English benchmark maintains a ranking order similar to the full-scale
version but at a fraction of the computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActionPiece: Contextually Tokenizing Action Sequences for Generative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, Derek Zhiyuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation (GR) is an emerging paradigm where user actions are
tokenized into discrete token patterns and autoregressively generated as
predictions. However, existing GR models tokenize each action independently,
assigning the same fixed tokens to identical actions across all sequences
without considering contextual relationships. This lack of context-awareness
can lead to suboptimal performance, as the same action may hold different
meanings depending on its surrounding context. To address this issue, we
propose ActionPiece to explicitly incorporate context when tokenizing action
sequences. In ActionPiece, each action is represented as a set of item
features, which serve as the initial tokens. Given the action sequence corpora,
we construct the vocabulary by merging feature patterns as new tokens, based on
their co-occurrence frequency both within individual sets and across adjacent
sets. Considering the unordered nature of feature sets, we further introduce
set permutation regularization, which produces multiple segmentations of action
sequences with the same semantics. Experiments on public datasets demonstrate
that ActionPiece consistently outperforms existing action tokenization methods,
improving NDCG@$10$ by $6.00\%$ to $12.82\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bursting Filter Bubble: Enhancing Serendipity Recommendations with
  Aligned Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjia Xi, Muyan Weng, Wen Chen, Chao Yi, Dian Chen, Gaoyang Guo, Mao Zhang, Jian Wu, Yuning Jiang, Qingwen Liu, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RSs) often suffer from the feedback loop phenomenon,
e.g., RSs are trained on data biased by their recommendations. This leads to
the filter bubble effect that reinforces homogeneous content and reduces user
satisfaction. To this end, serendipity recommendations, which offer unexpected
yet relevant items, are proposed. Recently, large language models (LLMs) have
shown potential in serendipity prediction due to their extensive world
knowledge and reasoning capabilities. However, they still face challenges in
aligning serendipity judgments with human assessments, handling long user
behavior sequences, and meeting the latency requirements of industrial RSs. To
address these issues, we propose SERAL (Serendipity Recommendations with
Aligned Large Language Models), a framework comprising three stages: (1)
Cognition Profile Generation to compress user behavior into multi-level
profiles; (2) SerenGPT Alignment to align serendipity judgments with human
preferences using enriched training data; and (3) Nearline Adaptation to
integrate SerenGPT into industrial RSs pipelines efficiently. Online
experiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and
transactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user
experience without much impact on overall revenue. Now, it has been fully
deployed in the "Guess What You Like" of the Taobao App homepage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuhan Chen, Zongwei Wang, Min Gao, Xin Xia, Feng Jiang, Junhao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional sequential recommendation (SR) methods heavily rely on explicit
item IDs to capture user preferences over time. This reliance introduces
critical limitations in cold-start scenarios and domain transfer tasks, where
unseen items and new contexts often lack established ID mappings. To overcome
these limitations, recent studies have shifted towards leveraging text-only
information for recommendation, thereby improving model generalization and
adaptability across domains. Although promising, text-based SR faces unique
difficulties: items' text descriptions often share semantic similarities that
lead to clustered item representations, compromising their uniformity, a
property essential for promoting diversity and enhancing generalization in
recommendation systems. In this paper, we explore a novel framework to improve
the uniformity of item representations in text-based SR. Our analysis reveals
that items within a sequence exhibit marked semantic similarity, meaning they
are closer in representation than items overall, and that this effect is more
pronounced for less popular items, which form tighter clusters compared to
their more popular counterparts. Based on these findings, we propose UniT, a
framework that employs three pairwise item sampling strategies: Unified General
Sampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven
Sampling Strategy. Each strategy applies varying degrees of repulsion to
selectively adjust the distances between item pairs, thereby refining
representation uniformity while considering both sequence context and item
popularity. Extensive experiments on multiple real-world datasets demonstrate
that our proposed approach outperforms state-of-the-art models, validating the
effectiveness of UniT in enhancing both representation uniformity and
recommendation accuracy.The source code is available at
https://github.com/ccwwhhh/Model-Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducing NevIR: Negation in Neural Information Retrieval <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coen van Elsen, Francien Barkhof, Thijmen Nijdam, Simon Lupart, Mohammad Alliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negation is a fundamental aspect of human communication, yet it remains a
challenge for Language Models (LMs) in Information Retrieval (IR). Despite the
heavy reliance of modern neural IR systems on LMs, little attention has been
given to their handling of negation. In this study, we reproduce and extend the
findings of NevIR, a benchmark study that revealed most IR models perform at or
below the level of random ranking when dealing with negation. We replicate
NevIR's original experiments and evaluate newly developed state-of-the-art IR
models. Our findings show that a recently emerging category - listwise Large
Language Model (LLM) rerankers - outperforms other models but still
underperforms human performance. Additionally, we leverage ExcluIR, a benchmark
dataset designed for exclusionary queries with extensive negation, to assess
the generalizability of negation understanding. Our findings suggest that
fine-tuning on one dataset does not reliably improve performance on the other,
indicating notable differences in their data distributions. Furthermore, we
observe that only cross-encoders and listwise LLM rerankers achieve reasonable
performance across both negation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, under review at SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM4Tag: Automatic Tagging System for Information Retrieval via Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiming Tang, Chenxu Zhu, Bo Chen, Weipeng Zhang, Menghui Zhu, Xinyi Dai, Huifeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tagging systems play an essential role in various information retrieval
applications such as search engines and recommender systems. Recently, Large
Language Models (LLMs) have been applied in tagging systems due to their
extensive world knowledge, semantic understanding, and reasoning capabilities.
Despite achieving remarkable performance, existing methods still have
limitations, including difficulties in retrieving relevant candidate tags
comprehensively, challenges in adapting to emerging domain-specific knowledge,
and the lack of reliable tag confidence quantification. To address these three
limitations above, we propose an automatic tagging system LLM4Tag. First, a
graph-based tag recall module is designed to effectively and comprehensively
construct a small-scale highly relevant candidate tag set. Subsequently, a
knowledge-enhanced tag generation module is employed to generate accurate tags
with long-term and short-term knowledge injection. Finally, a tag confidence
calibration module is introduced to generate reliable tag confidence scores.
Extensive experiments over three large-scale industrial datasets show that
LLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag
has been deployed online for content tagging to serve hundreds of millions of
users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HawkBench: Investigating Resilience of RAG Methods on Stratified
  Information-Seeking Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu, Chao Gao, Yankai Wang, Defu Lian, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world information-seeking scenarios, users have dynamic and diverse
needs, requiring RAG systems to demonstrate adaptable resilience. To
comprehensively evaluate the resilience of current RAG methods, we introduce
HawkBench, a human-labeled, multi-domain benchmark designed to rigorously
assess RAG performance across categorized task types. By stratifying tasks
based on information-seeking behaviors, HawkBench provides a systematic
evaluation of how well RAG systems adapt to diverse user needs.
  Unlike existing benchmarks, which focus primarily on specific task types
(mostly factoid queries) and rely on varying knowledge bases, HawkBench offers:
(1) systematic task stratification to cover a broad range of query types,
including both factoid and rationale queries, (2) integration of multi-domain
corpora across all task types to mitigate corpus bias, and (3) rigorous
annotation for high-quality evaluation.
  HawkBench includes 1,600 high-quality test samples, evenly distributed across
domains and task types. Using this benchmark, we evaluate representative RAG
methods, analyzing their performance in terms of answer quality and response
latency. Our findings highlight the need for dynamic task strategies that
integrate decision-making, query interpretation, and global knowledge
understanding to improve RAG generalizability. We believe HawkBench serves as a
pivotal benchmark for advancing the resilience of RAG methods and their ability
to achieve general-purpose information seeking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Retrieval for Large Language Model-based Conversational
  Recommender Systems <span class="chip">WWW'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaochen Zhu, Chao Wan, Harald Steck, Dawen Liang, Yesu Feng, Nathan Kallus, Jundong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRS) aim to provide personalized
recommendations via interactive dialogues with users. While large language
models (LLMs) enhance CRS with their superior understanding of context-aware
user preferences, they typically struggle to leverage behavioral data, which
have proven to be important for classical collaborative filtering (CF)-based
approaches. For this reason, we propose CRAG, Collaborative Retrieval Augmented
Generation for LLM-based CRS. To the best of our knowledge, CRAG is the first
approach that combines state-of-the-art LLMs with CF for conversational
recommendations. Our experiments on two publicly available movie conversational
recommendation datasets, i.e., a refined Reddit dataset (which we name
Reddit-v2) as well as the Redial dataset, demonstrate the superior item
coverage and recommendation performance of CRAG, compared to several CRS
baselines. Moreover, we observe that the improvements are mainly due to better
recommendation accuracy on recently released movies. The code and data are
available at https://github.com/yaochenzhu/CRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenglai Zeng, Pengfei He, Kai Guo, Tianqi Zheng, Hanqing Lu, Yue Xing, Hui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) enhanced with external contexts, such as through
retrieval-augmented generation (RAG), often face challenges in handling
imperfect evidence. They tend to over-rely on external knowledge, making them
vulnerable to misleading and unhelpful contexts. To address this, we propose
the concept of context-robust LLMs, which can effectively balance internal
knowledge with external context, similar to human cognitive processes.
Specifically, context-robust LLMs should rely on external context only when
lacking internal knowledge, identify contradictions between internal and
external knowledge, and disregard unhelpful contexts. To achieve this goal, we
introduce Grft, a lightweight and plug-and-play gated representation
fine-tuning approach. Grft consists of two key components: a gating mechanism
to detect and filter problematic inputs, and low-rank representation adapters
to adjust hidden representations. By training a lightweight intervention
function with only 0.0004\% of model size on fewer than 200 examples, Grft can
effectively adapt LLMs towards context-robust behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Non-Factoid Question Answering with Answer Paragraph
  Selection <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritwik Mishra, Sreeram Vennam, Rajiv Ratn Shah, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing Question Answering Datasets (QuADs) primarily focus on
factoid-based short-context Question Answering (QA) in high-resource languages.
However, the scope of such datasets for low-resource languages remains limited,
with only a few works centered on factoid-based QuADs and none on non-factoid
QuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with
non-factoid questions. It utilizes interrogative sub-headings from BBC news
articles as questions and the corresponding paragraphs as silver answers. The
dataset comprises over 578K QA pairs across 38 languages, encompassing several
low-resource languages, and stands as the largest multilingual QA dataset to
date. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden
set), we observe that 98\% of questions can be answered using their
corresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS)
model outperforms the baselines. The APS model attained an accuracy of 80\% and
72\%, as well as a macro F1 of 72\% and 66\%, on the MuNfQuAD testset and the
golden set, respectively. Furthermore, the APS model effectively generalizes a
certain language within the golden set, even after being fine-tuned on silver
labels. We also observe that the fine-tuned APS model is beneficial for
reducing the context of a question. These findings suggest that this resource
would be a valuable contribution to the QA research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shorter version accepted into DSFA, a special session in PAKDD 2025,
  Sydney</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emancipatory Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19241v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19241v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our world today is facing a confluence of several mutually reinforcing crises
each of which intersects with concerns of social justice and emancipation. This
paper is a provocation for the role of computer-mediated information access in
our emancipatory struggles. We define emancipatory information retrieval as the
study and development of information access methods that challenge various
forms of human oppression, and situates its activities within broader
collective emancipatory praxis. The term "emancipatory" here signifies the
moral concerns of universal humanization of all peoples and the elimination of
oppression to create the conditions under which we can collectively flourish.
To develop an emancipatory research agenda for information retrieval (IR), in
this paper we speculate about the practices that the community can adopt,
enumerate some of the projects that the field should undertake, and discuss
provocations to spark new ideas and directions for research. We challenge the
field of IR research to embrace humanistic values and commit to universal
emancipation and social justice. We also invite scholars from fields such as
human-computer interaction, information sciences, media studies, design, social
sciences, humanities, democratic theory, and critical theory, as well as legal
and policy experts, civil rights and social justice activists, and artists to
join us in realizing this transformation. In this process, we must both imagine
post-oppressive worlds, and reimagine the role of IR in that world and in the
journey that leads us there.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agentic Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, Jianghao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the 1970s, information retrieval (IR) has long been defined as the
process of acquiring relevant information items from a pre-defined corpus to
satisfy user information needs. Traditional IR systems, while effective in
domains like web search, are constrained by their reliance on static,
pre-defined information items. To this end, this paper introduces agentic
information retrieval (Agentic IR), a transformative next-generation paradigm
for IR driven by large language models (LLMs) and AI agents. The central shift
in agentic IR is the evolving definition of ``information'' from static,
pre-defined information items to dynamic, context-dependent information states.
Information state refers to a particular information context that the user is
right in within a dynamic environment, encompassing not only the acquired
information items but also real-time user preferences, contextual factors, and
decision-making processes. In such a way, traditional information retrieval,
focused on acquiring relevant information items based on user queries, can be
naturally extended to achieving the target information state given the user
instruction, which thereby defines the agentic information retrieval. We
systematically discuss agentic IR from various aspects, i.e., task formulation,
architecture, evaluation, case studies, as well as challenges and future
prospects. We believe that the concept of agentic IR introduced in this paper
not only broadens the scope of information retrieval research but also lays the
foundation for a more adaptive, interactive, and intelligent next-generation IR
paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, perspective paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterophily-Aware Fair Recommendation using Graph Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nemat Gholinejad, Mostafa Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, graph neural networks (GNNs) have become a popular tool to
improve the accuracy and performance of recommender systems. Modern recommender
systems are not only designed to serve end users, but also to benefit other
participants, such as items and item providers. These participants may have
different or conflicting goals and interests, which raises the need for
fairness and popularity bias considerations. GNN-based recommendation methods
also face the challenges of unfairness and popularity bias, and their
normalization and aggregation processes suffer from these challenges. In this
paper, we propose a fair GNN-based recommender system, called HetroFair, to
improve item-side fairness. HetroFair uses two separate components to generate
fairness-aware embeddings: i) Fairness-aware attention, which incorporates the
dot product in the normalization process of GNNs to decrease the effect of
nodes' degrees. ii) Heterophily feature weighting, to assign distinct weights
to different features during the aggregation process. To evaluate the
effectiveness of HetroFair, we conduct extensive experiments over six
real-world datasets. Our experimental results reveal that HetroFair not only
alleviates unfairness and popularity bias on the item side but also achieves
superior accuracy on the user side. Our implementation is publicly available at
https://github.com/NematGH/HetroFair.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Local to Global: A Graph RAG Approach to Query-Focused
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as "What are the main themes in the dataset?", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of
text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose GraphRAG, a graph-based approach to question
answering over private text corpora that scales with both the generality of
user questions and the quantity of source text. Our approach uses an LLM to
build a graph index in two stages: first, to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that GraphRAG
leads to substantial improvements over a conventional RAG baseline for both the
comprehensiveness and diversity of generated answers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Sequential Recommendation Models with Scaling Laws and
  Approximate Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00430v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00430v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingjia Shen, Hao Wang, Chuhan Wu, Jin Yao Chin, Wei Guo, Yong Liu, Huifeng Guo, Defu Lian, Ruiming Tang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling Laws have emerged as a powerful framework for understanding how model
performance evolves as they increase in size, providing valuable insights for
optimizing computational resources. In the realm of Sequential Recommendation
(SR), which is pivotal for predicting users' sequential preferences, these laws
offer a lens through which to address the challenges posed by the scalability
of SR models. However, the presence of structural and collaborative issues in
recommender systems prevents the direct application of the Scaling Law (SL) in
these systems. In response, we introduce the Performance Law for SR models,
which aims to theoretically investigate and model the relationship between
model performance and data quality. Specifically, we first fit the HR and NDCG
metrics to transformer-based SR models. Subsequently, we propose Approximate
Entropy (ApEn) to assess data quality, presenting a more nuanced approach
compared to traditional data quantity metrics. Our method enables accurate
predictions across various dataset scales and model sizes, demonstrating a
strong correlation in large SR models and offering insights into achieving
optimal performance for any given model configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planted vertex cover problem on regular random graphs and nonmonotonic
  temperature-dependence in the supercooled region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin-Yi Fan, Hai-Jun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a planted vertex cover problem on regular random graphs and
study it by the cavity method of statistical mechanics. Different from
conventional Ising models, the equilibrium ferromagnetic phase transition of
this binary-spin two-body interaction system is discontinuous, as the
paramagnetic phase is separated from the ferromagnetic phase by an extensive
free energy barrier. The free energy landscape can be distinguished into three
different types depending on the two degree parameters of the planted graph.
The critical inverse temperatures at which the paramagnetic phase becomes
locally unstable towards the ferromagnetic phase ($\beta_{\textrm{pf}}$) and
towards spin glass phases ($\beta_{\textrm{pg}}$) satisfy $\beta_{\textrm{pf}}
> \beta_{\textrm{pg}}$, $\beta_{\textrm{pf}} < \beta_{\textrm{pg}}$ and
$\beta_{\textrm{pf}} = \beta_{\textrm{pg}}$, respectively, in these three
landscapes. A locally stable anti-ferromagnetic phase emerges in the free
energy landscape if $\beta_{\textrm{pf}} < \beta_{\textrm{pg}}$. When exploring
the free energy landscape by stochastic local search dynamics, we find that in
agreement with our theoretical prediction, the first-passage time from the
paramagnetic phase to the ferromagnetic phase is nonmonotonic with the inverse
temperature. The potential relevance of the planted vertex cover model to
supercooled glass-forming liquids is briefly discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensively revised and expanded. Changed title. A mistake in
  numerical simulation corrected. Accepted for publication in PRE as a regular
  article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models in Recommendation Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Ruen Wei, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems remain an essential topic due to its wide application in
various domains and the business potential behind them. With the rise of deep
learning, common solutions have leveraged neural networks to facilitate
collaborative filtering, and some have turned to generative adversarial
networks to augment the dataset and tackle the data sparsity issue. However,
they are limited in learning the complex user and item distribution and still
suffer from model collapse. Given the great generation capability exhibited by
diffusion models in computer vision recently, many recommender systems have
adopted diffusion models and found improvements in performance for various
tasks. Diffusion models in recommender systems excel in managing complex user
and item distributions and do not suffer from mode collapse. With these
advantages, the amount of research in this domain have been growing rapidly and
calling for a systematic survey. In this survey paper, we present and propose a
taxonomy on past research papers in recommender systems that utilize diffusion
models. Distinct from a prior survey paper that categorizes based on the role
of the diffusion model, we categorize based on the recommendation task at hand.
The decision originates from the rationale that after all, the adoption of
diffusion models is to enhance the recommendation performance, not vice versa:
adapting the recommendation task to enable diffusion models. Nonetheless, we
offer a unique perspective for diffusion models in recommender systems
complementary to existing surveys. We present the foundation algorithms in
diffusion models and their applications in recommender systems to summarize the
rapid development in this field. Finally, we discuss open research directions
to prepare and encourage further efforts to advance the field. We compile the
relevant papers in a public GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Model for All: Large Language Models are Domain-Agnostic
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuoli Tang, Zhaoxin Huan, Zihao Li, Xiaolu Zhang, Jun Hu, Chilin Fu, Jun Zhou, Lixin Zou, Chenliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems aim to predict users' next likely
interaction based on their history. However, these systems face data sparsity
and cold-start problems. Utilizing data from other domains, known as
multi-domain methods, is useful for alleviating these problems. However,
traditional multi-domain methods rely on meaningless ID-based item
representation, which makes it difficult to align items with similar meanings
from different domains, yielding sup-optimal knowledge transfer. This paper
introduces LLM-Rec, a framework that utilizes pre-trained large language models
(LLMs) for domain-agnostic recommendation. Specifically, we mix user's
behaviors from multiple domains and concatenate item titles into a sentence,
then use LLMs for generating user and item representations. By mixing behaviors
across different domains, we can exploit the knowledge encoded in LLMs to
bridge the semantic across over multi-domain behaviors, thus obtaining
semantically rich representations and improving performance in all domains.
Furthermore, we explore the underlying reasons why LLMs are effective and
investigate whether LLMs can understand the semantic correlations as the
recommendation model, and if advanced techniques like scaling laws in NLP also
work in recommendations. We conduct extensive experiments with LLMs ranging
from 40M to 6.7B to answer the above questions and to verify the effectiveness
of LLM-Rec in multi-domain recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures, 7 tables, Accepted by TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open-Source Web-Based Tool for Evaluating Open-Source Large Language
  Models Leveraging Information Retrieval from Custom Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Godfrey I
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our work, we present the first-of-its-kind open-source web-based tool
which is able to demonstrate the impacts of a user's speech act during
discourse with conversational agents, which leverages open-source large
language models. With this software resource, it is possible for researchers
and experts to evaluate the performance of various dialogues, visualize the
user's communicative intents, and utilise uploaded specific documents for the
chat agent to use for its information retrieval to respond to the user query.
The context gathered by these models is obtained from a set of linguistic
features extracted, which forms the context embeddings of the models.
Regardless of these models showing good context understanding based on these
features, there still remains a gap in including deeper pragmatic features to
improve the model's comprehension of the query, hence the efforts to develop
this web resource, which is able to extract and then inject this overlooked
feature in the encoder-decoder pipeline of the conversational agent. To
demonstrate the effect and impact of the resource, we carried out an experiment
which evaluated the system using 2 knowledge files for information retrieval,
with two user queries each, across 5 open-source large language models using 10
standard metrics. Our results showed that larger open-source models,
demonstrated an improved alignment when the user speech act was included with
their query. The smaller models in contrast showed an increased perplexity and
mixed performance, which explicitly indicated struggles in processing queries
that explicitly included speech acts. The results from the analysis using the
developed web resource highlight the potential of speech acts towards enhancing
conversational depths while underscoring the need for model-specific
optimizations to address increased computational costs and response times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAR: A Simple Training-free Approach for Recommendations using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong-Ho Lee, Adam Kraft, Long Jin, Nikhil Mehta, Taibai Xu, Lichan Hong, Ed H. Chi, Xinyang Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models (LLMs) offers promising new
approaches for recommendation system tasks. While the current state-of-the-art
methods rely on fine-tuning LLMs to achieve optimal results, this process is
costly and introduces significant engineering complexities. Conversely, methods
that directly use LLMs without additional fine-tuning result in a large drop in
recommendation quality, often due to the inability to capture collaborative
information. In this paper, we propose a Simple Training-free Approach for
Recommendation (STAR), a framework that utilizes LLMs and can be applied to
various recommendation tasks without the need for fine-tuning, while
maintaining high quality recommendation performance. Our approach involves a
retrieval stage that uses semantic embeddings from LLMs combined with
collaborative user information to retrieve candidate items. We then apply an
LLM for pairwise ranking to enhance next-item prediction. Experimental results
on the Amazon Review dataset show competitive performance for next item
prediction, even with our retrieval stage alone. Our full method achieves
Hits@10 performance of +23.8% on Beauty, +37.5% on Toys & Games, and -1.8% on
Sports & Outdoors relative to the best supervised models. This framework offers
an effective alternative to traditional supervised models, highlighting the
potential of LLMs in recommendation systems without extensive training or
custom architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical
components of modern applications in information retrieval, question answering,
or knowledge-based text generation. However, existing solutions are often
fragmented, lacking a unified framework that easily integrates these essential
processes. The absence of a standardized implementation, coupled with the
complexity of retrieval and re-ranking workflows, makes it challenging for
researchers to compare and evaluate different approaches in a consistent
environment. While existing toolkits such as Rerankers and RankLLM provide
general-purpose reranking pipelines, they often lack the flexibility required
for fine-grained experimentation and benchmarking. In response to these
challenges, we introduce Rankify, a powerful and modular open-source toolkit
designed to unify retrieval, re-ranking, and RAG within a cohesive framework.
Rankify supports a wide range of retrieval techniques, including dense and
sparse retrievers, while incorporating state-of-the-art re-ranking models to
enhance retrieval quality. Additionally, Rankify includes a collection of
pre-retrieved datasets to facilitate benchmarking, available at Huggingface
(https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To
encourage adoption and ease of integration, we provide comprehensive
documentation (http://rankify.readthedocs.io/), an open-source implementation
on GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for
easy installation (https://pypi.org/project/rankify/). As a unified and
lightweight framework, Rankify allows researchers and practitioners to advance
retrieval and re-ranking methodologies while ensuring consistency, scalability,
and ease of use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy-Guided Zero-Shot Recommendations with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14043v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14043v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqing Liang, Liangwei Yang, Chen Wang, Xiongxiao Xu, Philip S. Yu, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of large language models (LLMs) and their ability to
perform a variety of tasks, their application in recommender systems (RecSys)
has shown promise. However, we are facing significant challenges when deploying
LLMs into RecSys, such as limited prompt length, unstructured item information,
and un-constrained generation of recommendations, leading to sub-optimal
performance. To address these issues, we propose a novel method using a
taxonomy dictionary. This method provides a systematic framework for
categorizing and organizing items, improving the clarity and structure of item
information. By incorporating the taxonomy dictionary into LLM prompts, we
achieve efficient token utilization and controlled feature generation, leading
to more accurate and contextually relevant recommendations. Our Taxonomy-guided
Recommendation (TaxRec) approach features a two-step process: one-time taxonomy
categorization and LLM-based recommendation, enabling zero-shot recommendations
without the need for domain-specific fine-tuning. Experimental results
demonstrate TaxRec significantly enhances recommendation quality compared to
traditional zero-shot approaches, showcasing its efficacy as personal
recommender with LLMs. Code is available at
https://github.com/yueqingliang1/TaxRec.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-18T00:00:00Z">2025-02-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">131</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Mamba: Decoder-only Multimodal State Space Model via
  Quadratic to Linear Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Multimodal Large Language Models (MLLMs) have achieved remarkable
performance but face deployment challenges due to their quadratic computational
complexity, growing Key-Value cache requirements, and reliance on separate
vision encoders. We propose mmMamba, a framework for developing
linear-complexity native multimodal state space models through progressive
distillation from existing MLLMs using moderate academic computational
resources. Our approach enables the direct conversion of trained decoder-only
MLLMs to linear-complexity architectures without requiring pre-trained
RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba
from trained Transformer and a three-stage distillation recipe, which can
effectively transfer the knowledge from Transformer to Mamba while preserving
multimodal capabilities. Our method also supports flexible hybrid architectures
that combine Transformer and Mamba layers for customizable
efficiency-performance trade-offs. Distilled from the Transformer-based
decoder-only HoVLE, mmMamba-linear achieves competitive performance against
existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further
improves performance significantly, approaching HoVLE's capabilities. At 103K
tokens, mmMamba-linear demonstrates 20.6$\times$ speedup and 75.8% GPU memory
reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\times$ speedup
and 60.2% memory savings. Code and models are released at
https://github.com/hustvl/mmMamba
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and model are available at https://github.com/hustvl/mmMamba</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large Vision Language Models (VLMs) has broadened the scope
and capabilities of single-modal Large Language Models (LLMs) by integrating
visual modalities, thereby unlocking transformative cross-modal applications in
a variety of real-world scenarios. Despite their impressive performance, VLMs
are prone to significant hallucinations, particularly in the form of
cross-modal inconsistencies. Building on the success of Reinforcement Learning
from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused
on applying direct preference optimization (DPO) on carefully curated datasets
to mitigate these issues. Yet, such approaches typically introduce preference
signals in a brute-force manner, neglecting the crucial role of visual
information in the alignment process. In this paper, we introduce Re-Align, a
novel alignment framework that leverages image retrieval to construct a
dual-preference dataset, effectively incorporating both textual and visual
preference signals. We further introduce rDPO, an extension of the standard
direct preference optimization that incorporates an additional visual
preference objective during fine-tuning. Our experimental results demonstrate
that Re-Align not only mitigates hallucinations more effectively than previous
methods but also yields significant performance gains in general visual
question-answering (VQA) tasks. Moreover, we show that Re-Align maintains
robustness and scalability across a wide range of VLM sizes and architectures.
This work represents a significant step forward in aligning multimodal LLMs,
paving the way for more reliable and effective cross-modal applications. We
release all the code in https://github.com/taco-group/Re-Align.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, Ying Zhang, Wenyu Liu, Qian Zhang, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing end-to-end autonomous driving (AD) algorithms typically follow the
Imitation Learning (IL) paradigm, which faces challenges such as causal
confusion and the open-loop gap. In this work, we establish a 3DGS-based
closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS
techniques, we construct a photorealistic digital replica of the real physical
world, enabling the AD policy to extensively explore the state space and learn
to handle out-of-distribution scenarios through large-scale trial and error. To
enhance safety, we design specialized rewards that guide the policy to
effectively respond to safety-critical events and understand real-world causal
relationships. For better alignment with human driving behavior, IL is
incorporated into RL training as a regularization term. We introduce a
closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS
environments. Compared to IL-based methods, RAD achieves stronger performance
in most closed-loop metrics, especially 3x lower collision rate. Abundant
closed-loop results are presented at https://hgao-cv.github.io/RAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hgao-cv.github.io/RAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and
  Object Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng Zhang, He Wang, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial intelligence is a critical component of embodied AI, promoting robots
to understand and interact with their environments. While recent advances have
enhanced the ability of VLMs to perceive object locations and positional
relationships, they still lack the capability to precisely understand object
orientations-a key requirement for tasks involving fine-grained manipulations.
Addressing this limitation not only requires geometric reasoning but also an
expressive and intuitive way to represent orientation. In this context, we
propose that natural language offers a more flexible representation space than
canonical frames, making it particularly suitable for instruction-following
robotic systems. In this paper, we introduce the concept of semantic
orientation, which defines object orientations using natural language in a
reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the
''handle'' direction of a knife). To support this, we construct OrienText300K,
a large-scale dataset of 3D models annotated with semantic orientations that
link geometric understanding to functional semantics. By integrating semantic
orientation into a VLM system, we enable robots to generate manipulation
actions with both positional and orientational constraints. Extensive
experiments in simulation and real world demonstrate that our approach
significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy
on Open6DOR and 74.9% accuracy on SIMPLER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://qizekun.github.io/sofar/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Flow: Transforming Text to Audio-Visual Human-like Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aggelina Chatziagapi, Louis-Philippe Morency, Hongyu Gong, Michael Zollhoefer, Dimitris Samaras, Alexander Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AV-Flow, an audio-visual generative model that animates
photo-realistic 4D talking avatars given only text input. In contrast to prior
work that assumes an existing speech signal, we synthesize speech and vision
jointly. We demonstrate human-like speech synthesis, synchronized lip motion,
lively facial expressions and head pose; all generated from just text
characters. The core premise of our approach lies in the architecture of our
two parallel diffusion transformers. Intermediate highway connections ensure
communication between the audio and visual modalities, and thus, synchronized
speech intonation and facial dynamics (e.g., eyebrow motion). Our model is
trained with flow matching, leading to expressive results and fast inference.
In case of dyadic conversations, AV-Flow produces an always-on avatar, that
actively listens and reacts to the audio-visual input of a user. Through
extensive experiments, we show that our method outperforms prior work,
synthesizing natural-looking 4D talking avatars. Project page:
https://aggelinacha.github.io/AV-Flow/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magma: A Foundation Model for Multimodal AI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Magma, a foundation model that serves multimodal AI agentic tasks
in both the digital and physical worlds. Magma is a significant extension of
vision-language (VL) models in that it not only retains the VL understanding
ability (verbal intelligence) of the latter, but is also equipped with the
ability to plan and act in the visual-spatial world (spatial-temporal
intelligence) and complete agentic tasks ranging from UI navigation to robot
manipulation. To endow the agentic capabilities, Magma is pretrained on large
amounts of heterogeneous datasets spanning from images, videos to robotics
data, where the actionable visual objects (e.g., clickable buttons in GUI) in
images are labeled by Set-of-Mark (SoM) for action grounding, and the object
movements (e.g., the trace of human hands or robotic arms) in videos are
labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show
that SoM and ToM reach great synergy and facilitate the acquisition of
spatial-temporal intelligence for our Magma model, which is fundamental to a
wide range of tasks as shown in Fig.1. In particular, Magma creates new
state-of-the-art results on UI navigation and robotic manipulation tasks,
outperforming previous models that are specifically tailored to these tasks. On
image and video-related multimodal tasks, Magma also compares favorably to
popular large multimodal models that are trained on much larger datasets. We
make our model and code public for reproducibility at
https://microsoft.github.io/Magma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 16 figures, technical report from MSR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Noise Conditioning Necessary for Denoising Generative Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Sun, Zhicheng Jiang, Hanhong Zhao, Kaiming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is widely believed that noise conditioning is indispensable for denoising
diffusion models to work successfully. This work challenges this belief.
Motivated by research on blind image denoising, we investigate a variety of
denoising-based generative models in the absence of noise conditioning. To our
surprise, most models exhibit graceful degradation, and in some cases, they
even perform better without noise conditioning. We provide a theoretical
analysis of the error caused by removing noise conditioning and demonstrate
that our analysis aligns with empirical observations. We further introduce a
noise-unconditional model that achieves a competitive FID of 2.23 on CIFAR-10,
significantly narrowing the gap to leading noise-conditional models. We hope
our findings will inspire the community to revisit the foundations and
formulations of denoising generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WeedsGalore: A Multispectral and Multitemporal UAV-based <span class="highlight-title">Dataset</span> for
  Crop and Weed Segmentation in Agricultural Maize Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekin Celikkan, Timo Kunzmann, Yertay Yeskaliyev, Sibylle Itzerott, Nadja Klein, Martin Herold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weeds are one of the major reasons for crop yield loss but current weeding
practices fail to manage weeds in an efficient and targeted manner. Effective
weed management is especially important for crops with high worldwide
production such as maize, to maximize crop yield for meeting increasing global
demands. Advances in near-sensing and computer vision enable the development of
new tools for weed management. Specifically, state-of-the-art segmentation
models, coupled with novel sensing technologies, can facilitate timely and
accurate weeding and monitoring systems. However, learning-based approaches
require annotated data and show a lack of generalization to aerial imaging for
different crops. We present a novel dataset for semantic and instance
segmentation of crops and weeds in agricultural maize fields. The multispectral
UAV-based dataset contains images with RGB, red-edge, and near-infrared bands,
a large number of plant instances, dense annotations for maize and four weed
classes, and is multitemporal. We provide extensive baseline results for both
tasks, including probabilistic methods to quantify prediction uncertainty,
improve model calibration, and demonstrate the approach's applicability to
out-of-distribution data. The results show the effectiveness of the two
additional bands compared to RGB only, and better performance in our target
domain than models trained on existing datasets. We hope our dataset advances
research on methods and operational systems for fine-grained weed
identification, enhancing the robustness and applicability of UAV-based weed
management. The dataset and code are available at
https://github.com/GFZ/weedsgalore
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Rectifying Safety Perception Distortion in VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Zou, Jian Kang, George Kesidis, Lu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies reveal that vision-language models (VLMs) become more
susceptible to harmful requests and jailbreak attacks after integrating the
vision modality, exhibiting greater vulnerability than their text-only LLM
backbones. To uncover the root cause of this phenomenon, we conduct an in-depth
analysis and identify a key issue: multimodal inputs introduce an
modality-induced activation shift toward a "safer" direction compared to their
text-only counterparts, leading VLMs to systematically overestimate the safety
of harmful inputs. We refer to this issue as safety perception distortion. To
mitigate such distortion, we propose Activation Shift Disentanglement and
Calibration (ShiftDC), a training-free method that decomposes and calibrates
the modality-induced activation shift to reduce the impact of modality on
safety. By isolating and removing the safety-relevant component, ShiftDC
restores the inherent safety alignment of the LLM backbone while preserving the
vision-language capabilities of VLMs. Empirical results demonstrate that
ShiftDC significantly enhances alignment performance on safety benchmarks
without impairing model utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Image Generation with Deep Generative Models: A Decade
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wei, Yiheng Zheng, Yabo Zhang, Ming Liu, Zhilong Ji, Lei Zhang, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative models have significantly facilitated the
development of personalized content creation. Given a small set of images with
user-specific concept, personalized image generation allows to create images
that incorporate the specified concept and adhere to provided text
descriptions. Due to its wide applications in content creation, significant
effort has been devoted to this field in recent years. Nonetheless, the
technologies used for personalization have evolved alongside the development of
generative models, with their distinct and interrelated components. In this
survey, we present a comprehensive review of generalized personalized image
generation across various generative models, including traditional GANs,
contemporary text-to-image diffusion models, and emerging multi-model
autoregressive models. We first define a unified framework that standardizes
the personalization process across different generative models, encompassing
three key components, i.e., inversion spaces, inversion methods, and
personalization schemes. This unified framework offers a structured approach to
dissecting and comparing personalization techniques across different generative
architectures. Building upon this unified framework, we further provide an
in-depth analysis of personalization techniques within each generative model,
highlighting their unique contributions and innovations. Through comparative
analysis, this survey elucidates the current landscape of personalized image
generation, identifying commonalities and distinguishing features among
existing methods. Finally, we discuss the open challenges in the field and
propose potential directions for future research. We keep tracing related works
at https://github.com/csyxwei/Awesome-Personalized-Image-Generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages; under submission; more information:
  https://github.com/csyxwei/Awesome-Personalized-Image-Generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L4P: Low-Level 4D Vision Perception Unified 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Badki, Hang Su, Bowen Wen, Orazio Gallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spatio-temporal relationship between the pixels of a video carries
critical information for low-level 4D perception. A single model that reasons
about it should be able to solve several such tasks well. Yet, most
state-of-the-art methods rely on architectures specialized for the task at
hand. We present L4P (pronounced "LAP"), a feedforward, general-purpose
architecture that solves low-level 4D perception tasks in a unified framework.
L4P combines a ViT-based backbone with per-task heads that are lightweight and
therefore do not require extensive training. Despite its general and
feedforward formulation, our method matches or surpasses the performance of
existing specialized methods on both dense tasks, such as depth or optical flow
estimation, and sparse tasks, such as 2D/3D tracking. Moreover, it solves all
those tasks at once in a time comparable to that of individual single-task
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye
  View for 3D Object Detection <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtong Yue, Zhiwei Lin, Xin Lin, Xiaoyu Zhou, Xiangtai Li, Lu Qi, Yongtao Wang, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent low-cost radar-camera approaches have shown promising results in
multi-modal 3D object detection, both sensors face challenges from
environmental and intrinsic disturbances. Poor lighting or adverse weather
conditions degrade camera performance, while radar suffers from noise and
positional ambiguity. Achieving robust radar-camera 3D object detection
requires consistent performance across varying conditions, a topic that has not
yet been fully explored. In this work, we first conduct a systematic analysis
of robustness in radar-camera detection on five kinds of noises and propose
RobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D
Gaussian Expansion (3DGE) module to mitigate inaccuracies in radar points,
including position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS
and velocity priors to generate a deformable kernel map and variance for kernel
size adjustment and value distribution. Additionally, we introduce a
weather-adaptive fusion module, which adaptively fuses radar and camera
features based on camera signal confidence. Extensive experiments on the
popular benchmark, nuScenes, show that our model achieves competitive results
in regular and noisy conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Fine-Tuning of Large Multimodal Models for Hateful Meme
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hateful memes have become a significant concern on the Internet,
necessitating robust automated detection systems. While large multimodal models
have shown strong generalization across various tasks, they exhibit poor
generalization to hateful meme detection due to the dynamic nature of memes
tied to emerging social trends and breaking news. Recent work further
highlights the limitations of conventional supervised fine-tuning for large
multimodal models in this context. To address these challenges, we propose
Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a
novel two-stage fine-tuning framework designed to improve both in-domain
accuracy and cross-domain generalization. Experimental results on six widely
used meme classification datasets demonstrate that LMM-RGCL achieves
state-of-the-art performance, outperforming agent-based systems such as
VPD-PALI-X-55B. Furthermore, our method effectively generalizes to
out-of-domain memes under low-resource settings, surpassing models like GPT-4o.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Power Grid Inspections with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diogo Lavado, Ricardo Santos, Andre Coelho, Joao Santos, Alessandra Micheletti, Claudia Soares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safety and reliability of power grids is critical as global
energy demands continue to rise. Traditional inspection methods, such as manual
observations or helicopter surveys, are resource-intensive and lack
scalability. This paper explores the use of 3D computer vision to automate
power grid inspections, utilizing the TS40K dataset -- a high-density,
annotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic
segmentation, our approach addresses challenges like class imbalance and noisy
data to enhance the detection of critical grid components such as power lines
and towers. The benchmark results indicate significant performance
improvements, with IoU scores reaching 95.53% for the detection of power lines
using transformer-based models. Our findings illustrate the potential for
integrating ML into grid maintenance workflows, increasing efficiency and
enabling proactive risk management strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Generation from Visual Sequences: Challenges and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to use natural language to talk about visual content is at the
core of human intelligence and a crucial feature of any artificial intelligence
system. Various studies have focused on generating text for single images. In
contrast, comparatively little attention has been paid to exhaustively
analyzing and advancing work on multiple-image vision-to-text settings. In this
position paper, we claim that any task dealing with temporally ordered
sequences of multiple images or frames is an instance of a broader, more
general problem involving the understanding of intricate relationships between
the visual content and the corresponding text. We comprehensively analyze five
tasks that are instances of this problem and argue that they pose a common set
of challenges and share similarities in terms of modeling and evaluation
approaches. Based on the insights from these various aspects and stages of
multi-image-to-text generation, we highlight several open questions and suggest
future research directions. We believe that these directions can advance the
understanding of complex phenomena in this domain and the development of better
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A deep learning framework for efficient pathology image analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Neidlinger, Tim Lenz, Sebastian Foersch, Chiara M. L. Loeffler, Jan Clusmann, Marco Gustav, Lawrence A. Shaktah, Rupert Langer, Bastian Dislich, Lisa A. Boardman, Amy J. French, Ellen L. Goode, Andrea Gsur, Stefanie Brezina, Marc J. Gunter, Robert Steinfelder, Hans-Michael Behrens, Christoph Röcken, Tabitha Harrison, Ulrike Peters, Amanda I. Phipps, Giuseppe Curigliano, Nicola Fusco, Antonio Marra, Michael Hoffmeister, Hermann Brenner, Jakob Nikolas Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has transformed digital pathology by enabling
biomarker prediction from high-resolution whole slide images (WSIs). However,
current methods are computationally inefficient, processing thousands of
redundant tiles per WSI and requiring complex aggregator models. We introduce
EAGLE (Efficient Approach for Guided Local Examination), a deep learning
framework that emulates pathologists by selectively analyzing informative
regions. EAGLE incorporates two foundation models: CHIEF for efficient tile
selection and Virchow2 for extracting high-quality features. Benchmarking was
conducted against leading slide- and tile-level foundation models across 31
tasks from four cancer types, spanning morphology, biomarker prediction and
prognosis. EAGLE outperformed state-of-the-art foundation models by up to 23%
and achieved the highest AUROC overall. It processed a slide in 2.27 seconds,
reducing computational time by more than 99% compared to existing models. This
efficiency enables real-time workflows, allows pathologists to validate all
tiles which are used by the model during analysis, and eliminates dependence on
high-performance computing, making AI-powered pathology more accessible. By
reliably identifying meaningful regions and minimizing artifacts, EAGLE
provides robust and interpretable outputs, supporting rapid slide searches,
integration into multi-omics pipelines and emerging clinical foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection and Geographic Localization of Natural Objects in the Wild: A
  Case Study on Palms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Cui, Rongkun Zhu, Manqi Wang, Wei Tang, Gregory D. Larsen, Victor P. Pauca, Sarra Alqahtani, Fan Yang, David Segurado, David Lutz, Jean-Michel Morel, Miles R. Silman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Palms are ecologically and economically indicators of tropical forest health,
biodiversity, and human impact that support local economies and global forest
product supply chains. While palm detection in plantations is well-studied,
efforts to map naturally occurring palms in dense forests remain limited by
overlapping crowns, uneven shading, and heterogeneous landscapes. We develop
PRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline
for detecting and localizing palms in dense tropical forests using large
orthomosaic images. Orthomosaics are created from thousands of aerial images
and spanning several to hundreds of gigabytes. Our contributions are threefold.
First, we construct a large UAV-derived orthomosaic dataset collected across 21
ecologically diverse sites in western Ecuador, annotated with 8,830 bounding
boxes and 5,026 palm center points. Second, we evaluate multiple
state-of-the-art object detectors based on efficiency and performance,
integrating zero-shot SAM 2 as the segmentation backbone, and refining the
results for precise geographic mapping. Third, we apply calibration methods to
align confidence scores with IoU and explore saliency maps for feature
explainability. Though optimized for palms, PRISM is adaptable for identifying
other natural objects, such as eastern white pines. Future work will explore
transfer learning for lower-resolution datasets (0.5 to 1m).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mean of Means: Human Localization with Calibration-free and
  Unconstrained Camera Settings (extended version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Zhang, Wengyu Zhang, Xulu Zhang, Jiaxin Wu, Xiao-Yong Wei, Jiannong Cao, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate human localization is crucial for various applications, especially
in the Metaverse era. Existing high precision solutions rely on expensive,
tag-dependent hardware, while vision-based methods offer a cheaper, tag-free
alternative. However, current vision solutions based on stereo vision face
limitations due to rigid perspective transformation principles and error
propagation in multi-stage SVD solvers. These solutions also require multiple
high-resolution cameras with strict setup constraints.To address these
limitations, we propose a probabilistic approach that considers all points on
the human body as observations generated by a distribution centered around the
body's geometric center. This enables us to improve sampling significantly,
increasing the number of samples for each point of interest from hundreds to
billions. By modeling the relation between the means of the distributions of
world coordinates and pixel coordinates, leveraging the Central Limit Theorem,
we ensure normality and facilitate the learning process. Experimental results
demonstrate human localization accuracy of 96\% within a 0.3$m$ range and
nearly 100\% accuracy within a 0.5$m$ range, achieved at a low cost of only 10
USD using two web cameras with a resolution of 640$\times$480 pixels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2407.20870</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHADeS: <span class="highlight-title">Self-supervised</span> Monocular Depth Estimation Through
  Non-Lam<span class="highlight-title">bert</span>ian Image Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rema Daher, Francisco Vasconcelos, Danail Stoyanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Visual 3D scene reconstruction can support colonoscopy navigation.
It can help in recognising which portions of the colon have been visualised and
characterising the size and shape of polyps. This is still a very challenging
problem due to complex illumination variations, including abundant specular
reflections. We investigate how to effectively decouple light and depth in this
problem.
  Methods: We introduce a self-supervised model that simultaneously
characterises the shape and lighting of the visualised colonoscopy scene. Our
model estimates shading, albedo, depth, and specularities (SHADeS) from single
images. Unlike previous approaches (IID), we use a non-Lambertian model that
treats specular reflections as a separate light component. The implementation
of our method is available at https://github.com/RemaDaher/SHADeS.
  Results: We demonstrate on real colonoscopy images (Hyper Kvasir) that
previous models for light decomposition (IID) and depth estimation (MonoVIT,
ModoDepth2) are negatively affected by specularities. In contrast, SHADeS can
simultaneously produce light decomposition and depth maps that are robust to
specular regions. We also perform a quantitative comparison on phantom data
(C3VD) where we further demonstrate the robustness of our model.
  Conclusion: Modelling specular reflections improves depth estimation in
colonoscopy. We propose an effective self-supervised approach that uses this
insight to jointly estimate light decomposition and depth. Light decomposition
has the potential to help with other problems, such as place recognition within
the colon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PartSDF: Part-Based Implicit Neural Representation for Composite 3D
  Shape Parametrization and Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Talabot, Olivier Clerc, Arda Cinar Demirtas, Doruk Oner, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D shape representation is essential in engineering applications
such as design, optimization, and simulation. In practice, engineering
workflows require structured, part-aware representations, as objects are
inherently designed as assemblies of distinct components. However, most
existing methods either model shapes holistically or decompose them without
predefined part structures, limiting their applicability in real-world design
tasks. We propose PartSDF, a supervised implicit representation framework that
explicitly models composite shapes with independent, controllable parts while
maintaining shape consistency. Despite its simple single-decoder architecture,
PartSDF outperforms both supervised and unsupervised baselines in
reconstruction and generation tasks. We further demonstrate its effectiveness
as a structured shape prior for engineering applications, enabling precise
control over individual components while preserving overall coherence. Code
available at https://github.com/cvlab-epfl/PartSDF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Level Moving Object Segmentation from a Single Image with
  Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexiong Wan, Bin Fan, Le Hui, Yuchao Dai, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moving object segmentation plays a crucial role in understanding dynamic
scenes involving multiple moving objects, while the difficulties lie in taking
into account both spatial texture structures and temporal motion cues. Existing
methods based on video frames encounter difficulties in distinguishing whether
pixel displacements of an object are caused by camera motion or object motion
due to the complexities of accurate image-based motion modeling. Recent
advances exploit the motion sensitivity of novel event cameras to counter
conventional images' inadequate motion modeling capabilities, but instead lead
to challenges in segmenting pixel-level object masks due to the lack of dense
texture structures in events. To address these two limitations imposed by
unimodal settings, we propose the first instance-level moving object
segmentation framework that integrates complementary texture and motion cues.
Our model incorporates implicit cross-modal masked attention augmentation,
explicit contrastive feature learning, and flow-guided motion enhancement to
exploit dense texture information from a single image and rich motion
information from events, respectively. By leveraging the augmented texture and
motion features, we separate mask segmentation from motion classification to
handle varying numbers of independently moving objects. Through extensive
evaluations on multiple datasets, as well as ablation experiments with
different input settings and real-time efficiency analysis of the proposed
framework, we believe that our first attempt to incorporate image and event
data for practical deployment can provide new insights for future work in
event-based motion related works. The source code with model training and
pre-trained weights is released at https://npucvr.github.io/EvInsMOS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IJCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for
  Improved Text-Based Learning for LGE Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athira J Jacob, Puneet Sharma, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of hyperenhancement from cardiac LGE MRI images is a complex task
requiring significant clinical expertise. Although deep learning-based models
have shown promising results for the task, they require large amounts of data
with fine-grained annotations. Clinical reports generated for cardiac MR
studies contain rich, clinically relevant information, including the location,
extent and etiology of any scars present. Although recently developed
CLIP-based training enables pretraining models with image-text pairs, it
requires large amounts of data and further finetuning strategies on downstream
tasks. In this study, we use various strategies rooted in domain knowledge to
train a model for LGE detection solely using text from clinical reports, on a
relatively small clinical cohort of 965 patients. We improve performance
through the use of synthetic data augmentation, by systematically creating scar
images and associated text. In addition, we standardize the orientation of the
images in an anatomy-informed way to enable better alignment of spatial and
text features. We also use a captioning loss to enable fine-grained supervision
and explore the effect of pretraining of the vision encoder on performance.
Finally, ablation studies are carried out to elucidate the contributions of
each design component to the overall performance of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Poster at Workshop on Large Language Models and Generative AI for
  Health at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrast-Unity for Partially-Supervised Temporal Sentence Grounding <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haicheng Wang, Chen Ju, Weixiong Lin, Chaofan Ma, Shuai Xiao, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding aims to detect event timestamps described by the
natural language query from given untrimmed videos. The existing
fully-supervised setting achieves great results but requires expensive
annotation costs; while the weakly-supervised setting adopts cheap labels but
performs poorly. To pursue high performance with less annotation costs, this
paper introduces an intermediate partially-supervised setting, i.e., only
short-clip is available during training. To make full use of partial labels, we
specially design one contrast-unity framework, with the two-stage goal of
implicit-explicit progressive grounding. In the implicit stage, we align
event-query representations at fine granularity using comprehensive quadruple
contrastive learning: event-query gather, event-background separation,
intra-cluster compactness and inter-cluster separability. Then, high-quality
representations bring acceptable grounding pseudo-labels. In the explicit
stage, to explicitly optimize grounding objectives, we train one
fully-supervised model using obtained pseudo-labels for grounding refinement
and denoising. Extensive experiments and thoroughly ablations on Charades-STA
and ActivityNet Captions demonstrate the significance of partial supervision,
as well as our superior performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025.The first two authors share the same
  contribution. arXiv admin note: text overlap with arXiv:2302.09850</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, Jingyi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering high-quality 3D scenes from a single RGB image is a challenging
task in computer graphics. Current methods often struggle with domain-specific
limitations or low-quality object generation. To address these, we propose CAST
(Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel
method for 3D scene reconstruction and recovery. CAST starts by extracting
object-level 2D segmentation and relative depth information from the input
image, followed by using a GPT-based model to analyze inter-object spatial
relationships. This enables the understanding of how objects relate to each
other within the scene, ensuring more coherent reconstruction. CAST then
employs an occlusion-aware large-scale 3D generation model to independently
generate each object's full geometry, using MAE and point cloud conditioning to
mitigate the effects of occlusions and partial object information, ensuring
accurate alignment with the source image's geometry and texture. To align each
object with the scene, the alignment generation model computes the necessary
transformations, allowing the generated meshes to be accurately placed and
integrated into the scene's point cloud. Finally, CAST incorporates a
physics-aware correction step that leverages a fine-grained relation graph to
generate a constraint graph. This graph guides the optimization of object
poses, ensuring physical consistency and spatial coherence. By utilizing Signed
Distance Fields (SDF), the model effectively addresses issues such as
occlusions, object penetration, and floating objects, ensuring that the
generated scene accurately reflects real-world physical interactions. CAST can
be leveraged in robotics, enabling efficient real-to-simulation workflows and
providing realistic, scalable simulation environments for robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sites.google.com/view/cast4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept
  Extraction in Large Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Fel, Ekdeep Singh Lubana, Jacob S. Prince, Matthew Kowal, Victor Boutin, Isabel Papadimitriou, Binxu Wang, Martin Wattenberg, Demba Ba, Talia Konkle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine
learning interpretability, enabling the unsupervised decomposition of model
representations into a dictionary of abstract, human-interpretable concepts.
However, we reveal a fundamental limitation: existing SAEs exhibit severe
instability, as identical models trained on similar datasets can produce
sharply different dictionaries, undermining their reliability as an
interpretability tool. To address this issue, we draw inspiration from the
Archetypal Analysis framework introduced by Cutler & Breiman (1994) and present
Archetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the convex
hull of data. This geometric anchoring significantly enhances the stability of
inferred dictionaries, and their mildly relaxed variants RA-SAEs further match
state-of-the-art reconstruction abilities. To rigorously assess dictionary
quality learned by SAEs, we introduce two new benchmarks that test (i)
plausibility, if dictionaries recover "true" classification directions and (ii)
identifiability, if dictionaries disentangle synthetic concept mixtures. Across
all evaluations, RA-SAEs consistently yield more structured representations
while uncovering novel, semantically meaningful concepts in large-scale vision
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Experimental Study of SOTA LiDAR Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bike Chen, Antti Tikanmäki, Juha Röning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud segmentation (PCS) is to classify each point in point clouds. The
task enables robots to parse their 3D surroundings and run autonomously.
According to different point cloud representations, existing PCS models can be
roughly divided into point-, voxel-, and range image-based models. However, no
work has been found to report comprehensive comparisons among the
state-of-the-art point-, voxel-, and range image-based models from an
application perspective, bringing difficulty in utilizing these models for
real-world scenarios. In this paper, we provide thorough comparisons among the
models by considering the LiDAR data motion compensation and the metrics of
model parameters, max GPU memory allocated during testing, inference latency,
frames per second, intersection-over-union (IoU) and mean IoU (mIoU) scores.
The experimental results benefit engineers when choosing a reasonable PCS model
for an application and inspire researchers in the PCS field to design more
practical models for a real-world scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>No comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Intermediate Representations for Better Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Guglielmo, Marc Masana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, machine learning models must reliably detect
Out-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD
detection methods often rely on analyzing the logits or the embeddings of the
penultimate layer of a neural network. However, little work has been conducted
on the exploitation of the rich information encoded in intermediate layers. To
address this, we analyze the discriminative power of intermediate layers and
show that they can positively be used for OoD detection. Therefore, we propose
to regularize intermediate layers with an energy-based contrastive loss, and by
grouping multiple layers in a single aggregated response. We demonstrate that
intermediate layer activations improves OoD detection performance by running a
comprehensive evaluation across multiple datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/gigug/LIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hinrich Rahlfs, Markus Hüllebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Enabling a comprehensive and robust assessment of carotid artery
plaques in 3D through extraction and visualization of quantitative plaque
parameters. These parameters have potential applications in stroke risk
analysis, evaluation of therapy effectiveness, and plaque progression
prediction. Methods: We propose a novel method for extracting a plaque mesh
from 3D vessel wall segmentation using distance encoding on the inner and outer
wall mesh for precise plaque structure analysis. A case-specific threshold,
derived from the normal vessel wall thickness, was applied to extract plaques
from a dataset of 202 T1-weighted black-blood MRI scans of subjects with up to
50% stenosis. Applied to baseline and one-year follow-up data, the method
supports detailed plaque morphology analysis over time, including plaque volume
quantification, aided by improved visualization via mesh unfolding. Results: We
successfully extracted plaque meshes from 341 carotid arteries, capturing a
wide range of plaque shapes with volumes ranging from 2.69{\mu}l to
847.7{\mu}l. The use of a case-specific threshold effectively eliminated false
positives in young, healthy subjects. Conclusion: The proposed method enables
precise extraction of plaque meshes from 3D vessel wall segmentation masks
enabling a correspondence between baseline and one-year follow-up examinations.
Unfolding the plaque meshes enhances visualization, while the mesh-based
analysis allows quantification of plaque parameters independent of voxel
resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 Figures, Submitted to the International Journal of
  Computer Assisted Radiology and Surgery</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hinrich Rahlfs, Markus Hüllebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach that uses sparse annotations from clinical
studies to train a 3D segmentation of the carotid artery wall. We use a
centerline annotation to sample perpendicular cross-sections of the carotid
artery and use an adversarial 2D network to segment them. These annotations are
then transformed into 3D pseudo-labels for training of a 3D convolutional
neural network, circumventing the creation of manual 3D masks. For pseudo-label
creation in the bifurcation area we propose the use of cross-sections
perpendicular to the bifurcation axis and show that this enhances segmentation
performance. Different sampling distances had a lesser impact. The proposed
method allows for efficient training of 3D segmentation, offering potential
improvements in the assessment of carotid artery stenosis and allowing the
extraction of 3D biomarkers such as plaque volume.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at MICAD 2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Text-Image Interleaved Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Ziqi Dai, Yongqi Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Jun Yu, Wenjie Li, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal information retrieval studies mainly focus on single-image
inputs, which limits real-world applications involving multiple images and
text-image interleaved content. In this work, we introduce the text-image
interleaved retrieval (TIIR) task, where the query and document are interleaved
text-image sequences, and the model is required to understand the semantics
from the interleaved context for effective retrieval. We construct a TIIR
benchmark based on naturally interleaved wikiHow tutorials, where a specific
pipeline is designed to generate interleaved queries. To explore the task, we
adapt several off-the-shelf retrievers and build a dense baseline by
interleaved multimodal large language model (MLLM). We then propose a novel
Matryoshka Multimodal Embedder (MME), which compresses the number of visual
tokens at different granularity, to address the challenge of excessive visual
tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption
of existing models does not consistently yield effective results. Our MME
achieves significant improvements over the baseline by substantially fewer
visual tokens. We provide extensive analysis and will release the dataset and
code to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAPID: Retrieval Augmented Training of Differentially Private Diffusion
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanqiu Jiang, Changjiang Li, Fenglong Ma, Ting Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private diffusion models (DPDMs) harness the remarkable
generative capabilities of diffusion models while enforcing differential
privacy (DP) for sensitive data. However, existing DPDM training approaches
often suffer from significant utility loss, large memory footprint, and
expensive inference cost, impeding their practical uses. To overcome such
limitations, we present RAPID: Retrieval Augmented PrIvate Diffusion model, a
novel approach that integrates retrieval augmented generation (RAG) into DPDM
training. Specifically, RAPID leverages available public data to build a
knowledge base of sample trajectories; when training the diffusion model on
private data, RAPID computes the early sampling steps as queries, retrieves
similar trajectories from the knowledge base as surrogates, and focuses on
training the later sampling steps in a differentially private manner. Extensive
evaluation using benchmark datasets and models demonstrates that, with the same
privacy guarantee, RAPID significantly outperforms state-of-the-art approaches
by large margins in generative quality, memory footprint, and inference cost,
suggesting that retrieval-augmented DP training represents a promising
direction for developing future privacy-preserving generative models. The code
is available at: https://github.com/TanqiuJiang/RAPID
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Timesteps: A Novel Activation-wise Membrane Potential Propagation
  Mechanism for Spiking Neural Networks in 3D cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Song, Boxuan Zheng, Xiangfei Yang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the similar characteristics between event-based visual data and point
clouds, recent studies have emerged that treat event data as event clouds to
learn based on point cloud analysis. Additionally, some works approach point
clouds from the perspective of event vision, employing Spiking Neural Network
(SNN) due to their asynchronous nature. However, these contributions are often
domain-specific, making it difficult to extend their applicability to other
intersecting fields. Moreover, while SNN-based visual tasks have seen
significant growth, the conventional timestep-wise iterative activation
strategy largely limits their real-world applications by large timesteps,
resulting in significant delays and increased computational costs. Although
some innovative methods achieve good performance with short timesteps (<10),
few have fundamentally restructured the update strategy of spiking neurons to
completely overcome the limitations of timesteps. In response to these
concerns, we propose a novel and general activation strategy for spiking
neurons called Activation-wise Membrane Potential Propagation (AMP2). This
approach extends the concept of timesteps from a manually crafted parameter
within the activation function to any existing network structure. In
experiments on common point cloud tasks (classification, object, and scene
segmentation) and event cloud tasks (action recognition), we found that AMP2
stabilizes SNN training, maintains competitive performance, and reduces latency
compared to the traditional timestep-wise activation paradigm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, Christopher Schroers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in Novel View Synthesis (NVS), generating
high-fidelity views from single or sparse observations remains a significant
challenge. Existing splatting-based approaches often produce distorted geometry
due to splatting errors. While diffusion-based methods leverage rich 3D priors
to achieve improved geometry, they often suffer from texture hallucination. In
this paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion
model designed to synthesize high-fidelity novel views from a single image.
Specifically, we propose an aligned synthesis strategy for precise control of
target viewpoints and geometry-consistent view synthesis. To mitigate texture
hallucination, we design a texture bridge module that enables high-fidelity
texture generation through adaptive feature fusion. In this manner, SplatDiff
leverages the strengths of splatting and diffusion to generate novel views with
consistent geometry and high-fidelity details. Extensive experiments verify the
state-of-the-art performance of SplatDiff in single-view NVS. Additionally,
without extra training, SplatDiff shows remarkable zero-shot performance across
diverse tasks, including sparse-view NVS and stereo video conversion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from
  Cortical Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Bongratz, Yitong Li, Sama Elbaroudy, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in medical image generation, existing methods
struggle to produce anatomically plausible 3D structures. In synthetic brain
magnetic resonance images (MRIs), characteristic fissures are often missing,
and reconstructed cortical surfaces appear scattered rather than densely
convoluted. To address this issue, we introduce Cor2Vox, the first diffusion
model-based method that translates continuous cortical shape priors to
synthetic brain MRIs. To achieve this, we leverage a Brownian bridge process
which allows for direct structured mapping between shape contours and medical
images. Specifically, we adapt the concept of the Brownian bridge diffusion
model to 3D and extend it to embrace various complementary shape
representations. Our experiments demonstrate significant improvements in the
geometric accuracy of reconstructed structures compared to previous voxel-based
approaches. Moreover, Cor2Vox excels in image quality and diversity, yielding
high variation in non-target structures like the skull. Finally, we highlight
the capability of our approach to simulate cortical atrophy at the sub-voxel
level. Our code is available at https://github.com/ai-med/Cor2Vox.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Information Processing in Medical Imaging (IPMI) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaiya Vaibhaw Kumar, Deepti Rawat, Tanvi Kandalla, Aarnav Nagariya, Kavita Vemuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the myEye2Wheeler dataset, a unique resource of
real-world gaze behaviour of two-wheeler drivers navigating complex Indian
traffic. Most datasets are from four-wheeler drivers on well-planned roads and
homogeneous traffic. Our dataset offers a critical lens into the unique visual
attention patterns and insights into the decision-making of Indian two-wheeler
drivers. The analysis demonstrates that existing saliency models, like
TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to
when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),
highlighting the need for models specifically tailored to the traffic
conditions. By introducing the dataset, we not only fill a significant gap in
two-wheeler driver behaviour research in India but also emphasise the critical
need for developing context-specific saliency models. The larger aim is to
improve road safety for two-wheeler users and lane-planning to support a
cost-effective mode of transport.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Propagation for Echocardiography Clinical Metric Estimation
  via Contour Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thierry Judge, Olivier Bernard, Woo-Jin Cho Kim, Alberto Gomez, Arian Beqiri, Agisilaos Chartsias, Pierre-Marc Jodoin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Echocardiography plays a fundamental role in the extraction of important
clinical parameters (e.g. left ventricular volume and ejection fraction)
required to determine the presence and severity of heart-related conditions.
When deploying automated techniques for computing these parameters, uncertainty
estimation is crucial for assessing their utility. Since clinical parameters
are usually derived from segmentation maps, there is no clear path for
converting pixel-wise uncertainty values into uncertainty estimates in the
downstream clinical metric calculation. In this work, we propose a novel
uncertainty estimation method based on contouring rather than segmentation. Our
method explicitly predicts contour location uncertainty from which contour
samples can be drawn. Finally, the sampled contours can be used to propagate
uncertainty to clinical metrics. Our proposed method not only provides accurate
uncertainty estimations for the task of contouring but also for the downstream
clinical metrics on two cardiac ultrasound datasets. Code is available at:
https://github.com/ThierryJudge/contouring-uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, submitted to IEEE TMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Data Aware Neural Architecture Search via Supernet Accelerated
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emil Njor, Colby Banbury, Xenofon Fafoutis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tiny machine learning (TinyML) promises to revolutionize fields such as
healthcare, environmental monitoring, and industrial maintenance by running
machine learning models on low-power embedded systems. However, the complex
optimizations required for successful TinyML deployment continue to impede its
widespread adoption. A promising route to simplifying TinyML is through
automatic machine learning (AutoML), which can distill elaborate optimization
workflows into accessible key decisions. Notably, Hardware Aware Neural
Architecture Searches - where a computer searches for an optimal TinyML model
based on predictive performance and hardware metrics - have gained significant
traction, producing some of today's most widely used TinyML models.
Nevertheless, limiting optimization solely to neural network architectures can
prove insufficient. Because TinyML systems must operate under extremely tight
resource constraints, the choice of input data configuration, such as
resolution or sampling rate, also profoundly impacts overall system efficiency.
Achieving truly optimal TinyML systems thus requires jointly tuning both input
data and model architecture. Despite its importance, this "Data Aware Neural
Architecture Search" remains underexplored. To address this gap, we propose a
new state-of-the-art Data Aware Neural Architecture Search technique and
demonstrate its effectiveness on the novel TinyML ``Wake Vision'' dataset. Our
experiments show that across varying time and hardware constraints, Data Aware
Neural Architecture Search consistently discovers superior TinyML systems
compared to purely architecture-focused methods, underscoring the critical role
of data-aware optimization in advancing TinyML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking Vision <span class="highlight-title">Transformer</span> with Saccadic Attention <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Malu Zhang, Dehao Zhang, Ammar Belatreche, Yichen Xiao, Yu Liang, Yimeng Shan, Qian Sun, Enqi Zhang, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of Spiking Neural Networks (SNNs) and Vision Transformers
(ViTs) holds potential for achieving both energy efficiency and high
performance, particularly suitable for edge vision applications. However, a
significant performance gap still exists between SNN-based ViTs and their ANN
counterparts. Here, we first analyze why SNN-based ViTs suffer from limited
performance and identify a mismatch between the vanilla self-attention
mechanism and spatio-temporal spike trains. This mismatch results in degraded
spatial relevance and limited temporal interactions. To address these issues,
we draw inspiration from biological saccadic attention mechanisms and introduce
an innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the
spatial domain, SSSA employs a novel spike distribution-based method to
effectively assess the relevance between Query and Key pairs in SNN-based ViTs.
Temporally, SSSA employs a saccadic interaction module that dynamically focuses
on selected visual areas at each timestep and significantly enhances whole
scene understanding through temporal interactions. Building on the SSSA
mechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive
experiments across various visual tasks demonstrate that SNN-ViT achieves
state-of-the-art performance with linear computational complexity. The
effectiveness and efficiency of the SNN-ViT highlight its potential for
power-critical edge vision applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by
  NeRFs Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quoc-Anh Bui, Gilles Rougeron, Géraldine Morin, Simone Gasparini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate 3D reconstruction is essential for applications in
cultural heritage. This study addresses the challenge of visualizing objects
within large-scale scenes at a high level of detail (LOD) using Neural Radiance
Fields (NeRFs). The aim is to improve the visual fidelity of chosen objects
while maintaining the efficiency of the computations by focusing on details
only for relevant content. The proposed ROI-NeRFs framework divides the scene
into a Scene NeRF, which represents the overall scene at moderate detail, and
multiple ROI NeRFs that focus on user-defined objects of interest. An
object-focused camera selection module automatically groups relevant cameras
for each NeRF training during the decomposition phase. In the composition
phase, a Ray-level Compositional Rendering technique combines information from
the Scene NeRF and ROI NeRFs, allowing simultaneous multi-object rendering
composition. Quantitative and qualitative experiments conducted on two
real-world datasets, including one on a complex eighteen's century cultural
heritage room, demonstrate superior performance compared to baseline methods,
improving LOD for object regions, minimizing artifacts, and without
significantly increasing inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages including appendix, 16 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecDreamer: Consistent Text-to-3D Generation via Uniform Score
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Zheng, Yihong Lin, Bangzhen Liu, Xuemiao Xu, Yongwei Nie, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current text-to-3D generation methods based on score distillation often
suffer from geometric inconsistencies, leading to repeated patterns across
different poses of 3D assets. This issue, known as the Multi-Face Janus
problem, arises because existing methods struggle to maintain consistency
across varying poses and are biased toward a canonical pose. While recent work
has improved pose control and approximation, these efforts are still limited by
this inherent bias, which skews the guidance during generation. To address
this, we propose a solution called RecDreamer, which reshapes the underlying
data distribution to achieve a more consistent pose representation. The core
idea behind our method is to rectify the prior distribution, ensuring that pose
variation is uniformly distributed rather than biased toward a canonical form.
By modifying the prescribed distribution through an auxiliary function, we can
reconstruct the density of the distribution to ensure compliance with specific
marginal constraints. In particular, we ensure that the marginal distribution
of poses follows a uniform distribution, thereby eliminating the biases
introduced by the prior knowledge. We incorporate this rectified data
distribution into existing score distillation algorithms, a process we refer to
as uniform score distillation. To efficiently compute the posterior
distribution required for the auxiliary function, RecDreamer introduces a
training-free classifier that estimates pose categories in a plug-and-play
manner. Additionally, we utilize various approximation techniques for noisy
states, significantly improving system performance. Our experimental results
demonstrate that RecDreamer effectively mitigates the Multi-Face Janus problem,
leading to more consistent 3D asset generation across different poses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in
  Visual Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Gou, Hansi Yang, Zhili Liu, Kai Chen, Yihan Zeng, Lanqing Hong, Zhenguo Li, Qun Liu, James T. Kwok, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Instruction Tuning (VIT) enhances Multimodal Large Language Models
(MLLMs) but it is hindered by corrupted datasets containing hallucinated
content, incorrect responses, and poor OCR quality. While prior works focus on
dataset refinement through high-quality data collection or rule-based
filtering, they are costly or limited to specific types of corruption. To
deeply understand how corrupted data affects MLLMs, in this paper, we
systematically investigate this issue and find that while corrupted data
degrades the performance of MLLMs, its effects are largely superficial in that
the performance of MLLMs can be largely restored by either disabling a small
subset of parameters or post-training with a small amount of clean data.
Additionally, corrupted MLLMs exhibit improved ability to distinguish clean
samples from corrupted ones, enabling the dataset cleaning without external
help. Based on those insights, we propose a corruption-robust training paradigm
combining self-validation and post-training, which significantly outperforms
existing corruption mitigation strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MALT Diffusion: Memory-Augmented Latent <span class="highlight-title">Transformer</span>s for Any-Length
  Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihyun Yu, Meera Hahn, Dan Kondratyuk, Jinwoo Shin, Agrim Gupta, José Lezama, Irfan Essa, David Ross, Jonathan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are successful for synthesizing high-quality videos but are
limited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained
footage (e.g. over minutes) still remains an open research question. In this
paper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers),
a new diffusion model specialized for long video generation. MALT Diffusion (or
just MALT) handles long videos by subdividing them into short segments and
doing segment-level autoregressive generation. To achieve this, we first
propose recurrent attention layers that encode multiple segments into a compact
memory latent vector; by maintaining this memory vector over time, MALT is able
to condition on it and continuously generate new footage based on a long
temporal context. We also present several training techniques that enable the
model to generate frames over a long horizon with consistent quality and
minimal degradation. We validate the effectiveness of MALT through experiments
on long video benchmarks. We first perform extensive analysis of MALT in
long-contextual understanding capability and stability using popular long video
benchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video
generation on UCF-101, outperforming the previous state-of-the-art of 648.4.
Finally, we explore MALT's capabilities in a text-to-video generation setting
and show that it can produce long videos compared with recent techniques for
long text-to-video generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint. 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAMamba: Vision State Space Model with Dynamic Adaptive Scan 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanzhe Li, Caoshuo Li, Jiayi Lyu, Hongjuan Pei, Baochang Zhang, Taisong Jin, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State space models (SSMs) have recently garnered significant attention in
computer vision. However, due to the unique characteristics of image data,
adapting SSMs from natural language processing to computer vision has not
outperformed the state-of-the-art convolutional neural networks (CNNs) and
Vision Transformers (ViTs). Existing vision SSMs primarily leverage manually
designed scans to flatten image patches into sequences locally or globally.
This approach disrupts the original semantic spatial adjacency of the image and
lacks flexibility, making it difficult to capture complex image structures. To
address this limitation, we propose Dynamic Adaptive Scan (DAS), a data-driven
method that adaptively allocates scanning orders and regions. This enables more
flexible modeling capabilities while maintaining linear computational
complexity and global modeling capacity. Based on DAS, we further propose the
vision backbone DAMamba, which significantly outperforms current
state-of-the-art vision Mamba models in vision tasks such as image
classification, object detection, instance segmentation, and semantic
segmentation. Notably, it surpasses some of the latest state-of-the-art CNNs
and ViTs. Code will be available at https://github.com/ltzovo/DAMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S2C: Learning Noise-Resistant Differences for Unsupervised Change
  Detection in Multimodal Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Ding, Xibing Zuo, Danfeng Hong, Haitao Guo, Jun Lu, Zhihui Gong, Lorenzo Bruzzone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Change Detection (UCD) in multimodal Remote Sensing (RS) images
remains a difficult challenge due to the inherent spatio-temporal complexity
within data, and the heterogeneity arising from different imaging sensors.
Inspired by recent advancements in Visual Foundation Models (VFMs) and
Contrastive Learning (CL) methodologies, this research aims to develop CL
methodologies to translate implicit knowledge in VFM into change
representations, thus eliminating the need for explicit supervision. To this
end, we introduce a Semantic-to-Change (S2C) learning framework for UCD in both
homogeneous and multimodal RS images. Differently from existing CL
methodologies that typically focus on learning multi-temporal similarities, we
introduce a novel triplet learning strategy that explicitly models temporal
differences, which are crucial to the CD task. Furthermore, random spatial and
spectral perturbations are introduced during the training to enhance robustness
to temporal noise. In addition, a grid sparsity regularization is defined to
suppress insignificant changes, and an IoU-matching algorithm is developed to
refine the CD results. Experiments on four benchmark CD datasets demonstrate
that the proposed S2C learning framework achieves significant improvements in
accuracy, surpassing current state-of-the-art by over 31\%, 9\%, 23\%, and
15\%, respectively. It also demonstrates robustness and sample efficiency,
suitable for training and adaptation of various Visual Foundation Models (VFMs)
or backbone neural networks. The relevant code will be available at:
github.com/DingLei14/S2C.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Generalization Problem of Low-level Vision Models Through
  the Lens of Image Deraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfan Hu, Zhiyuan You, Jinjin Gu, Kaiwen Zhu, Tianfan Xue, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization remains a significant challenge for low-level vision models,
which often struggle with unseen degradations in real-world scenarios despite
their success in controlled benchmarks. In this paper, we revisit the
generalization problem in low-level vision models. Image deraining is selected
as a case study due to its well-defined and easily decoupled structure,
allowing for more effective observation and analysis. Through comprehensive
experiments, we reveal that the generalization issue is not primarily due to
limited network capacity but rather the failure of existing training
strategies, which leads networks to overfit specific degradation patterns. Our
findings show that guiding networks to focus on learning the underlying image
content, rather than the degradation patterns, is key to improving
generalization. We demonstrate that balancing the complexity of background
images and degradations in the training data helps networks better fit the
image distribution. Furthermore, incorporating content priors from pre-trained
generative models significantly enhances generalization. Experiments on both
image deraining and image denoising validate the proposed strategies. We
believe the insights and solutions will inspire further research and improve
the generalization of low-level vision models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2305.15134</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CutPaste&Find: Efficient Multimodal Hallucination Detector with
  Visual-aid Knowledge Base 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong-Duy Nguyen, Xiaobao Wu, Duc Anh Vu, Shuai Zhao, Thong Nguyen, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal
reasoning capabilities, but they remain susceptible to hallucination,
particularly object hallucination where non-existent objects or incorrect
attributes are fabricated in generated descriptions. Existing detection methods
achieve strong performance but rely heavily on expensive API calls and
iterative LVLM-based validation, making them impractical for large-scale or
offline use. To address these limitations, we propose CutPaste\&Find, a
lightweight and training-free framework for detecting hallucinations in
LVLM-generated outputs. Our approach leverages off-the-shelf visual and
linguistic modules to perform multi-step verification efficiently without
requiring LVLM inference. At the core of our framework is a Visual-aid
Knowledge Base that encodes rich entity-attribute relationships and associated
image representations. We introduce a scaling factor to refine similarity
scores, mitigating the issue of suboptimal alignment values even for
ground-truth image-text pairs. Comprehensive evaluations on benchmark datasets,
including POPE and R-Bench, demonstrate that CutPaste\&Find achieves
competitive hallucination detection performance while being significantly more
efficient and cost-effective than previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juefeng Xiao, Tianqi Xiang, Zhigang Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world action recognition systems, incorporating more attributes helps
achieve a more comprehensive understanding of human behavior. However, using a
single model to simultaneously recognize multiple attributes can lead to a
decrease in accuracy. In this work, we propose a novel method i.e. Adaptive
Attribute Prototype Model (AAPM) for human action recognition, which captures
rich action-relevant attribute information and strikes a balance between
accuracy and robustness. Firstly, we introduce the Text-Constrain Module (TCM)
to incorporate textual information from potential labels, and constrain the
construction of different attributes prototype representations. In addition, we
explore the Attribute Assignment Method (AAM) to address the issue of training
bias and increase robustness during the training process.Furthermore, we
construct a new video dataset with attribute-based multi-label called
Multi-Kinetics for evaluation, which contains various attribute labels (e.g.
action, scene, object, etc.) related to human behavior. Extensive experiments
demonstrate that our AAPM achieves the state-of-the-art performance in both
attribute-based multi-label few-shot action recognition and single-label
few-shot action recognition. The project and dataset are available at an
anonymous account https://github.com/theAAPM/AAPM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Fu, Guo-Hua Wang, Liangfu Cao, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as a dominant approach for text-to-image
generation. Key components such as the human preference alignment and
classifier-free guidance play a crucial role in ensuring generation quality.
However, their independent application in current text-to-image models
continues to face significant challenges in achieving strong text-image
alignment, high generation quality, and consistency with human aesthetic
standards. In this work, we for the first time, explore facilitating the
collaboration of human performance alignment and test-time sampling to unlock
the potential of text-to-image models. Consequently, we introduce CHATS
(Combining Human-Aligned optimization and Test-time Sampling), a novel
generative framework that separately models the preferred and dispreferred
distributions and employs a proxy-prompt-based sampling strategy to utilize the
useful information contained in both distributions. We observe that CHATS
exhibits exceptional data efficiency, achieving strong performance with only a
small, high-quality funetuning dataset. Extensive experiments demonstrate that
CHATS surpasses traditional preference alignment methods, setting new
state-of-the-art across various standard benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GVTNet: Graph Vision <span class="highlight-title">Transformer</span> For Face Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yang, Yong Fan, Cheng Lu, Minghao Yuan, Zhijing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in face super-resolution research have utilized the
Transformer architecture. This method processes the input image into a series
of small patches. However, because of the strong correlation between different
facial components in facial images. When it comes to super-resolution of
low-resolution images, existing algorithms cannot handle the relationships
between patches well, resulting in distorted facial components in the
super-resolution results. To solve the problem, we propose a transformer
architecture based on graph neural networks called graph vision transformer
network. We treat each patch as a graph node and establish an adjacency matrix
based on the information between patches. In this way, the patch only interacts
between neighboring patches, further processing the relationship of facial
components. Quantitative and visualization experiments have underscored the
superiority of our algorithm over state-of-the-art techniques. Through detailed
comparisons, we have demonstrated that our algorithm possesses more advanced
super-resolution capabilities, particularly in enhancing facial components. The
PyTorch code is available at https://github.com/continueyang/GVTNet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeltaDiff: A Residual-Guided Diffusion Model for Enhanced Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yang, Yong Fan, Cheng Lu, Zhijing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the application of diffusion models in super-resolution tasks has
become a popular research direction. Existing work is focused on fully
migrating diffusion models to SR tasks. The diffusion model is proposed in the
field of image generation, so in order to make the generated results diverse,
the diffusion model combines random Gaussian noise and distributed sampling to
increase the randomness of the model.
  However, the essence of super-resolution tasks requires the model to generate
high-resolution images with fidelity. Excessive addition of random factors can
result in the model generating detailed information that does not belong to the
HR image. To address this issue, we propose a new diffusion model called
Deltadiff, which uses only residuals between images for diffusion, making the
entire diffusion process more stable. The experimental results show that our
method surpasses state-of-the-art models and generates results with better
fidelity. Our code and model are publicly available at
https://github.com/continueyang/DeltaDiff
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment
  Retrieval Within Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaying Yuan, Jian Ni, Yueze Wang, Junjie Zhou, Zhengyang Liang, Zheng Liu, Zhao Cao, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) holds great promise in addressing
challenges associated with long video understanding. These methods retrieve
useful moments from long videos for their presented tasks, thereby enabling
multimodal large language models (MLLMs) to generate high-quality answers in a
cost-effective way. In this work, we present MomentSeeker, a comprehensive
benchmark to evaluate retrieval models' performance in handling general
long-video moment retrieval (LVMR) tasks. MomentSeeker offers three key
advantages. First, it incorporates long videos of over 500 seconds on average,
making it the first benchmark specialized for long-video moment retrieval.
Second, it covers a wide range of task categories (including Moment Search,
Caption Alignment, Image-conditioned Moment Search, and Video-conditioned
Moment Search) and diverse application scenarios (e.g., sports, movies,
cartoons, and ego), making it a comprehensive tool for assessing retrieval
models' general LVMR performance. Additionally, the evaluation tasks are
carefully curated through human annotation, ensuring the reliability of
assessment. We further fine-tune an MLLM-based LVMR retriever on synthetic
data, which demonstrates strong performance on our benchmark. We perform
extensive experiments with various popular multimodal retrievers based on our
benchmark, whose results highlight the challenges of LVMR and limitations for
existing methods. Our created resources will be shared with community to
advance future research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporal Multi-Camera Calibration using Freely Moving People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang-Eun Lee, Ko Nishino, Shohei Nobuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method for spatiotemporal multi-camera calibration using
freely moving people in multiview videos. Since calibrating multiple cameras
and finding matches across their views are inherently interdependent,
performing both in a unified framework poses a significant challenge. We
address these issues as a single registration problem of matching two sets of
3D points, leveraging human motion in dynamic multi-person scenes. To this end,
we utilize 3D human poses obtained from an off-the-shelf monocular 3D human
pose estimator and transform them into 3D points on a unit sphere, to solve the
rotation, time offset, and the association alternatingly. We employ a
probabilistic approach that can jointly solve both problems of aligning
spatiotemporal data and establishing correspondences through soft assignment
between two views. The translation is determined by applying coplanarity
constraints. The pairwise registration results are integrated into a multiview
setup, and then a nonlinear optimization method is used to improve the accuracy
of the camera poses, temporal offsets, and multi-person associations. Extensive
experiments on synthetic and real data demonstrate the effectiveness and
flexibility of the proposed method as a practical marker-free calibration tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with
  360$^\circ$ Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongki Jung, Jaehoon Choi, Yonghan Lee, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel 3D reconstruction pipeline for 360$^\circ$ cameras for 3D
mapping and rendering of indoor environments. Traditional Structure-from-Motion
(SfM) methods may not work well in large-scale indoor scenes due to the
prevalence of textureless and repetitive regions. To overcome these challenges,
our approach (IM360) leverages the wide field of view of omnidirectional images
and integrates the spherical camera model into every core component of the SfM
pipeline. In order to develop a comprehensive 3D reconstruction solution, we
integrate a neural implicit surface reconstruction technique to generate
high-quality surfaces from sparse input data. Additionally, we utilize a
mesh-based neural rendering approach to refine texture maps and accurately
capture view-dependent properties by combining diffuse and specular components.
We evaluate our pipeline on large-scale indoor scenes from the Matterport3D and
Stanford2D3D datasets. In practice, IM360 demonstrate superior performance in
terms of textured mesh reconstruction over SOTA. We observe accuracy
improvements in terms of camera localization and registration as well as
rendering high frequency details.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Segmentation Meets Hyperspectral Image: New Paradigm for
  Hyperspectral Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilian Zhou, Weixuan Xie, Sei-ichiro Kamata, Man Sing Wong,  Huiying,  Hou, Haipeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) classification is a cornerstone of remote sensing,
enabling precise material and land-cover identification through rich spectral
information. While deep learning has driven significant progress in this task,
small patch-based classifiers, which account for over 90% of the progress, face
limitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach
considers a limited receptive field, resulting in insufficient spatial
structural information critical for object-level identification and noise-like
misclassifications even within uniform regions; (2) undefined optimal patch
sizes lead to coarse label predictions, which degrade performance; and (3) a
lack of multi-shape awareness around objects. To address these challenges, we
draw inspiration from large-scale image segmentation techniques, which excel at
handling object boundaries-a capability essential for semantic labeling in HSI
classification. However, their application remains under-explored in this task
due to (1) the prevailing notion that larger patch sizes degrade performance,
(2) the extensive unlabeled regions in HSI groundtruth, and (3) the
misalignment of input shapes between HSI data and segmentation models. Thus, in
this study, we propose a novel paradigm and baseline, HSIseg, for HSI
classification that leverages segmentation techniques combined with a novel
Dynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We
also introduce an intuitive progressive learning framework with adaptive
pseudo-labeling to iteratively incorporate unlabeled regions into the training
process, thereby advancing the application of segmentation techniques.
Additionally, we incorporate auxiliary data through multi-source data
collaboration, promoting better feature interaction. Validated on five public
HSI datasets, our proposal outperforms state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Transformation-Isomorphic Latent Space for Accurate Hand Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Ren, Lei Hu, Zhiheng Zhang, Yongjing Ye, Shihong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based regression tasks, such as hand pose estimation, have achieved
higher accuracy and faster convergence through representation learning.
However, existing representation learning methods often encounter the following
issues: the high semantic level of features extracted from images is inadequate
for regressing low-level information, and the extracted features include
task-irrelevant information, reducing their compactness and interfering with
regression tasks. To address these challenges, we propose TI-Net, a highly
versatile visual Network backbone designed to construct a Transformation
Isomorphic latent space. Specifically, we employ linear transformations to
model geometric transformations in the latent space and ensure that {\rm
TI-Net} aligns them with those in the image space. This ensures that the latent
features capture compact, low-level information beneficial for pose estimation
tasks. We evaluated TI-Net on the hand pose estimation task to demonstrate the
network's superiority. On the DexYCB dataset, TI-Net achieved a 10% improvement
in the PA-MPJPE metric compared to specialized state-of-the-art (SOTA) hand
pose estimation methods. Our code will be released in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Assessment and Analysis for NSFW Content Erasure in
  Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Die Chen, Zhiwen Li, Cen Chen, Xiaodan Li, Jinyan Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) diffusion models have gained widespread application
across various domains, demonstrating remarkable creative potential. However,
the strong generalization capabilities of these models can inadvertently led
they to generate NSFW content even with efforts on filtering NSFW content from
the training dataset, posing risks to their safe deployment. While several
concept erasure methods have been proposed to mitigate this issue, a
comprehensive evaluation of their effectiveness remains absent. To bridge this
gap, we present the first systematic investigation of concept erasure methods
for NSFW content and its sub-themes in text-to-image diffusion models. At the
task level, we provide a holistic evaluation of 11 state-of-the-art baseline
methods with 14 variants. Specifically, we analyze these methods from six
distinct assessment perspectives, including three conventional perspectives,
i.e., erasure proportion, image quality, and semantic alignment, and three new
perspectives, i.e., excessive erasure, the impact of explicit and implicit
unsafe prompts, and robustness. At the tool level, we perform a detailed
toxicity analysis of NSFW datasets and compare the performance of different
NSFW classifiers, offering deeper insights into their performance alongside a
compilation of comprehensive evaluation metrics. Our benchmark not only
systematically evaluates concept erasure methods, but also delves into the
underlying factors influencing their performance at the insight level. By
synthesizing insights from various evaluation perspectives, we provide a deeper
understanding of the challenges and opportunities in the field, offering
actionable guidance and inspiration for advancing research and practical
applications in concept erasure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLOv12: Attention-Centric Real-Time Object Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Tian, Qixiang Ye, David Doermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the network architecture of the YOLO framework has been crucial for
a long time, but has focused on CNN-based improvements despite the proven
superiority of attention mechanisms in modeling capabilities. This is because
attention-based models cannot match the speed of CNN-based models. This paper
proposes an attention-centric YOLO framework, namely YOLOv12, that matches the
speed of previous CNN-based ones while harnessing the performance benefits of
attention mechanisms. YOLOv12 surpasses all popular real-time object detectors
in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP
with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced
YOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage
extends to other model scales. YOLOv12 also surpasses end-to-end real-time
detectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats
RT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the
computation and 45% of the parameters. More comparisons are shown in Figure 1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/sunsmarterjie/yolov12</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFEERASER: Enhancing Safety in Multimodal Large Language Models through
  Multimodal Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Chen, Zhijie Deng, Kening Zheng, Yibo Yan, Shuliang Liu, PeiJun Wu, Peijie Jiang, Jia Liu, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Multimodal Large Language Models (MLLMs) develop, their potential security
issues have become increasingly prominent. Machine Unlearning (MU), as an
effective strategy for forgetting specific knowledge in training data, has been
widely used in privacy protection. However, MU for safety in MLLM has yet to be
fully explored. To address this issue, we propose SAFEERASER, a safety
unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs.
We comprehensively evaluate unlearning methods from two perspectives: forget
quality and model utility. Our findings show that existing MU methods struggle
to maintain model performance while implementing the forget operation and often
suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to
alleviate over-forgetting through decouple prompt during unlearning process. To
quantitatively measure over-forgetting mitigated by PD Loss, we propose a new
metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate
that combining PD Loss with existing unlearning methods can effectively prevent
over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B
and LLaVA-13B, while maintaining forget quality and model utility. Our code and
dataset will be released upon acceptance. Warning: This paper contains examples
of harmful language and images, and reader discretion is recommended.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RealSyn: An Effective and Scalable Multimodal Interleaved Document
  Transformation Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After pre-training on extensive image-text pairs, Contrastive Language-Image
Pre-training (CLIP) demonstrates promising performance on a wide variety of
benchmarks. However, a substantial volume of non-paired data, such as
multimodal interleaved documents, remains underutilized for vision-language
representation learning. To fully leverage these unpaired documents, we
initially establish a Real-World Data Extraction pipeline to extract
high-quality images and texts. Then we design a hierarchical retrieval method
to efficiently associate each image with multiple semantically relevant
realistic texts. To further enhance fine-grained visual information, we propose
an image semantic augmented generation module for synthetic text production.
Furthermore, we employ a semantic balance sampling strategy to improve dataset
diversity, enabling better learning of long-tail concepts. Based on these
innovations, we construct RealSyn, a dataset combining realistic and synthetic
texts, available in three scales: 15M, 30M, and 100M. Extensive experiments
demonstrate that RealSyn effectively advances vision-language representation
learning and exhibits strong scalability. Models pre-trained on RealSyn achieve
state-of-the-art performance on multiple downstream tasks. To facilitate future
research, the RealSyn dataset and pre-trained model weights are released at
https://github.com/deepglint/RealSyn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures, Webpage: https://garygutc.github.io/RealSyn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Audio-Visual Spiking Neural Networks through
  Semantic-Alignment and Cross-Modal Residual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang He, Dongcheng Zhao, Yiting Dong, Guobin Shen, Xin Yang, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans interpret and perceive the world by integrating sensory information
from multiple modalities, such as vision and hearing. Spiking Neural Networks
(SNNs), as brain-inspired computational models, exhibit unique advantages in
emulating the brain's information processing mechanisms. However, existing SNN
models primarily focus on unimodal processing and lack efficient cross-modal
information fusion, thereby limiting their effectiveness in real-world
multimodal scenarios. To address this challenge, we propose a
semantic-alignment cross-modal residual learning (S-CMRL) framework, a
Transformer-based multimodal SNN architecture designed for effective
audio-visual integration. S-CMRL leverages a spatiotemporal spiking attention
mechanism to extract complementary features across modalities, and incorporates
a cross-modal residual learning strategy to enhance feature integration.
Additionally, a semantic alignment optimization mechanism is introduced to
align cross-modal features within a shared semantic space, improving their
consistency and complementarity. Extensive experiments on three benchmark
datasets CREMA-D, UrbanSound8K-AV, and MNISTDVS-NTIDIGITS demonstrate that
S-CMRL significantly outperforms existing multimodal SNN methods, achieving the
state-of-the-art performance. The code is publicly available at
https://github.com/Brain-Cog-Lab/S-CMRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript is under review and the code is available
  https://github.com/Brain-Cog-Lab/S-CMRL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicate Hierarchies Improve Few-Shot State Classification <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Jin, Joy Hsu, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State classification of objects and their relations is core to many
long-horizon tasks, particularly in robot planning and manipulation. However,
the combinatorial explosion of possible object-predicate combinations, coupled
with the need to adapt to novel real-world environments, makes it a desideratum
for state classification models to generalize to novel queries with few
examples. To this end, we propose PHIER, which leverages predicate hierarchies
to generalize effectively in few-shot scenarios. PHIER uses an object-centric
scene encoder, self-supervised losses that infer semantic relations between
predicates, and a hyperbolic distance metric that captures hierarchical
structure; it learns a structured latent space of image-predicate pairs that
guides reasoning over state classification queries. We evaluate PHIER in the
CALVIN and BEHAVIOR robotic environments and show that PHIER significantly
outperforms existing methods in few-shot, out-of-distribution state
classification, and demonstrates strong zero- and few-shot generalization from
simulated to real-world tasks. Our results demonstrate that leveraging
predicate hierarchies improves performance on state classification tasks with
limited data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. First two authors contributed equally. Project page:
  https://emilyzjin.github.io/projects/phier.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not-So-Optimal Transport Flows for 3D Point Cloud Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ka-Hei Hui, Chao Liu, Xiaohui Zeng, Chi-Wing Fu, Arash Vahdat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning generative models of 3D point clouds is one of the fundamental
problems in 3D generative learning. One of the key properties of point clouds
is their permutation invariance, i.e., changing the order of points in a point
cloud does not change the shape they represent. In this paper, we analyze the
recently proposed equivariant OT flows that learn permutation invariant
generative models for point-based molecular data and we show that these models
scale poorly on large point clouds. Also, we observe learning (equivariant) OT
flows is generally challenging since straightening flow trajectories makes the
learned flow model complex at the beginning of the trajectory. To remedy these,
we propose not-so-optimal transport flow models that obtain an approximate OT
by an offline OT precomputation, enabling an efficient construction of OT pairs
for training. During training, we can additionally construct a hybrid coupling
by combining our approximate OT and independent coupling to make the target
flow models easier to learn. In an extensive empirical study, we show that our
proposed model outperforms prior diffusion- and flow-based approaches on a wide
range of unconditional generation and shape completion on the ShapeNet
benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Zero-Shot Facial Emotion Annotation with Large Language
  Models: A Multi-Class and Multi-Frame Approach in DailyLife 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Xinyi Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the feasibility and performance of using large
language models (LLMs) to automatically annotate human emotions in everyday
scenarios. We conducted experiments on the DailyLife subset of the publicly
available FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot
labeling of key frames extracted from video segments. Under a seven-class
emotion taxonomy ("Angry," "Disgust," "Fear," "Happy," "Neutral," "Sad,"
"Surprise"), the LLM achieved an average precision of approximately 50%. In
contrast, when limited to ternary emotion classification
(negative/neutral/positive), the average precision increased to approximately
64%. Additionally, we explored a strategy that integrates multiple frames
within 1-2 second video clips to enhance labeling performance and reduce costs.
The results indicate that this approach can slightly improve annotation
accuracy. Overall, our preliminary findings highlight the potential application
of zero-shot LLMs in human facial emotion annotation tasks, offering new
avenues for reducing labeling costs and broadening the applicability of LLMs in
complex multimodal environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YUNet: Improved YOLOv11 Network for Skyline Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Yang, Miao Wang, Quan Zhou, Jiangchuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skyline detection plays an important role in geolocalizaion, flight control,
visual navigation, port security, etc. The appearance of the sky and non-sky
areas are variable, because of different weather or illumination environment,
which brings challenges to skyline detection. In this research, we proposed the
YUNet algorithm, which improved the YOLOv11 architecture to segment the sky
region and extract the skyline in complicated and variable circumstances. To
improve the ability of multi-scale and large range contextual feature fusion,
the YOLOv11 architecture is extended as an UNet-like architecture, consisting
of an encoder, neck and decoder submodule. The encoder extracts the multi-scale
features from the given images. The neck makes fusion of these multi-scale
features. The decoder applies the fused features to complete the prediction
rebuilding. To validate the proposed approach, the YUNet was tested on
Skyfinder and CH1 datasets for segmentation and skyline detection respectively.
Our test shows that the IoU of YUnet segmentation can reach 0.9858, and the
average error of YUnet skyline detection is just 1.36 pixels. The
implementation is published at
https://github.com/kuazhangxiaoai/SkylineDet-YOLOv11Seg.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Image Super Resolution Modeling for Earth System Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Super-resolution (SR) techniques are essential for improving Earth System
Model (ESM) data's spatial resolution, which helps better understand complex
environmental processes. This paper presents a new algorithm, ViFOR, which
combines Vision Transformers (ViT) and Implicit Neural Representation Networks
(INRs) to generate High-Resolution (HR) images from Low-Resolution (LR) inputs.
ViFOR introduces a novel integration of Fourier-based activation functions
within the Vision Transformer architecture, enabling it to effectively capture
global context and high-frequency details critical for accurate SR
reconstruction. The results show that ViFOR outperforms state-of-the-art
methods such as ViT, Sinusoidal Representation Networks (SIREN), and SR
Generative Adversarial Networks (SRGANs) based on metrics like Peak
Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) both for global as
well as the local imagery. ViFOR improves PSNR of up to 4.18 dB, 1.56 dB, and
1.73 dB over ViT for full images in the Source Temperature, Shortwave, and
Longwave Flux.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Disentangled Counterfactual Learning for Physical Audiovisual
  Commonsense Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengshi Qi, Changsheng Lv, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new Robust Disentangled Counterfactual Learning
(RDCL) approach for physical audiovisual commonsense reasoning. The task aims
to infer objects' physics commonsense based on both video and audio input, with
the main challenge being how to imitate the reasoning ability of humans, even
under the scenario of missing modalities. Most of the current methods fail to
take full advantage of different characteristics in multi-modal data, and
lacking causal reasoning ability in models impedes the progress of implicit
physical knowledge inferring. To address these issues, our proposed RDCL method
decouples videos into static (time-invariant) and dynamic (time-varying)
factors in the latent space by the disentangled sequential encoder, which
adopts a variational autoencoder (VAE) to maximize the mutual information with
a contrastive loss function. Furthermore, we introduce a counterfactual
learning module to augment the model's reasoning ability by modeling physical
knowledge relationships among different objects under counterfactual
intervention. To alleviate the incomplete modality data issue, we introduce a
robust multimodal learning method to recover the missing data by decomposing
the shared features and model-specific features. Our proposed method is a
plug-and-play module that can be incorporated into any baseline including VLMs.
In experiments, we show that our proposed method improves the reasoning
accuracy and robustness of baseline methods and achieves the state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Illuminant Estimation in Deep Color Constancy through Enhancing
  Brightness Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengda Xie, Chengzhi Zhong, Yiling He, Zhan Qin, Meie Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color constancy estimates illuminant chromaticity to correct color-biased
images. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models
have made substantial advancements. Nevertheless, the potential risks in DNNCC
due to the vulnerability of deep neural networks have not yet been explored. In
this paper, we conduct the first investigation into the impact of a key factor
in color constancy-brightness-on DNNCC from a robustness perspective. Our
evaluation reveals that several mainstream DNNCC models exhibit high
sensitivity to brightness despite their focus on chromaticity estimation. This
sheds light on a potential limitation of existing DNNCC models: their
sensitivity to brightness may hinder performance given the widespread
brightness variations in real-world datasets. From the insights of our
analysis, we propose a simple yet effective brightness robustness enhancement
strategy for DNNCC models, termed BRE. The core of BRE is built upon the
adaptive step-size adversarial brightness augmentation technique, which
identifies high-risk brightness variation and generates augmented images via
explicit brightness adjustment. Subsequently, BRE develops a
brightness-robustness-aware model optimization strategy that integrates
adversarial brightness training and brightness contrastive loss, significantly
bolstering the brightness robustness of DNNCC models. BRE is
hyperparameter-free and can be integrated into existing DNNCC models, without
incurring additional overhead during the testing phase. Experiments on two
public color constancy datasets-ColorChecker and Cube+-demonstrate that the
proposed BRE consistently enhances the illuminant estimation performance of
existing DNNCC models, reducing the estimation error by an average of 5.04%
across six mainstream DNNCC models, underscoring the critical role of enhancing
brightness robustness in these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaseous Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailai Zhou, Yibo Wang, Tao Lv, Qiu Shen, Xun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection, a fundamental and challenging problem in computer vision,
has experienced rapid development due to the effectiveness of deep learning.
The current objects to be detected are mostly rigid solid substances with
apparent and distinct visual characteristics. In this paper, we endeavor on a
scarcely explored task named Gaseous Object Detection (GOD), which is
undertaken to explore whether the object detection techniques can be extended
from solid substances to gaseous substances. Nevertheless, the gas exhibits
significantly different visual characteristics: 1) saliency deficiency, 2)
arbitrary and ever-changing shapes, 3) lack of distinct boundaries. To
facilitate the study on this challenging task, we construct a GOD-Video dataset
comprising 600 videos (141,017 frames) that cover various attributes with
multiple types of gases. A comprehensive benchmark is established based on this
dataset, allowing for a rigorous evaluation of frame-level and video-level
detectors. Deduced from the Gaussian dispersion model, the physics-inspired
Voxel Shift Field (VSF) is designed to model geometric irregularities and
ever-changing shapes in potential 3D space. By integrating VSF into Faster
RCNN, the VSF RCNN serves as a simple but strong baseline for gaseous object
detection. Our work aims to attract further research into this valuable albeit
challenging area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-vision-based Picking Point Localisation of Target Fruit for
  Harvesting Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. Beldek, A. Dunn, J. Cunningham, E. Sariyildiz, S. L. Phung, G. Alici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents multi-vision-based localisation strategies for harvesting
robots. Identifying picking points accurately is essential for robotic
harvesting because insecure grasping can lead to economic loss through fruit
damage and dropping. In this study, two multi-vision-based localisation
methods, namely the analytical approach and model-based algorithms, were
employed. The actual geometric centre points of fruits were collected using a
motion capture system (mocap), and two different surface points Cfix and Ceih
were extracted using two Red-Green-Blue-Depth (RGB-D) cameras. First, the
picking points of the target fruit were detected using analytical methods.
Second, various primary and ensemble learning methods were employed to predict
the geometric centre of target fruits by taking surface points as input.
Adaboost regression, the most successful model-based localisation algorithm,
achieved 88.8% harvesting accuracy with a Mean Euclidean Distance (MED) of 4.40
mm, while the analytical approach reached 81.4% picking success with a MED of
14.25 mm, both demonstrating better performance than the single-camera, which
had a picking success rate of 77.7% with a MED of 24.02 mm. To evaluate the
effect of picking point accuracy in collecting fruits, a series of robotic
harvesting experiments were performed utilising a collaborative robot (cobot).
It is shown that multi-vision systems can improve picking point localisation,
resulting in higher success rates of picking in robotic harvesting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Aware Diffusion Models for Multiview Scene Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Salimi, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Konstantinos G. Derpanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on 3D scene inpainting, where parts of an input image
set, captured from different viewpoints, are masked out. The main challenge
lies in generating plausible image completions that are geometrically
consistent across views. Most recent work addresses this challenge by combining
generative models with a 3D radiance field to fuse information across
viewpoints. However, a major drawback of these methods is that they often
produce blurry images due to the fusion of inconsistent cross-view images. To
avoid blurry inpaintings, we eschew the use of an explicit or implicit radiance
field altogether and instead fuse cross-view information in a learned space. In
particular, we introduce a geometry-aware conditional generative model, capable
of inpainting multi-view consistent images based on both geometric and
appearance cues from reference images. A key advantage of our approach over
existing methods is its unique ability to inpaint masked scenes with a limited
number of views (i.e., few-view inpainting), whereas previous methods require
relatively large image sets for their 3D model fitting step. Empirically, we
evaluate and compare our scene-centric inpainting method on two datasets,
SPIn-NeRF and NeRFiller, which contain images captured at narrow and wide
baselines, respectively, and achieve state-of-the-art 3D inpainting performance
on both. Additionally, we demonstrate the efficacy of our approach in the
few-view setting compared to prior methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project page is available at https://geomvi.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MotionMatcher: Motion Customization of Text-to-Video Diffusion Models
  via Motion Feature Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Siang Wu, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video (T2V) diffusion models have shown promising capabilities in
synthesizing realistic videos from input text prompts. However, the input text
description alone provides limited control over the precise objects movements
and camera framing. In this work, we tackle the motion customization problem,
where a reference video is provided as motion guidance. While most existing
methods choose to fine-tune pre-trained diffusion models to reconstruct the
frame differences of the reference video, we observe that such strategy suffer
from content leakage from the reference video, and they cannot capture complex
motion accurately. To address this issue, we propose MotionMatcher, a motion
customization framework that fine-tunes the pre-trained T2V diffusion model at
the feature level. Instead of using pixel-level objectives, MotionMatcher
compares high-level, spatio-temporal motion features to fine-tune diffusion
models, ensuring precise motion learning. For the sake of memory efficiency and
accessibility, we utilize a pre-trained T2V diffusion model, which contains
considerable prior knowledge about video motion, to compute these motion
features. In our experiments, we demonstrate state-of-the-art motion
customization performances, validating the design of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.csie.ntu.edu.tw/~b09902097/motionmatcher/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting
  View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance
Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to
represent complex geometry and appearance, GS achieves faster rendering times
and reduced memory consumption compared to the neural network approach used in
NeRF. However, quality assessment of GS-generated static content is not yet
explored in-depth. This paper describes a subjective quality assessment study
that aims to evaluate synthesized videos obtained with several static GS
state-of-the-art methods. The methods were applied to diverse visual scenes,
covering both 360-degree and forward-facing (FF) camera trajectories. Moreover,
the performance of 18 objective quality metrics was analyzed using the scores
resulting from the subjective study, providing insights into their strengths,
limitations, and alignment with human perception. All videos and scores are
made available providing a comprehensive database that can be used as benchmark
on GS view synthesis and objective quality metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicArticulate: Make Your 3D Models Articulation-Ready 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth of 3D content creation, there is an increasing
demand for automatically converting static 3D models into articulation-ready
versions that support realistic animation. Traditional approaches rely heavily
on manual annotation, which is both time-consuming and labor-intensive.
Moreover, the lack of large-scale benchmarks has hindered the development of
learning-based solutions. In this work, we present MagicArticulate, an
effective framework that automatically transforms static 3D models into
articulation-ready assets. Our key contributions are threefold. First, we
introduce Articulation-XL, a large-scale benchmark containing over 33k 3D
models with high-quality articulation annotations, carefully curated from
Objaverse-XL. Second, we propose a novel skeleton generation method that
formulates the task as a sequence modeling problem, leveraging an
auto-regressive transformer to naturally handle varying numbers of bones or
joints within skeletons and their inherent dependencies across different 3D
models. Third, we predict skinning weights using a functional diffusion process
that incorporates volumetric geodesic distance priors between vertices and
joints. Extensive experiments demonstrate that MagicArticulate significantly
outperforms existing methods across diverse object categories, achieving
high-quality articulation that enables realistic animation. Project page:
https://chaoyuesong.github.io/MagicArticulate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://chaoyuesong.github.io/MagicArticulate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2VEval: T2V-generated Videos Benchmark <span class="highlight-title">Dataset</span> and Objective Evaluation
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08545v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08545v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelu Qi, Ping Shi, Shuqi Wang, Zhaoyang Zhang, Fei Zhao, Zefeng Ying, Da Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video (T2V) technology, as demonstrated by models
such as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the
applicability and popularity of the technology. This progress has created a
growing demand for accurate quality assessment metrics to evaluate the
perceptual quality of T2V-generated videos and optimize video generation
models. However, assessing the quality of text-to-video outputs remain
challenging due to the presence of highly complex distortions, such as
unnatural actions and phenomena that defy human cognition. To address these
challenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset
for text-to-video quality evaluation, which contains 148 textual prompts and
1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation,
we scored each video on four dimensions in the subjective experiment, which are
overall impression, text-video consistency, realness, and technical quality.
Based on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for
T2V quality evaluation. T2VEval assesses videos across three branches:
text-video consistency, realness, and technical quality. Using an
attention-based fusion module, T2VEval effectively integrates features from
each branch and predicts scores with the aid of a large language model.
Additionally, we implemented a divide-and-conquer training strategy, enabling
each branch to learn targeted knowledge while maintaining synergy with the
others. Experimental results demonstrate that T2VEval achieves state-of-the-art
performance across multiple metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iMOVE: Instance-Motion-Aware Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaze Li, Yaya Shi, Zongyang Ma, Haoran Xu, Feng Cheng, Huihui Xiao, Ruiwen Kang, Fan Yang, Tingting Gao, Di Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the fine-grained instance spatiotemporal motion perception
capabilities of Video Large Language Models is crucial for improving their
temporal and general video understanding. However, current models struggle to
perceive detailed and complex instance motions. To address these challenges, we
have made improvements from both data and model perspectives. In terms of data,
we have meticulously curated iMOVE-IT, the first large-scale
instance-motion-aware video instruction-tuning dataset. This dataset is
enriched with comprehensive instance motion annotations and spatiotemporal
mutual-supervision tasks, providing extensive training for the model's
instance-motion-awareness. Building on this foundation, we introduce iMOVE, an
instance-motion-aware video foundation model that utilizes Event-aware
Spatiotemporal Efficient Modeling to retain informative instance spatiotemporal
motion details while maintaining computational efficiency. It also incorporates
Relative Spatiotemporal Position Tokens to ensure awareness of instance
spatiotemporal positions. Evaluations indicate that iMOVE excels not only in
video temporal understanding and general video understanding but also
demonstrates significant advantages in long-term video understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Multimodal Models Solve Caption Generation for Scientific
  Figures? Lessons Learned from SciCap Challenge 2023 <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19353v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19353v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the SciCap datasets launch in 2021, the research community has made
significant progress in generating captions for scientific figures in scholarly
articles. In 2023, the first SciCap Challenge took place, inviting global teams
to use an expanded SciCap dataset to develop models for captioning diverse
figure types across various academic fields. At the same time, text generation
models advanced quickly, with many powerful pre-trained large multimodal models
(LMMs) emerging that showed impressive capabilities in various
vision-and-language tasks. This paper presents an overview of the first SciCap
Challenge and details the performance of various models on its data, capturing
a snapshot of the fields state. We found that professional editors
overwhelmingly preferred figure captions generated by GPT-4V over those from
all other models and even the original captions written by authors. Following
this key finding, we conducted detailed analyses to answer this question: Have
advanced LMMs solved the task of generating captions for scientific figures?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantically Consistent Person Image Generation <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data-driven approach for context-aware person image generation.
Specifically, we attempt to generate a person image such that the synthesized
instance can blend into a complex scene. In our method, the position, scale,
and appearance of the generated person are semantically conditioned on the
existing persons in the scene. The proposed technique is divided into three
sequential steps. At first, we employ a Pix2PixHD model to infer a coarse
semantic mask that represents the new person's spatial location, scale, and
potential pose. Next, we use a data-centric approach to select the closest
representation from a precomputed cluster of fine semantic masks. Finally, we
adopt a multi-scale, attention-guided architecture to transfer the appearance
attributes from an exemplar image. The proposed strategy enables us to
synthesize semantically coherent realistic persons that can blend into an
existing scene without altering the global context. We conclude our findings
with relevant qualitative and quantitative evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward
  Modeling <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiyu Zhang, Huan-ang Gao, Zijian Jiang, Hao Zhao, Zhedong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on the task of conditional image generation, where an
image is synthesized according to user instructions. The critical challenge
underpinning this task is ensuring both the fidelity of the generated images
and their semantic alignment with the provided conditions. To tackle this
issue, previous studies have employed supervised perceptual losses derived from
pre-trained models, i.e., reward models, to enforce alignment between the
condition and the generated result. However, we observe one inherent
shortcoming: considering the diversity of synthesized images, the reward model
usually provides inaccurate feedback when encountering newly generated data,
which can undermine the training process. To address this limitation, we
propose an uncertainty-aware reward modeling, called Ctrl-U, including
uncertainty estimation and uncertainty-aware regularization, designed to reduce
the adverse effects of imprecise feedback from the reward model. Given the
inherent cognitive uncertainty within reward models, even images generated
under identical conditions often result in a relatively large discrepancy in
reward loss. Inspired by the observation, we explicitly leverage such
prediction variance as an uncertainty indicator. Based on the uncertainty
estimation, we regularize the model training by adaptively rectifying the
reward. In particular, rewards with lower uncertainty receive higher loss
weights, while those with higher uncertainty are given reduced weights to allow
for larger variability. The proposed uncertainty regularization facilitates
reward fine-tuning through consistency construction. Extensive experiments
validate the effectiveness of our methodology in improving the controllability
and generation quality, as well as its scalability across diverse conditional
scenarios. Codes are publicly available at
https://grenoble-zhang.github.io/Ctrl-U-Page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scene Aware Person Image Generation through Global Contextual
  Conditioning <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person image generation is an intriguing yet challenging problem. However,
this task becomes even more difficult under constrained situations. In this
work, we propose a novel pipeline to generate and insert contextually relevant
person images into an existing scene while preserving the global semantics.
More specifically, we aim to insert a person such that the location, pose, and
scale of the person being inserted blends in with the existing persons in the
scene. Our method uses three individual networks in a sequential pipeline. At
first, we predict the potential location and the skeletal structure of the new
person by conditioning a Wasserstein Generative Adversarial Network (WGAN) on
the existing human skeletons present in the scene. Next, the predicted skeleton
is refined through a shallow linear network to achieve higher structural
accuracy in the generated image. Finally, the target image is generated from
the refined skeleton using another generative network conditioned on a given
image of the target person. In our experiments, we achieve high-resolution
photo-realistic generation results while preserving the general context of the
scene. We conclude our paper with multiple qualitative and quantitative
benchmarks on the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIPS: Text-Induced Pose Synthesis <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, human pose synthesis and transfer deal with probabilistic
image generation of a person in a previously unseen pose from an already
available observation of that person. Though researchers have recently proposed
several methods to achieve this task, most of these techniques derive the
target pose directly from the desired target image on a specific dataset,
making the underlying process challenging to apply in real-world scenarios as
the generation of the target image is the actual aim. In this paper, we first
present the shortcomings of current pose transfer algorithms and then propose a
novel text-based pose transfer technique to address those issues. We divide the
problem into three independent stages: (a) text to pose representation, (b)
pose refinement, and (c) pose rendering. To the best of our knowledge, this is
one of the first attempts to develop a text-based pose transfer framework where
we also introduce a new dataset DF-PASS, by adding descriptive pose annotations
for the images of the DeepFashion dataset. The proposed method generates
promising results with significant qualitative and quantitative scores in our
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The European Conference on Computer Vision (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BenthicNet: A global compilation of seafloor images for deep learning
  applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05241v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05241v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott C. Lowe, Benjamin Misiuk, Isaac Xu, Shakhboz Abdulazizov, Amit R. Baroi, Alex C. Bastos, Merlin Best, Vicki Ferrini, Ariell Friedman, Deborah Hart, Ove Hoegh-Guldberg, Daniel Ierodiaconou, Julia Mackin-McLaughlin, Kathryn Markey, Pedro S. Menandro, Jacquomo Monk, Shreya Nemani, John O'Brien, Elizabeth Oh, Luba Y. Reshitnyk, Katleen Robert, Chris M. Roelfsema, Jessica A. Sameoto, Alexandre C. G. Schimel, Jordan A. Thomson, Brittany R. Wilson, Melisa C. Wong, Craig J. Brown, Thomas Trappenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in underwater imaging enable collection of extensive seafloor image
datasets necessary for monitoring important benthic ecosystems. The ability to
collect seafloor imagery has outpaced our capacity to analyze it, hindering
mobilization of this crucial environmental information. Machine learning
approaches provide opportunities to increase the efficiency with which seafloor
imagery is analyzed, yet large and consistent datasets to support development
of such approaches are scarce. Here we present BenthicNet: a global compilation
of seafloor imagery designed to support the training and evaluation of
large-scale image recognition models. An initial set of over 11.4 million
images was collected and curated to represent a diversity of seafloor
environments using a representative subset of 1.3 million images. These are
accompanied by 3.1 million annotations translated to the CATAMI scheme, which
span 190,000 of the images. A large deep learning model was trained on this
compilation and preliminary results suggest it has utility for automating large
and small-scale image analysis tasks. The compilation and model are made openly
available for reuse at https://doi.org/10.20383/103.0614.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-scale Attention Guided Pose Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose transfer refers to the probabilistic image generation of a person with a
previously unseen novel pose from another image of that person having a
different pose. Due to potential academic and commercial applications, this
problem is extensively studied in recent years. Among the various approaches to
the problem, attention guided progressive generation is shown to produce
state-of-the-art results in most cases. In this paper, we present an improved
network architecture for pose transfer by introducing attention links at every
resolution level of the encoder and decoder. By utilizing such dense
multi-scale attention guided approach, we are able to achieve significant
improvement over the existing methods both visually and analytically. We
conclude our findings with extensive qualitative and quantitative comparisons
against several existing methods on the DeepFashion dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Pattern Recognition (PR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Event-based Frame Interpolation with Ad-hoc
  Deblurring in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sun, Daniel Gehrig, Christos Sakaridis, Mathias Gehrig, Jingyun Liang, Peng Sun, Zhijie Xu, Kaiwei Wang, Luc Van Gool, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective video frame interpolation hinges on the adept handling of motion in
the input scene. Prior work acknowledges asynchronous event information for
this, but often overlooks whether motion induces blur in the video, limiting
its scope to sharp frame interpolation. We instead propose a unified framework
for event-based frame interpolation that performs deblurring ad-hoc and thus
works both on sharp and blurry input videos. Our model consists in a
bidirectional recurrent network that incorporates the temporal dimension of
interpolation and fuses information from the input frames and the events
adaptively based on their temporal proximity. To enhance the generalization
from synthetic data to real event cameras, we integrate self-supervised
framework with the proposed model to enhance the generalization on real-world
datasets in the wild. At the dataset level, we introduce a novel real-world
high-resolution dataset with events and color videos named HighREV, which
provides a challenging evaluation setting for the examined task. Extensive
experiments show that our network consistently outperforms previous
state-of-the-art methods on frame interpolation, single image deblurring, and
the joint task of both. Experiments on domain transfer reveal that
self-supervised training effectively mitigates the performance degradation
observed when transitioning from synthetic data to real-world data. Code and
datasets are available at https://github.com/AHupuJR/REFID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to T-PAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLMaterial: Procedural Material Generation with Large Vision-Language
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beichen Li, Rundi Wu, Armando Solar-Lezama, Changxi Zheng, Liang Shi, Bernd Bickel, Wojciech Matusik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural materials, represented as functional node graphs, are ubiquitous
in computer graphics for photorealistic material appearance design. They allow
users to perform intuitive and precise editing to achieve desired visual
appearances. However, creating a procedural material given an input image
requires professional knowledge and significant effort. In this work, we
leverage the ability to convert procedural materials into standard Python
programs and fine-tune a large pre-trained vision-language model (VLM) to
generate such programs from input images. To enable effective fine-tuning, we
also contribute an open-source procedural material dataset and propose to
perform program-level augmentation by prompting another pre-trained large
language model (LLM). Through extensive evaluation, we show that our method
outperforms previous methods on both synthetic and real-world examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LieRE: Generalizing Rotary Position Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10322v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10322v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Ostmeier, Brian Axelrod, Michael E. Moseley, Akshay Chaudhari, Curtis Langlotz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architectures rely on position encodings to capture token
dependencies. Rotary Position Encoding (RoPE) has emerged as a popular choice
in language models due to its efficient encoding of relative position
information through key-query rotations. However, RoPE faces significant
limitations beyond language processing: it is constrained to one-dimensional
sequence data and, even with learnable phases, offers limited representational
capacity. We address these challenges with Lie Relative Encodings (LieRE),
which replaces RoPE's block-2D rotation matrix with a learned, dense,
high-dimensional rotation matrix of variable sparsity. Through extensive
evaluation on three image datasets across 2D and 3D classification tasks, LieRE
achieves 2\% relative improvement over state-of-the-art baselines on 2D tasks
and 1.5\% on 3D tasks, while demonstrating superior generalization to higher
resolutions. Our implementation is computationally efficient, with results
reproducible on 4 A100 GPUs in 30 minutes on CIFAR100, and we release our code
to facilitate further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Real-to-Sim-to-Real Approach to Robotic Manipulation with
  VLM-Generated Iterative Keypoint Rewards <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivansh Patel, Xinchen Yin, Wenlong Huang, Shubham Garg, Hooshang Nayyeri, Li Fei-Fei, Svetlana Lazebnik, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task specification for robotic manipulation in open-world environments is
challenging, requiring flexible and adaptive objectives that align with human
intentions and can evolve through iterative feedback. We introduce Iterative
Keypoint Reward (IKER), a visually grounded, Python-based reward function that
serves as a dynamic task specification. Our framework leverages VLMs to
generate and refine these reward functions for multi-step manipulation tasks.
Given RGB-D observations and free-form language instructions, we sample
keypoints in the scene and generate a reward function conditioned on these
keypoints. IKER operates on the spatial relationships between keypoints,
leveraging commonsense priors about the desired behaviors, and enabling precise
SE(3) control. We reconstruct real-world scenes in simulation and use the
generated rewards to train reinforcement learning (RL) policies, which are then
deployed into the real world-forming a real-to-sim-to-real loop. Our approach
demonstrates notable capabilities across diverse scenarios, including both
prehensile and non-prehensile tasks, showcasing multi-step task execution,
spontaneous error recovery, and on-the-fly strategy adjustments. The results
highlight IKER's effectiveness in enabling robots to perform multi-step tasks
in dynamic environments through iterative reward shaping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025, Project Page: https://iker-robot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Air-writing refers to virtually writing linguistic characters through hand
gestures in three-dimensional space with six degrees of freedom. This paper
proposes a generic video camera-aided convolutional neural network (CNN) based
air-writing framework. Gestures are performed using a marker of fixed color in
front of a generic video camera, followed by color-based segmentation to
identify the marker and track the trajectory of the marker tip. A pre-trained
CNN is then used to classify the gesture. The recognition accuracy is further
improved using transfer learning with the newly acquired data. The performance
of the system varies significantly on the illumination condition due to
color-based segmentation. In a less fluctuating illumination condition, the
system is able to recognize isolated unistroke numerals of multiple languages.
The proposed framework has achieved 97.7%, 95.4% and 93.7% recognition rates in
person independent evaluations on English, Bengali and Devanagari numerals,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Frontiers of Handwriting
  Recognition (ICFHR) 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locality-aware Cross-modal Correspondence Learning for Dense
  Audio-Visual Events Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Xing, Hongyu Qu, Rui Yan, Xiangbo Shu, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense-localization Audio-Visual Events (DAVE) aims to identify time
boundaries and corresponding categories for events that can be heard and seen
concurrently in an untrimmed video. Existing DAVE solutions extract audio and
visual features through modality-specific encoders and fuse them via dense
cross-attention. The independent processing of each modality neglects their
complementarity, resulting in modality-specific noise, while dense attention
fails to account for local temporal continuity of events, causing irrelevant
signal distractions. In this paper, we present LoCo, a Locality-aware
cross-modal Correspondence learning framework for DAVE. The core idea is to
explore local temporal continuity nature of audio-visual events, which serves
as informative yet free supervision signals to guide the filtering of
irrelevant information and inspire the extraction of complementary multimodal
information during both unimodal and cross-modal learning stages. i)
Specifically, LoCo applies Locality-aware Correspondence Correction (LCC) to
unimodal features via leveraging cross-modal local-correlated properties
without any extra annotations. This enforces unimodal encoders to highlight
similar semantics shared by audio and visual features. ii) To better aggregate
such audio and visual features, we further customize Cross-modal Dynamic
Perception layer (CDP) in cross-modal feature pyramid to understand local
temporal patterns of audio-visual events by imposing local consistency within
multimodal features in a data-driven manner. By incorporating LCC and CDP, LoCo
provides solid performance gains and outperforms existing DAVE methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery is crucial to diagnose and mitigate model errors.
Current clustering or discrete attribute-based slice discovery methods face key
limitations: 1) clustering results in incoherent slices, while assigning
discrete attributes to slices leads to incomplete coverage of error patterns
due to missing or insufficient attributes; 2) these methods lack complex
reasoning, preventing them from fully explaining model biases; 3) they fail to
integrate \textit{domain knowledge}, limiting their usage in specialized fields
\eg radiology. We propose\ladder (\underline{La}nguage-\underline{D}riven
\underline{D}iscovery and \underline{E}rror \underline{R}ectification), to
address the limitations by: (1) leveraging the flexibility of natural language
to address incompleteness, (2) employing LLM's latent \textit{domain knowledge}
and advanced reasoning to analyze sentences and derive testable hypotheses
directly, identifying biased attributes, and form coherent error slices without
clustering. Existing mitigation methods typically address only the
worst-performing group, often amplifying errors in other subgroups. In
contrast,\ladder generates pseudo attributes from the discovered hypotheses to
mitigate errors across all biases without explicit attribute annotations or
prior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural
and medical images -- comparing 200+ classifiers with diverse architectures,
pretraining strategies, and LLMs -- show that\ladder consistently outperforms
existing baselines in discovering and mitigating biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Where Do We Stand with Implicit Neural Representations? A Technical and
  Performance <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amer Essakine, Yanqi Cheng, Chun-Wun Cheng, Lipei Zhang, Zhongying Deng, Lei Zhu, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have emerged as a paradigm in
knowledge representation, offering exceptional flexibility and performance
across a diverse range of applications. INRs leverage multilayer perceptrons
(MLPs) to model data as continuous implicit functions, providing critical
advantages such as resolution independence, memory efficiency, and
generalisation beyond discretised data structures. Their ability to solve
complex inverse problems makes them particularly effective for tasks including
audio reconstruction, image representation, 3D object reconstruction, and
high-dimensional data synthesis. This survey provides a comprehensive review of
state-of-the-art INR methods, introducing a clear taxonomy that categorises
them into four key areas: activation functions, position encoding, combined
strategies, and network structure optimisation. We rigorously analyse their
critical properties, such as full differentiability, smoothness, compactness,
and adaptability to varying resolutions while also examining their strengths
and limitations in addressing locality biases and capturing fine details. Our
experimental comparison offers new insights into the trade-offs between
different approaches, showcasing the capabilities and challenges of the latest
INR techniques across various tasks. In addition to identifying areas where
current methods excel, we highlight key limitations and potential avenues for
improvement, such as developing more expressive activation functions, enhancing
positional encoding mechanisms, and improving scalability for complex,
high-dimensional data. This survey serves as a roadmap for researchers,
offering practical guidance for future exploration in the field of INRs. We aim
to foster new methodologies by outlining promising research directions for INRs
and applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position and Rotation Invariant Sign Language Recognition from 3D Kinect
  Data with Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.12669v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.12669v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Partha Pratim Roy, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language is a gesture-based symbolic communication medium among speech
and hearing impaired people. It also serves as a communication bridge between
non-impaired and impaired populations. Unfortunately, in most situations, a
non-impaired person is not well conversant in such symbolic languages
restricting the natural information flow between these two categories.
Therefore, an automated translation mechanism that seamlessly translates sign
language into natural language can be highly advantageous. In this paper, we
attempt to perform recognition of 30 basic Indian sign gestures. Gestures are
represented as temporal sequences of 3D maps (RGB + depth), each consisting of
3D coordinates of 20 body joints captured by the Kinect sensor. A recurrent
neural network (RNN) is employed as the classifier. To improve the classifier's
performance, we use geometric transformation for the alignment correction of
depth frames. In our experiments, the model achieves 84.81% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R3L: Relative Representations for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Pio Ricciardi, Valentino Maiorca, Luca Moschella, Riccardo Marin, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Reinforcement Learning is a popular and powerful framework that takes
full advantage of the Deep Learning breakthrough. It is known that variations
in input domains (e.g., different panorama colors due to seasonal changes) or
task domains (e.g., altering the target speed of a car) can disrupt agent
performance, necessitating new training for each variation. Recent advancements
in the field of representation learning have demonstrated the possibility of
combining components from different neural networks to create new models in a
zero-shot fashion. In this paper, we build upon relative representations, a
framework that maps encoder embeddings to a universal space. We adapt this
framework to the Visual Reinforcement Learning setting, allowing to combine
agents components to create new agents capable of effectively handling novel
visual-task pairs not encountered during training. Our findings highlight the
potential for model reuse, significantly reducing the need for retraining and,
consequently, the time and computational resources required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PTQ4RIS: Post-Training Quantization for Referring Image Segmentation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Jiang, Hang Yang, Kaiying Zhu, Xihe Qiu, Shibo Zhao, Sifan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS), aims to segment the object referred by a
given sentence in an image by understanding both visual and linguistic
information. However, existing RIS methods tend to explore top-performance
models, disregarding considerations for practical applications on
resources-limited edge devices. This oversight poses a significant challenge
for on-device RIS inference. To this end, we propose an effective and efficient
post-training quantization framework termed PTQ4RIS. Specifically, we first
conduct an in-depth analysis of the root causes of performance degradation in
RIS model quantization and propose dual-region quantization (DRQ) and
reorder-based outlier-retained quantization (RORQ) to address the quantization
difficulties in visual and text encoders. Extensive experiments on three
benchmarks with different bits settings (from 8 to 4 bits) demonstrates its
superior performance. Importantly, we are the first PTQ method specifically
designed for the RIS task, highlighting the feasibility of PTQ in RIS
applications. Code and video are available at
{https://github.com/gugu511yy/PTQ4RIS}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025.(Update the code link.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't drop your samples! Coherence-aware training benefits Conditional
  diffusion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, David Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional diffusion models are powerful generative models that can leverage
various types of conditional information, such as class labels, segmentation
masks, or text captions. However, in many real-world scenarios, conditional
information may be noisy or unreliable due to human annotation errors or weak
alignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a
novel method that integrates coherence in conditional information into
diffusion models, allowing them to learn from noisy annotations without
discarding data. We assume that each data point has an associated coherence
score that reflects the quality of the conditional information. We then
condition the diffusion model on both the conditional information and the
coherence score. In this way, the model learns to ignore or discount the
conditioning when the coherence is low. We show that CAD is theoretically sound
and empirically effective on various conditional generation tasks. Moreover, we
show that leveraging coherence generates realistic and diverse samples that
respect conditional information better than models trained on cleaned datasets
where samples with low coherence have been discarded.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024 as a Highlight. Project page:
  https://nicolas-dufour.github.io/cad.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Post-processing of coronary and myocardial spatial data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.14624v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.14624v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jay Aodh Mackenzie, Megan Jeanne Miller, Nicholas Hill, Mette Olufsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical simulations of real-world phenomena require a computational scheme
and a computational domain. In the context of haemodynamics, the computational
domain is the blood vessel network through which blood flows. Such networks
contain millions of vessels that are joined in series and in parallel. It is
computationally unfeasible to explicitly simulate blood flow throughout the
network. From a single porcine left coronary arterial tree, we develop a data
pipeline to obtain computational domains for haemodynamic simulations in the
myocardium from a graph representing a partial coronary arterial tree. In
addition, we develop a method to ascertain which subregions of the
left-ventricular wall are more likely to be perfused via a given artery, using
a comparison with the American Heart Association division of the left ventricle
for validation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Weijian Chen, Na Cheng, Jingyuan Xu, Dong Li, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread
attention due to its excellent performance in real-time high-fidelity
rendering. However, in real-world environments with dynamic objects, existing
3DGS-based SLAM systems often face mapping errors and tracking drift issues. To
address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM
system tailored for dynamic scenes. In terms of tracking, unlike traditional
methods, we directly perform dynamic segmentation on Gaussians and map them
back to the front-end to obtain dynamic point labels through a Gaussian pyramid
network, achieving precise dynamic removal and robust tracking. For mapping, we
impose rendering penalties on dynamically labeled Gaussians, which are updated
through the network, to avoid irreversible erroneous removal caused by simple
pruning. Our results on real-world datasets demonstrate that our method is
competitive in tracking compared to baseline methods, generating fewer
artifacts and higher-quality reconstructions in rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Modality Prior-Induced Hallucinations in Multimodal Large
  Language Models via Deciphering Attention Causality <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyu Zhou, Yibo Yan, Xin Zou, Kun Wang, Aiwei Liu, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have emerged as a central focus in
both industry and academia, but often suffer from biases introduced by visual
and language priors, which can lead to multimodal hallucination. These biases
arise from the visual encoder and the Large Language Model (LLM) backbone,
affecting the attention mechanism responsible for aligning multimodal inputs.
Existing decoding-based mitigation methods focus on statistical correlations
and overlook the causal relationships between attention mechanisms and model
output, limiting their effectiveness in addressing these biases. To tackle this
issue, we propose a causal inference framework termed CausalMM that applies
structural causal modeling to MLLMs, treating modality priors as a confounder
between attention mechanisms and output. Specifically, by employing backdoor
adjustment and counterfactual reasoning at both the visual and language
attention levels, our method mitigates the negative effects of modality priors
and enhances the alignment of MLLM's inputs and outputs, with a maximum score
improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME
Benchmark compared to conventional methods. Extensive experiments validate the
effectiveness of our approach while being a plug-and-play solution. Our code is
available at: https://github.com/The-Martyr/CausalMM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Thirteenth International Conference on Learning
  Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian
  Neural Networks <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20891v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20891v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Gia Doan, Afshar Shamsi, Xiao-Yu Guo, Arash Mohammadi, Hamid Alinejad-Rokny, Dino Sejdinovic, Damien Teney, Damith C. Ranasinghe, Ehsan Abbasnejad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational complexity of Bayesian learning is impeding its adoption in
practical, large-scale tasks. Despite demonstrations of significant merits such
as improved robustness and resilience to unseen or out-of-distribution inputs
over their non- Bayesian counterparts, their practical use has faded to near
insignificance. In this study, we introduce an innovative framework to mitigate
the computational burden of Bayesian neural networks (BNNs). Our approach
follows the principle of Bayesian techniques based on deep ensembles, but
significantly reduces their cost via multiple low-rank perturbations of
parameters arising from a pre-trained neural network. Both vanilla version of
ensembles as well as more sophisticated schemes such as Bayesian learning with
Stein Variational Gradient Descent (SVGD), previously deemed impractical for
large models, can be seamlessly implemented within the proposed framework,
called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a
dramatic reduction in the number of trainable parameters required to
approximate a Bayesian posterior; and ii) it not only maintains, but in some
instances, surpasses the performance of conventional Bayesian learning methods
and non-Bayesian baselines. Our results with large-scale tasks such as
ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the
effectiveness and versatility of Bella in building highly scalable and
practical Bayesian deep models for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in AAAI'25", and the code is available at
  https://bnn-bella.github.io/BNN-Bella/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Ji, Yue Liu, Zhicheng Zhang, Zhao Zhang, Yuting Zhao, Xiaoshuai Hao, Gang Zhou, Xingwei Zhang, Xiaolong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) play a crucial role in the advancement of
Artificial General Intelligence (AGI). As AGI rapidly evolves, addressing
security concerns has emerged as one of the most significant challenges for
VLMs. In this paper, we present extensive experiments that expose the
vulnerabilities of conventional adaptation methods for VLMs, highlighting
significant security risks. Moreover, as VLMs grow in size, the application of
traditional adversarial adaptation techniques incurs substantial computational
costs. To address these issues, we propose a parameter-efficient adversarial
adaptation method called \textbf{\textit{AdvLoRA}} based on Low-Rank
Adaptation. We investigate and reveal the inherent low-rank properties involved
in adversarial adaptation for VLMs. Different from LoRA, we enhance the
efficiency and robustness of adversarial adaptation by introducing a novel
reparameterization method that leverages parameter clustering and alignment.
Additionally, we propose an adaptive parameter update strategy to further
bolster robustness. These innovations enable our AdvLoRA to mitigate issues
related to model security and resource wastage. Extensive experiments confirm
the effectiveness and efficiency of AdvLoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeRCULES: Heterogeneous Radar <span class="highlight-title">Dataset</span> in Complex Urban Environment for
  Multi-session Radar SLAM <span class="chip">ICRA
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjun Kim, Minwoo Jung, Chiyun Noh, Sangwoo Jung, Hyunho Song, Wooseong Yang, Hyesu Jang, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, radars have been widely featured in robotics for their robustness
in challenging weather conditions. Two commonly used radar types are spinning
radars and phased-array radars, each offering distinct sensor characteristics.
Existing datasets typically feature only a single type of radar, leading to the
development of algorithms limited to that specific kind. In this work, we
highlight that combining different radar types offers complementary advantages,
which can be leveraged through a heterogeneous radar dataset. Moreover, this
new dataset fosters research in multi-session and multi-robot scenarios where
robots are equipped with different types of radars. In this context, we
introduce the HeRCULES dataset, a comprehensive, multi-modal dataset with
heterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first
dataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering
unparalleled localization, mapping, and place recognition capabilities. The
dataset covers diverse weather and lighting conditions and a range of urban
traffic scenarios, enabling a comprehensive analysis across various
environments. The sequence paths with multiple revisits and ground truth pose
for each sensor enhance its suitability for place recognition research. We
expect the HeRCULES dataset to facilitate odometry, mapping, place recognition,
and sensor fusion research. The dataset and development tools are available at
https://sites.google.com/view/herculesdataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Robotics and Automation (ICRA
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TS40K: a 3D Point Cloud <span class="highlight-title">Dataset</span> of Rural Terrain and Electrical
  Transmission System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diogo Lavado, Cláudia Soares, Alessandra Micheletti, Ricardo Santos, André Coelho, João Santos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on supervised learning algorithms in 3D scene understanding has
risen in prominence and witness great increases in performance across several
datasets. The leading force of this research is the problem of autonomous
driving followed by indoor scene segmentation. However, openly available 3D
data on these tasks mainly focuses on urban scenarios. In this paper, we
propose TS40K, a 3D point cloud dataset that encompasses more than 40,000 Km on
electrical transmission systems situated in European rural terrain. This is not
only a novel problem for the research community that can aid in the high-risk
mission of power-grid inspection, but it also offers 3D point clouds with
distinct characteristics from those in self-driving and indoor 3D data, such as
high point-density and no occlusion. In our dataset, each 3D point is labeled
with 1 out of 22 annotated classes. We evaluate the performance of
state-of-the-art methods on our dataset concerning 3D semantic segmentation and
3D object detection. Finally, we provide a comprehensive analysis of the
results along with key challenges such as using labels that were not originally
intended for learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RedundancyLens: Revealing and Exploiting Visual Token Processing
  Redundancy for Efficient Decoder-Only MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongliang Li, Jiaxin Zhang, Wenhui Liao, Dezhi Peng, Kai Ding, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Multimodal Large Language Model (MLLM) architectures face a critical
tradeoff between performance and efficiency: decoder-only architectures achieve
higher performance but lower efficiency, while cross-attention-based
architectures offer greater efficiency but lower performance. The key
distinction lies in how visual tokens are processed. Decoder-only architectures
apply self-attention and FFN operations on visual tokens, while cross-attention
architectures skip these computations. To investigate whether redundancy exists
in this computationally expensive process, we propose a training-free framework
for analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and
Hollow Attention, which enable adjustable reductions in computations for visual
tokens, as well as a Layer Ranking Algorithm that prioritizes layers for these
reductions. Extensive experiments demonstrate substantial, structured, and
clustered redundancy unique to decoder-only MLLMs, offering valuable insights
for future MLLM architecture design. Furthermore, by leveraging our reduction
framework as a training-free inference acceleration approach, we achieve
performance comparable to or better than state-of-the-art methods while
remaining compatible with them. Code will be publicly available at
https://github.com/L-Hugh/RedundancyLens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The
  answer is No! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Fazli Imam, Chenyang Lyu, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have achieved significant
advancements in tasks like Visual Question Answering (VQA) by leveraging
foundational Large Language Models (LLMs). However, their abilities in specific
areas such as visual temporal understanding, which is crucial for comprehending
real-world dynamics, remain underexplored. To address this, we propose a
challenging evaluation benchmark named TemporalVQA, consisting of two parts: 1)
Temporal Order Understanding and 2) Time-lapse Estimation. The first part
requires MLLMs to determine the sequence of events by analyzing temporally
consecutive video frames. The second part presents image pairs with varying
time differences, framed as multiple-choice questions, asking MLLMs to estimate
the time-lapse between images with options ranging from seconds to years. Our
evaluations of advanced MLLMs, including models like GPT-4o and Gemini-1.5-Pro,
reveal significant challenges: GPT-4o achieved only 49.1% average consistent
accuracy in temporal order task and 70% in time-lapse estimation, with
open-source models performing even poorly. These findings underscore the
limitations of current MLLMs in visual temporal understanding and reasoning,
highlighting the need for further improvements for their temporal capability.
Our dataset can be found at
https://huggingface.co/datasets/fazliimam/temporal-vqa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our dataset can be found at
  \url{https://huggingface.co/datasets/fazliimam/temporal-vqa}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SNAT-YOLO: Efficient Cross-Layer Aggregation Network for Edge-Oriented
  Gangue Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the issues of slow detection speed,low accuracy,difficulty in
deployment on industrial edge devices,and large parameter and computational
requirements in deep learning-based coal gangue target detection methods,we
propose a lightweight coal gangue target detection algorithm based on an
improved YOLOv11.First,we use the lightweight network ShuffleNetV2 as the
backbone to enhance detection speed.Second,we introduce a lightweight
downsampling operation,ADown,which reduces model complexity while improving
average detection accuracy.Third,we improve the C2PSA module in YOLOv11 by
incorporating the Triplet Attention mechanism,resulting in the proposed
C2PSA-TriAtt module,which enhances the model's ability to focus on different
dimensions of images.Fourth,we propose the Inner-FocalerIoU loss function to
replace the existing CIoU loss function.Experimental results show that our
model achieves a detection accuracy of 99.10% in coal gangue detection
tasks,reduces the model size by 38%,the number of parameters by 41%,and the
computational cost by 40%,while decreasing the average detection time per image
by 1 ms.The improved model demonstrates enhanced detection speed and
accuracy,making it suitable for deployment on industrial edge mobile
devices,thus contributing positively to coal processing and efficient
utilization of coal resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Figure 1, due to our mistake, some parts of the picture are
  incorrect. We are making changes for resubmission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physical Coherence Benchmark for Evaluating Video Generation Models
  via Optical Flow-guided Frame Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongfan Chen, Xiuwen Zhu, Tianyu Li, Hao Chen, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in video generation models demonstrate their potential as
world simulators, but they often struggle with videos deviating from physical
laws, a key concern overlooked by most text-to-video benchmarks. We introduce a
benchmark designed specifically to assess the Physical Coherence of generated
videos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of
physical principles, capturing key physical laws observable in video content.
We evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and
conducted manual assessments. Additionally, we propose an automated evaluation
model: PhyCoPredictor, a diffusion model that generates optical flow and video
frames in a cascade manner. Through a consistency evaluation comparing
automated and manual sorting, the experimental results show that PhyCoPredictor
currently aligns most closely with human evaluation. Therefore, it can
effectively evaluate the physical coherence of videos, providing insights for
future model optimization. Our benchmark, including physical coherence prompts,
the automatic evaluation tool PhyCoPredictor, and the generated video dataset,
has been released on GitHub at https://github.com/Jeckinchen/PhyCoBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explanation Bottleneck Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17663v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17663v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shin'ya Yamaguchi, Kosuke Nishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent concept-based interpretable models have succeeded in providing
meaningful explanations by pre-defined concept sets. However, the dependency on
the pre-defined concepts restricts the application because of the limited
number of concepts for explanations. This paper proposes a novel interpretable
deep neural network called explanation bottleneck models (XBMs). XBMs generate
a text explanation from the input without pre-defined concepts and then predict
a final task prediction based on the generated explanation by leveraging
pre-trained vision-language encoder-decoder models. To achieve both the target
task performance and the explanation quality, we train XBMs through the target
task loss with the regularization penalizing the explanation decoder via the
distillation from the frozen pre-trained decoder. Our experiments, including a
comparison to state-of-the-art concept bottleneck models, confirm that XBMs
provide accurate and fluent natural language explanations without pre-defined
concept sets. Code is available at https://github.com/yshinya6/xbm/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VividMed: Vision Language Model with Versatile Visual Grounding for
  Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vision Language Models (VLMs) have demonstrated
remarkable promise in generating visually grounded responses. However, their
application in the medical domain is hindered by unique challenges. For
instance, most VLMs rely on a single method of visual grounding, whereas
complex medical tasks demand more versatile approaches. Additionally, while
most VLMs process only 2D images, a large portion of medical images are 3D. The
lack of medical data further compounds these obstacles. To address these
challenges, we present VividMed, a vision language model with versatile visual
grounding for medicine. Our model supports generating both semantic
segmentation masks and instance-level bounding boxes, and accommodates various
imaging modalities, including both 2D and 3D data. We design a three-stage
training procedure and an automatic data synthesis pipeline based on open
datasets and models. Besides visual grounding tasks, VividMed also excels in
other common downstream tasks, including Visual Question Answering (VQA) and
report generation. Ablation studies empirically show that the integration of
visual grounding ability leads to improved performance on these tasks. Our code
is publicly available at https://github.com/function2-llx/MMMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08557v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08557v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huali Xu, Shuaifeng Zhi, Shuzhou Sun, Vishal M. Patel, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning excels in computer vision tasks with abundant labeled
data, its performance diminishes significantly in scenarios with limited
labeled samples. To address this, Few-shot learning (FSL) enables models to
perform the target tasks with very few labeled examples by leveraging prior
knowledge from related tasks. However, traditional FSL assumes that both the
related and target tasks come from the same domain, which is a restrictive
assumption in many real-world scenarios where domain differences are common. To
overcome this limitation, Cross-domain few-shot learning (CDFSL) has gained
attention, as it allows source and target data to come from different domains
and label spaces. This paper presents the first comprehensive review of
Cross-domain Few-shot Learning (CDFSL), a field that has received less
attention compared to traditional FSL due to its unique challenges. We aim to
provide both a position paper and a tutorial for researchers, covering key
problems, existing methods, and future research directions. The review begins
with a formal definition of CDFSL, outlining its core challenges, followed by a
systematic analysis of current approaches, organized under a clear taxonomy.
Finally, we discuss promising future directions in terms of problem setups,
applications, and theoretical advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM Computing Surveys; 35 pages, 12 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dreamweaver: Learning Compositional World Representations from Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyeob Baek, Yi-Fu Wu, Gautam Singh, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans have an innate ability to decompose their perceptions of the world
into objects and their attributes, such as colors, shapes, and movement
patterns. This cognitive process enables us to imagine novel futures by
recombining familiar concepts. However, replicating this ability in artificial
intelligence systems has proven challenging, particularly when it comes to
modeling videos into compositional concepts and generating unseen, recomposed
futures without relying on auxiliary data, such as text, masks, or bounding
boxes. In this paper, we propose Dreamweaver, a neural architecture designed to
discover hierarchical and compositional representations from raw videos and
generate compositional future simulations. Our approach leverages a novel
Recurrent Block-Slot Unit (RBSU) to decompose videos into their constituent
objects and attributes. In addition, Dreamweaver uses a multi-future-frame
prediction objective to capture disentangled representations for dynamic
concepts more effectively as well as static concepts. In experiments, we
demonstrate our model outperforms current state-of-the-art baselines for world
modeling when evaluated under the DCI framework across multiple datasets.
Furthermore, we show how the modularized concept representations of our model
enable compositional imagination, allowing the generation of novel videos by
recombining attributes from different objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VarGes: Improving Variation in Co-Speech 3D Gesture Generation via
  StyleCLIPS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Meng, Ke Mu, Yonggui Zhu, Zhe Zhu, Haoyu Sun, Heyang Yan, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating expressive and diverse human gestures from audio is crucial in
fields like human-computer interaction, virtual reality, and animation. Though
existing methods have achieved remarkable performance, they often exhibit
limitations due to constrained dataset diversity and the restricted amount of
information derived from audio inputs. To address these challenges, we present
VarGes, a novel variation-driven framework designed to enhance co-speech
gesture generation by integrating visual stylistic cues while maintaining
naturalness. Our approach begins with the Variation-Enhanced Feature Extraction
(VEFE) module, which seamlessly incorporates \textcolor{blue}{style-reference}
video data into a 3D human pose estimation network to extract StyleCLIPS,
thereby enriching the input with stylistic information. Subsequently, we employ
the Variation-Compensation Style Encoder (VCSE), a transformer-style encoder
equipped with an additive attention mechanism pooling layer, to robustly encode
diverse StyleCLIPS representations and effectively manage stylistic variations.
Finally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio
features with StyleCLIPS encodings via cross-attention, injecting this fused
data into a cross-conditional autoregressive model to modulate 3D human gesture
generation based on audio input and stylistic clues. The efficacy of our
approach is validated on benchmark datasets, where it outperforms existing
methods in terms of gesture diversity and naturalness. The code and video
results will be made publicly available upon
acceptance:https://github.com/mookerr/VarGES/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyRefill: A Unified, Data-Efficient Framework for Left-<span class="highlight-title">Prompt</span>-Guided
  Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Xie, Chenjie Cao, Yunuo Cai, Xiangyang Xue, Yu-Gang Jiang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel Left-Prompt-Guided (LPG) paradigm to
address a diverse range of reference-based vision tasks. Inspired by the human
creative process, we reformulate these tasks using a left-right stitching
formulation to construct contextual input. Building upon this foundation, we
propose AnyRefill, an extension of LeftRefill, that effectively adapts
Text-to-Image (T2I) models to various vision tasks. AnyRefill leverages the
inpainting priors of advanced T2I model based on the Diffusion Transformer
(DiT) architecture, and incorporates flexible components to enhance its
capabilities. By combining task-specific LoRAs with the stitching input,
AnyRefill unlocks its potential across diverse tasks, including conditional
generation, visual perception, and image editing, without requiring additional
visual encoders. Meanwhile, AnyRefill exhibits remarkable data efficiency,
requiring minimal task-specific fine-tuning while maintaining high generative
performance. Through extensive ablation studies, we demonstrate that AnyRefill
outperforms other image condition injection methods and achieves competitive
results compared to state-of-the-art open-source methods. Notably, AnyRefill
delivers results comparable to advanced commercial tools, such as IC-Light and
SeedEdit, even in challenging scenarios. Comprehensive experiments and ablation
studies across versatile tasks validate the strong generation of the proposed
simple yet effective LPG formulation, establishing AnyRefill as a unified,
highly data-efficient solution for reference-based vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, submitted to TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RU-AI: A Large Multimodal <span class="highlight-title">Dataset</span> for Machine-Generated Content
  Detection <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04906v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04906v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liting Huang, Zhihao Zhang, Yiran Zhang, Xiyue Zhou, Shoujin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent generative AI models' capability of creating realistic and
human-like content is significantly transforming the ways in which people
communicate, create and work. The machine-generated content is a double-edged
sword. On one hand, it can benefit the society when used appropriately. On the
other hand, it may mislead people, posing threats to the society, especially
when mixed together with natural content created by humans. Hence, there is an
urgent need to develop effective methods to detect machine-generated content.
However, the lack of aligned multimodal datasets inhibited the development of
such methods, particularly in triple-modality settings (e.g., text, image, and
voice). In this paper, we introduce RU-AI, a new large-scale multimodal dataset
for robust and effective detection of machine-generated content in text, image
and voice. Our dataset is constructed on the basis of three large publicly
available datasets: Flickr8K, COCO and Places205, by adding their corresponding
AI duplicates, resulting in a total of 1,475,370 instances. In addition, we
created an additional noise variant of the dataset for testing the robustness
of detection models. We conducted extensive experiments with the current SOTA
detection methods on our dataset. The results reveal that existing models still
struggle to achieve accurate and robust detection on our dataset. We hope that
this new data set can promote research in the field of machine-generated
content detection, fostering the responsible use of generative AI. The source
code and datasets are available at https://github.com/ZhihaoZhang97/RU-AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'25 Resource Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human and AI Perceptual Differences in Image Classification Errors <span class="chip">AAAI 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Liu, Jiaheng Wei, Yang Liu, James Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) models for computer vision trained with
supervised machine learning are assumed to solve classification tasks by
imitating human behavior learned from training labels. Most efforts in recent
vision research focus on measuring the model task performance using
standardized benchmarks such as accuracy. However, limited work has sought to
understand the perceptual difference between humans and machines. To fill this
gap, this study first analyzes the statistical distributions of mistakes from
the two sources and then explores how task difficulty level affects these
distributions. We find that even when AI learns an excellent model from the
training data, one that outperforms humans in overall accuracy, these AI models
have significant and consistent differences from human perception. We
demonstrate the importance of studying these differences with a simple human-AI
teaming algorithm that outperforms humans alone, AI alone, or AI-AI teaming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 25 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex
  Diagrams in Coding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12381v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12381v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, Jacky Keung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and reasoning over diagrams is a fundamental aspect of human
intelligence. While Large Multimodal Models (LMMs) have demonstrated impressive
capabilities across various tasks, existing benchmarks lack comprehensive
evaluation of their diagram interpretation and reasoning abilities,
particularly in coding contexts. We present HumanEval-V, a rigorous benchmark
of human-annotated coding tasks that spans six task types and evaluates diverse
visual reasoning capabilities. Each task features carefully crafted diagrams
paired with function signatures and test cases, employing novel code generation
tasks to thoroughly assess models' diagram comprehension. Through extensive
experiments with 22 LMMs, we find that even top-performing models achieve
modest success rates, with Claude 3.5 Sonnet reaching only 36.8% pass@1,
highlighting substantial room for improvement. Our analysis reveals that
current LMMs struggle with spatial transformations, topological relationships,
and dynamic patterns that humans find intuitive. These findings provide
valuable insights for advancing LMMs' visual reasoning abilities. We have
open-sourced our code and benchmark at
https://github.com/HumanEval-V/HumanEval-V-Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>homepage https://humaneval-v.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20756v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20756v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Liu, Hao Liang, Bozhou Li, Tianyi Bai, Wentao Xiong, Chong Chen, Conghui He, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have recently emerged, demonstrating remarkable
vision-understanding capabilities. However, training these models requires
large-scale datasets, which brings challenges related to efficiency,
effectiveness, quality, and privacy of web data. In this paper, we introduce
SynthVLM, a novel data synthesis and curation method for generating
image-caption pairs. Unlike traditional methods, where captions are generated
from images, SynthVLM utilizes advanced diffusion models and high-quality
captions to automatically synthesize and select high-resolution images from
text descriptions, thereby creating precisely aligned image-text pairs. To
demonstrate the power of SynthVLM, we introduce SynthVLM-100K, a high-quality
dataset consisting of 100,000 curated and synthesized image-caption pairs. In
both model and human evaluations, SynthVLM-100K outperforms traditional
real-world datasets. Leveraging this dataset, we develop a new family of
multimodal large language models (MLLMs), SynthVLM-7B and SynthVLM-13B, which
achieve state-of-the-art (SOTA) performance on various vision
question-answering (VQA) tasks. Notably, our models outperform LLaVA across
most metrics with only 18\% pretrain data. Furthermore, SynthVLM-7B and
SynthVLM-13B attain SOTA performance on the MMLU benchmark, demonstrating that
the high-quality SynthVLM-100K dataset preserves language abilities. To
facilitate future research, our dataset and the complete data generating and
curating methods are open-sourced at
https://github.com/starriver030515/SynthVLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FrGNet: A fourier-guided weakly-supervised framework for nuclear
  instance segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Ling, Wenxiao Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nuclear instance segmentation has played a critical role in pathology image
analysis. The main challenges arise from the difficulty in accurately
segmenting instances and the high cost of precise mask-level annotations for
fully-supervised training.In this work, we propose a fourier guidance framework
for solving the weakly-supervised nuclear instance segmentation problem. In
this framework, we construct a fourier guidance module to fuse the priori
information into the training process of the model, which facilitates the model
to capture the relevant features of the nuclear. Meanwhile, in order to further
improve the model's ability to represent the features of nuclear, we propose
the guide-based instance level contrastive module. This module makes full use
of the framework's own properties and guide information to effectively enhance
the representation features of nuclear. We show on two public datasets that our
model can outperform current SOTA methods under fully-supervised design, and in
weakly-supervised experiments, with only a small amount of labeling our model
still maintains close to the performance under full supervision.In addition, we
also perform generalization experiments on a private dataset, and without any
labeling, our model is able to segment nuclear images that have not been seen
during training quite effectively. As open science, all codes and pre-trained
models are available at https://github.com/LQY404/FrGNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking and Improving Large Vision-Language Models for Fundamental
  Visual Graph Understanding and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across diverse tasks. Despite great success, recent studies show that LVLMs
encounter substantial limitations when engaging with visual graphs. To study
the reason behind these limitations, we propose VGCure, a comprehensive
benchmark covering 22 tasks for examining the fundamental graph understanding
and reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs
reveal that LVLMs are weak in basic graph understanding and reasoning tasks,
particularly those concerning relational or structurally complex information.
Based on this observation, we propose a structure-aware fine-tuning framework
to enhance LVLMs with structure learning abilities through three
self-supervised learning tasks. Experiments validate the effectiveness of our
method in improving LVLMs' performance on fundamental and downstream graph
learning tasks, as well as enhancing their robustness against complex visual
graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-han Li, Sandeep P. Chinchali, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal encoders like CLIP excel in tasks such as zero-shot image
classification and cross-modal retrieval. However, they require excessive
training data. We propose canonical similarity analysis (CSA), which uses two
unimodal encoders to replicate multimodal encoders using limited data. CSA maps
unimodal features into a multimodal space, using a new similarity score to
retain only the multimodal information. CSA only involves the inference of
unimodal encoders and a cubic-complexity matrix decomposition, eliminating the
need for extensive GPU-based model training. Experiments show that CSA
outperforms CLIP while requiring $50,000\times$ fewer multimodal data pairs to
bridge the modalities given pre-trained unimodal encoders on ImageNet
classification and misinformative news caption detection. CSA surpasses the
state-of-the-art method to map unimodal features to multimodal features. We
also demonstrate the ability of CSA with modalities beyond image and text,
paving the way for future modality pairs with limited paired multimodal data
but abundant unpaired unimodal data, such as lidar and text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Liu, Lei Zhu, Lijian Lin, Ye Zhu, Ailing Zhang, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D facial reconstruction from a single in-the-wild image is a crucial task in
human-centered computer vision tasks. While existing methods can recover
accurate facial shapes, there remains significant space for improvement in
fine-grained expression capture. Current approaches struggle with irregular
mouth shapes, exaggerated expressions, and asymmetrical facial movements. We
present TEASER (Token EnhAnced Spatial modeling for Expressions
Reconstruction), which addresses these challenges and enhances 3D facial
geometry performance. TEASER tackles two main limitations of existing methods:
insufficient photometric loss for self-reconstruction and inaccurate
localization of subtle expressions. We introduce a multi-scale tokenizer to
extract facial appearance information. Combined with a neural renderer, these
tokens provide precise geometric guidance for expression reconstruction.
Furthermore, TEASER incorporates a pose-dependent landmark loss to further
improve geometric performances. Our approach not only significantly enhances
expression reconstruction quality but also offers interpretable tokens suitable
for various downstream applications, such as photorealistic facial video
driving, expression transfer, and identity swapping. Quantitative and
qualitative experimental results across multiple datasets demonstrate that
TEASER achieves state-of-the-art performance in precise expression
reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Stop Overthinking at Test Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu Tran Bao, Nguyen Cong Dat, Nguyen Duc Anh, Hoang Thanh-Tung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test time scaling is currently one of the most active research areas that
shows promise after training time scaling has reached its limits. Deep-thinking
(DT) models are a class of recurrent models that can perform easy-to-hard
generalization by assigning more compute to harder test samples. However, due
to their inability to determine the complexity of a test sample, DT models have
to use a large amount of computation for both easy and hard test samples.
Excessive test time computation is wasteful and can cause the ``overthinking''
problem where more test time computation leads to worse results. In this paper,
we introduce a test time training method for determining the optimal amount of
computation needed for each sample during test time. We also propose
Conv-LiGRU, a novel recurrent architecture for efficient and robust visual
reasoning. Extensive experiments demonstrate that Conv-LiGRU is more stable
than DT, effectively mitigates the ``overthinking'' phenomenon, and achieves
superior accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Causally Informed <span class="highlight-title">Pretrain</span>ing Approach for Multimodal Foundation
  Models: Applications in Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19660v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19660v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praveen Ravirathinam, Ankush Khandelwal, Rahul Ghosh, Vipin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has emerged as a powerful paradigm for pretraining
foundation models using large-scale data. Existing pretraining approaches
predominantly rely on masked reconstruction or next-token prediction
strategies, demonstrating strong performance across various downstream tasks,
including geoscience applications. However, these approaches do not fully
capture the causal interplay between different geospatial and environmental
variables. To address this limitation, we propose Causally Informed
Variable-Step Forecasting (CI-VSF), a novel pretraining task that models
forecasting as a conditional generation task, where driver variables (e.g.,
weather) inform the prediction of response variables (e.g., satellite imagery).
We demonstrate that pretraining in such a fashion leads to enhanced performance
when finetuned on both prediction (e.g., crop mapping, missing image
prediction, soil moisture estimation) and forecasting (e.g., future image
forecasting, soil moisture forecasting) downstream tasks when compared to other
pretraining approaches. While we use remote sensing as our main application to
demonstrate the efficacy of our proposed pretraining strategy over existing
paradigms, it is applicable to any domain that involves known causal
relationships amongst a set of variables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages with appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Migician: Revealing the Magic of Free-Form Multi-Image Grounding in
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05767v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05767v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Li, Heyu Huang, Chi Chen, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancement of Multimodal Large Language Models (MLLMs) has
significantly improved their fine-grained perception of single images and
general comprehension across multiple images. However, existing MLLMs still
face challenges in achieving precise grounding in complex multi-image
scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework
that integrates single-image grounding with multi-image comprehension. While
partially effective, it remains unstable and struggles to capture abstract
visual information due to its non-end-to-end nature. Therefore, we introduce
Migician, the first multi-image grounding model capable of performing free-form
and accurate grounding across multiple images. To support this, we present the
MGrounding-630k dataset, which comprises data for several multi-image grounding
tasks derived from existing datasets, along with newly generated free-form
grounding instruction-following data. Furthermore, we propose MIG-Bench, a
comprehensive benchmark specifically designed for evaluating multi-image
grounding capabilities. Experimental results demonstrate that our model
achieves significantly superior multi-image grounding capabilities,
outperforming the best existing MLLMs by 24.94% and even surpassing much larger
70B models. Our code, model, dataset, and benchmark are fully open-sourced at
https://migician-vg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Visual Language Priors in VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00569v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00569v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, Honglak Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in Vision-Language Models (VLMs), they may over-rely
on visual language priors existing in their training data rather than true
visual reasoning. To investigate this, we introduce ViLP, a benchmark featuring
deliberately out-of-distribution images synthesized via image generation models
and out-of-distribution Q&A pairs. Each question in ViLP is coupled with three
potential answers and three corresponding images: one that can be resolved by
text priors alone and two that demand visual reasoning. Although, humans
achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves
only 66.17% on ViLP. To alleviate this, we propose a self-improving framework
in which models generate new VQA data, then apply pixel-level and semantic
corruptions to form "good-bad" image pairs for self-training. Our training
objectives compel VLMs to focus more on the actual visual inputs, and we
demonstrate their effectiveness in boosting the performance of open-source
VLMs, including LLaVA-v1.5 and Cambrian.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/ViLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DarSwin-Unet: Distortion Aware Encoder-Decoder Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshaya Athwale, Ichrak Shili, Émile Bergeron, Ola Ahmad, Jean-François Lalonde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wide-angle fisheye images are becoming increasingly common for perception
tasks in applications such as robotics, security, and mobility (e.g. drones,
avionics). However, current models often either ignore the distortions in
wide-angle images or are not suitable to perform pixel-level tasks. In this
paper, we present an encoder-decoder model based on a radial transformer
architecture that adapts to distortions in wide-angle lenses by leveraging the
physical characteristics defined by the radial distortion profile. In contrast
to the original model, which only performs classification tasks, we introduce a
U-Net architecture, DarSwin-Unet, designed for pixel level tasks. Furthermore,
we propose a novel strategy that minimizes sparsity when sampling the image for
creating its input tokens. Our approach enhances the model capability to handle
pixel-level tasks in wide-angle fisheye images, making it more effective for
real-world applications. Compared to other baselines, DarSwin-Unet achieves the
best results across different datasets, with significant gains when trained on
bounded levels of distortions (very low, low, medium, and high) and tested on
all, including out-of-distribution distortions. We demonstrate its performance
on depth estimation and show through extensive experiments that DarSwin-Unet
can perform zero-shot adaptation to unseen distortions of different wide-angle
lenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttributionScanner: A Visual Analytics System for Model Validation with
  Metadata-Free Slice Finding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwei Xuan, Jorge Piazentin Ono, Liang Gou, Kwan-Liu Ma, Liu Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data slice finding is an emerging technique for validating machine learning
(ML) models by identifying and analyzing subgroups in a dataset that exhibit
poor performance, often characterized by distinct feature sets or descriptive
metadata. However, in the context of validating vision models involving
unstructured image data, this approach faces significant challenges, including
the laborious and costly requirement for additional metadata and the complex
task of interpreting the root causes of underperformance. To address these
challenges, we introduce AttributionScanner, an innovative human-in-the-loop
Visual Analytics (VA) system, designed for metadata-free data slice finding.
Our system identifies interpretable data slices that involve common model
behaviors and visualizes these patterns through an Attribution Mosaic design.
Our interactive interface provides straightforward guidance for users to
detect, interpret, and annotate predominant model issues, such as spurious
correlations (model biases) and mislabeled data, with minimal effort.
Additionally, it employs a cutting-edge model regularization technique to
mitigate the detected issues and enhance the model's performance. The efficacy
of AttributionScanner is demonstrated through use cases involving two benchmark
datasets, with qualitative and quantitative evaluations showcasing its
substantial effectiveness in vision model validation, ultimately leading to
more reliable and accurate models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Skin Lesion Classification Generalization with Active Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to improve the generalization of skin lesion
classification models by combining self-supervised learning (SSL) and active
domain adaptation (ADA). The main steps of the approach include selection of an
SSL pre-trained model on natural image datasets, subsequent SSL retraining on
all available skin-lesion datasets, fine-tuning of the model on source domain
data with labels, and application of ADA methods on target domain data. The
efficacy of the proposed approach is assessed in ten skin lesion datasets with
five different ADA methods, demonstrating its potential to improve
generalization in settings with different amounts of domain shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 2 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Logic For Pediatric Brain Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Bengtsson, Elif Keles, Gorkem Durak, Syed Anwar, Yuri S. Velichko, Marius G. Linguraru, Angela J. Waanders, Ulas Bagci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel approach for segmenting pediatric brain
tumors using a deep learning architecture, inspired by expert radiologists'
segmentation strategies. Our model delineates four distinct tumor labels and is
benchmarked on a held-out PED BraTS 2024 test set (i.e., pediatric brain tumor
datasets introduced by BraTS). Furthermore, we evaluate our model's performance
against the state-of-the-art (SOTA) model using a new external dataset of 30
patients from CBTN (Children's Brain Tumor Network), labeled in accordance with
the PED BraTS 2024 guidelines and 2023 BraTS Adult Glioma dataset. We compare
segmentation outcomes with the winning algorithm from the PED BraTS 2023
challenge as the SOTA model. Our proposed algorithm achieved an average Dice
score of 0.642 and an HD95 of 73.0 mm on the CBTN test data, outperforming the
SOTA model, which achieved a Dice score of 0.626 and an HD95 of 84.0 mm.
Moreover, our model exhibits strong generalizability, attaining a 0.877 Dice
score in whole tumor segmentation on the BraTS 2023 Adult Glioma dataset,
surpassing existing SOTA. Our results indicate that the proposed model is a
step towards providing more accurate segmentation for pediatric brain tumors,
which is essential for evaluating therapy response and monitoring patient
progress. Our source code is available at
https://github.com/NUBagciLab/Pediatric-Brain-Tumor-Segmentation-Model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Text-to-Image Evaluation with Gecko: On Metrics, <span class="highlight-title">Prompt</span>s, and
  Human Ratings <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajić, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset, Aida Nematzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-image (T2I) generative models have become ubiquitous, they do
not necessarily generate images that align with a given prompt. While previous
work has evaluated T2I alignment by proposing metrics, benchmarks, and
templates for collecting human judgements, the quality of these components is
not systematically measured. Human-rated prompt sets are generally small and
the reliability of the ratings -- and thereby the prompt set used to compare
models -- is not evaluated. We address this gap by performing an extensive
study evaluating auto-eval metrics and human templates. We provide three main
contributions: (1) We introduce a comprehensive skills-based benchmark that can
discriminate models across different human templates. This skills-based
benchmark categorises prompts into sub-skills, allowing a practitioner to
pinpoint not only which skills are challenging, but at what level of complexity
a skill becomes challenging. (2) We gather human ratings across four templates
and four T2I models for a total of >100K annotations. This allows us to
understand where differences arise due to inherent ambiguity in the prompt and
where they arise due to differences in metric and model quality. (3) Finally,
we introduce a new QA-based auto-eval metric that is better correlated with
human ratings than existing metrics for our new dataset, across different human
templates, and on TIFA160.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VAPO: Visibility-Aware Keypoint Localization for Efficient 6DoF Object
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyi Lian, Yuewei Lin, Longin Jan Latecki, Haibin Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localizing predefined 3D keypoints in a 2D image is an effective way to
establish 3D-2D correspondences for 6DoF object pose estimation. However,
unreliable localization results of invisible keypoints degrade the quality of
correspondences. In this paper, we address this issue by localizing the
important keypoints in terms of visibility. Since keypoint visibility
information is currently missing in the dataset collection process, we propose
an efficient way to generate binary visibility labels from available
object-level annotations, for keypoints of both asymmetric objects and
symmetric objects. We further derive real-valued visibility-aware importance
from binary labels based on the PageRank algorithm. Taking advantage of the
flexibility of our visibility-aware importance, we construct VAPO
(Visibility-Aware POse estimator) by integrating the visibility-aware
importance with a state-of-the-art pose estimation algorithm, along with
additional positional encoding. VAPO can work in both CAD-based and CAD-free
settings. Extensive experiments are conducted on popular pose estimation
benchmarks including Linemod, Linemod-Occlusion, and YCB-V, demonstrating that
VAPO clearly achieves state-of-the-art performances. Our code is available at
https://github.com/RuyiLian/VAPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerAct: Advanced Activation Mechanism for Robust Inference of CNNs <span class="chip">AAAI 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04940v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04940v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kihyuk Yoon, Chiehyeon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel activation mechanism called LayerAct for
CNNs. This approach is motivated by our theoretical and experimental analyses,
which demonstrate that Layer Normalization (LN) can mitigate a limitation of
existing activation functions regarding noise robustness. However, LN is known
to be disadvantageous in CNNs due to its tendency to make activation outputs
homogeneous. The proposed method is designed to be more robust than existing
activation functions by reducing the upper bound of influence caused by input
shifts without inheriting LN's limitation. We provide analyses and experiments
showing that LayerAct functions exhibit superior robustness compared to
ElementAct functions. Experimental results on three clean and noisy benchmark
datasets for image classification tasks indicate that LayerAct functions
outperform other activation functions in handling noisy datasets while
achieving superior performance on clean datasets in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 4 tables except acknowledge, reference, and
  appendix. Accepted for the main track of AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEFANN: Scene Text Editor using Font Adaptive Neural Network <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1903.01192v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1903.01192v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual information in a captured scene plays an important role in scene
interpretation and decision making. Though there exist methods that can
successfully detect and interpret complex text regions present in a scene, to
the best of our knowledge, there is no significant prior work that aims to
modify the textual information in an image. The ability to edit text directly
on images has several advantages including error correction, text restoration
and image reusability. In this paper, we propose a method to modify text in an
image at character-level. We approach the problem in two stages. At first, the
unobserved character (target) is generated from an observed character (source)
being modified. We propose two different neural network architectures - (a)
FANnet to achieve structural consistency with source font and (b) Colornet to
preserve source color. Next, we replace the source character with the generated
character maintaining both geometric and visual consistency with neighboring
characters. Our method works as a unified platform for modifying text in
images. We present the effectiveness of our method on COCO-Text and ICDAR
datasets both qualitatively and quantitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2020</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning More Effective Representations for Dense Retrieval through
  Deliberate Thinking Before Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Ji, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan Liu, Yu Gu, Ge Yu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent dense retrievers usually thrive on the emergency capabilities of Large
Language Models (LLMs), using them to encode queries and documents into an
embedding space for retrieval. These LLM-based dense retrievers have shown
promising performance across various retrieval scenarios. However, relying on a
single embedding to represent documents proves less effective in capturing
different perspectives of documents for matching. In this paper, we propose
Deliberate Thinking based Dense Retriever (DEBATER), which enhances these
LLM-based retrievers by enabling them to learn more effective document
representations through a step-by-step thinking process. DEBATER introduces the
Chain-of-Deliberation mechanism to iteratively optimize document
representations using a continuous chain of thought. To consolidate information
from various thinking steps, DEBATER also incorporates the Self Distillation
mechanism, which identifies the most informative thinking steps and integrates
them into a unified text embedding. Experimental results show that DEBATER
significantly outperforms existing methods across several retrieval benchmarks,
demonstrating superior accuracy and robustness. All codes are available at
https://github.com/OpenBMB/DEBATER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Text-Image Interleaved Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Ziqi Dai, Yongqi Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Jun Yu, Wenjie Li, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal information retrieval studies mainly focus on single-image
inputs, which limits real-world applications involving multiple images and
text-image interleaved content. In this work, we introduce the text-image
interleaved retrieval (TIIR) task, where the query and document are interleaved
text-image sequences, and the model is required to understand the semantics
from the interleaved context for effective retrieval. We construct a TIIR
benchmark based on naturally interleaved wikiHow tutorials, where a specific
pipeline is designed to generate interleaved queries. To explore the task, we
adapt several off-the-shelf retrievers and build a dense baseline by
interleaved multimodal large language model (MLLM). We then propose a novel
Matryoshka Multimodal Embedder (MME), which compresses the number of visual
tokens at different granularity, to address the challenge of excessive visual
tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption
of existing models does not consistently yield effective results. Our MME
achieves significant improvements over the baseline by substantially fewer
visual tokens. We provide extensive analysis and will release the dataset and
code to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Context Information in Lifelong Sequential Modeling using
  Temporal Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Guo, Zhaoyang Yang, Qinsong Zeng, Ming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of lifelong sequential modeling (LSM) is growing in the realm
of social media recommendation systems. A key component in this process is the
attention module, which derives interest representations with respect to
candidate items from the sequence. Typically, attention modules function in a
point-wise fashion, concentrating only on the relevance of individual items in
the sequence to the candidate item. However, the context information in the
neighboring items that is useful for more accurately evaluating the
significance of each item has not been taken into account. In this study, we
introduce a novel network which employs the Temporal Convolutional Network
(TCN) to generate context-aware representations for each item throughout the
lifelong sequence. These improved representations are then utilized in the
attention module to produce context-aware interest representations. Expanding
on this TCN framework, we present a enhancement module which includes multiple
TCN layers and their respective attention modules to capture interest
representations across different context scopes. Additionally, we also
incorporate a lightweight sub-network to create convolution filters based on
users' basic profile features. These personalized filters are then applied in
the TCN layers instead of the original global filters to produce more
user-specific representations. We performed experiments on both a public
dataset and a proprietary dataset. The findings indicate that the proposed
network surpasses existing methods in terms of prediction accuracy and online
performance metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, including 1 page of reference, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Li, Xinni Zhang, Linhao Luo, Heng Chang, Yuxiang Ren, Irwin King, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable recommendation has demonstrated significant advantages in
informing users about the logic behind recommendations, thereby increasing
system transparency, effectiveness, and trustworthiness. To provide
personalized and interpretable explanations, existing works often combine the
generation capabilities of large language models (LLMs) with collaborative
filtering (CF) information. CF information extracted from the user-item
interaction graph captures the user behaviors and preferences, which is crucial
for providing informative explanations. However, due to the complexity of graph
structure, effectively extracting the CF information from graphs still remains
a challenge. Moreover, existing methods often struggle with the integration of
extracted CF information with LLMs due to its implicit representation and the
modality gap between graph structures and natural language explanations. To
address these challenges, we propose G-Refer, a framework using graph
retrieval-augmented large language models (LLMs) for explainable
recommendation. Specifically, we first employ a hybrid graph retrieval
mechanism to retrieve explicit CF signals from both structural and semantic
perspectives. The retrieved CF information is explicitly formulated as
human-understandable text by the proposed graph translation and accounts for
the explanations generated by LLMs. To bridge the modality gap, we introduce
knowledge pruning and retrieval-augmented fine-tuning to enhance the ability of
LLMs to process and utilize the retrieved CF information to generate
explanations. Extensive experiments show that G-Refer achieves superior
performance compared with existing methods in both explainability and
stability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025, research track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Principles to Applications: A Comprehensive <span class="highlight-title">Survey</span> of Discrete
  Tokenizers in Generation, Comprehension, Recommendation, and Information
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Jia, Jingtong Gao, Ben Xue, Junhao Wang, Qingpeng Cai, Quan Chen, Xiangyu Zhao, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete tokenizers have emerged as indispensable components in modern
machine learning systems, particularly within the context of autoregressive
modeling and large language models (LLMs). These tokenizers serve as the
critical interface that transforms raw, unstructured data from diverse
modalities into discrete tokens, enabling LLMs to operate effectively across a
wide range of tasks. Despite their central role in generation, comprehension,
and recommendation systems, a comprehensive survey dedicated to discrete
tokenizers remains conspicuously absent in the literature. This paper addresses
this gap by providing a systematic review of the design principles,
applications, and challenges of discrete tokenizers. We begin by dissecting the
sub-modules of tokenizers and systematically demonstrate their internal
mechanisms to provide a comprehensive understanding of their functionality and
design. Building on this foundation, we synthesize state-of-the-art methods,
categorizing them into multimodal generation and comprehension tasks, and
semantic tokens for personalized recommendations. Furthermore, we critically
analyze the limitations of existing tokenizers and outline promising directions
for future research. By presenting a unified framework for understanding
discrete tokenizers, this survey aims to guide researchers and practitioners in
addressing open challenges and advancing the field, ultimately contributing to
the development of more robust and versatile AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems often struggle with imperfect
retrieval, as traditional retrievers focus on lexical or semantic similarity
rather than logical relevance. To address this, we propose HopRAG, a novel RAG
framework that augments retrieval with logical reasoning through
graph-structured knowledge exploration. During indexing, HopRAG constructs a
passage graph, with text chunks as vertices and logical connections established
via LLM-generated pseudo-queries as edges. During retrieval, it employs a
retrieve-reason-prune mechanism: starting with lexically or semantically
similar passages, the system explores multi-hop neighbors guided by
pseudo-queries and LLM reasoning to identify truly relevant ones. Extensive
experiments demonstrate HopRAG's superiority, achieving 76.78\% higher answer
accuracy and 65.07\% improved retrieval F1 score compared to conventional
methods. The repository is available at https://github.com/LIU-Hao-2002/HopRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving the Cold Start Problem on One's Own as an End User via
  Preference Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach that enables end users to directly solve the cold
start problem by themselves. The cold start problem is a common issue in
recommender systems, and many methods have been proposed to address the problem
on the service provider's side. However, when the service provider does not
take action, users are left with poor recommendations and no means to improve
their experience. We propose an algorithm, Pretender, that allows end users to
proactively solve the cold start problem on their own. Pretender does not
require any special support from the service provider and can be deployed
independently by users. We formulate the problem as minimizing the distance
between the source and target distributions and optimize item selection from
the target service accordingly. Furthermore, we establish theoretical
guarantees for Pretender based on a discrete quadrature problem. We conduct
experiments on real-world datasets to demonstrate the effectiveness of
Pretender.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Range Retrieval with Graph-Based Indices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalen Dobson Manohar, Taekseung Kim, Guy E. Belloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving points based on proximity in a high-dimensional vector space is a
crucial step in information retrieval applications. The approximate nearest
neighbor search (ANNS) problem, which identifies the $k$ nearest neighbors for
a query (approximately, since exactly is hard), has been extensively studied in
recent years. However, comparatively little attention has been paid to the
related problem of finding all points within a given distance of a query, the
range retrieval problem, despite its applications in areas such as duplicate
detection, plagiarism checking, and facial recognition. In this paper, we
present a set of algorithms for range retrieval on graph-based vector indices,
which are known to achieve excellent performance on ANNS queries. Since a range
query may have anywhere from no matching results to thousands of matching
results in the database, we introduce a set of range retrieval algorithms based
on modifications of the standard graph search that adapt to terminate quickly
on queries in the former group, and to put more resources into finding results
for the latter group. Due to the lack of existing benchmarks for range
retrieval, we also undertake a comprehensive study of range characteristics of
existing embedding datasets, and select a suitable range retrieval radius for
eight existing datasets with up to 100 million points in addition to the one
existing benchmark. We test our algorithms on these datasets, and find up to
100x improvement in query throughput over a naive baseline approach, with 5-10x
improvement on average, and strong performance up to 100 million data points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question
  Answering? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in general
domains but often struggle with tasks requiring specialized knowledge.
Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve
external information from static knowledge bases, which can be outdated or
incomplete, missing fine-grained clinical details essential for accurate
medical question answering. In this work, we propose SearchRAG, a novel
framework that overcomes these limitations by leveraging real-time search
engines. Our method employs synthetic query generation to convert complex
medical questions into search-engine-friendly queries and utilizes
uncertainty-based knowledge selection to filter and incorporate the most
relevant and informative medical knowledge into the LLM's input. Experimental
results demonstrate that our method significantly improves response accuracy in
medical question answering tasks, particularly for complex questions requiring
detailed and up-to-date knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, three figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CuriousLLM: Elevating Multi-Document Question Answering with
  LLM-Enhanced Knowledge Graph Reasoning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zukang Yang, Zixuan Zhu, Xuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success in open-domain
question answering. However, they continue to face challenges such as
hallucinations and knowledge cutoffs. These issues can be mitigated through
in-context learning by providing LLMs with relevant context before generating
answers. Recent literature proposes Knowledge Graph Prompting (KGP) which
integrates knowledge graphs with an LLM-based traversal agent to substantially
enhance document retrieval quality. However, KGP requires costly fine-tuning
with large datasets and remains prone to hallucination. In this paper, we
propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning
mechanism into an LLM agent. This mechanism enables the agent to generate
relevant follow-up questions, thereby guiding the information retrieval process
more efficiently. Central to our approach is the development of the new
Follow-upQA dataset, which includes questions and supporting evidence as input,
with follow-up questions serving as ground truths. These follow-up questions
either inquire about what is still missing to fully answer the user's query or
use special tokens to signify that the retrieved evidence is sufficient. Our
experiments show that CuriousLLM significantly boosts LLM performance in
multi-document question answering (MD-QA), circumventing the substantial
computational costs and latency from the original KGP framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in NAACL 2025. The official version will be
  available in the ACL Anthology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unbiased Learning to Rank with Query-Level Click Propensity Estimation:
  Beyond Pointwise Observation and Relevance <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lulu Yu, Keping Bi, Jiafeng Guo, Shihao Liu, Dawei Yin, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing unbiased learning-to-rank (ULTR) approaches are based on the
user examination hypothesis, which assumes that users will click a result only
if it is both relevant and observed (typically modeled by position). However,
in real-world scenarios, users often click only one or two results after
examining multiple relevant options, due to limited patience or because their
information needs have already been satisfied. Motivated by this, we propose a
query-level click propensity model to capture the probability that users will
click on different result lists, allowing for non-zero probabilities that users
may not click on an observed relevant result. We hypothesize that this
propensity increases when more potentially relevant results are present, and
refer to this user behavior as relevance saturation bias. Our method introduces
a Dual Inverse Propensity Weighting (DualIPW) mechanism -- combining
query-level and position-level IPW -- to address both relevance saturation and
position bias. Through theoretical derivation, we prove that DualIPW can learn
an unbiased ranking model. Experiments on the real-world Baidu-ULTR dataset
demonstrate that our approach significantly outperforms state-of-the-art ULTR
baselines. The code and dataset information can be found at
https://github.com/Trustworthy-Information-Access/DualIPW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, accepted by The ACM Web Conference (WWW) 2025
  Short Paper Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SessionRec: Next Session Prediction Paradigm For Generative Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Huang, Hao Guo, Linzhi Peng, Long Zhang, Xiaoteng Wang, Daoyuan Wang, Shichao Wang, Jinpeng Wang, Lei Wang, Sheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for
generative sequential recommendation, addressing the fundamental misalignment
between conventional next-item prediction paradigm (NIPP) and real-world
recommendation scenarios. Unlike NIPP's item-level autoregressive generation
that contradicts actual session-based user interactions, our framework
introduces a session-aware representation learning through hierarchical
sequence aggregation (intra/inter-session), reducing attention computation
complexity while enabling implicit modeling of massive negative interactions,
and a session-based prediction objective that better captures users' diverse
interests through multi-item recommendation in next sessions. Moreover, we
found that incorporating a rank loss for items within the session under the
next session prediction paradigm can significantly improve the ranking
effectiveness of generative sequence recommendation models. We also verified
that SessionRec exhibits clear power-law scaling laws similar to those observed
in LLMs. Extensive experiments conducted on public datasets and online A/B test
in Meituan App demonstrate the effectiveness of SessionRec. The proposed
paradigm establishes new foundations for developing industrial-scale generative
recommendation systems through its model-agnostic architecture and
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CODE-ACCORD: A Corpus of building regulatory data for rule generation
  towards automatic compliance checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02231v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02231v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Amna Dridi, Mohamed Medhat Gaber, Pouyan Parsafard, Nicoleta Bocaneala, Katja Breitenfelder, Gonçal Costa, Maria Hedblom, Mihaela Juganaru-Mathieu, Thamer Mecharnia, Sumee Park, He Tan, Abdel-Rahman H. Tawil, Edlira Vakaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and
Construction (AEC) sector necessitates automating the interpretation of
building regulations to achieve its full potential. Converting textual rules
into machine-readable formats is challenging due to the complexities of natural
language and the scarcity of resources for advanced Machine Learning (ML).
Addressing these challenges, we introduce CODE-ACCORD, a dataset of 862
sentences from the building regulations of England and Finland. Only the
self-contained sentences, which express complete rules without needing
additional context, were considered as they are essential for ACC. Each
sentence was manually annotated with entities and relations by a team of 12
annotators to facilitate machine-readable rule generation, followed by careful
curation to ensure accuracy. The final dataset comprises 4,297 entities and
4,329 relations across various categories, serving as a robust ground truth.
CODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,
including text classification, entity recognition, and relation extraction. It
enables applying recent trends, such as deep neural networks and large language
models, to ACC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of an article published in the Scientific Data
  Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Ended and Knowledge-Intensive Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Zarif Ul Alam, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question answering that requires external knowledge beyond the visual
content remains a significant challenge in AI systems. While models can
effectively answer questions based on direct visual observations, they often
falter when faced with questions requiring broader contextual knowledge. To
address this limitation, we investigate knowledge-intensive video question
answering (KI-VideoQA) through the lens of multi-modal retrieval-augmented
generation, with a particular focus on handling open-ended questions rather
than just multiple-choice formats. Our comprehensive analysis examines various
retrieval augmentation approaches using cutting-edge retrieval and vision
language models, testing both zero-shot and fine-tuned configurations. We
investigate several critical dimensions: the interplay between different
information sources and modalities, strategies for integrating diverse
multi-modal contexts, and the dynamics between query formulation and retrieval
result utilization. Our findings reveal that while retrieval augmentation shows
promise in improving model performance, its success is heavily dependent on the
chosen modality and retrieval methodology. The study also highlights the
critical role of query construction and retrieval depth optimization in
effective knowledge integration. Through our proposed approach, we achieve a
substantial 17.5% improvement in accuracy on multiple choice questions in the
KnowIT VQA dataset, establishing new state-of-the-art performance levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Evaluators for Conversational Recommender
  Systems: Benchmarking System Performance from a User-Centric Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Quanyu Dai, Xiaoyu Dong, Xiao-Ming Wu, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRS) involve both recommendation and
dialogue tasks, which makes their evaluation a unique challenge. Although past
research has analyzed various factors that may affect user satisfaction with
CRS interactions from the perspective of user studies, few evaluation metrics
for CRS have been proposed. Recent studies have shown that LLMs can align with
human preferences, and several LLM-based text quality evaluation measures have
been introduced. However, the application of LLMs in CRS evaluation remains
relatively limited. To address this research gap and advance the development of
user-centric conversational recommender systems, this study proposes an
automated LLM-based CRS evaluation framework, building upon existing research
in human-computer interaction and psychology. The framework evaluates CRS from
four dimensions: dialogue behavior, language expression, recommendation items,
and response content. We use this framework to evaluate four different
conversational recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African
  clean water access, sanitation and hygiene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kloker, Alex Cedric Luyima, Matthew Bazanya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper. Accepted at IST-Africa Conference 2025, Nairobi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creating a Taxonomy for Retrieval Augmented Generation Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02854v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02854v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Nikishina, Özge Sevgili, Mahei Manhai Li, Chris Biemann, Martin Semmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we develop a taxonomy to conceptualize a comprehensive
overview of the constituting characteristics that define retrieval augmented
generation (RAG) applications, facilitating the adoption of this technology for
different application domains. To the best of our knowledge, no holistic RAG
application taxonomies have been developed so far. We employ the method foreign
to ACL and thus contribute to the set of methods in the taxonomy creation. It
comprises four iterative phases designed to refine and enhance our
understanding and presentation of RAG's core dimensions. We have developed a
total of five meta-dimensions and sixteen dimensions to comprehensively capture
the concept of RAG applications. Thus, the taxonomy can be used to better
understand RAG applications and to derive design knowledge for future solutions
in specific application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy and Analysis of Sensitive User Queries in Generative AI Search <span class="chip">NAACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwiyeol Jo, Taiwoo Park, Hyunwoo Lee, Nayoung Choi, Changbong Kim, Ohjoon Kwon, Donghyeon Jeon, Eui-Hyeon Lee, Kyoungho Shin, Sun Suk Lim, Kyungmi Kim, Jihye Lee, Sun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although there has been a growing interest among industries in integrating
generative LLMs into their services, limited experience and scarcity of
resources act as a barrier in launching and servicing large-scale LLM-based
services. In this paper, we share our experiences in developing and operating
generative AI models within a national-scale search engine, with a specific
focus on the sensitiveness of user queries. We propose a taxonomy for sensitive
search queries, outline our approaches, and present a comprehensive analysis
report on sensitive queries from actual users. We believe that our experiences
in launching generative AI search systems can contribute to reducing the
barrier in building generative LLM-based services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL2025(Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Learning for Trustworthy Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Li, Shoujin Wang, Qi Zhang, Longbing Cao, Fang Chen, Xiuzhen Zhang, Dietmar Jannach, Charu C. Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RS) have significantly advanced online content filtering
and personalized decision-making. However, emerging vulnerabilities in RS have
catalyzed a paradigm shift towards Trustworthy RS (TRS). Despite substantial
progress on TRS, most efforts focus on data correlations while overlooking the
fundamental causal nature of recommendations. This drawback hinders TRS from
identifying the root cause of trustworthiness issues, leading to limited
fairness, robustness, and explainability. To bridge this gap, causal learning
emerges as a class of promising methods to augment TRS. These methods, grounded
in reliable causality, excel in mitigating various biases and noise while
offering insightful explanations for TRS. However, there is a lack of timely
and dedicated surveys in this vibrant area. This paper creates an overview of
TRS from the perspective of causal learning. We begin by presenting the
advantages and common procedures of Causality-oriented TRS (CTRS). Then, we
identify potential trustworthiness challenges at each stage and link them to
viable causal solutions, followed by a classification of CTRS methods. Finally,
we discuss several future directions for advancing this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating with Fairness: A Modality-Diffused Counterfactual Framework
  for Incomplete Multimodal Recommendations <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Li, Shoujin Wang, Qi Zhang, Shui Yu, Fang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete scenario is a prevalent, practical, yet challenging setting in
Multimodal Recommendations (MMRec), where some item modalities are missing due
to various factors. Recently, a few efforts have sought to improve the
recommendation accuracy by exploring generic structures from incomplete data.
However, two significant gaps persist: 1) the difficulty in accurately
generating missing data due to the limited ability to capture modality
distributions; and 2) the critical but overlooked visibility bias, where items
with missing modalities are more likely to be disregarded due to the
prioritization of items' multimodal data over user preference alignment. This
bias raises serious concerns about the fair treatment of items. To bridge these
two gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)
framework for incomplete multimodal recommendations. MoDiCF features two key
modules: a novel modality-diffused data completion module and a new
counterfactual multimodal recommendation module. The former, equipped with a
particularly designed multimodal generative framework, accurately generates and
iteratively refines missing data from learned modality-specific distribution
spaces. The latter, grounded in the causal perspective, effectively mitigates
the negative causal effects of visibility bias and thus assures fairness in
recommendations. Both modules work collaboratively to address the two
aforementioned significant gaps for generating more accurate and fair results.
Extensive experiments on three real-world datasets demonstrate the superior
performance of MoDiCF in terms of both recommendation accuracy and fairness.
The code and processed datasets are released at
https://github.com/JinLi-i/MoDiCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPRM: A LLM-based Continual <span class="highlight-title">Pre-train</span>ing Framework for Relevance
  Modeling in Commercial Search <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01269v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01269v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixin Wu, Yixin Ji, Zeyuan Chen, Qiang Wang, Cunxiang Wang, Hong Liu, Baijun Ji, Jia Xu, Zhongyi Liu, Jinjie Gu, Yuan Zhou, Linjian Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relevance modeling between queries and items stands as a pivotal component in
commercial search engines, directly affecting the user experience. Given the
remarkable achievements of large language models (LLMs) in various natural
language processing (NLP) tasks, LLM-based relevance modeling is gradually
being adopted within industrial search systems. Nevertheless, foundational LLMs
lack domain-specific knowledge and do not fully exploit the potential of
in-context learning. Furthermore, structured item text remains underutilized,
and there is a shortage in the supply of corresponding queries and background
knowledge. We thereby propose CPRM (Continual Pre-training for Relevance
Modeling), a framework designed for the continual pre-training of LLMs to
address these issues. Our CPRM framework includes three modules: 1) employing
both queries and multi-field item to jointly pre-train for enhancing domain
knowledge, 2) applying in-context pre-training, a novel approach where LLMs are
pre-trained on a sequence of related queries or items, and 3) conducting
reading comprehension on items to produce associated domain knowledge and
background information (e.g., generating summaries and corresponding queries)
to further strengthen LLMs. Results on offline experiments and online A/B
testing demonstrate that our model achieves convincing performance compared to
strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aspect-Aware Decomposition for Opinion Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Jey Han Lau, Eduard Hovy, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion summarization plays a key role in deriving meaningful insights from
large-scale online reviews. To make this process more explainable and grounded,
we propose a modular approach guided by review aspects which separates the
tasks of aspect identification, opinion consolidation, and meta-review
synthesis, enabling greater transparency and ease of inspection. We conduct
extensive experiments across datasets representing scientific research,
business, and product domains. Results show that our method generates more
grounded summaries compared to strong baseline models, as verified through
automated and human evaluations. Additionally, our modular approach, which
incorporates reasoning based on review aspects, produces more informative
intermediate outputs than knowledge-agnostic decomposed prompting. These
intermediate outputs can also effectively support humans in summarizing
opinions from large volumes of reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive In-Context Learning with Large Language Models for Bundle
  Generation <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Sun, Kaidong Feng, Jie Yang, Xinghua Qu, Hui Fang, Yew-Soon Ong, Wenyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing bundle generation approaches fall short in generating
fixed-size bundles. Furthermore, they often neglect the underlying user intents
reflected by the bundles in the generation process, resulting in less
intelligible bundles. This paper addresses these limitations through the
exploration of two interrelated tasks, i.e., personalized bundle generation and
the underlying intent inference, based on different user sessions. Inspired by
the reasoning capabilities of large language models (LLMs), we propose an
adaptive in-context learning paradigm, which allows LLMs to draw tailored
lessons from related sessions as demonstrations, enhancing the performance on
target sessions. Specifically, we first employ retrieval augmented generation
to identify nearest neighbor sessions, and then carefully design prompts to
guide LLMs in executing both tasks on these neighbor sessions. To tackle
reliability and hallucination challenges, we further introduce (1) a
self-correction strategy promoting mutual improvements of the two tasks without
supervision signals and (2) an auto-feedback mechanism for adaptive supervision
based on the distinct mistakes made by LLMs on different neighbor sessions.
Thereby, the target session can gain customized lessons for improved
performance by observing the demonstrations of its neighbor sessions.
Experiments on three real-world datasets demonstrate the effectiveness of our
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Graph Diffusion with Applications in Personalized
  PageRanks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00077v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00077v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhe Wei, Eli Chien, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph diffusion, which iteratively propagates real-valued substances among
the graph, is used in numerous graph/network-involved applications. However,
releasing diffusion vectors may reveal sensitive linking information in the
data such as transaction information in financial network data. However,
protecting the privacy of graph data is challenging due to its interconnected
nature. This work proposes a novel graph diffusion framework with edge-level
differential privacy guarantees by using noisy diffusion iterates. The
algorithm injects Laplace noise per diffusion iteration and adopts a
degree-based thresholding function to mitigate the high sensitivity induced by
low-degree nodes. Our privacy loss analysis is based on Privacy Amplification
by Iteration (PABI), which to our best knowledge, is the first effort that
analyzes PABI with Laplace noise and provides relevant applications. We also
introduce a novel Infinity-Wasserstein distance tracking method, which tightens
the analysis of privacy leakage and makes PABI more applicable in practice. We
evaluate this framework by applying it to Personalized Pagerank computation for
ranking tasks. Experiments on real-world network data demonstrate the
superiority of our method under stringent privacy conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Code Available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-han Li, Sandeep P. Chinchali, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal encoders like CLIP excel in tasks such as zero-shot image
classification and cross-modal retrieval. However, they require excessive
training data. We propose canonical similarity analysis (CSA), which uses two
unimodal encoders to replicate multimodal encoders using limited data. CSA maps
unimodal features into a multimodal space, using a new similarity score to
retain only the multimodal information. CSA only involves the inference of
unimodal encoders and a cubic-complexity matrix decomposition, eliminating the
need for extensive GPU-based model training. Experiments show that CSA
outperforms CLIP while requiring $50,000\times$ fewer multimodal data pairs to
bridge the modalities given pre-trained unimodal encoders on ImageNet
classification and misinformative news caption detection. CSA surpasses the
state-of-the-art method to map unimodal features to multimodal features. We
also demonstrate the ability of CSA with modalities beyond image and text,
paving the way for future modality pairs with limited paired multimodal data
but abundant unpaired unimodal data, such as lidar and text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging AI and Science: Implications from a Large-Scale Literature
  Analysis of AI4Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xie, Yijun Pan, Hua Xu, Qiaozhu Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence has proven to be a transformative tool for advancing
scientific research across a wide range of disciplines. However, a significant
gap still exists between AI and scientific communities, limiting the full
potential of AI methods in driving broad scientific discovery. Existing efforts
in identifying and bridging this gap have often relied on qualitative
examination of small samples of literature, offering a limited perspective on
the broader AI4Science landscape. In this work, we present a large-scale
analysis of the AI4Science literature, starting by using large language models
to identify scientific problems and AI methods in publications from top science
and AI venues. Leveraging this new dataset, we quantitatively highlight key
disparities between AI methods and scientific problems, revealing substantial
opportunities for deeper AI integration across scientific disciplines.
Furthermore, we explore the potential and challenges of facilitating
collaboration between AI and scientific communities through the lens of link
prediction. Our findings and tools aim to promote more impactful
interdisciplinary collaborations and accelerate scientific discovery through
deeper and broader AI integration. Our code and dataset are available at:
https://github.com/charles-pyj/Bridging-AI-and-Science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do We Need Domain-Specific Embedding Models? An Empirical Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18511v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18511v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a crucial role in representing and retrieving
information across various NLP applications. Recent advancements in Large
Language Models (LLMs) have further enhanced the performance of embedding
models, which are trained on massive amounts of text covering almost every
domain. These models are often benchmarked on general-purpose datasets like
Massive Text Embedding Benchmark (MTEB), where they demonstrate superior
performance. However, a critical question arises: Is the development of
domain-specific embedding models necessary when general-purpose models are
trained on vast corpora that already include specialized domain texts? In this
paper, we empirically investigate this question, choosing the finance domain as
an example. We introduce the Finance Massive Text Embedding Benchmark
(FinMTEB), a counterpart to MTEB that consists of financial domain-specific
text datasets. We evaluate the performance of seven state-of-the-art embedding
models on FinMTEB and observe a significant performance drop compared to their
performance on MTEB. To account for the possibility that this drop is driven by
FinMTEB's higher complexity, we propose four measures to quantify dataset
complexity and control for this factor in our analysis. Our analysis provides
compelling evidence that state-of-the-art embedding models struggle to capture
domain-specific linguistic and semantic patterns. Moreover, we find that the
performance of general-purpose embedding models on MTEB is not correlated with
their performance on FinMTEB, indicating the need for domain-specific embedding
benchmarks for domain-specific embedding models. This study sheds light on
developing domain-specific embedding models in the LLM era. FinMTEB comes with
open-source code at https://github.com/yixuantt/FinMTEB
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yixuantt/FinMTEB, The newer version:
  arXiv:2502.10990</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yu Meng, Xuan Wang, Sheng Wang, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering latent topics from text corpora has been studied for decades.
Many existing topic models adopt a fully unsupervised setting, and their
discovered topics may not cater to users' particular interests due to their
inability of leveraging user guidance. Although there exist seed-guided topic
discovery approaches that leverage user-provided seeds to discover
topic-representative terms, they are less concerned with two factors: (1) the
existence of out-of-vocabulary seeds and (2) the power of pre-trained language
models (PLMs). In this paper, we generalize the task of seed-guided topic
discovery to allow out-of-vocabulary seeds. We propose a novel framework, named
SeeTopic, wherein the general knowledge of PLMs and the local semantics learned
from the input corpus can mutually benefit each other. Experiments on three
real datasets from different domains demonstrate the effectiveness of SeeTopic
in terms of topic coherence, accuracy, and diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages; Accepted to NAACL 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepResonance: Enhancing Multimodal Music Understanding via
  Music-centric Multi-way Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Mao, Mengjie Zhao, Qiyu Wu, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in music large language models (LLMs) have significantly
improved music understanding tasks, which involve the model's ability to
analyze and interpret various musical elements. These improvements primarily
focused on integrating both music and text inputs. However, the potential of
incorporating additional modalities such as images, videos and textual music
features to enhance music understanding remains unexplored. To bridge this gap,
we propose DeepResonance, a multimodal music understanding LLM fine-tuned via
multi-way instruction tuning with multi-way aligned music, text, image, and
video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and
Music4way-Any2T, three 4-way training and evaluation datasets designed to
enable DeepResonance to integrate both visual and textual music feature
content. We also introduce multi-sampled ImageBind embeddings and a
pre-alignment Transformer to enhance modality fusion prior to input into text
LLMs, tailoring DeepResonance for multi-way instruction tuning. Our model
achieves state-of-the-art performances across six music understanding tasks,
highlighting the benefits of the auxiliary modalities and the structural
superiority of DeepResonance. We plan to open-source the models and the newly
constructed datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEA: Low-Resource Safety Alignment for Multimodal Large Language Models
  via Synthetic Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have serious security
vulnerabilities.While safety alignment using multimodal datasets consisting of
text and data of additional modalities can effectively enhance MLLM's security,
it is costly to construct these datasets. Existing low-resource security
alignment methods, including textual alignment, have been found to struggle
with the security risks posed by additional modalities. To address this, we
propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes
embeddings of additional modality through gradient updates to expand textual
datasets. This enables multimodal safety alignment training even when only
textual data is available. Extensive experiments on image, video, and
audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding
on a single RTX3090 GPU within 24 seconds. SEA significantly improves the
security of MLLMs when faced with threats from additional modalities. To assess
the security risks introduced by video and audio, we also introduced a new
benchmark called VA-SafetyBench. High attack success rates across multiple
MLLMs validate its challenge. Our code and data will be available at
https://github.com/ZeroNLP/SEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> on Generative AI for Video-to-Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Songruoyao Wu, Zihao Wang, Shuyu Li, Kejun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning growth of video-to-music generation can be attributed to the
ascendancy of multimodal generative models. However, there is a lack of
literature that comprehensively combs through the work in this field. To fill
this gap, this paper presents a comprehensive review of video-to-music
generation using deep generative AI techniques, focusing on three key
components: visual feature extraction, music generation frameworks, and
conditioning mechanisms. We categorize existing approaches based on their
designs for each component, clarifying the roles of different strategies.
Preceding this, we provide a fine-grained classification of video and music
modalities, illustrating how different categories influence the design of
components within the generation pipelines. Furthermore, we summarize available
multimodal datasets and evaluation metrics while highlighting ongoing
challenges in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting
  View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance
Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to
represent complex geometry and appearance, GS achieves faster rendering times
and reduced memory consumption compared to the neural network approach used in
NeRF. However, quality assessment of GS-generated static content is not yet
explored in-depth. This paper describes a subjective quality assessment study
that aims to evaluate synthesized videos obtained with several static GS
state-of-the-art methods. The methods were applied to diverse visual scenes,
covering both 360-degree and forward-facing (FF) camera trajectories. Moreover,
the performance of 18 objective quality metrics was analyzed using the scores
resulting from the subjective study, providing insights into their strengths,
limitations, and alignment with human perception. All videos and scores are
made available providing a comprehensive database that can be used as benchmark
on GS view synthesis and objective quality metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantically Consistent Person Image Generation <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data-driven approach for context-aware person image generation.
Specifically, we attempt to generate a person image such that the synthesized
instance can blend into a complex scene. In our method, the position, scale,
and appearance of the generated person are semantically conditioned on the
existing persons in the scene. The proposed technique is divided into three
sequential steps. At first, we employ a Pix2PixHD model to infer a coarse
semantic mask that represents the new person's spatial location, scale, and
potential pose. Next, we use a data-centric approach to select the closest
representation from a precomputed cluster of fine semantic masks. Finally, we
adopt a multi-scale, attention-guided architecture to transfer the appearance
attributes from an exemplar image. The proposed strategy enables us to
synthesize semantically coherent realistic persons that can blend into an
existing scene without altering the global context. We conclude our findings
with relevant qualitative and quantitative evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scene Aware Person Image Generation through Global Contextual
  Conditioning <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person image generation is an intriguing yet challenging problem. However,
this task becomes even more difficult under constrained situations. In this
work, we propose a novel pipeline to generate and insert contextually relevant
person images into an existing scene while preserving the global semantics.
More specifically, we aim to insert a person such that the location, pose, and
scale of the person being inserted blends in with the existing persons in the
scene. Our method uses three individual networks in a sequential pipeline. At
first, we predict the potential location and the skeletal structure of the new
person by conditioning a Wasserstein Generative Adversarial Network (WGAN) on
the existing human skeletons present in the scene. Next, the predicted skeleton
is refined through a shallow linear network to achieve higher structural
accuracy in the generated image. Finally, the target image is generated from
the refined skeleton using another generative network conditioned on a given
image of the target person. In our experiments, we achieve high-resolution
photo-realistic generation results while preserving the general context of the
scene. We conclude our paper with multiple qualitative and quantitative
benchmarks on the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Pattern Recognition
  (ICPR) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIPS: Text-Induced Pose Synthesis <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, human pose synthesis and transfer deal with probabilistic
image generation of a person in a previously unseen pose from an already
available observation of that person. Though researchers have recently proposed
several methods to achieve this task, most of these techniques derive the
target pose directly from the desired target image on a specific dataset,
making the underlying process challenging to apply in real-world scenarios as
the generation of the target image is the actual aim. In this paper, we first
present the shortcomings of current pose transfer algorithms and then propose a
novel text-based pose transfer technique to address those issues. We divide the
problem into three independent stages: (a) text to pose representation, (b)
pose refinement, and (c) pose rendering. To the best of our knowledge, this is
one of the first attempts to develop a text-based pose transfer framework where
we also introduce a new dataset DF-PASS, by adding descriptive pose annotations
for the images of the DeepFashion dataset. The proposed method generates
promising results with significant qualitative and quantitative scores in our
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The European Conference on Computer Vision (ECCV) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-scale Attention Guided Pose Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose transfer refers to the probabilistic image generation of a person with a
previously unseen novel pose from another image of that person having a
different pose. Due to potential academic and commercial applications, this
problem is extensively studied in recent years. Among the various approaches to
the problem, attention guided progressive generation is shown to produce
state-of-the-art results in most cases. In this paper, we present an improved
network architecture for pose transfer by introducing attention links at every
resolution level of the encoder and decoder. By utilizing such dense
multi-scale attention guided approach, we are able to achieve significant
improvement over the existing methods both visually and analytically. We
conclude our findings with extensive qualitative and quantitative comparisons
against several existing methods on the DeepFashion dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Pattern Recognition (PR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UMETTS: A Unified Framework for Emotional Text-to-Speech Synthesis with
  Multimodal <span class="highlight-title">Prompt</span>s <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Qi Cheng, Xiang Li, Jun-Yan He, Junyao Chen, Xiaomao Fan, Xiaojiang Peng, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional Text-to-Speech (E-TTS) synthesis has garnered significant attention
in recent years due to its potential to revolutionize human-computer
interaction. However, current E-TTS approaches often struggle to capture the
intricacies of human emotions, primarily relying on oversimplified emotional
labels or single-modality input. In this paper, we introduce the Unified
Multimodal Prompt-Induced Emotional Text-to-Speech System (UMETTS), a novel
framework that leverages emotional cues from multiple modalities to generate
highly expressive and emotionally resonant speech. The core of UMETTS consists
of two key components: the Emotion Prompt Alignment Module (EP-Align) and the
Emotion Embedding-Induced TTS Module (EMI-TTS). (1) EP-Align employs
contrastive learning to align emotional features across text, audio, and visual
modalities, ensuring a coherent fusion of multimodal information. (2)
Subsequently, EMI-TTS integrates the aligned emotional embeddings with
state-of-the-art TTS models to synthesize speech that accurately reflects the
intended emotions. Extensive evaluations show that UMETTS achieves significant
improvements in emotion accuracy and speech naturalness, outperforming
traditional E-TTS methods on both objective and subjective metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025, Code available at
  https://github.com/KTTRCDL/UMETTS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaDesigner: Advancing Artistic Typography Through AI-Driven,
  User-Centric, and Multilingual WordArt Synthesis <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19859v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19859v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng Xiang, Hanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng Geng, Xuansong Xie, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MetaDesigner introduces a transformative framework for artistic typography
synthesis, powered by Large Language Models (LLMs) and grounded in a
user-centric design paradigm. Its foundation is a multi-agent system comprising
the Pipeline, Glyph, and Texture agents, which collectively orchestrate the
creation of customizable WordArt, ranging from semantic enhancements to
intricate textural elements. A central feedback mechanism leverages insights
from both multimodal models and user evaluations, enabling iterative refinement
of design parameters. Through this iterative process, MetaDesigner dynamically
adjusts hyperparameters to align with user-defined stylistic and thematic
preferences, consistently delivering WordArt that excels in visual quality and
contextual resonance. Empirical evaluations underscore the system's versatility
and effectiveness across diverse WordArt applications, yielding outputs that
are both aesthetically compelling and context-sensitive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025, Project:
  https://modelscope.cn/studios/WordArt/WordArt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEFANN: Scene Text Editor using Font Adaptive Neural Network <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1903.01192v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1903.01192v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual information in a captured scene plays an important role in scene
interpretation and decision making. Though there exist methods that can
successfully detect and interpret complex text regions present in a scene, to
the best of our knowledge, there is no significant prior work that aims to
modify the textual information in an image. The ability to edit text directly
on images has several advantages including error correction, text restoration
and image reusability. In this paper, we propose a method to modify text in an
image at character-level. We approach the problem in two stages. At first, the
unobserved character (target) is generated from an observed character (source)
being modified. We propose two different neural network architectures - (a)
FANnet to achieve structural consistency with source font and (b) Colornet to
preserve source color. Next, we replace the source character with the generated
character maintaining both geometric and visual consistency with neighboring
characters. Our method works as a unified platform for modifying text in
images. We present the effectiveness of our method on COCO-Text and ICDAR
datasets both qualitatively and quantitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2020</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-17T00:00:00Z">2025-02-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">167</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models without Classifier-free Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Jianmin Bao, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Model-guidance (MG), a novel objective for training
diffusion model that addresses and removes of the commonly used Classifier-free
guidance (CFG). Our innovative approach transcends the standard modeling of
solely data distribution to incorporating the posterior probability of
conditions. The proposed technique originates from the idea of CFG and is easy
yet effective, making it a plug-and-play module for existing models. Our method
significantly accelerates the training process, doubles the inference speed,
and achieve exceptional quality that parallel and even surpass concurrent
diffusion models with CFG. Extensive experiments demonstrate the effectiveness,
efficiency, scalability on different models and datasets. Finally, we establish
state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.
Our code is available at https://github.com/tzco/Diffusion-wo-CFG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoLUT: Efficient Volumetric streaming enhanced by LUT-based
  super-resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendong Wang, Anlan Zhang, Yifan Yang, Lili Qiu, Yuqing Yang, Xinyang Jiang, Feng Qian, Suman Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D volumetric video provides immersive experience and is gaining traction in
digital media. Despite its rising popularity, the streaming of volumetric video
content poses significant challenges due to the high data bandwidth
requirement. A natural approach to mitigate the bandwidth issue is to reduce
the volumetric video's data rate by downsampling the content prior to
transmission. The video can then be upsampled at the receiver's end using a
super-resolution (SR) algorithm to reconstruct the high-resolution details.
While super-resolution techniques have been extensively explored and advanced
for 2D video content, there is limited work on SR algorithms tailored for
volumetric videos.
  To address this gap and the growing need for efficient volumetric video
streaming, we have developed VoLUT with a new SR algorithm specifically
designed for volumetric content. Our algorithm uniquely harnesses the power of
lookup tables (LUTs) to facilitate the efficient and accurate upscaling of
low-resolution volumetric data. The use of LUTs enables our algorithm to
quickly reference precomputed high-resolution values, thereby significantly
reducing the computational complexity and time required for upscaling. We
further apply adaptive video bit rate algorithm (ABR) to dynamically determine
the downsampling rate according to the network condition and stream the
selected video rate to the receiver. Compared to related work, VoLUT is the
first to enable high-quality 3D SR on commodity mobile devices at line-rate.
Our evaluation shows VoLUT can reduce bandwidth usage by 70% , boost QoE by
36.7% for volumetric video streaming and achieve
  3D SR speed-up with no quality compromise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Yang, Xinchen Zhang, Ye Tian, Chenming Shang, Minghao Xu, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of the autoregressive paradigm has made significant
advancement in Multimodal Large Language Models (MLLMs), with powerful models
like Show-o, Transfusion and Emu3 achieving notable progress in unified image
understanding and generation. For the first time, we uncover a common
phenomenon: the understanding capabilities of MLLMs are typically stronger than
their generative capabilities, with a significant gap between the two. Building
on this insight, we propose HermesFlow, a simple yet general framework designed
to seamlessly bridge the gap between understanding and generation in MLLMs.
Specifically, we take the homologous data as input to curate homologous
preference data of both understanding and generation. Through Pair-DPO and
self-play iterative optimization, HermesFlow effectively aligns multimodal
understanding and generation using homologous preference data. Extensive
experiments demonstrate the significant superiority of our approach over prior
methods, particularly in narrowing the gap between multimodal understanding and
generation. These findings highlight the potential of HermesFlow as a general
alignment framework for next-generation multimodal foundation models. Code:
https://github.com/Gen-Verse/HermesFlow
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Gen-Verse/HermesFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising
  Trajectory Sharpening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Tian, Ling Yang, Xinchen Zhang, Yunhai Tong, Mengdi Wang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Diffusion-Sharpening, a fine-tuning approach that enhances
downstream alignment by optimizing sampling trajectories. Existing RL-based
fine-tuning methods focus on single training timesteps and neglect
trajectory-level alignment, while recent sampling trajectory optimization
methods incur significant inference NFE costs. Diffusion-Sharpening overcomes
this by using a path integral framework to select optimal trajectories during
training, leveraging reward feedback, and amortizing inference costs. Our
method demonstrates superior training efficiency with faster convergence, and
best inference efficiency without requiring additional NFEs. Extensive
experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning
methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods
(e.g., Inference Scaling) across diverse metrics including text alignment,
compositional capabilities, and human preferences, offering a scalable and
efficient solution for future diffusion model fine-tuning. Code:
https://github.com/Gen-Verse/Diffusion-Sharpening
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Gen-Verse/Diffusion-Sharpening</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLARE: Feed-forward Geometry, Appearance and Camera Estimation from
  Uncalibrated Sparse Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FLARE, a feed-forward model designed to infer high-quality camera
poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8
inputs), which is a challenging yet practical setting in real-world
applications. Our solution features a cascaded learning paradigm with camera
pose serving as the critical bridge, recognizing its essential role in mapping
3D structures onto 2D image planes. Concretely, FLARE starts with camera pose
estimation, whose results condition the subsequent learning of geometric
structure and appearance, optimized through the objectives of geometry
reconstruction and novel-view synthesis. Utilizing large-scale public datasets
for training, our method delivers state-of-the-art performance in the tasks of
pose estimation, geometry reconstruction, and novel view synthesis, while
maintaining the inference efficiency (i.e., less than 0.5 seconds). The project
page and code can be found at: https://zhanghe3z.github.io/FLARE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Website: https://zhanghe3z.github.io/FLARE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRISM: Self-Pruning Intrinsic Selection Method for Training-Free
  Multimodal Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning refines pre-trained Multimodal Large Language
Models (MLLMs) to enhance their real-world task performance. However, the rapid
expansion of visual instruction datasets introduces significant data
redundancy, leading to excessive computational costs. Existing data selection
methods predominantly rely on proxy models or loss-based metrics, both of which
impose substantial computational overheads due to the necessity of model
inference and backpropagation. To address this challenge, we propose PRISM, a
novel training-free approach for efficient multimodal data selection. Unlike
existing methods, PRISM eliminates the reliance on proxy models, warm-up
pretraining, and gradient-based optimization. Instead, it leverages Pearson
correlation analysis to quantify the intrinsic visual encoding properties of
MLLMs, computing a task-specific correlation score to identify high-value
instances. This not only enbles data-efficient selection,but maintains the
original performance. Empirical evaluations across multiple MLLMs demonstrate
that PRISM reduces the overall time required for visual instruction tuning and
data selection to just 30% of conventional methods, while surpassing fully
fine-tuned models across eight multimodal and three language understanding
benchmarks, achieving a 101.7% relative improvement in final performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Monocular Event-Camera Motion Capture System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion capture systems are a widespread tool in research to record
ground-truth poses of objects. Commercial systems use reflective markers
attached to the object and then triangulate pose of the object from multiple
camera views. Consequently, the object must be visible to multiple cameras
which makes such multi-view motion capture systems unsuited for deployments in
narrow, confined spaces (e.g. ballast tanks of ships). In this technical report
we describe a monocular event-camera motion capture system which overcomes this
limitation and is ideally suited for narrow spaces. Instead of passive markers
it relies on active, blinking LED markers such that each marker can be uniquely
identified from the blinking frequency. The markers are placed at known
locations on the tracking object. We then solve the PnP (perspective-n-points)
problem to obtain the position and orientation of the object. The developed
system has millimeter accuracy, millisecond latency and we demonstrate that its
state estimate can be used to fly a small, agile quadrotor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Communications: A Unified Framework for Cross-modal Context-aware
  Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Rahim Tafazolli, Mehdi Bennis, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce token communications (TokCom), a unified
framework to leverage cross-modal context information in generative semantic
communications (GenSC). TokCom is a new paradigm, motivated by the recent
success of generative foundation models and multimodal large language models
(GFM/MLLMs), where the communication units are tokens, enabling efficient
transformer-based token processing at the transmitter and receiver. In this
paper, we introduce the potential opportunities and challenges of leveraging
context in GenSC, explore how to integrate GFM/MLLMs-based token processing
into semantic communication systems to leverage cross-modal context
effectively, present the key principles for efficient TokCom at various layers
in future wireless networks. We demonstrate the corresponding TokCom benefits
in a GenSC setup for image, leveraging cross-modal context information, which
increases the bandwidth efficiency by 70.8% with negligible loss of
semantic/perceptual quality. Finally, the potential research directions are
identified to facilitate adoption of TokCom in future wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Descriminative-Generative Custom Tokens for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pramuditha Perera, Matthew Trager, Luca Zancato, Alessandro Achille, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the possibility of learning custom tokens for
representing new concepts in Vision-Language Models (VLMs). Our aim is to learn
tokens that can be effective for both discriminative and generative tasks while
composing well with words to form new input queries. The targeted concept is
specified in terms of a small set of images and a parent concept described
using text. We operate on CLIP text features and propose to use a combination
of a textual inversion loss and a classification loss to ensure that text
features of the learned token are aligned with image features of the concept in
the CLIP embedding space. We restrict the learned token to a low-dimensional
subspace spanned by tokens for attributes that are appropriate for the given
super-class. These modifications improve the quality of compositions of the
learned token with natural language for generating new scenes. Further, we show
that learned custom tokens can be used to form queries for text-to-image
retrieval task, and also have the important benefit that composite queries can
be visualized to ensure that the desired concept is faithfully encoded. Based
on this, we introduce the method of Generation Aided Image Retrieval, where the
query is modified at inference time to better suit the search intent. On the
DeepFashion2 dataset, our method improves Mean Reciprocal Retrieval (MRR) over
relevant baselines by 7%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unhackable Temporal Rewarding for Scalable Video MLLMs <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of superior video-processing MLLMs, we have encountered a
perplexing paradox: the "anti-scaling law", where more data and larger models
lead to worse performance. This study unmasks the culprit: "temporal hacking",
a phenomenon where models shortcut by fixating on select frames, missing the
full video narrative. In this work, we systematically establish a comprehensive
theory of temporal hacking, defining it from a reinforcement learning
perspective, introducing the Temporal Perplexity (TPL) score to assess this
misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework
to mitigate the temporal hacking. Both theoretically and empirically, TPL
proves to be a reliable indicator of temporal modeling quality, correlating
strongly with frame activation patterns. Extensive experiments reveal that UTR
not only counters temporal hacking but significantly elevates video
comprehension capabilities. This work not only advances video-AI systems but
also illuminates the critical importance of aligning proxy rewards with true
objectives in MLLM development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumanGif: Single-View Human Diffusion with Generative Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While previous single-view-based 3D human reconstruction methods made
significant progress in novel view synthesis, it remains a challenge to
synthesize both view-consistent and pose-consistent results for animatable
human avatars from a single image input. Motivated by the success of 2D
character animation, we propose <strong>HumanGif</strong>, a single-view human
diffusion model with generative prior. Specifically, we formulate the
single-view-based 3D human novel view and pose synthesis as a
single-view-conditioned human diffusion process, utilizing generative priors
from foundational diffusion models. To ensure fine-grained and consistent novel
view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn
spatially aligned features from the input image, implicitly capturing the
relative camera and human pose transformation. Furthermore, we introduce an
image-level loss during optimization to bridge the gap between latent and image
spaces in diffusion models. Extensive experiments on RenderPeople and
DNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual
performance, with better generalizability for novel view and pose synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://skhu101.github.io/HumanGif/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Transparent Object Pose Estimation: A Fusion of GDR-Net and
  Edge Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tessa Pulli, Peter Hönig, Stefan Thalhammer, Matthias Hirschmanner, Markus Vincze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object pose estimation of transparent objects remains a challenging task in
the field of robot vision due to the immense influence of lighting, background,
and reflections. However, the edges of clear objects have the highest contrast,
which leads to stable and prominent features. We propose a novel approach by
incorporating edge detection in a pre-processing step for the tasks of object
detection and object pose estimation. We conducted experiments to investigate
the effect of edge detectors on transparent objects. We examine the performance
of the state-of-the-art 6D object pose estimation pipeline GDR-Net and the
object detector YOLOX when applying different edge detectors as pre-processing
steps (i.e., Canny edge detection with and without color information, and
holistically-nested edges (HED)). We evaluate the physically-based rendered
dataset Trans6D-32 K of transparent objects with parameters proposed by the BOP
Challenge. Our results indicate that applying edge detection as a
pre-processing enhances performance for certain objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at First Austrian Symposium on AI, Robotics, and Vision
  (AIROV 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Next-Day Wildfire Spread with Time Series and Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Lahrichi, Jesse Johnson, Jordan Malof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has demonstrated the potential of deep neural networks (DNNs)
to accurately predict next-day wildfire spread, based upon the current extent
of a fire and geospatial rasters of influential environmental covariates e.g.,
vegetation, topography, climate, and weather. In this work, we investigate a
recent transformer-based model, termed the SwinUnet, for next-day wildfire
prediction. We benchmark Swin-based models against several current
state-of-the-art models on WildfireSpreadTS (WFTS), a large public benchmark
dataset of historical wildfire events. We consider two next-day fire prediction
scenarios: when the model is given input of (i) a single previous day of data,
or (ii) five previous days of data. We find that, with the proper
modifications, SwinUnet achieves state-of-the-art accuracy on next-day
prediction for both the single-day and multi-day scenarios. SwinUnet's success
depends heavily upon utilizing pre-trained weights from ImageNet. Consistent
with prior work, we also found that models with multi-day-input always
outperformed models with single-day input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis
  with Differential Digital Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Liang, Fangkun Liu, Andong Li, Xiaodong Li, Chengshi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in visual speech recognition (VSR) have promoted progress
in lip-to-speech synthesis, where pre-trained VSR models enhance the
intelligibility of synthesized speech by providing valuable semantic
information. The success achieved by cascade frameworks, which combine
pseudo-VSR with pseudo-text-to-speech (TTS) or implicitly utilize the
transcribed text, highlights the benefits of leveraging VSR models. However,
these methods typically rely on mel-spectrograms as an intermediate
representation, which may introduce a key bottleneck: the domain gap between
synthetic mel-spectrograms, generated from inherently error-prone lip-to-speech
mappings, and real mel-spectrograms used to train vocoders. This mismatch
inevitably degrades synthesis quality. To bridge this gap, we propose Natural
Lip-to-Speech (NaturalL2S), an end-to-end framework integrating acoustic
inductive biases with differentiable speech generation components.
Specifically, we introduce a fundamental frequency (F0) predictor to capture
prosodic variations in synthesized speech. The predicted F0 then drives a
Differentiable Digital Signal Processing (DDSP) synthesizer to generate a
coarse signal which serves as prior information for subsequent speech
synthesis. Additionally, instead of relying on a reference speaker embedding as
an auxiliary input, our approach achieves satisfactory performance on speaker
similarity without explicitly modelling speaker characteristics. Both objective
and subjective evaluation results demonstrate that NaturalL2S can effectively
enhance the quality of the synthesized speech when compared to state-of-the-art
methods. Our demonstration page is accessible at
https://yifan-liang.github.io/NaturalL2S/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiFlow: A unified deep learning framework for multi-vessel
  classification, segmentation and clustering of phase-contrast MRI validated
  on a multi-site single ventricle patient cohort 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Yao, Nicole St. Clair, Gabriel F. Miller, FORCE Investigators, Jennifer A. Steeden, Rahul H. Rathod, Vivek Muthurangu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a unified deep learning (DL) framework, MultiFlowSeg, for
classification and segmentation of velocity-encoded phase-contrast magnetic
resonance imaging data, and MultiFlowDTC for temporal clustering of flow
phenotypes. Applied to the FORCE registry of Fontan procedure patients,
MultiFlowSeg achieved 100% classification accuracy for the aorta, SVC, and IVC,
and 94% for the LPA and RPA. It demonstrated robust segmentation with a median
Dice score of 0.91 (IQR: 0.86-0.93). The automated pipeline processed registry
data, achieving high segmentation success despite challenges like poor image
quality and dextrocardia. Temporal clustering identified five distinct patient
subgroups, with significant differences in clinical outcomes, including
ejection fraction, exercise tolerance, liver disease, and mortality. These
results demonstrate the potential of combining DL and time-varying flow data
for improved CHD prognosis and personalized care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Logic Elements Associated with Round-Off Errors and Gaussian Blur
  in Image Registration: A Simple Case of Commingling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serap A. Savari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete image registration can be a strategy to reconstruct signals from
samples corrupted by blur and noise. We examine superresolution and discrete
image registration for one-dimensional spatially-limited piecewise constant
functions which are subject to blur which is Gaussian or a mixture of Gaussians
as well as to round-off errors. Previous approaches address the signal recovery
problem as an optimization problem. We focus on a regime with low blur and
suggest that the operations of blur, sampling, and quantization are not unlike
the operation of a computer program and have an abstraction that can be studied
with a type of logic. When the minimum distance between discontinuity points is
between $1.5$ and 2 times the sampling interval, we can encounter the simplest
form of a type of interference between discontinuity points that we call
``commingling.'' We describe a way to reason about two sets of samples of the
same signal that will often result in the correct recovery of signal
amplitudes. We also discuss ways to estimate bounds on the distances between
discontinuity points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing Photorealism and Artifacts in Diffusion Model-Generated
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Kamali, Karyn Nakamura, Aakriti Kumar, Angelos Chatzimparmpas, Jessica Hullman, Matthew Groh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion model-generated images can appear indistinguishable from authentic
photographs, but these images often contain artifacts and implausibilities that
reveal their AI-generated provenance. Given the challenge to public trust in
media posed by photorealistic AI-generated images, we conducted a large-scale
experiment measuring human detection accuracy on 450 diffusion-model generated
images and 149 real images. Based on collecting 749,828 observations and 34,675
comments from 50,444 participants, we find that scene complexity of an image,
artifact types within an image, display time of an image, and human curation of
AI-generated images all play significant roles in how accurately people
distinguish real from AI-generated images. Additionally, we propose a taxonomy
characterizing artifacts often appearing in images generated by diffusion
models. Our empirical observations and taxonomy offer nuanced insights into the
capabilities and limitations of diffusion models to generate photorealistic
images in 2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 24 Figures, Accepted by ACM CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Inversion: A <span class="highlight-title">Survey</span> from GANs to Diffusion and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Chen, Jiangning Zhang, Yali Bi, Xiaobin Hu, Teng Hu, Zhucun Xue, Ran Yi, Yong Liu, Ying Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image inversion is a fundamental task in generative models, aiming to map
images back to their latent representations to enable downstream applications
such as editing, restoration, and style transfer. This paper provides a
comprehensive review of the latest advancements in image inversion techniques,
focusing on two main paradigms: Generative Adversarial Network (GAN) inversion
and diffusion model inversion. We categorize these techniques based on their
optimization methods. For GAN inversion, we systematically classify existing
methods into encoder-based approaches, latent optimization approaches, and
hybrid approaches, analyzing their theoretical foundations, technical
innovations, and practical trade-offs. For diffusion model inversion, we
explore training-free strategies, fine-tuning methods, and the design of
additional trainable modules, highlighting their unique advantages and
limitations. Additionally, we discuss several popular downstream applications
and emerging applications beyond image tasks, identifying current challenges
and future research directions. By synthesizing the latest developments, this
paper aims to provide researchers and practitioners with a valuable reference
resource, promoting further advancements in the field of image inversion. We
keep track of the latest works at https://github.com/RyanChenYN/ImageInversion
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust 6DoF Pose Tracking Considering Contour and Interior
  Correspondence Uncertainty for AR Assembly Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmented reality assembly guidance is essential for intelligent
manufacturing and medical applications, requiring continuous measurement of the
6DoF poses of manipulated objects. Although current tracking methods have made
significant advancements in accuracy and efficiency, they still face challenges
in robustness when dealing with cluttered backgrounds, rotationally symmetric
objects, and noisy sequences. In this paper, we first propose a robust
contour-based pose tracking method that addresses error-prone contour
correspondences and improves noise tolerance. It utilizes a fan-shaped search
strategy to refine correspondences and models local contour shape and noise
uncertainty as mixed probability distribution, resulting in a highly robust
contour energy function. Secondly, we introduce a CPU-only strategy to better
track rotationally symmetric objects and assist the contour-based method in
overcoming local minima by exploring sparse interior correspondences. This is
achieved by pre-sampling interior points from sparse viewpoint templates
offline and using the DIS optical flow algorithm to compute their
correspondences during tracking. Finally, we formulate a unified energy
function to fuse contour and interior information, which is solvable using a
re-weighted least squares algorithm. Experiments on public datasets and real
scenarios demonstrate that our method significantly outperforms
state-of-the-art monocular tracking methods and can achieve more than 100 FPS
using only a CPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Instrumentation and Measurement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Generalizable <span class="highlight-title">Prompt</span> for CLIP with Class Similarity Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehun Jung, Hyang-won Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In vision-language models (VLMs), prompt tuning has shown its effectiveness
in adapting models to downstream tasks. However, learned prompts struggle to
generalize to unseen classes, as they tend to overfit to the classes that are
targeted during prompt tuning. Examining failure cases, we observed that
learned prompts disrupt the semantics of unseen classes, generating text
embeddings with incorrect semantic relationships among classes. To address
this, we propose Similarity Alignment Regularization (SAR), which regularizes
learnable prompts to preserve the semantic relationships among classes captured
by hand-crafted prompts. Specifically, we first obtain novel classes related to
base classes using ChatGPT-4o and utilize them as potential unseen classes
during prompt tuning. Then, by targeting both base and novel classes, SAR
aligns the similarity relationships among text embeddings generated by
learnable prompts with the similarity relationships from hand-crafted prompts.
Extensive experiments applying SAR to existing prompt tuning methods
demonstrate its effectiveness in improving generalization to unseen classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Freda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  pySLAM is an open-source Python framework for Visual SLAM, supporting
monocular, stereo, and RGB-D cameras. It provides a flexible interface for
integrating both classical and modern local features, making it adaptable to
various SLAM tasks. The framework includes different loop closure methods, a
volumetric reconstruction pipeline, and support for depth prediction models.
Additionally, it offers a suite of tools for visual odometry and SLAM
applications. Designed for both beginners and experienced researchers, pySLAM
encourages community contributions, fostering collaborative development in the
field of Visual SLAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAPH<span class="highlight-title">GPT</span>-O: Synergistic Multimodal Comprehension and Generation on
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Multimodal Large Language Models (MLLMs) has enabled
the integration of multiple modalities, including texts and images, within the
large language model (LLM) framework. However, texts and images are usually
interconnected, forming a multimodal attributed graph (MMAG). It is
underexplored how MLLMs can incorporate the relational information
(\textit{i.e.}, graph structure) and semantic information (\textit{i.e.,} texts
and images) on such graphs for multimodal comprehension and generation. In this
paper, we propose GraphGPT-o, which supports omni-multimodal understanding and
creation on MMAGs. We first comprehensively study linearization variants to
transform semantic and structural information as input for MLLMs. Then, we
propose a hierarchical aligner that enables deep graph encoding, bridging the
gap between MMAGs and MLLMs. Finally, we explore the inference choices,
adapting MLLM to interleaved text and image generation in graph scenarios.
Extensive experiments on three datasets from different domains demonstrate the
effectiveness of our proposed method. Datasets and codes will be open-sourced
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihang Yuan, Siyuan Wang, Rui Xie, Hanling Zhang, Tongcheng Fang, Yuzhang Shang, Shengen Yan, Guohao Dai, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a
training-free paradigm that can make use of adaptive temporal compression in
latent space. While existing video generative models apply fixed compression
rates via pretrained VAE, we observe that real-world video content exhibits
substantial temporal non-uniformity, with high-motion segments containing more
information than static scenes. Based on this insight, DLFR-VAE dynamically
adjusts the latent frame rate according to the content complexity.
Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent
Frame Rate Scheduler that partitions videos into temporal chunks and adaptively
determines optimal frame rates based on information-theoretic content
complexity, and (2) A training-free adaptation mechanism that transforms
pretrained VAE architectures into a dynamic VAE that can process features with
variable frame rates. Our simple but effective DLFR-VAE can function as a
plug-and-play module, seamlessly integrating with existing video generation
models and accelerating the video generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Open-Vocabulary to Vocabulary-Free Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klara Reichard, Giulia Rizzoli, Stefano Gasperini, Lukas Hoyer, Pietro Zanuttigh, Nassir Navab, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary semantic segmentation enables models to identify novel object
categories beyond their training data. While this flexibility represents a
significant advancement, current approaches still rely on manually specified
class names as input, creating an inherent bottleneck in real-world
applications. This work proposes a Vocabulary-Free Semantic Segmentation
pipeline, eliminating the need for predefined class vocabularies. Specifically,
we address the chicken-and-egg problem where users need knowledge of all
potential objects within a scene to identify them, yet the purpose of
segmentation is often to discover these objects. The proposed approach
leverages Vision-Language Models to automatically recognize objects and
generate appropriate class names, aiming to solve the challenge of class
specification and naming quality. Through extensive experiments on several
public datasets, we highlight the crucial role of the text encoder in model
performance, particularly when the image text classes are paired with generated
descriptions. Despite the challenges introduced by the sensitivity of the
segmentation text encoder to false negatives within the class tagging process,
which adds complexity to the task, we demonstrate that our fully automated
pipeline significantly enhances vocabulary-free segmentation accuracy across
diverse real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to: Pattern Recognition Letters, Klara Reichard and Giulia
  Rizzoli equally contributed to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Knowledge About Perceptual Uncertainty Help an Agent in Automated
  Driving? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Grabowsky, Annika Mütze, Joshua Wendland, Nils Jansen, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents in real-world scenarios like automated driving deal with uncertainty
in their environment, in particular due to perceptual uncertainty. Although,
reinforcement learning is dedicated to autonomous decision-making under
uncertainty these algorithms are typically not informed about the uncertainty
currently contained in their environment. On the other hand, uncertainty
estimation for perception itself is typically directly evaluated in the
perception domain, e.g., in terms of false positive detection rates or
calibration errors based on camera images. Its use for deciding on
goal-oriented actions remains largely unstudied. In this paper, we investigate
how an agent's behavior is influenced by an uncertain perception and how this
behavior changes if information about this uncertainty is available. Therefore,
we consider a proxy task, where the agent is rewarded for driving a route as
fast as possible without colliding with other road users. For controlled
experiments, we introduce uncertainty in the observation space by perturbing
the perception of the given agent while informing the latter. Our experiments
show that an unreliable observation space modeled by a perturbed perception
leads to a defensive driving behavior of the agent. Furthermore, when adding
the information about the current uncertainty directly to the observation
space, the agent adapts to the specific situation and in general accomplishes
its task faster while, at the same time, accounting for risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining and Evaluating Visual Language Models' Basic Spatial Abilities:
  A Perspective from Psychometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Theory of Multiple Intelligences underscores the hierarchical nature of
cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer
a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual
Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial
Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13
mainstream VLMs through nine validated psychometric experiments reveals
significant gaps versus humans (average score 24.95 vs. 68.38), with three key
findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,
weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller
models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading
(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought
(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from
architectural constraints. Identified barriers include weak geometry encoding
and missing dynamic simulation. By linking psychometric BSAs to VLM
capabilities, we provide a diagnostic toolkit for spatial intelligence
evaluation, methodological foundations for embodied AI development, and a
cognitive science-informed roadmap for achieving human-like spatial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Audio-Visual Adversarial Vulnerability from Temporal and
  Modality Perspectives <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeliang Zhang, Susan Liang, Daiki Shimada, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While audio-visual learning equips models with a richer understanding of the
real world by leveraging multiple sensory modalities, this integration also
introduces new vulnerabilities to adversarial attacks.
  In this paper, we present a comprehensive study of the adversarial robustness
of audio-visual models, considering both temporal and modality-specific
vulnerabilities. We propose two powerful adversarial attacks: 1) a temporal
invariance attack that exploits the inherent temporal redundancy across
consecutive time segments and 2) a modality misalignment attack that introduces
incongruence between the audio and visual modalities. These attacks are
designed to thoroughly assess the robustness of audio-visual models against
diverse threats. Furthermore, to defend against such attacks, we introduce a
novel audio-visual adversarial training framework. This framework addresses key
challenges in vanilla adversarial training by incorporating efficient
adversarial perturbation crafting tailored to multi-modal data and an
adversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds
dataset demonstrate that our proposed temporal and modality-based attacks in
degrading model performance can achieve state-of-the-art performance, while our
adversarial training defense largely improves the adversarial robustness as
well as the adversarial training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aras Yurtman, Daan Van Wesenbeeck, Wannes Meert, Hendrik Blockeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Motif Discovery (TSMD) identifies repeating patterns in time
series data, but its unsupervised nature might result in motifs that are not
interesting to the user. To address this, we propose a framework that allows
the user to impose constraints on the motifs to be discovered, where
constraints can easily be defined according to the properties of the desired
motifs in the application domain. We also propose an efficient implementation
of the framework, the LoCoMotif-DoK algorithm. We demonstrate that
LoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic
data, outperforming other TSMD techniques which only support a limited form of
domain knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio
  Chord Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Waseem Akram, Stefano Dettori, Valentina Colla, Giorgio Carlo Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord recognition serves as a critical task in music information retrieval
due to the abstract and descriptive nature of chords in music analysis. While
audio chord recognition systems have achieved significant accuracy for small
vocabularies (e.g., major/minor chords), large-vocabulary chord recognition
remains a challenging problem. This complexity also arises from the inherent
long-tail distribution of chords, where rare chord types are underrepresented
in most datasets, leading to insufficient training samples. Effective chord
recognition requires leveraging contextual information from audio sequences,
yet existing models, such as combinations of convolutional neural networks,
bidirectional long short-term memory networks, and bidirectional transformers,
face limitations in capturing long-term dependencies and exhibit suboptimal
performance on large-vocabulary chord recognition tasks. This work proposes
ChordFormer, a novel conformer-based architecture designed to tackle structural
chord recognition (e.g., triads, bass, sevenths) for large vocabularies.
ChordFormer leverages conformer blocks that integrate convolutional neural
networks with transformers, thus enabling the model to capture both local
patterns and global dependencies effectively. By addressing challenges such as
class imbalance through a reweighted loss function and structured chord
representations, ChordFormer outperforms state-of-the-art models, achieving a
2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy
on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling
class imbalance, providing robust and balanced recognition across chord types.
This approach bridges the gap between theoretical music knowledge and practical
applications, advancing the field of large-vocabulary chord recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Intuitive physics understanding emerges from <span class="highlight-title">self-supervised</span> <span class="highlight-title">pretrain</span>ing
  on natural videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, <span class="highlight-author">Yann LeCun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the emergence of intuitive physics understanding in
general-purpose deep neural network models trained to predict masked regions in
natural videos. Leveraging the violation-of-expectation framework, we find that
video prediction models trained to predict outcomes in a learned representation
space demonstrate an understanding of various intuitive physics properties,
such as object permanence and shape consistency. In contrast, video prediction
in pixel space and multimodal large language models, which reason through text,
achieve performance closer to chance. Our comparisons of these architectures
reveal that jointly learning an abstract representation space while predicting
missing parts of sensory input, akin to predictive coding, is sufficient to
acquire an understanding of intuitive physics, and that even models trained on
one week of unique video achieve above chance performance. This challenges the
idea that core knowledge -- a set of innate systems to help understand the
world -- needs to be hardwired to develop an understanding of intuitive
physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages,14 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Bias Formation in Deep Neural Networks Through the Geometric
  Mechanisms of Human Visual Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ma, Bowei Liu, Wei Dai, Jiayi Chen, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often exhibit biases toward certain categories
during object recognition, even under balanced training data conditions. The
intrinsic mechanisms underlying these biases remain unclear. Inspired by the
human visual system, which decouples object manifolds through hierarchical
processing to achieve object recognition, we propose a geometric analysis
framework linking the geometric complexity of class-specific perceptual
manifolds in DNNs to model bias. Our findings reveal that differences in
geometric complexity can lead to varying recognition capabilities across
categories, introducing biases. To support this analysis, we present the
Perceptual-Manifold-Geometry library, designed for calculating the geometric
properties of perceptual manifolds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When performing 3D inpainting using novel-view rendering methods like Neural
Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture
and geometry consistency across camera views has been a challenge. In this
paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided
Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided
by the rendered depth information from each training view, our 3DGIC exploits
background pixels visible across different views for updating the inpainting
mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive
experiments on benchmark datasets, we confirm that our 3DGIC outperforms
current state-of-the-art 3D inpainting methods quantitatively and
qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Neural Networks for Accurate Depth Estimation with Latent Space
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddiqui Muhammad Yasir, Hyunsik Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation plays a pivotal role in advancing human-robot interactions,
especially in indoor environments where accurate 3D scene reconstruction is
essential for tasks like navigation and object handling. Monocular depth
estimation, which relies on a single RGB camera, offers a more affordable
solution compared to traditional methods that use stereo cameras or LiDAR.
However, despite recent progress, many monocular approaches struggle with
accurately defining depth boundaries, leading to less precise reconstructions.
In response to these challenges, this study introduces a novel depth estimation
framework that leverages latent space features within a deep convolutional
neural network to enhance the precision of monocular depth maps. The proposed
model features dual encoder-decoder architecture, enabling both color-to-depth
and depth-to-depth transformations. This structure allows for refined depth
estimation through latent space encoding. To further improve the accuracy of
depth boundaries and local features, a new loss function is introduced. This
function combines latent loss with gradient loss, helping the model maintain
the integrity of depth boundaries. The framework is thoroughly tested using the
NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in
complex indoor scenarios. The results clearly show that this approach
effectively reduces depth ambiguities and blurring, making it a promising
solution for applications in human-robot interaction and 3D scene
reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Sun, Yudong Yang, Jimin Zhuang, Changli Tang, Yixuan Li, Wei Li, Zejun MA, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advancements in reasoning optimization have significantly
enhanced the capabilities of large language models (LLMs), existing efforts to
improve reasoning have been limited to solving mathematical problems and
focusing on visual graphical inputs, neglecting broader applications in general
video understanding.This paper proposes video-SALMONN-o1, the first open-source
reasoning-enhanced audio-visual LLM designed for general video understanding
tasks. To enhance its reasoning abilities, we develop a reasoning-intensive
dataset featuring challenging audio-visual questions with step-by-step
solutions. We also propose process direct preference optimization (pDPO), which
leverages contrastive step selection to achieve efficient step-level reward
modelling tailored for multimodal inputs. Additionally, we introduce RivaBench,
the first reasoning-intensive video understanding benchmark, featuring over
4,000 high-quality, expert-curated question-answer pairs across scenarios such
as standup comedy, academic presentations, and synthetic video detection.
video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision
baseline across different video reasoning benchmarks. Besides, pDPO achieves
6-8% improvements compared to the supervised fine-tuning model on RivaBench.
Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Deepfake Detection Based on Multi-Feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddiqui Muhammad Yasir, Hyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake technology utilizes deep learning based face manipulation techniques
to seamlessly replace faces in videos creating highly realistic but
artificially generated content. Although this technology has beneficial
applications in media and entertainment misuse of its capabilities may lead to
serious risks including identity theft cyberbullying and false information. The
integration of DL with visual cognition has resulted in important technological
improvements particularly in addressing privacy risks caused by artificially
generated deepfake images on digital media platforms. In this study we propose
an efficient and lightweight method for detecting deepfake images and videos
making it suitable for devices with limited computational resources. In order
to reduce the computational burden usually associated with DL models our method
integrates machine learning classifiers in combination with keyframing
approaches and texture analysis. Moreover the features extracted with a
histogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands
were integrated to evaluate using random forest extreme gradient boosting extra
trees and support vector classifier algorithms. Our findings show a
feature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and
96% on FaceForensics++ and Celeb-DFv2 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Computation of the Fisher Information in Continual Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most popular methods for continual learning with deep neural
networks is Elastic Weight Consolidation (EWC), which involves computing the
Fisher Information. The exact way in which the Fisher Information is computed
is however rarely described, and multiple different implementations for it can
be found online. This blog post discusses and empirically compares several
often-used implementations, which highlights that many currently reported
results for EWC could likely be improved by changing the way the Fisher
Information is computed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the blogpost track at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Can See Better: Visual Contrastive Decoding For LLM
  Multimodal Reasoning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Pang, Bowen Yang, Haoqin Tu, Yun Cao, Zeyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) excel in reasoning and generation for
language tasks, they are not specifically designed for multimodal challenges.
Training Multimodal Large Language Models (MLLMs), however, is
resource-intensive and constrained by various training limitations. In this
paper, we propose the Modular-based Visual Contrastive Decoding (MVCD)
framework to move this obstacle. Our framework leverages LLMs' In-Context
Learning (ICL) capability and the proposed visual contrastive-example decoding
(CED), specifically tailored for this framework, without requiring any
additional training. By converting visual signals into text and focusing on
contrastive output distributions during decoding, we can highlight the new
information introduced by contextual examples, explore their connections, and
avoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual
perception to make it see and reason over the input visuals. To demonstrate
MVCD's effectiveness, we conduct experiments with four LLMs across five
question answering datasets. Our results not only show consistent improvement
in model accuracy but well explain the effective components inside our decoding
strategy. Our code will be available at https://github.com/Pbhgit/MVCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse Unrolling
  Network for Accelerating Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Zhang, Haiyan Gui, Ningdi Yang, Yue Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint low-rank and sparse unrolling networks have shown superior performance
in dynamic MRI reconstruction. However, existing works mainly utilized matrix
low-rank priors, neglecting the tensor characteristics of dynamic MRI images,
and only a global threshold is applied for the sparse constraint to the
multi-channel data, limiting the flexibility of the network. Additionally, most
of them have inherently complex network structure, with intricate interactions
among variables. In this paper, we propose a novel deep unrolling network,
JotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank
and attention-based sparse priors. Specifically, we utilize tensor low-rank
prior to exploit the structural correlations in high-dimensional data.
Convolutional neural networks are used to adaptively learn the low-rank and
sparse transform domains. A novel attention-based soft thresholding operator is
proposed to assign a unique learnable threshold to each channel of the data in
the CNN-learned sparse domain. The network is unrolled from the elaborately
designed composite splitting algorithm and thus features a simple yet efficient
parallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon)
demonstrate the superior performance of JotlasNet in dynamic MRI
reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, accepted by Magnetic Resonance Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILIAS: Instance-Level Image retrieval At Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgos Kordopatis-Zilos, Vladan Stojnić, Anna Manko, Pavel Šuma, Nikolaos-Antonios Ypsilantis, Nikos Efthymiadis, Zakaria Laskar, Jiří Matas, Ondřej Chum, Giorgos Tolias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces ILIAS, a new test dataset for Instance-Level Image
retrieval At Scale. It is designed to evaluate the ability of current and
future foundation models and retrieval techniques to recognize particular
objects. The key benefits over existing datasets include large scale, domain
diversity, accurate ground truth, and a performance that is far from saturated.
ILIAS includes query and positive images for 1,000 object instances, manually
collected to capture challenging conditions and diverse domains. Large-scale
retrieval is conducted against 100 million distractor images from YFCC100M. To
avoid false negatives without extra annotation effort, we include only query
objects confirmed to have emerged after 2014, i.e. the compilation date of
YFCC100M. An extensive benchmarking is performed with the following
observations: i) models fine-tuned on specific domains, such as landmarks or
products, excel in that domain but fail on ILIAS ii) learning a linear
adaptation layer using multi-domain class supervision results in performance
improvements, especially for vision-language models iii) local descriptors in
retrieval re-ranking are still a key ingredient, especially in the presence of
severe background clutter iv) the text-to-image performance of the
vision-language foundation models is surprisingly close to the corresponding
image-to-image case. website: https://vrg.fel.cvut.cz/ilias/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FUNCTO: Function-Centric One-Shot Imitation Learning for Tool
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning tool use from a single human demonstration video offers a highly
intuitive and efficient approach to robot teaching. While humans can
effortlessly generalize a demonstrated tool manipulation skill to diverse tools
that support the same function (e.g., pouring with a mug versus a teapot),
current one-shot imitation learning (OSIL) methods struggle to achieve this. A
key challenge lies in establishing functional correspondences between
demonstration and test tools, considering significant geometric variations
among tools with the same function (i.e., intra-function variations). To
address this challenge, we propose FUNCTO (Function-Centric OSIL for Tool
Manipulation), an OSIL method that establishes function-centric correspondences
with a 3D functional keypoint representation, enabling robots to generalize
tool manipulation skills from a single human demonstration video to novel tools
with the same function despite significant intra-function variations. With this
formulation, we factorize FUNCTO into three stages: (1) functional keypoint
extraction, (2) function-centric correspondence establishment, and (3)
functional keypoint-based action planning. We evaluate FUNCTO against exiting
modular OSIL methods and end-to-end behavioral cloning methods through
real-robot experiments on diverse tool manipulation tasks. The results
demonstrate the superiority of FUNCTO when generalizing to novel tools with
intra-function geometric variations. More details are available at
https://sites.google.com/view/functo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Peng, Fan Lu, Bin Li, Yuan Huang, Sanqing Qu, Guang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-point cloud cross-modal Visual Place Recognition (VPR) is a
challenging task where the query is an RGB image, and the database samples are
LiDAR point clouds. Compared to single-modal VPR, this approach benefits from
the widespread availability of RGB cameras and the robustness of point clouds
in providing accurate spatial geometry and distance information. However,
current methods rely on intermediate modalities that capture either the
vertical or horizontal field of view, limiting their ability to fully exploit
the complementary information from both sensors. In this work, we propose an
innovative initial retrieval + re-rank method that effectively combines
information from range (or RGB) images and Bird's Eye View (BEV) images. Our
approach relies solely on a computationally efficient global descriptor
similarity search process to achieve re-ranking. Additionally, we introduce a
novel similarity label supervision technique to maximize the utility of limited
training data. Specifically, we employ points average distance to approximate
appearance similarity and incorporate an adaptive margin, based on similarity
differences, into the vanilla triplet loss. Experimental results on the KITTI
dataset demonstrate that our method significantly outperforms state-of-the-art
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submmitted to IEEE IV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via
  Modality-decoupled Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wu, Yuxin Xiong, Xintong Li, Yu Xia, Ruoyu Wang, Yu Wang, Tong Yu, Sungchul Kim, Ryan A. Rossi, Lina Yao, Jingbo Shang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent MLLMs have shown emerging visual understanding and reasoning abilities
after being pre-trained on large-scale multimodal datasets. Unlike
pre-training, where MLLMs receive rich visual-text alignment,
instruction-tuning is often text-driven with weaker visual supervision, leading
to the degradation of pre-trained visual understanding and causing visual
forgetting. Existing approaches, such as direct fine-tuning and continual
learning methods, fail to explicitly address this issue, often compressing
visual representations and prioritizing task alignment over visual retention,
which further worsens visual forgetting. To overcome this limitation, we
introduce a novel perspective leveraging effective rank to quantify the
degradation of visual representation richness, interpreting this degradation
through the information bottleneck principle as excessive compression that
leads to the degradation of crucial pre-trained visual knowledge. Building on
this view, we propose a modality-decoupled gradient descent (MDGD) method that
regulates gradient updates to maintain the effective rank of visual
representations while mitigating the over-compression effects described by the
information bottleneck. By explicitly disentangling the optimization of visual
understanding from task-specific alignment, MDGD preserves pre-trained visual
knowledge while enabling efficient task adaptation. To enable lightweight
instruction-tuning, we further develop a memory-efficient fine-tuning approach
using gradient masking, which selectively updates a subset of model parameters
to enable parameter-efficient fine-tuning (PEFT), reducing computational
overhead while preserving rich visual representations. Extensive experiments
across various downstream tasks and backbone MLLMs demonstrate that MDGD
effectively mitigates visual forgetting from pre-trained tasks while enabling
strong adaptation to new tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphMorph: Tubular Structure Extraction by Morphing Predicted Graphs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Zhang, Ziwei Zhao, Dong Wang, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately restoring topology is both challenging and crucial in tubular
structure extraction tasks, such as blood vessel segmentation and road network
extraction. Diverging from traditional approaches based on pixel-level
classification, our proposed method, named GraphMorph, focuses on branch-level
features of tubular structures to achieve more topologically accurate
predictions. GraphMorph comprises two main components: a Graph Decoder and a
Morph Module. Utilizing multi-scale features extracted from an image patch by
the segmentation network, the Graph Decoder facilitates the learning of
branch-level features and generates a graph that accurately represents the
tubular structure in this patch. The Morph Module processes two primary inputs:
the graph and the centerline probability map, provided by the Graph Decoder and
the segmentation network, respectively. Employing a novel SkeletonDijkstra
algorithm, the Morph Module produces a centerline mask that aligns with the
predicted graph. Furthermore, we observe that employing centerline masks
predicted by GraphMorph significantly reduces false positives in the
segmentation task, which is achieved by a simple yet effective post-processing
strategy. The efficacy of our method in the centerline extraction and
segmentation tasks has been substantiated through experimental evaluations
across various datasets. Source code will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No-reference geometry quality assessment for colorless point clouds via
  list-wise rank learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Li, Bingxu Xie, Chao Chu, Weiqing Li, Zhiyong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry quality assessment (GQA) of colorless point clouds is crucial for
evaluating the performance of emerging point cloud-based solutions (e.g.,
watermarking, compression, and 3-Dimensional (3D) reconstruction).
Unfortunately, existing objective GQA approaches are traditional full-reference
metrics, whereas state-of-the-art learning-based point cloud quality assessment
(PCQA) methods target both color and geometry distortions, neither of which are
qualified for the no-reference GQA task. In addition, the lack of large-scale
GQA datasets with subjective scores, which are always imprecise, biased, and
inconsistent, also hinders the development of learning-based GQA metrics.
Driven by these limitations, this paper proposes a no-reference geometry-only
quality assessment approach based on list-wise rank learning, termed LRL-GQA,
which comprises of a geometry quality assessment network (GQANet) and a
list-wise rank learning network (LRLNet). The proposed LRL-GQA formulates the
no-reference GQA as a list-wise rank problem, with the objective of directly
optimizing the entire quality ordering. Specifically, a large dataset
containing a variety of geometry-only distortions is constructed first, named
LRL dataset, in which each sample is label-free but coupled with quality
ranking information. Then, the GQANet is designed to capture intrinsic
multi-scale patch-wise geometric features in order to predict a quality index
for each point cloud. After that, the LRLNet leverages the LRL dataset and a
likelihood loss to train the GQANet and ranks the input list of degraded point
clouds according to their distortion levels. In addition, the pre-trained
GQANet can be fine-tuned further to obtain absolute quality scores.
Experimental results demonstrate the superior performance of the proposed
no-reference LRL-GQA method compared with existing full-reference GQA metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Croce, Christian Schlarmann, Naman Deep Singh, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring perceptual similarity is a key tool in computer vision. In recent
years perceptual metrics based on features extracted from neural networks with
large and diverse training sets, e.g. CLIP, have become popular. At the same
time, the metrics extracted from features of neural networks are not
adversarially robust. In this paper we show that adversarially robust CLIP
models, called R-CLIP$_\textrm{F}$, obtained by unsupervised adversarial
fine-tuning induce a better and adversarially robust perceptual metric that
outperforms existing metrics in a zero-shot setting, and further matches the
performance of state-of-the-art metrics while being robust after fine-tuning.
Moreover, our perceptual metric achieves strong performance on related tasks
such as robust image-to-image retrieval, which becomes especially relevant when
applied to "Not Safe for Work" (NSFW) content detection and dataset filtering.
While standard perceptual metrics can be easily attacked by a small
perturbation completely degrading NSFW detection, our robust perceptual metric
maintains high accuracy under an attack while having similar performance for
unperturbed images. Finally, perceptual metrics induced by robust CLIP models
have higher interpretability: feature inversion can show which images are
considered similar, while text inversion can find what images are associated to
a given prompt. This also allows us to visualize the very rich visual concepts
learned by a CLIP model, including memorized persons, paintings and complex
queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication in the IEEE Conference on
  Secure and Trustworthy Machine Learning (SaTML). The final version will be
  available on IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incomplete Modality Disentangled Representation for Ophthalmic Disease
  Grading and Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzhi Liu, Zile Huang, Zhe Chen, Feilong Tang, Yu Tian, Zhongxing Xu, Zihong Luo, Yalin Zheng, Yanda Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ophthalmologists typically require multimodal data sources to improve
diagnostic accuracy in clinical decisions. However, due to medical device
shortages, low-quality data and data privacy concerns, missing data modalities
are common in real-world scenarios. Existing deep learning methods tend to
address it by learning an implicit latent subspace representation for different
modality combinations. We identify two significant limitations of these
methods: (1) implicit representation constraints that hinder the model's
ability to capture modality-specific information and (2) modality
heterogeneity, causing distribution gaps and redundancy in feature
representations. To address these, we propose an Incomplete Modality
Disentangled Representation (IMDR) strategy, which disentangles features into
explicit independent modal-common and modal-specific features by guidance of
mutual information, distilling informative knowledge and enabling it to
reconstruct valuable missing semantics and produce robust multimodal
representations. Furthermore, we introduce a joint proxy learning module that
assists IMDR in eliminating intra-modality redundancy by exploiting the
extracted proxies from each class. Experiments on four ophthalmology multimodal
datasets demonstrate that the proposed IMDR outperforms the state-of-the-art
methods significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "See the World, Discover Knowledge": A Chinese Factuality Evaluation for
  Large Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of factual accuracy in large vision language models (LVLMs)
has lagged behind their rapid development, making it challenging to fully
reflect these models' knowledge capacity and reliability. In this paper, we
introduce the first factuality-based visual question-answering benchmark in
Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of
LVLMs across 8 major topics and 56 subtopics. The key features of this
benchmark include a focus on the Chinese language, diverse knowledge types, a
multi-hop question construction, high-quality data, static consistency, and
easy-to-evaluate through short answers. Moreover, we contribute a rigorous data
construction pipeline and decouple the visual factuality into two parts: seeing
the world (i.e., object recognition) and discovering knowledge. This decoupling
allows us to analyze the capability boundaries and execution mechanisms of
LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source
models, revealing critical performance gaps within this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Component-aware Unsupervised Logical Anomaly Generation for Industrial
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Tong, Yang Chang, Qing Zhao, Jiawen Yu, Boyang Wang, Junxiong Lin, Yuxuan Lin, Xinji Mai, Haoran Wang, Zeng Tao, Yan Wang, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is critical in industrial manufacturing for ensuring
product quality and improving efficiency in automated processes. The scarcity
of anomalous samples limits traditional detection methods, making anomaly
generation essential for expanding the data repository. However, recent
generative models often produce unrealistic anomalies increasing false
positives, or require real-world anomaly samples for training. In this work, we
treat anomaly generation as a compositional problem and propose ComGEN, a
component-aware and unsupervised framework that addresses the gap in logical
anomaly generation. Our method comprises a multi-component learning strategy to
disentangle visual components, followed by subsequent generation editing
procedures. Disentangled text-to-component pairs, revealing intrinsic logical
constraints, conduct attention-guided residual mapping and model training with
iteratively matched references across multiple scales. Experiments on the
MVTecLOCO dataset confirm the efficacy of ComGEN, achieving the best AUROC
score of 91.2%. Additional experiments on the real-world scenario of Diesel
Engine and widely-used MVTecAD dataset demonstrate significant performance
improvements when integrating simulated anomalies generated by ComGEN into
automated production workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Worse The Better: Content-Aware Viewpoint Generation Network for
  Projection-related Point Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyong Su, Bingxu Xie, Zheng Li, Jincan Wu, Weiqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through experimental studies, however, we observed the instability of final
predicted quality scores, which change significantly over different viewpoint
settings. Inspired by the "wooden barrel theory", given the default
content-independent viewpoints of existing projection-related PCQA approaches,
this paper presents a novel content-aware viewpoint generation network (CAVGN)
to learn better viewpoints by taking the distribution of geometric and
attribute features of degraded point clouds into consideration. Firstly, the
proposed CAVGN extracts multi-scale geometric and texture features of the
entire input point cloud, respectively. Then, for each default
content-independent viewpoint, the extracted geometric and texture features are
refined to focus on its corresponding visible part of the input point cloud.
Finally, the refined geometric and texture features are concatenated to
generate an optimized viewpoint. To train the proposed CAVGN, we present a
self-supervised viewpoint ranking network (SSVRN) to select the viewpoint with
the worst quality projected image to construct a default-optimized viewpoint
dataset, which consists of thousands of paired default viewpoints and
corresponding optimized viewpoints. Experimental results show that the
projection-related PCQA methods can achieve higher performance using the
viewpoints generated by the proposed CAVGN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE Transactions on Circuits and Systems for
  Video Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVTokenFlow: High-quality 4D Content Generation using Multiview Token
  Flow <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhuo Huang, Yuan Liu, Ge Zheng, Jiepeng Wang, Zhiyang Dou, Sibei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present MVTokenFlow for high-quality 4D content creation
from monocular videos. Recent advancements in generative models such as video
diffusion models and multiview diffusion models enable us to create videos or
3D models. However, extending these generative models for dynamic 4D content
creation is still a challenging task that requires the generated content to be
consistent spatially and temporally. To address this challenge, MVTokenFlow
utilizes the multiview diffusion model to generate multiview images on
different timesteps, which attains spatial consistency across different
viewpoints and allows us to reconstruct a reasonable coarse 4D field. Then,
MVTokenFlow further regenerates all the multiview images using the rendered 2D
flows as guidance. The 2D flows effectively associate pixels from different
timesteps and improve the temporal consistency by reusing tokens in the
regeneration process. Finally, the regenerated images are spatiotemporally
consistent and utilized to refine the coarse 4D field to get a high-quality 4D
field. Experiments demonstrate the effectiveness of our design and show
significantly improved quality than baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://soolab.github.io/MVTokenFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaskGWM: A Generalizable Driving World Model with Video Mask
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, Zehuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models that forecast environmental changes from actions are vital for
autonomous driving models with strong generalization. The prevailing driving
world model mainly build on video prediction model. Although these models can
produce high-fidelity video sequences with advanced diffusion-based generator,
they are constrained by their predictive duration and overall generalization
capabilities. In this paper, we explore to solve this problem by combining
generation loss with MAE-style feature-level context learning. In particular,
we instantiate this target with three key design: (1) A more scalable Diffusion
Transformer (DiT) structure trained with extra mask construction task. (2) we
devise diffusion-related mask tokens to deal with the fuzzy relations between
mask reconstruction and generative diffusion process. (3) we extend mask
construction task to spatial-temporal domain by utilizing row-wise mask for
shifted self-attention rather than masked self-attention in MAE. Then, we adopt
a row-wise cross-view module to align with this mask design. Based on above
improvement, we propose MaskGWM: a Generalizable driving World Model embodied
with Video Mask reconstruction. Our model contains two variants: MaskGWM-long,
focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view
generation. Comprehensive experiments on standard benchmarks validate the
effectiveness of the proposed method, which contain normal validation of
Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot
validation of Waymo dataset. Quantitative metrics on these datasets show our
method notably improving state-of-the-art driving world model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Image to Video Generation with Language Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel Villar-Corrales, Gjergj Plepi, Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and flexible world models are crucial for autonomous systems to
understand their environment and predict future events. Object-centric models,
with structured latent spaces, have shown promise in modeling object dynamics
and interactions, but often face challenges in scaling to complex datasets and
incorporating external guidance, limiting their applicability in robotics. To
address these limitations, we propose TextOCVP, an object-centric model for
image-to-video generation guided by textual descriptions. TextOCVP parses an
observed scene into object representations, called slots, and utilizes a
text-conditioned transformer predictor to forecast future object states and
video frames. Our approach jointly models object dynamics and interactions
while incorporating textual guidance, thus leading to accurate and controllable
predictions. Our method's structured latent space offers enhanced control over
the prediction process, outperforming several image-to-video generative
baselines. Additionally, we demonstrate that structured object-centric
representations provide superior controllability and interpretability,
facilitating the modeling of object dynamics and enabling more precise and
understandable predictions. Videos and code are available at
https://play-slot.github.io/TextOCVP/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMXU: A Multi-Modal and Multi-X-ray Understanding <span class="highlight-title">Dataset</span> for Disease
  Progression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linjie Mu, Zhongzhen Huang, Shengqian Qin, Yakun Zhu, Shaoting Zhang, Xiaofan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have shown great promise in medical
applications, particularly in visual question answering (MedVQA) and diagnosis
from medical images. However, existing datasets and models often fail to
consider critical aspects of medical diagnostics, such as the integration of
historical records and the analysis of disease progression over time. In this
paper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel
dataset for MedVQA that focuses on identifying changes in specific regions
between two patient visits. Unlike previous datasets that primarily address
single-image questions, MMXU enables multi-image questions, incorporating both
current and historical patient data. We demonstrate the limitations of current
LVLMs in identifying disease progression on MMXU-\textit{test}, even those that
perform well on traditional benchmarks. To address this, we propose a
MedRecord-Augmented Generation (MAG) approach, incorporating both global and
regional historical records. Our experiments show that integrating historical
records significantly enhances diagnostic accuracy by at least 20\%, bridging
the gap between current LVLMs and human expert performance. Additionally, we
fine-tune models with MAG on MMXU-\textit{dev}, which demonstrates notable
improvements. We hope this work could illuminate the avenue of advancing the
use of LVLMs in medical diagnostics by emphasizing the importance of historical
context in interpreting medical images. Our dataset is released at
\href{https://github.com/linjiemu/MMXU}{https://github.com/linjiemu/MMXU}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with
  Pose Guidance from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyumin Shim, Sangmin Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce GaussianMotion, a novel human rendering model
that generates fully animatable scenes aligned with textual descriptions using
Gaussian Splatting. Although existing methods achieve reasonable text-to-3D
generation of human bodies using various 3D representations, they often face
limitations in fidelity and efficiency, or primarily focus on static models
with limited pose control. In contrast, our method generates fully animatable
3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score
distillation, achieving high fidelity and efficient rendering for arbitrary
poses. By densely generating diverse random poses during optimization, our
deformable 3D human model learns to capture a wide range of natural motions
distilled from a pose-conditioned diffusion model in an end-to-end manner.
Furthermore, we propose Adaptive Score Distillation that effectively balances
realistic detail and smoothness to achieve optimal 3D results. Experimental
results demonstrate that our approach outperforms existing baselines by
producing high-quality textures in both static and animated results, and by
generating diverse 3D human models from various textual inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Out-of-Distribution Detection in Medical Imaging with
  Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dariush Lotfi, Mohammad-Ali Nikouei Mahani, Mohamad Koohi-Moghadam, Kyongtae Ty Bae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging
to ensure reliability and safety by identifying inputs outside a model's
training distribution. Existing methods often require retraining or
modifications to pre-trained models, which is impractical for clinical
applications. This study introduces a post-hoc normalizing flow-based approach
that seamlessly integrates with pre-trained models. By leveraging normalizing
flows, it estimates the likelihood of feature vectors extracted from
pre-trained models, capturing semantically meaningful representations without
relying on pixel-level statistics. The method was evaluated using the MedMNIST
benchmark and a newly curated MedOOD dataset simulating clinically relevant
distributional shifts. Performance was measured using standard OOD detection
metrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses
comparing it against ten baseline methods. On MedMNIST, the proposed model
achieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD,
it achieved an AUROC of 84.61%, demonstrating superior performance against
other methods. Its post-hoc nature ensures compatibility with existing clinical
workflows, addressing the limitations of previous approaches. The model and
code to build OOD datasets are available at
https://github.com/dlotfi/MedOODFlow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference Attacks for Face Images Against Fine-Tuned Latent
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauritz Christian Holme, Anton Mosquera Storgaard, Siavash Arjomand Bigdeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of generative image models leads to privacy concerns when it comes
to the huge datasets used to train such models. This paper investigates the
possibility of inferring if a set of face images was used for fine-tuning a
Latent Diffusion Model (LDM). A Membership Inference Attack (MIA) method is
presented for this task. Using generated auxiliary data for the training of the
attack model leads to significantly better performance, and so does the use of
watermarks. The guidance scale used for inference was found to have a
significant influence. If a LDM is fine-tuned for long enough, the text prompt
used for inference has no significant influence. The proposed MIA is found to
be viable in a realistic black-box setup against LDMs fine-tuned on
face-images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 20th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP
  2025) - Volume 2: VISAPP, pages 439-446</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Neural Rendering of LiDAR Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joni Vanherck, Brent Zoomers, Tom Mertens, Lode Jorissen, Nick Michiels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Static LiDAR scanners produce accurate, dense, colored point clouds, but
often contain obtrusive artifacts which makes them ill-suited for direct
display. We propose an efficient method to render photorealistic images of such
scans without any expensive preprocessing or training of a scene-specific
model. A naive projection of the point cloud to the output view using 1x1
pixels is fast and retains the available detail, but also results in
unintelligible renderings as background points leak in between the foreground
pixels. The key insight is that these projections can be transformed into a
realistic result using a deep convolutional model in the form of a U-Net, and a
depth-based heuristic that prefilters the data. The U-Net also handles
LiDAR-specific problems such as missing parts due to occlusion, color
inconsistencies and varying point densities. We also describe a method to
generate synthetic training data to deal with imperfectly-aligned ground truth
images. Our method achieves real-time rendering rates using an off-the-shelf
GPU and outperforms the state-of-the-art in both speed and quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis
  from Japanese Haiku <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunan Yu, Yidong Han, Chaotao Ding, Ying Zang, Lanyun Zhu, Xinhao Chen, Zejian Li, Renjun Xu, Tianrun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of the metaverse, where immersive technologies redefine human
experiences, translating abstract literary concepts into navigable 3D
environments presents a fundamental challenge in preserving semantic and
emotional fidelity. This research introduces HaikuVerse, a novel framework for
transforming poetic abstraction into spatial representation, with Japanese
Haiku serving as an ideal test case due to its sophisticated encapsulation of
profound emotions and imagery within minimal text. While existing text-to-3D
methods struggle with nuanced interpretations, we present a literary-guided
approach that synergizes traditional poetry analysis with advanced generative
technologies. Our framework centers on two key innovations: (1) Hierarchical
Literary-Criticism Theory Grounded Parsing (H-LCTGP), which captures both
explicit imagery and implicit emotional resonance through structured semantic
decomposition, and (2) Progressive Dimensional Synthesis (PDS), a multi-stage
pipeline that systematically transforms poetic elements into coherent 3D scenes
through sequential diffusion processes, geometric optimization, and real-time
enhancement. Extensive experiments demonstrate that HaikuVerse significantly
outperforms conventional text-to-3D approaches in both literary fidelity and
visual quality, establishing a new paradigm for preserving cultural heritage in
immersive digital spaces. Project website at:
https://syllables-to-scenes.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, submitted to IJCAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Trustworthy Anomaly Detection for Critical Applications
  through Approximated Partial AUC Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Bougaham, Benoît Frénay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly Detection is a crucial step for critical applications such in the
industrial, medical or cybersecurity domains. These sectors share the same
requirement of handling differently the different types of classification
errors. Indeed, even if false positives are acceptable, false negatives are
not, because it would reflect a missed detection of a quality issue, a disease
or a cyber threat. To fulfill this requirement, we propose a method that
dynamically applies a trustworthy approximated partial AUC ROC loss (tapAUC). A
binary classifier is trained to optimize the specific range of the AUC ROC
curve that prevents the True Positive Rate (TPR) to reach 100% while minimizing
the False Positive Rate (FPR). The optimal threshold that does not trigger any
false negative is then kept and used at the test step. The results show a TPR
of 92.52% at a 20.43% FPR for an average across 6 datasets, representing a TPR
improvement of 4.3% for a FPR cost of 12.2% against other state-of-the-art
methods. The code is available at https://github.com/ArnaudBougaham/tapAUC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurgPose: a <span class="highlight-title">Dataset</span> for Articulated Robotic Surgical Tool Pose
  Estimation and Tracking <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Wu, Adam Schmidt, Randy Moore, Haoying Zhou, Alexandre Banks, Peter Kazanzides, Septimiu E. Salcudean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient surgical robotic tool pose estimation is of
fundamental significance to downstream applications such as augmented reality
(AR) in surgical training and learning-based autonomous manipulation. While
significant advancements have been made in pose estimation for humans and
animals, it is still a challenge in surgical robotics due to the scarcity of
published data. The relatively large absolute error of the da Vinci end
effector kinematics and arduous calibration procedure make calibrated
kinematics data collection expensive. Driven by this limitation, we collected a
dataset, dubbed SurgPose, providing instance-aware semantic keypoints and
skeletons for visual surgical tool pose estimation and tracking. By marking
keypoints using ultraviolet (UV) reactive paint, which is invisible under white
light and fluorescent under UV light, we execute the same trajectory under
different lighting conditions to collect raw videos and keypoint annotations,
respectively. The SurgPose dataset consists of approximately 120k surgical
instrument instances (80k for training and 40k for validation) of 6 categories.
Each instrument instance is labeled with 7 semantic keypoints. Since the videos
are collected in stereo pairs, the 2D pose can be lifted to 3D based on
stereo-matching depth. In addition to releasing the dataset, we test a few
baseline approaches to surgical instrument tracking to demonstrate the utility
of SurgPose. More details can be found at surgpose.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-CLIP: Decoupling Category and Style Guidance in CLIP for
  Specific-Domain Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexi Jia, Chuanwei Huang, Hongyan Fei, Yeshuang Zhu, Zhiqiang Yuan, Jinchao Zhang, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have shown remarkable capabilities of
generating high-quality images closely aligned with textual inputs. However,
the effectiveness of text guidance heavily relies on the CLIP text encoder,
which is trained to pay more attention to general content but struggles to
capture semantics in specific domains like styles. As a result, generation
models tend to fail on prompts like "a photo of a cat in Pokemon style" in
terms of simply producing images depicting "a photo of a cat". To fill this
gap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that
enables the CLIP model to learn the meaning of category and style in a
complement manner. With specially designed fine-tuning tasks on minimal data
and a modified cross-attention mechanism, Control-CLIP can precisely guide the
diffusion model to a specific domain. Moreover, the parameters of the diffusion
model remain unchanged at all, preserving the original generation performance
and diversity. Experiments across multiple domains confirm the effectiveness of
our approach, particularly highlighting its robust plug-and-play capability in
generating content with various specific styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SayAnything: Audio-Driven Lip Synchronization with Conditional Video
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxian Ma, Shiwen Wang, Jian Yang, Junyi Hu, Jian Liang, Guosheng Lin, Jingbo chen, Kai Li, Yu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have led to significant progress in
audio-driven lip synchronization. However, existing methods typically rely on
constrained audio-visual alignment priors or multi-stage learning of
intermediate representations to force lip motion synthesis. This leads to
complex training pipelines and limited motion naturalness. In this paper, we
present SayAnything, a conditional video diffusion framework that directly
synthesizes lip movements from audio input while preserving speaker identity.
Specifically, we propose three specialized modules including identity
preservation module, audio guidance module, and editing control module. Our
novel design effectively balances different condition signals in the latent
space, enabling precise control over appearance, motion, and region-specific
generation without requiring additional supervision signals or intermediate
representations. Extensive experiments demonstrate that SayAnything generates
highly realistic videos with improved lip-teeth coherence, enabling unseen
characters to say anything, while effectively generalizing to animated
characters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Pruning in Multimodal Large Language Models: Are We Solving the
  Right Problem? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown remarkable performance
for cross-modal understanding and generation, yet still suffer from severe
inference costs. Recently, abundant works have been proposed to solve this
problem with token pruning, which identifies the redundant tokens in MLLMs and
then prunes them to reduce the computation and KV storage costs, leading to
significant acceleration without training. While these methods claim efficiency
gains, critical questions about their fundamental design and evaluation remain
unanswered: Why do many existing approaches underperform even compared to naive
random token selection? Are attention-based scoring sufficient for reliably
identifying redundant tokens? Is language information really helpful during
token pruning? What makes a good trade-off between token importance and
duplication? Are current evaluation protocols comprehensive and unbiased? The
ignorance of previous research on these problems hinders the long-term
development of token pruning. In this paper, we answer these questions one by
one, providing insights into the design of future token pruning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stop Looking for Important Tokens in Multimodal Language Models:
  Duplication Matters More 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision tokens in multimodal large language models often dominate huge
computational overhead due to their excessive length compared to linguistic
modality. Abundant recent methods aim to solve this problem with token pruning,
which first defines an importance criterion for tokens and then prunes the
unimportant vision tokens during inference. However, in this paper, we show
that the importance is not an ideal indicator to decide whether a token should
be pruned. Surprisingly, it usually results in inferior performance than random
token pruning and leading to incompatibility to efficient attention computation
operators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),
which prunes tokens based on its duplication with other tokens, leading to
significant and training-free acceleration. Concretely, DART selects a small
subset of pivot tokens and then retains the tokens with low duplication to the
pivots, ensuring minimal information loss during token pruning. Experiments
demonstrate that DART can prune 88.9% vision tokens while maintaining
comparable performance, leading to a 1.99$\times$ and 2.99$\times$ speed-up in
total time and prefilling stage, respectively, with good compatibility to
efficient attention operators. Our codes are available at
https://github.com/ZichenWen1/DART.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Vision Language Models Struggle with Visual Arithmetic? Towards
  Enhanced Chart and Geometry Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have achieved remarkable progress in multimodal
tasks, yet they often struggle with visual arithmetic, seemingly simple
capabilities like object counting or length comparison, which are essential for
relevant complex tasks like chart understanding and geometric reasoning. In
this work, we first investigate the root causes of this deficiency through a
suite of probing tasks focusing on basic visual arithmetic. Our analysis
reveals that while pre-trained vision encoders typically capture sufficient
information, the text decoder often fails to decode it correctly for arithmetic
reasoning. To address this, we propose CogAlign, a novel post-training strategy
inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to
recognize invariant properties under visual transformations. We demonstrate
that this approach significantly improves the performance of three diverse VLMs
on our proposed probing tasks. Furthermore, CogAlign enhances performance by an
average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching
supervised fine-tuning methods while requiring only 60% less training data.
These results highlight the effectiveness and generalizability of CogAlign in
improving fundamental visual arithmetic capabilities and their transfer to
downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxiang Cui, Zhongyu Li, Xiayue Fan, Peng Huang, Ying Wang, Meng Yang, Shi Chang, Jihua Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intersection of medical imaging and artificial intelligence has become an
important research direction in intelligent medical treatment, particularly in
the analysis of medical images using deep learning for clinical diagnosis.
Despite the advances, existing keyframe classification methods lack extraction
of time series features, while ultrasonic video classification based on
three-dimensional convolution requires uniform frame numbers across patients,
resulting in poor feature extraction efficiency and model classification
performance. This study proposes a novel video classification method based on
CNN and LSTM, introducing NLP's long and short sentence processing scheme into
video classification for the first time. The method reduces CNN-extracted image
features to 1x512 dimension, followed by sorting and compressing feature
vectors for LSTM training. Specifically, feature vectors are sorted by patient
video frame numbers and populated with padding value 0 to form variable
batches, with invalid padding values compressed before LSTM training to
conserve computing resources. Experimental results demonstrate that our
variable-frame CNNLSTM method outperforms other approaches across all metrics,
showing improvements of 3-6% in F1 score and 1.5% in specificity compared to
keyframe methods. The variable-frame CNNLSTM also achieves better accuracy and
precision than equal-frame CNNLSTM. These findings validate the effectiveness
of our approach in classifying variable-frame ultrasound videos and suggest
potential applications in other medical imaging modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Sample Effective and Diverse <span class="highlight-title">Prompt</span>s for Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, Ling Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image diffusion models have achieved impressive
image generation capabilities. However, it remains challenging to control the
generation process with desired properties (e.g., aesthetic quality, user
intention), which can be expressed as black-box reward functions. In this
paper, we focus on prompt adaptation, which refines the original prompt into
model-preferred prompts to generate desired images. While prior work uses
reinforcement learning (RL) to optimize prompts, we observe that applying RL
often results in generating similar postfixes and deterministic behaviors. To
this end, we introduce \textbf{P}rompt \textbf{A}daptation with
\textbf{G}FlowNets (\textbf{PAG}), a novel approach that frames prompt
adaptation as a probabilistic inference problem. Our key insight is that
leveraging Generative Flow Networks (GFlowNets) allows us to shift from reward
maximization to sampling from an unnormalized density function, enabling both
high-quality and diverse prompt generation. However, we identify that a naive
application of GFlowNets suffers from mode collapse and uncovers a previously
overlooked phenomenon: the progressive loss of neural plasticity in the model,
which is compounded by inefficient credit assignment in sequential prompt
generation. To address this critical challenge, we develop a systematic
approach in PAG with flow reactivation, reward-prioritized sampling, and reward
decomposition for prompt adaptation. Extensive experiments validate that PAG
successfully learns to sample effective and diverse prompts for text-to-image
generation. We also show that PAG exhibits strong robustness across various
reward functions and transferability to different text-to-image models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 14 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically Robust Unsupervised Image Translation for Paired Remote
  Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Fang, Kaiyu Li, Zhe Li, Jianli Zhao, Xingli Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image translation for change detection or classification in bi-temporal
remote sensing images is unique. Although it can acquire paired images, it is
still unsupervised. Moreover, strict semantic preservation in translation is
always needed instead of multimodal outputs. In response to these problems,
this paper proposes a new method, SRUIT (Semantically Robust Unsupervised
Image-to-image Translation), which ensures semantically robust translation and
produces deterministic output. Inspired by previous works, the method explores
the underlying characteristics of bi-temporal Remote Sensing images and designs
the corresponding networks. Firstly, we assume that bi-temporal Remote Sensing
images share the same latent space, for they are always acquired from the same
land location. So SRUIT makes the generators share their high-level layers, and
this constraint will compel two domain mapping to fall into the same latent
space. Secondly, considering land covers of bi-temporal images could evolve
into each other, SRUIT exploits the cross-cycle-consistent adversarial networks
to translate from one to the other and recover them. Experimental results show
that constraints of sharing weights and cross-cycle consistency enable
translated images with both good perceptual image quality and semantic
preservation for significant differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning
  Network for Semi-supervised 3D Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyan Wang, Kechen Song, Yuyuan Liu, Shuai Ma, Yunhui Yan, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised 3D medical image segmentation aims to achieve accurate
segmentation using few labelled data and numerous unlabelled data. The main
challenge in the design of semi-supervised learning methods consists in the
effective use of the unlabelled data for training. A promising solution
consists of ensuring consistent predictions across different views of the data,
where the efficacy of this strategy depends on the accuracy of the
pseudo-labels generated by the model for this consistency learning strategy. In
this paper, we introduce a new methodology to produce high-quality
pseudo-labels for a consistency learning strategy to address semi-supervised 3D
medical image segmentation. The methodology has three important contributions.
The first contribution is the Cooperative Rectification Learning Network (CRLN)
that learns multiple prototypes per class to be used as external knowledge
priors to adaptively rectify pseudo-labels at the voxel level. The second
contribution consists of the Dynamic Interaction Module (DIM) to facilitate
pairwise and cross-class interactions between prototypes and multi-resolution
image features, enabling the production of accurate voxel-level clues for
pseudo-label rectification. The third contribution is the Cooperative Positive
Supervision (CPS), which optimises uncertain representations to align with
unassertive representations of their class distributions, improving the model's
accuracy in classifying uncertain regions. Extensive experiments on three
public 3D medical segmentation datasets demonstrate the effectiveness and
superiority of our semi-supervised learning method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Registration Meets Vision Foundation Model: Prototype
  Learning and Contour Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xu, Tengfei Xue, Jianan Fan, Dongnan Liu, Yuqian Chen, Fan Zhang, Carl-Fredrik Westin, Ron Kikinis, Lauren J. O'Donnell, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image registration is a fundamental task in medical image analysis,
aiming to establish spatial correspondences between paired images. However,
existing unsupervised deformable registration methods rely solely on
intensity-based similarity metrics, lacking explicit anatomical knowledge,
which limits their accuracy and robustness. Vision foundation models, such as
the Segment Anything Model (SAM), can generate high-quality segmentation masks
that provide explicit anatomical structure knowledge, addressing the
limitations of traditional methods that depend only on intensity similarity.
Based on this, we propose a novel SAM-assisted registration framework
incorporating prototype learning and contour awareness. The framework includes:
(1) Explicit anatomical information injection, where SAM-generated segmentation
masks are used as auxiliary inputs throughout training and testing to ensure
the consistency of anatomical information; (2) Prototype learning, which
leverages segmentation masks to extract prototype features and aligns
prototypes to optimize semantic correspondences between images; and (3)
Contour-aware loss, a contour-aware loss is designed that leverages the edges
of segmentation masks to improve the model's performance in fine-grained
deformation fields. Extensive experiments demonstrate that the proposed
framework significantly outperforms existing methods across multiple datasets,
particularly in challenging scenarios with complex anatomical structures and
ambiguous boundaries. Our code is available at
https://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Information Processing in Medical Imaging (IPMI) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do we Really Need Visual Instructions? Towards Visual Instruction-Free
  Fine-tuning for Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning has become the predominant technology in eliciting
the multimodal task-solving capabilities of large vision-language models
(LVLMs). Despite the success, as visual instructions require images as the
input, it would leave the gap in inheriting the task-solving capabilities from
the backbone LLMs, and make it costly to collect a large-scale dataset. To
address it, we propose ViFT, a visual instruction-free fine-tuning framework
for LVLMs. In ViFT, we only require the text-only instructions and image
caption data during training, to separately learn the task-solving and visual
perception abilities. During inference, we extract and combine the
representations of the text and image inputs, for fusing the two abilities to
fulfill multimodal tasks. Experimental results demonstrate that ViFT can
achieve state-of-the-art performance on several visual reasoning and visual
instruction following benchmarks, with rather less training data. Our code and
data will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise GPS-Denied UAV Self-Positioning via Context-Enhanced Cross-View
  Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanze Xu, Ming Dai, Wenxiao Cai, Wankou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval has been employed as a robust complementary technique to
address the challenge of Unmanned Aerial Vehicles (UAVs) self-positioning.
However, most existing methods primarily focus on localizing objects captured
by UAVs through complex part-based representations, often overlooking the
unique challenges associated with UAV self-positioning, such as fine-grained
spatial discrimination requirements and dynamic scene variations. To address
the above issues, we propose the Context-Enhanced method for precise UAV
Self-Positioning (CEUSP), specifically designed for UAV self-positioning tasks.
CEUSP integrates a Dynamic Sampling Strategy (DSS) to efficiently select
optimal negative samples, while the Rubik's Cube Attention (RCA) module,
combined with the Context-Aware Channel Integration (CACI) module, enhances
feature representation and discrimination by exploiting interdimensional
interactions, inspired by the rotational mechanics of a Rubik's Cube. Extensive
experimental validate the effectiveness of the proposed method, demonstrating
notable improvements in feature representation and UAV self-positioning
accuracy within complex urban environments. Our approach achieves
state-of-the-art performance on the DenseUAV dataset, which is specifically
designed for dense urban contexts, and also delivers competitive results on the
widely recognized University-1652 benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARS: Mesh AutoRegressive Model for 3D Shape Detailization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingnan Gao, Weizhe Liu, Weixuan Sun, Senbo Wang, Xibin Song, Taizhang Shang, Shenzhou Chen, Hongdong Li, Xiaokang Yang, Yichao Yan, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for mesh detailization predominantly utilize
Generative Adversarial Networks (GANs) to generate detailed meshes from coarse
ones. These methods typically learn a specific style code for each category or
similar categories without enforcing geometry supervision across different
Levels of Detail (LODs). Consequently, such methods often fail to generalize
across a broader range of categories and cannot ensure shape consistency
throughout the detailization process. In this paper, we introduce MARS, a novel
approach for 3D shape detailization. Our method capitalizes on a novel
multi-LOD, multi-category mesh representation to learn shape-consistent mesh
representations in latent space across different LODs. We further propose a
mesh autoregressive model capable of generating such latent representations
through next-LOD token prediction. This approach significantly enhances the
realism of the generated shapes. Extensive experiments conducted on the
challenging 3D Shape Detailization benchmark demonstrate that our proposed MARS
model achieves state-of-the-art performance, surpassing existing methods in
both qualitative and quantitative assessments. Notably, the model's capability
to generate fine-grained details while preserving the overall shape integrity
is particularly commendable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Physics-Informed Blur Learning Framework for Imaging Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqun Chen, Yuxuan Li, Jun Dai, Jinwei Gu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate blur estimation is essential for high-performance imaging across
various applications. Blur is typically represented by the point spread
function (PSF). In this paper, we propose a physics-informed PSF learning
framework for imaging systems, consisting of a simple calibration followed by a
learning process. Our framework could achieve both high accuracy and universal
applicability. Inspired by the Seidel PSF model for representing spatially
varying PSF, we identify its limitations in optimization and introduce a novel
wavefront-based PSF model accompanied by an optimization strategy, both
reducing optimization complexity and improving estimation accuracy. Moreover,
our wavefront-based PSF model is independent of lens parameters, eliminate the
need for prior knowledge of the lens. To validate our approach, we compare it
with recent PSF estimation methods (Degradation Transfer and Fast Two-step)
through a deblurring task, where all the estimated PSFs are used to train
state-of-the-art deblurring algorithms. Our approach demonstrates improvements
in image quality in simulation and also showcases noticeable visual quality
improvements on real captured images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Without Paired Labeled Data: An End-to-End <span class="highlight-title">Self-Supervised</span> Paradigm for
  UAV-View Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UAV-View Geo-Localization (UVGL) aims to ascertain the precise location of a
UAV by retrieving the most similar GPS-tagged satellite image. However,
existing methods predominantly rely on supervised learning paradigms that
necessitate annotated paired data for training, which incurs substantial
annotation costs and impedes large-scale deployment. To overcome this
limitation, we propose the Dynamic Memory-Driven and Neighborhood Information
Learning (DMNIL) network, a lightweight end-to-end self-supervised framework
for UAV-view geo-localization. The DMNIL framework utilizes a dual-path
clustering-based contrastive learning architecture as its baseline to model
intra-view structural relationships, enhancing feature consistency and
discriminability. Additionally, a dynamic memory-driven hierarchical learning
module is proposed to progressively mine local and global information,
reinforcing multi-level feature associations to improve model robustness. To
bridge the domain gap between UAV and satellite views, we design an
information-consistent evolutionary learning mechanism that systematically
explores latent correlations within intra-view neighborhoods and across
cross-view domains, ultimately constructing a unified cross-view feature
representation space. Extensive experiments on three benchmarks
(University-1652, SUES-200, and DenseUAV) demonstrate that DMNIL achieves
competitive performance against state-of-the-art supervised methods while
maintaining computational efficiency. Notably, this superiority is attained
without relying on paired training data, underscoring the framework's
practicality for real-world deployment. Codes will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee, Dongwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GeoDANO, a geometric vision-language model (VLM) with a
domain-agnostic vision encoder, for solving plane geometry problems. Although
VLMs have been employed for solving geometry problems, their ability to
recognize geometric features remains insufficiently analyzed. To address this
gap, we propose a benchmark that evaluates the recognition of visual geometric
features, including primitives such as dots and lines, and relations such as
orthogonality. Our preliminary study shows that vision encoders often used in
general-purpose VLMs, e.g., OpenCLIP, fail to detect these features and
struggle to generalize across domains. We develop GeoCLIP, a CLIP based model
trained on synthetic geometric diagram-caption pairs to overcome the
limitation. Benchmark results show that GeoCLIP outperforms existing vision
encoders in recognizing geometric features. We then propose our VLM, GeoDANO,
which augments GeoCLIP with a domain adaptation strategy for unseen diagram
styles. GeoDANO outperforms specialized methods for plane geometry problems and
GPT-4o on MathVerse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld
  Radiographic Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunyi Zhou, Kun Shi, Gang Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiographic testing is a fundamental non-destructive evaluation technique
for identifying weld defects and assessing quality in industrial applications
due to its high-resolution imaging capabilities. Over the past decade, deep
learning techniques have significantly advanced weld defect identification in
radiographic images. However, conventional approaches, which rely on training
small-scale, task-specific models on single-scenario datasets, exhibit poor
cross-scenario generalization. Recently, the Segment Anything Model (SAM), a
pre-trained visual foundation model trained on large-scale datasets, has
demonstrated exceptional zero-shot generalization capabilities. Fine-tuning SAM
with limited domain-specific data has yielded promising results in fields such
as medical image segmentation and anomaly detection. To the best of our
knowledge, this work is the first to introduce SAM-based segmentation for
general weld radiographic testing images. We propose WRT-SAM, a novel weld
radiographic defect segmentation model that leverages SAM through an
adapter-based integration with a specialized prompt generator architecture. To
improve adaptability to grayscale weld radiographic images, we introduce a
frequency prompt generator module, which enhances the model's sensitivity to
frequency-domain information. Furthermore, to address the multi-scale nature of
weld defects, we incorporate a multi-scale prompt generator module, enabling
the model to effectively extract and encode defect information across varying
scales. Extensive experimental evaluations demonstrate that WRT-SAM achieves a
recall of 78.87%, a precision of 84.04%, and an AUC of 0.9746, setting a new
state-of-the-art (SOTA) benchmark. Moreover, the model exhibits superior
zero-shot generalization performance, highlighting its potential for practical
deployment in diverse radiographic testing scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparison of Human and Machine Learning Errors in Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Estévez-Almenzar, Ricardo Baeza-Yates, Carlos Castillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning applications in high-stakes scenarios should always operate
under human oversight. Developing an optimal combination of human and machine
intelligence requires an understanding of their complementarities, particularly
regarding the similarities and differences in the way they make mistakes. We
perform extensive experiments in the area of face recognition and compare two
automated face recognition systems against human annotators through a
demographically balanced user study. Our research uncovers important ways in
which machine learning errors and human errors differ from each other, and
suggests potential strategies in which human-machine collaboration can improve
accuracy in face recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially private fine-tuned NF-Net to predict GI cancer type 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Venkatesh Chilukoti, Imran Hossen Md, Liqun Shan, Vijay Srinivas Tida, Xiali Hei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on global genomic status, the cancer tumor is classified as
Microsatellite Instable (MSI) and Microsatellite Stable (MSS). Immunotherapy is
used to diagnose MSI, whereas radiation and chemotherapy are used for MSS.
Therefore, it is significant to classify a gastro-intestinal (GI) cancer tumor
into MSI vs. MSS to provide appropriate treatment. The existing literature
showed that deep learning could directly predict the class of GI cancer tumors
from histological images. However, deep learning (DL) models are susceptible to
various threats, including membership inference attacks, model extraction
attacks, etc. These attacks render the use of DL models impractical in
real-world scenarios. To make the DL models useful and maintain privacy, we
integrate differential privacy (DP) with DL. In particular, this paper aims to
predict the state of GI cancer while preserving the privacy of sensitive data.
We fine-tuned the Normalizer Free Net (NF-Net) model. We obtained an accuracy
of 88.98\% without DP to predict (GI) cancer status. When we fine-tuned the
NF-Net using DP-AdamW and adaptive DP-AdamW, we got accuracies of 74.58% and
76.48%, respectively. Moreover, we investigate the Weighted Random Sampler
(WRS) and Class weighting (CW) to solve the data imbalance. We also evaluated
and analyzed the DP algorithms in different settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCT Data is All You Need: How Vision <span class="highlight-title">Transformer</span>s with and without
  <span class="highlight-title">Pre-train</span>ing Benefit Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Han, Philippe De Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Coherence Tomography (OCT) provides high-resolution cross-sectional
images useful for diagnosing various diseases, but their distinct
characteristics from natural images raise questions about whether large-scale
pre-training on datasets like ImageNet is always beneficial. In this paper, we
investigate the impact of ImageNet-based pre-training on Vision Transformer
(ViT) performance for OCT image classification across different dataset sizes.
Our experiments cover four-category retinal pathologies (CNV, DME, Drusen,
Normal). Results suggest that while pre-training can accelerate convergence and
potentially offer better performance in smaller datasets, training from scratch
may achieve comparable or even superior accuracy when sufficient OCT data is
available. Our findings highlight the importance of matching domain
characteristics in pre-training and call for further study on large-scale
OCT-specific pre-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alignment and Adversarial Robustness: Are More Human-Like Models More
  Secure? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blaine Hoak, Kunyang Li, Patrick McDaniel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representational alignment refers to the extent to which a model's internal
representations mirror biological vision, offering insights into both neural
similarity and functional correspondence. Recently, some more aligned models
have demonstrated higher resiliency to adversarial examples, raising the
question of whether more human-aligned models are inherently more secure. In
this work, we conduct a large-scale empirical analysis to systematically
investigate the relationship between representational alignment and adversarial
robustness. We evaluate 118 models spanning diverse architectures and training
paradigms, measuring their neural and behavioral alignment and engineering task
performance across 106 benchmarks as well as their adversarial robustness via
AutoAttack. Our findings reveal that while average alignment and robustness
exhibit a weak overall correlation, specific alignment benchmarks serve as
strong predictors of adversarial robustness, particularly those that measure
selectivity towards texture or shape. These results suggest that different
forms of alignment play distinct roles in model robustness, motivating further
investigation into how alignment-driven approaches can be leveraged to build
more secure and perceptually-grounded vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Systematic Weaknesses in Vision Models along Predefined
  Human-Understandable Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujan Sai Gannamaneni, Rohil Prakash Rao, Michael Mock, Maram Akila, Stefan Wrobel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying systematic weaknesses of DNNs has gained prominence in the last few
years with the rising focus on building safe AI systems. Slice discovery
methods (SDMs) are prominent algorithmic approaches for finding such systematic
weaknesses. They identify top-k semantically coherent slices/subsets of data
where a DNN-under-test has low performance. For being directly useful, e.g., as
evidences in a safety argumentation, slices should be aligned with
human-understandable (safety-relevant) dimensions, which, for example, are
defined by safety and domain experts as parts of the operational design domain
(ODD). While straightforward for structured data, the lack of semantic metadata
makes these investigations challenging for unstructured data. Therefore, we
propose a complete workflow which combines contemporary foundation models with
algorithms for combinatorial search that consider structured data and DNN
errors for finding systematic weaknesses in images. In contrast to existing
approaches, ours identifies weak slices that are in line with predefined
human-understandable dimensions. As the workflow includes foundation models,
its intermediate and final results may not always be exact. Therefore, we build
into our workflow an approach to address the impact of noisy metadata. We
evaluate our approach w.r.t. its quality on four popular computer vision
datasets, including autonomous driving datasets like Cityscapes, BDD100k, and
RailSem19, while using multiple state-of-the-art models as DNNs-under-test.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LanP: Rethinking the Impact of Language Priors in Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyu Wu, Yuwei Niu, Hongcheng Gao, Minhua Lin, Zhiwei Zhang, Zhifang Zhang, Qi Shi, Yilong Wang, Sike Fu, Junjie Xu, Junjie Ao, Enyan Dai, Lei Feng, Xiang Zhang, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have shown impressive performance in
various tasks. However, LVLMs suffer from hallucination, which hinders their
adoption in the real world. Existing studies emphasized that the strong
language priors of LVLMs can overpower visual information, causing
hallucinations. However, the positive role of language priors is the key to a
powerful LVLM. If the language priors are too weak, LVLMs will struggle to
leverage rich parameter knowledge and instruction understanding abilities to
complete tasks in challenging visual scenarios where visual information alone
is insufficient. Therefore, we propose a benchmark called LanP to rethink the
impact of Language Priors in LVLMs. It is designed to investigate how strong
language priors are in current LVLMs. LanP consists of 170 images and 340
corresponding well-designed questions. Extensive experiments on 25 popular
LVLMs reveal that many LVLMs' language priors are not strong enough to
effectively aid question answering when objects are partially hidden. Many
models, including GPT-4 Turbo, exhibit an accuracy below 0.5 in such a
scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navve Wasserman, Roi Pony, Oshri Naparstek, Adi Raz Goldfarb, Eli Schwartz, Udi Barzelay, Leonid Karlinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate multi-modal document retrieval is crucial for Retrieval-Augmented
Generation (RAG), yet existing benchmarks do not fully capture real-world
challenges with their current design. We introduce REAL-MM-RAG, an
automatically generated benchmark designed to address four key properties
essential for real-world retrieval: (i) multi-modal documents, (ii) enhanced
difficulty, (iii) Realistic-RAG queries and (iv) accurate labeling.
Additionally, we propose a multi-difficulty-level scheme based on query
rephrasing to evaluate models' semantic understanding beyond keyword matching.
Our benchmark reveals significant model weaknesses, particularly in handling
table-heavy documents and robustness to query rephrasing. To mitigate these
shortcomings, we curate a rephrased training set and introduce a new
finance-focused, table-heavy dataset. Fine-tuning on these datasets enables
models to achieve state-of-the-art retrieval performance on REAL-MM-RAG
benchmark. Our work offers a better way to evaluate and improve retrieval in
multi-modal RAG systems while also providing training data and models that
address current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Gaming to Research: GTA V for Synthetic Data Generation for
  Robotics and Navigations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Scucchia, Matteo Ferrara, Davide Maltoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, the development of robust algorithms capable of
generalizing effectively in real-world scenarios more and more often requires
large-scale datasets collected under diverse environmental conditions. However,
acquiring such datasets is time-consuming, costly, and sometimes unfeasible. To
address these limitations, the use of synthetic data has gained attention as a
viable alternative, allowing researchers to generate vast amounts of data while
simulating various environmental contexts in a controlled setting. In this
study, we investigate the use of synthetic data in robotics and navigation,
specifically focusing on Simultaneous Localization and Mapping (SLAM) and
Visual Place Recognition (VPR). In particular, we introduce a synthetic dataset
created using the virtual environment of the video game Grand Theft Auto V (GTA
V), along with an algorithm designed to generate a VPR dataset, without human
supervision. Through a series of experiments centered on SLAM and VPR, we
demonstrate that synthetic data derived from GTA V are qualitatively comparable
to real-world data. Furthermore, these synthetic data can complement or even
substitute real-world data in these applications. This study sets the stage for
the creation of large-scale synthetic datasets, offering a cost-effective and
scalable solution for future research and development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Per-channel autoregressive linear prediction padding in tiled CNN
  processing of 2D spatial data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olli Niemitalo, Otto Rosenberg, Nathaniel Narra, Olli Koskela, Iivari Kunttu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present linear prediction as a differentiable padding method. For each
channel, a stochastic autoregressive linear model is fitted to the padding
input by minimizing its noise terms in the least-squares sense. The padding is
formed from the expected values of the autoregressive model given the known
pixels. We trained the convolutional RVSR super-resolution model from scratch
on satellite image data, using different padding methods. Linear prediction
padding slightly reduced the mean square super-resolution error compared to
zero and replication padding, with a moderate increase in time cost. Linear
prediction padding better approximated satellite image data and RVSR feature
map data. With zero padding, RVSR appeared to use more of its capacity to
compensate for the high approximation error. Cropping the network output by a
few pixels reduced the super-resolution error and the effect of the choice of
padding method on the error, favoring output cropping with the faster
replication and zero padding methods, for the studied workload.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 20 figures including appendix; to be submitted for review;
  for source code, see https://doi.org/10.5281/zenodo.14871260</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duo Streamers: A Streaming Gesture Recognition Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxuan Zhu, Sicheng Yang, Zhuo Wang, Haining Liang, Junxiao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gesture recognition in resource-constrained scenarios faces significant
challenges in achieving high accuracy and low latency. The streaming gesture
recognition framework, Duo Streamers, proposed in this paper, addresses these
challenges through a three-stage sparse recognition mechanism, an RNN-lite
model with an external hidden state, and specialized training and
post-processing pipelines, thereby making innovative progress in real-time
performance and lightweight design. Experimental results show that Duo
Streamers matches mainstream methods in accuracy metrics, while reducing the
real-time factor by approximately 92.3%, i.e., delivering a nearly 13-fold
speedup. In addition, the framework shrinks parameter counts to 1/38 (idle
state) and 1/9 (busy state) compared to mainstream models. In summary, Duo
Streamers not only offers an efficient and practical solution for streaming
gesture recognition in resource-constrained devices but also lays a solid
foundation for extended applications in multimodal and diverse scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale
  Convolutions and Multiview Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuesong Liu, Emmett J. Ientilucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient segmentation of smoke plumes is crucial for environmental
monitoring and industrial safety, enabling the detection and mitigation of
harmful emissions from activities like quarry blasts and wildfires. Accurate
segmentation facilitates environmental impact assessments, timely
interventions, and compliance with safety standards. However, existing models
often face high computational demands and limited adaptability to diverse smoke
appearances, restricting their deployment in resource-constrained environments.
To address these issues, we introduce SmokeNet, a novel deep learning
architecture that leverages multiscale convolutions and multiview linear
attention mechanisms combined with layer-specific loss functions to handle the
complex dynamics of diverse smoke plumes, ensuring efficient and accurate
segmentation across varied environments. Additionally, we evaluate SmokeNet's
performance and versatility using four datasets, including our quarry blast
smoke dataset made available to the community. The results demonstrate that
SmokeNet maintains a favorable balance between computational efficiency and
segmentation accuracy, making it suitable for deployment in environmental
monitoring and safety management systems. By contributing a new dataset and
offering an efficient segmentation model, SmokeNet advances smoke segmentation
capabilities in diverse and challenging environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Whole-body Grasp Synthesis with Directional Controllability <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Paschalidis, Romana Wilschut, Dimitrije Antić, Omid Taheri, Dimitrios Tzionas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing 3D whole bodies that realistically grasp objects is useful for
animation, mixed reality, and robotics. This is challenging, because the hands
and body need to look natural w.r.t. each other, the grasped object, as well as
the local scene (i.e., a receptacle supporting the object). Moreover, training
data for this task is really scarce, while capturing new data is expensive.
Recent work goes beyond finite datasets via a divide-and-conquer approach; it
first generates a "guiding" right-hand grasp, and then searches for bodies that
match this. However, the guiding-hand synthesis lacks controllability and
receptacle awareness, so it likely has an implausible direction (i.e., a body
can't match this without penetrating the receptacle) and needs corrections
through major post-processing. Moreover, the body search needs exhaustive
sampling and is expensive. These are strong limitations. We tackle these with a
novel method called CWGrasp. Our key idea is that performing geometry-based
reasoning "early on," instead of "too late," provides rich "control" signals
for inference. To this end, CWGrasp first samples a plausible
reaching-direction vector (used later for both the arm and hand) from a
probabilistic model built via ray-casting from the object and collision
checking. Moreover, CWGrasp uniquely tackles both right and left-hand grasps.
We evaluate on the GRAB and ReplicaGrasp datasets. CWGrasp outperforms
baselines, at lower runtime and budget, while all components help performance.
Code and models are available at https://gpaschalidis.github.io/cwgrasp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting models' contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is quite effective and sensitive in identifying varying degrees
of contamination, and can highlight significant performance improvements due to
the leakage of multimodal benchmark training sets. Furthermore, we explore
whether the contamination originates from the base LLMs used by MLLMs or the
multimodal training phase, providing new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaVILA: Legged Robot Vision-Language-Action Model for Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes to solve the problem of Vision-and-Language Navigation
with legged robots, which not only provides a flexible way for humans to
command but also allows the robot to navigate through more challenging and
cluttered scenes. However, it is non-trivial to translate human language
instructions all the way to low-level leg joint actions. We propose NaVILA, a
2-level framework that unifies a Vision-Language-Action model (VLA) with
locomotion skills. Instead of directly predicting low-level actions from VLA,
NaVILA first generates mid-level actions with spatial information in the form
of language, (e.g., "moving forward 75cm"), which serves as an input for a
visual locomotion RL policy for execution. NaVILA substantially improves
previous approaches on existing benchmarks. The same advantages are
demonstrated in our newly developed benchmarks with IsaacLab, featuring more
realistic scenes, low-level controls, and real-world robot experiments. We show
more results at https://navila-bot.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://navila-bot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generation and Detection of Sign Language Deepfakes - A Linguistic and
  Visual Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahzeb Naeem, Muhammad Riyyan Khan, Usman Tariq, Abhinav Dhall, Carlos Ivan Colon, Hasan Al-Nashash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research explores the positive application of deepfake technology for
upper body generation, specifically sign language for the Deaf and Hard of
Hearing (DHoH) community. Given the complexity of sign language and the
scarcity of experts, the generated videos are vetted by a sign language expert
for accuracy. We construct a reliable deepfake dataset, evaluating its
technical and visual credibility using computer vision and natural language
processing models. The dataset, consisting of over 1200 videos featuring both
seen and unseen individuals, is also used to detect deepfake videos targeting
vulnerable individuals. Expert annotations confirm that the generated videos
are comparable to real sign language content. Linguistic analysis, using
textual similarity scores and interpreter evaluations, shows that the
interpretation of generated videos is at least 90% similar to authentic sign
language. Visual analysis demonstrates that convincingly realistic deepfakes
can be produced, even for new subjects. Using a pose/style transfer model, we
pay close attention to detail, ensuring hand movements are accurate and align
with the driving video. We also apply machine learning algorithms to establish
a baseline for deepfake detection on this dataset, contributing to the
detection of fraudulent sign language videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures, IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL
  SYSTEM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLEAR: Character Unlearning in Textual and Visual Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin, Boris Mikheev, Denis Bobkov, Aibek Alanov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning (MU) is critical for removing private or hazardous
information from deep learning models. While MU has advanced significantly in
unimodal (text or vision) settings, multimodal unlearning (MMU) remains
underexplored due to the lack of open benchmarks for evaluating cross-modal
data removal. To address this gap, we introduce CLEAR, the first open-source
benchmark designed specifically for MMU. CLEAR contains 200 fictitious
individuals and 3,700 images linked with corresponding question-answer pairs,
enabling a thorough evaluation across modalities. We conduct a comprehensive
analysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four
evaluation sets, demonstrating that jointly unlearning both modalities
outperforms single-modality approaches. The dataset is available at
https://huggingface.co/datasets/therem/CLEAR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision CNNs trained to estimate spatial latents learned similar
  ventral-stream-aligned representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies of the functional role of the primate ventral visual stream have
traditionally focused on object categorization, often ignoring -- despite much
prior evidence -- its role in estimating "spatial" latents such as object
position and pose. Most leading ventral stream models are derived by optimizing
networks for object categorization, which seems to imply that the ventral
stream is also derived under such an objective. Here, we explore an alternative
hypothesis: Might the ventral stream be optimized for estimating spatial
latents? And a closely related question: How different -- if at all -- are
representations learned from spatial latent estimation compared to
categorization? To ask these questions, we leveraged synthetic image datasets
generated by a 3D graphic engine and trained convolutional neural networks
(CNNs) to estimate different combinations of spatial and category latents. We
found that models trained to estimate just a few spatial latents achieve neural
alignment scores comparable to those trained on hundreds of categories, and the
spatial latent performance of models strongly correlates with their neural
alignment. Spatial latent and category-trained models have very similar -- but
not identical -- internal representations, especially in their early and middle
layers. We provide evidence that this convergence is partly driven by
non-target latent variability in the training data, which facilitates the
implicit learning of representations of those non-target latents. Taken
together, these results suggest that many training objectives, such as spatial
latents, can lead to similar models aligned neurally with the ventral stream.
Thus, one should not assume that the ventral stream is optimized for object
categorization only. As a field, we need to continue to sharpen our measures of
comparing models to brains to better understand the functional roles of the
ventral stream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 21 figures, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Figurative Meaning through Explainable Visual Entailment <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in
tasks requiring a fine-grained understanding of literal meaning in images and
text, such as visual question-answering or visual entailment. However, there
has been little exploration of the capabilities of these models when presented
with images and captions containing figurative meaning, such as metaphors or
humor. To close this gap, we propose a new task framing the figurative meaning
understanding problem as an explainable visual entailment task, where the model
has to predict whether the image (premise) entails a caption (hypothesis) and
justify the predicted label with a textual explanation. The figurative
phenomena can be present in the image, in the caption, or both. Using a
human-AI collaboration approach, we build the accompanying expert-verified
dataset V-FLUTE, containing 6,027 {image, caption, label, explanation}
instances spanning five diverse figurative phenomena: metaphors, similes,
idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs
struggle to generalize from literal to figurative meaning, particularly when it
is present in images. Further, we identify common types of errors in VLM
reasoning (hallucination and incomplete or unsound reasoning) across classes of
models via human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Health<span class="highlight-title">GPT</span>: A Medical Large Vision-Language Model for Unifying
  Comprehension and Generation via Heterogeneous Knowledge Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HealthGPT, a powerful Medical Large Vision-Language Model
(Med-LVLM) that integrates medical visual comprehension and generation
capabilities within a unified autoregressive paradigm. Our bootstrapping
philosophy is to progressively adapt heterogeneous comprehension and generation
knowledge to pre-trained large language models (LLMs). This is achieved through
a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is
complemented by a tailored hierarchical visual perception approach and a
three-stage learning strategy. To effectively learn the HealthGPT, we devise a
comprehensive medical domain-specific comprehension and generation dataset
called VL-Health. Experimental results demonstrate exceptional performance and
scalability of HealthGPT in medical visual unified tasks. Our project can be
accessed at https://github.com/DCDmllm/HealthGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: added project page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18592v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18592v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View
  Gaussian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onat Şahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have significantly improved 3D
generation, enabling the use of assets generated from an image for embodied AI
simulations. However, the one-to-many nature of the image-to-3D problem limits
their use due to inconsistent content and quality across views. Previous models
optimize a 3D model by sampling views from a view-conditioned diffusion prior,
but diffusion models cannot guarantee view consistency. Instead, we present
ConsistentDreamer, where we first generate a set of fixed multi-view prior
images and sample random views between them with another diffusion model
through a score distillation sampling (SDS) loss. Thereby, we limit the
discrepancies between the views guided by the SDS loss and ensure a consistent
rough shape. In each iteration, we also use our generated multi-view prior
images for fine-detail reconstruction. To balance between the rough shape and
the fine-detail optimizations, we introduce dynamic task-dependent weights
based on homoscedastic uncertainty, updated automatically in each iteration.
Additionally, we employ opacity, depth distortion, and normal alignment losses
to refine the surface for mesh extraction. Our method ensures better view
consistency and visual quality compared to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted by Pattern Recognition Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Multimodal Models Solve Caption Generation for Scientific
  Figures? Lessons Learned from SCICAP Challenge 2023 <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the SCICAP datasets launch in 2021, the research community has made
significant progress in generating captions for scientific figures in scholarly
articles. In 2023, the first SCICAP Challenge took place, inviting global teams
to use an expanded SCICAP dataset to develop models for captioning diverse
figure types across various academic fields. At the same time, text generation
models advanced quickly, with many powerful pre-trained large multimodal models
(LMMs) emerging that showed impressive capabilities in various
vision-and-language tasks. This paper presents an overview of the first SCICAP
Challenge and details the performance of various models on its data, capturing
a snapshot of the fields state. We found that professional editors
overwhelmingly preferred figure captions generated by GPT-4V over those from
all other models and even the original captions written by authors. Following
this key finding, we conducted detailed analyses to answer this question: Have
advanced LMMs solved the task of generating captions for scientific figures?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Compressed Image Latents and Multimodal Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever study of adapting compressed image latents
to suit the needs of downstream vision tasks that adopt Multimodal Large
Language Models (MLLMs). MLLMs have extended the success of large language
models to modalities (e.g. images) beyond text, but their billion scale hinders
deployment on resource-constrained end devices. While cloud-hosted MLLMs could
be available, transmitting raw, uncompressed images captured by end devices to
the cloud requires an efficient image compression system. To address this, we
focus on emerging neural image compression and propose a novel framework with a
lightweight transform-neck and a surrogate loss to adapt compressed image
latents for MLLM-based vision tasks. Given the huge scale of MLLMs, our
framework excludes the entire downstream MLLM except part of its visual encoder
from training our system. This stands out from most existing coding for machine
approaches that involve downstream networks in training and thus could be
impractical when the networks are MLLMs. The proposed framework is general in
that it is applicable to various MLLMs, neural image codecs, and multiple
application scenarios, where the neural image codec can be (1) pre-trained for
human perception without updating, (2) fully updated for joint human and
machine perception, or (3) fully updated for only machine perception. Extensive
experiments on different neural image codecs and various MLLMs show that our
method achieves great rate-accuracy performance with much less complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iFormer: Integrating ConvNet and <span class="highlight-title">Transformer</span> for Mobile Application <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new family of mobile hybrid vision networks, called iFormer,
with a focus on optimizing latency and accuracy on mobile applications. iFormer
effectively integrates the fast local representation capacity of convolution
with the efficient global modeling ability of self-attention. The local
interactions are derived from transforming a standard convolutional network,
\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly
introduced mobile modulation attention removes memory-intensive operations in
MHA and employs an efficient modulation mechanism to boost dynamic global
representational capacity. We conduct comprehensive experiments demonstrating
that iFormer outperforms existing lightweight networks across various tasks.
Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\% on ImageNet-1k
with a latency of only 1.10 ms on an iPhone 13, surpassing the recently
proposed MobileNetV4 under similar latency constraints. Additionally, our
method shows significant improvements in downstream tasks, including COCO
object detection, instance segmentation, and ADE20k semantic segmentation,
while still maintaining low latency on mobile devices for high-resolution
inputs in these scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code:
  https://github.com/ChuanyangZheng/iFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Long Videos with Multimodal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16998v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16998v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have allowed recent LLM-based approaches to
achieve excellent performance on long-video understanding benchmarks. We
investigate how extensive world knowledge and strong reasoning skills of
underlying LLMs influence this strong performance. Surprisingly, we discover
that LLM-based approaches can yield surprisingly good accuracy on long-video
tasks with limited video information, sometimes even with no video specific
information. Building on this, we exploring injecting video-specific
information into an LLM-based framework. We utilize off-the-shelf vision tools
to extract three object-centric information modalities from videos and then
leverage natural language as a medium for fusing this information. Our
resulting Multimodal Video Understanding (MVU) framework demonstrates
state-of-the-art performance across multiple video understanding benchmarks.
Strong performance also on robotics domain tasks establish its strong
generality. Our code will be released publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/kahnchana/mvu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of End-to-End Continuous Spanish Lipreading in Different Data
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual speech recognition remains an open research problem where different
challenges must be considered by dispensing with the auditory sense, such as
visual ambiguities, the inter-personal variability among speakers, and the
complex modeling of silence. Nonetheless, recent remarkable results have been
achieved in the field thanks to the availability of large-scale databases and
the use of powerful attention mechanisms. Besides, multiple languages apart
from English are nowadays a focus of interest. This paper presents noticeable
advances in automatic continuous lipreading for Spanish. First, an end-to-end
system based on the hybrid CTC/Attention architecture is presented. Experiments
are conducted on two corpora of disparate nature, reaching state-of-the-art
results that significantly improve the best performance obtained to date for
both databases. In addition, a thorough ablation study is carried out, where it
is studied how the different components that form the architecture influence
the quality of speech recognition. Then, a rigorous error analysis is carried
out to investigate the different factors that could affect the learning of the
automatic system. Finally, a new Spanish lipreading benchmark is consolidated.
Code and trained models are available at
https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the "Language Resources and Evaluation" journal, Springer
  Nature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable Insect Monitoring: Ultra-Lightweight CNNs as On-Device
  Triggers for Insect Camera Traps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Gardiner, Sareh Rowands, Benno I. Simmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera traps, combined with AI, have emerged as a way to achieve automated,
scalable biodiversity monitoring. However, the passive infrared (PIR) sensors
that trigger camera traps are poorly suited for detecting small, fast-moving
ectotherms such as insects. Insects comprise over half of all animal species
and are key components of ecosystems and agriculture. The need for an
appropriate and scalable insect camera trap is critical in the wake of
concerning reports of declines in insect populations. This study proposes an
alternative to the PIR trigger: ultra-lightweight convolutional neural networks
running on low-powered hardware to detect insects in a continuous stream of
captured images. We train a suite of models to distinguish insect images from
backgrounds. Our design achieves zero latency between trigger and image
capture. Our models are rigorously tested and achieve high accuracy ranging
from 91.8% to 96.4% AUC on validation data and >87% AUC on data from
distributions unseen during training. The high specificity of our models
ensures minimal saving of false positive images, maximising deployment storage
efficiency. High recall scores indicate a minimal false negative rate,
maximising insect detection. Further analysis with saliency maps shows the
learned representation of our models to be robust, with low reliance on
spurious background features. Our system is also shown to operate deployed on
off-the-shelf, low-powered microcontroller units, consuming a maximum power
draw of less than 300mW. This enables longer deployment times using cheap and
readily available battery components. Overall we offer a step change in the
cost, efficiency and scope of insect monitoring. Solving the challenging
trigger problem, we demonstrate a system which can be deployed for far longer
than existing designs and budgets power and bandwidth effectively, moving
towards a generic insect camera trap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BitStack: Any-Size Compression of Large Language Models in Variable
  Memory Environments <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novel computational workflows for natural and biomedical image
  processing based on hypercomplex algebras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nektarios A. Valous, Eckhard Hitzer, Dragoş Duşe, Rodrigo Rojas Moraleda, Ferdinand Popp, Meggy Suarez-Carmona, Anna Berthel, Ismini Papageorgiou, Carlo Fremd, Alexander Rölle, Christina C. Westhoff, Bénédicte Lenoir, Niels Halama, Inka Zörnig, Dirk Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypercomplex image processing extends conventional techniques in a unified
paradigm encompassing algebraic and geometric principles. This work leverages
quaternions and the two-dimensional orthogonal planes split framework
(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D
planes) for natural/biomedical image analysis through the following
computational workflows and outcomes: natural/biomedical image re-colorization,
natural image de-colorization, natural/biomedical image contrast enhancement,
computational re-staining and stain separation in histological images, and
performance gains in machine/deep learning pipelines for histological images.
The workflows are analyzed separately for natural and biomedical images to
showcase the effectiveness of the proposed approaches. The proposed workflows
can regulate color appearance (e.g. with alternative renditions and grayscale
conversion) and image contrast, be part of automated image processing pipelines
(e.g. isolating stain components, boosting learning models), and assist in
digital pathology applications (e.g. enhancing biomarker visibility, enabling
colorblind-friendly renditions). Employing only basic arithmetic and matrix
operations, this work offers a computationally accessible methodology - in the
hypercomplex domain - that showcases versatility and consistency across image
processing tasks and a range of computer vision and biomedical applications.
The proposed non-data-driven methods achieve comparable or better results
(particularly in cases involving well-known methods) to those reported in the
literature, showcasing the potential of robust theoretical frameworks with
practical effectiveness. Results, methods, and limitations are detailed
alongside discussion of promising extensions, emphasizing the potential of
feature-rich mathematical/computational frameworks for natural and biomedical
images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 18 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Language Models Exhibit Higher Visual Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How well do text-only Large Language Models (LLMs) naturally align with the
visual world? We provide the first direct analysis by utilizing frozen text
representations in a discriminative vision-language model framework and
measuring zero-shot generalization on unseen classes. We find decoder-based
LLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs
reliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs
leads to strong gains in cross-lingual settings, where our approach surpasses
CLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves
both robustness and generalization and also significantly reduces the need for
paired data and compute, making vision-language models more accessible and
adaptable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Meta-Learning from a Learning Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Wang, Wenwen Qiang, Chuxiong Sun, Changwen Zheng, Jiangmeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-learning has emerged as a powerful approach for leveraging knowledge
from previous tasks to solve new tasks. The mainstream methods focus on
training a well-generalized model initialization, which is then adapted to
different tasks with limited data and updates. However, it pushes the model
overfitting on the training tasks. Previous methods mainly attributed this to
the lack of data and used augmentations to address this issue, but they were
limited by sufficient training and effective augmentation strategies. In this
work, we focus on the more fundamental learning to learn strategy of
meta-learning to explore what causes errors and how to eliminate these errors
without changing the environment. Specifically, we first rethink the
algorithmic procedure of meta-learning from a learning lens. Through
theoretical and empirical analyses, we find that (i) this paradigm faces the
risk of both overfitting and underfitting and (ii) the model adapted to
different tasks promote each other where the effect is stronger if the tasks
are more similar. Based on this insight, we propose using task relations to
calibrate the optimization process of meta-learning and propose a plug-and-play
method called Task Relation Learner (TRLearner) to achieve this goal.
Specifically, it first obtains task relation matrices from the extracted
task-specific meta-data. Then, it uses the obtained matrices with
relation-aware consistency regularization to guide optimization. Extensive
theoretical and empirical analyses demonstrate the effectiveness of TRLearner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangwen Wu, Lechao Cheng, Shengeng Tang, Xiaofeng Zhu, Chaowei Fang, Dingwen Zhang, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) seeks to enable a model to sequentially
learn new classes while retaining knowledge of previously learned ones.
Balancing flexibility and stability remains a significant challenge,
particularly when the task ID is unknown. To address this, our study reveals
that the gap in feature distribution between novel and existing tasks is
primarily driven by differences in mean and covariance moments. Building on
this insight, we propose a novel semantic drift calibration method that
incorporates mean shift compensation and covariance calibration. Specifically,
we calculate each class's mean by averaging its sample embeddings and estimate
task shifts using weighted embedding changes based on their proximity to the
previous mean, effectively capturing mean shifts for all learned classes with
each new task. We also apply Mahalanobis distance constraint for covariance
calibration, aligning class-specific embedding covariances between old and
current networks to mitigate the covariance shift. Additionally, we integrate a
feature-level self-distillation approach to enhance generalization.
Comprehensive experiments on commonly used datasets demonstrate the
effectiveness of our approach. The source code is available at
\href{https://github.com/fwu11/MACIL.git}{https://github.com/fwu11/MACIL.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Swapping via Learning and Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Xing, Lechao Cheng, Shengeng Tang, Yaxiong Wang, Zhun Zhong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce \textbf{Knowledge Swapping}, a novel task designed to
selectively regulate knowledge of a pretrained model by enabling the forgetting
of user\-specified information, retaining essential knowledge, and acquiring
new knowledge simultaneously. By delving into the analysis of knock-on feature
hierarchy, we find that incremental learning typically progresses from
low\-level representations to higher\-level semantics, whereas forgetting tends
to occur in the opposite direction\-starting from high-level semantics and
moving down to low-level features. Building upon this, we propose to benchmark
the knowledge swapping task with the strategy of \textit{Learning Before
Forgetting}. Comprehensive experiments on various tasks like image
classification, object detection, and semantic segmentation validate the
effectiveness of the proposed strategy. The source code is available at
\href{https://github.com/xingmingyu123456/KnowledgeSwapping}{https://github.com/xingmingyu123456/KnowledgeSwapping}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCo: Synthetic Hard Negatives for Contrastive Visual Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning, but efficiently leveraging hard negatives, which are
samples closely resembling the anchor, remains challenging. We introduce SynCo
(Synthetic negatives in Contrastive learning), a novel approach that improves
model performance by generating synthetic hard negatives on the representation
space. Building on the MoCo framework, SynCo introduces six strategies for
creating diverse synthetic hard negatives on-the-fly with minimal computational
overhead. SynCo achieves faster training and strong representation learning,
surpassing MoCo-v2 by +0.4% and MoCHI by +1.0% on ImageNet ILSVRC-2012 linear
evaluation. It also transfers more effectively to detection tasks achieving
strong results on PASCAL VOC detection (57.2% AP) and significantly improving
over MoCo-v2 on COCO detection (+1.0% AP) and instance segmentation (+0.8% AP).
Our synthetic hard negative generation approach significantly enhances visual
representations learned through self-supervised contrastive learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code: https://github.com/giakoumoglou/synco, Supplementary:
  https://giakoumoglou.com/src/synco_suppl.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Catastrophic Forgetting in Two-Stage Incremental Object
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qirui Wu, Shizhou Zhang, De Cheng, Yinghui Xing, Di Xu, Peng Wang, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting is a critical chanllenge for incremental object
detection (IOD). Most existing methods treat the detector monolithically,
relying on instance replay or knowledge distillation without analyzing
component-specific forgetting. Through dissection of Faster R-CNN, we reveal a
key insight: Catastrophic forgetting is predominantly localized to the RoI Head
classifier, while regressors retain robustness across incremental stages. This
finding challenges conventional assumptions, motivating us to develop a
framework termed NSGP-RePRE. Regional Prototype Replay (RePRE) mitigates
classifier forgetting via replay of two types of prototypes: coarse prototypes
represent class-wise semantic centers of RoI features, while fine-grained
prototypes model intra-class variations. Null Space Gradient Projection (NSGP)
is further introduced to eliminate prototype-feature misalignment by updating
the feature extractor in directions orthogonal to subspace of old inputs via
gradient projection, aligning RePRE with incremental learning dynamics. Our
simple yet effective design allows NSGP-RePRE to achieve state-of-the-art
performance on the Pascal VOC and MS COCO datasets under various settings. Our
work not only advances IOD methodology but also provide pivotal insights for
catastrophic forgetting mitigation in IOD. Code will be available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PrototypeFormer: Learning to Explore Prototype Relationships for
  Few-shot Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meijuan Su, Feihong He, Fanzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot image classification has received considerable attention for
overcoming the challenge of limited classification performance with limited
samples in novel classes. Most existing works employ sophisticated learning
strategies and feature learning modules to alleviate this challenge. In this
paper, we propose a novel method called PrototypeFormer, exploring the
relationships among category prototypes in the few-shot scenario. Specifically,
we utilize a transformer architecture to build a prototype extraction module,
aiming to extract class representations that are more discriminative for
few-shot classification. Besides, during the model training process, we propose
a contrastive learning-based optimization approach to optimize prototype
features in few-shot learning scenarios. Despite its simplicity, our method
performs remarkably well, with no bells and whistles. We have experimented with
our approach on several popular few-shot image classification benchmark
datasets, which shows that our method outperforms all current state-of-the-art
methods. In particular, our method achieves 97.07\% and 90.88\% on 5-way 5-shot
and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art
results with accuracy of 0.57\% and 6.84\%, respectively. The code will be
released later.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Attribute-Relation Representation Based Video Semantic
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Du, Yiping Duan, Qianqian Yang, Xiaoming Tao, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of multimedia data volume, there is an increasing need
for efficient video transmission in applications such as virtual reality and
future video streaming services. Semantic communication is emerging as a vital
technique for ensuring efficient and reliable transmission in low-bandwidth,
high-noise settings. However, most current approaches focus on joint
source-channel coding (JSCC) that depends on end-to-end training. These methods
often lack an interpretable semantic representation and struggle with
adaptability to various downstream tasks. In this paper, we introduce the use
of object-attribute-relation (OAR) as a semantic framework for videos to
facilitate low bit-rate coding and enhance the JSCC process for more effective
video transmission. We utilize OAR sequences for both low bit-rate
representation and generative video reconstruction. Additionally, we
incorporate OAR into the image JSCC model to prioritize communication resources
for areas more critical to downstream tasks. Our experiments on traffic
surveillance video datasets assess the effectiveness of our approach in terms
of video transmission performance. The empirical findings demonstrate that our
OAR-based video coding method not only outperforms H.265 coding at lower
bit-rates but also synergizes with JSCC to deliver robust and efficient video
transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Vision Language Model Training via High Quality Data Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, Jiao Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning
via High QuaLity Data Curation), an open-source vision language model (VLM)
series achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.
The following three key improvements contribute to SAIL-VL's leading
performance: (1) Scalable high-quality visual understanding data construction:
We implement a data construction pipeline to enable hundred-million-scale
high-quality recaption data annotation, and the resulted dataset SAIL-Caption
is validated to be of the highest data quality compared with opensource
alternatives. (2) Scalable Pretraining with High-Quality Visual Understanding
Data: We scale SAIL-VL's pretraining budget up to 655B tokens and show that
even a 2B VLM benefits from scaled up training data sizes, exhibiting expected
data size scaling laws in visual understanding and instruction following
performance. (3) Scalable SFT via data quantity and complexity scaling: We
curate a high-quality SFT dataset collection which outperforms opensource
alternatives in data quantity scaling effectiveness. We also demonstrate that
training with progressively higher-complexity data surpasses baseline one-stage
training by a large margin. SAIL-VL series models achieve the highest average
score in 18 widely used VLM benchmarks in our evaluation, with the 2B model
takes the top position over VLMs of comparable sizes on OpenCompass 2024
(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust
visual comprehension abilities. SAIL-VL series models are released at
HuggingFace (https://huggingface.co/BytedanceDouyinContent).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Is That Talk About? A Video-to-Text Summarization <span class="highlight-title">Dataset</span> for
  Scientific Presentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transforming recorded videos into concise and accurate textual summaries is a
growing challenge in multimodal learning. This paper introduces VISTA, a
dataset specifically designed for video-to-text summarization in scientific
domains. VISTA contains 18,599 recorded AI conference presentations paired with
their corresponding paper abstracts. We benchmark the performance of
state-of-the-art large models and apply a plan-based framework to better
capture the structured nature of abstracts. Both human and automated
evaluations confirm that explicit planning enhances summary quality and factual
consistency. However, a considerable gap remains between models and human
performance, highlighting the challenges of scientific video summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global-Local Distillation Network-Based Audio-Visual Speaker Tracking
  with Incomplete Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidi Li, Yihan Li, Yixin Guo, Bin Ren, Zhenhuan Xu, Hao Guo, Hong Liu, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In speaker tracking research, integrating and complementing multi-modal data
is a crucial strategy for improving the accuracy and robustness of tracking
systems. However, tracking with incomplete modalities remains a challenging
issue due to noisy observations caused by occlusion, acoustic noise, and sensor
failures. Especially when there is missing data in multiple modalities, the
performance of existing multi-modal fusion methods tends to decrease. To this
end, we propose a Global-Local Distillation-based Tracker (GLDTracker) for
robust audio-visual speaker tracking. GLDTracker is driven by a teacher-student
distillation model, enabling the flexible fusion of incomplete information from
each modality. The teacher network processes global signals captured by camera
and microphone arrays, and the student network handles local information
subject to visual occlusion and missing audio channels. By transferring
knowledge from teacher to student, the student network can better adapt to
complex dynamic scenes with incomplete observations. In the student network, a
global feature reconstruction module based on the generative adversarial
network is constructed to reconstruct global features from feature embedding
with missing local information. Furthermore, a multi-modal multi-level fusion
attention is introduced to integrate the incomplete feature and the
reconstructed feature, leveraging the complementarity and consistency of
audio-visual and global-local features. Experimental results on the AV16.3
dataset demonstrate that the proposed GLDTracker outperforms existing
state-of-the-art audio-visual trackers and achieves leading performance on both
standard and incomplete modalities datasets, highlighting its superiority and
robustness in complex conditions. The code and models will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We request to withdraw our paper from arXiv due to unresolved author
  disagreements about the data interpretation and study conclusions. To
  maintain scientific integrity, we believe withdrawing the paper is necessary.
  We regret any confusion caused</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parametric PerceptNet: A bio-inspired deep-net trained for Image Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Vila-Tomás, Pablo Hernández-Cámara, Valero Laparra, Jesús Malo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision models are at the core of image processing. For instance,
classical approaches to the problem of image quality are based on models that
include knowledge about human vision. However, nowadays, deep learning
approaches have obtained competitive results by simply approaching this problem
as regression of human decisions, and training an standard network on
human-rated datasets. These approaches have the advantages of being easily
adaptable to a particular problem and they fit very efficiently when data is
available. However, mainly due to the excess of parameters, they have the
problems of lack of interpretability, and over-fitting. Here we propose a
vision model that combines the best of both worlds by using a parametric neural
network architecture. We parameterize the layers to have bioplausible
functionality, and provide a set of bioplausible parameters. We analyzed
different versions of the model and compared it with the non-parametric
version. The parametric models achieve a three orders of magnitude reduction in
the number of parameters without suffering in regression performance.
Furthermore, we show that the parametric models behave better during training
and are easier to interpret as vision models. Interestingly, we find that, even
initialized with bioplausible trained for regression using human rated
datasets, which we call the feature-spreading problem. This suggests that the
deep learning approach is inherently flawed, and emphasizes the need to
evaluate and train models beyond regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for
  Traversability Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01395v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01395v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Santo, Juan J. Cabrera, David Valiente, Carlos Viegas, Arturo Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents TE-NeXt, a novel and efficient architecture for
Traversability Estimation (TE) from sparse LiDAR point clouds based on a
residual convolution block. TE-NeXt block fuses notions of current trends such
as attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate
high capacity for generalisation in a variety of urban and natural
environments, using well-known and accessible datasets such as SemanticKITTI,
Rellis-3D and SemanticUSL. Thus, the designed architecture ouperforms
state-of-the-art methods in the problem of semantic segmentation, demonstrating
better results in unstructured environments and maintaining high reliability
and robustness in urbans environments, which leads to better abstraction.
Implementation is available in a open repository to the scientific community
with the aim of ensuring the reproducibility of results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DINeuro: Distilling Knowledge from 2D Natural Images via Deformable
  Tubular Transferring Strategy for 3D Neuron Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yik San Cheng, Runkai Zhao, Heng Wang, Hanchuan Peng, Yui Lo, Yuqian Chen, Lauren J. O'Donnell, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing neuron morphology from 3D light microscope imaging data is
critical to aid neuroscientists in analyzing brain networks and neuroanatomy.
With the boost from deep learning techniques, a variety of learning-based
segmentation models have been developed to enhance the signal-to-noise ratio of
raw neuron images as a pre-processing step in the reconstruction workflow.
However, most existing models directly encode the latent representative
features of volumetric neuron data but neglect their intrinsic morphological
knowledge. To address this limitation, we design a novel framework that
distills the prior knowledge from a 2D Vision Transformer pre-trained on
extensive 2D natural images to facilitate neuronal morphological learning of
our 3D Vision Transformer. To bridge the knowledge gap between the 2D natural
image and 3D microscopic morphologic domains, we propose a deformable tubular
transferring strategy that adapts the pre-trained 2D natural knowledge to the
inherent tubular characteristics of neuronal structure in the latent embedding
space. The experimental results on the Janelia dataset of the BigNeuron project
demonstrate that our method achieves a segmentation performance improvement of
4.53% in mean Dice and 3.56% in mean 95% Hausdorff distance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, and 2 tables. This work has been accepted to 2025
  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-based Ensemble Learning in CMR Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Liu, Ziyi Wu, Liang Zhong, Lingyi Wen, Yuankai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing models typically segment either the entire 3D frame or 2D slices
independently to derive clinical functional metrics from ventricular
segmentation in cardiac cine sequences. While performing well overall, they
struggle at the end slices. To address this, we leverage spatial continuity to
extract global uncertainty from segmentation variance and use it as memory in
our ensemble learning method, Streaming, for classifier weighting, balancing
overall and end-slice performance. Additionally, we introduce the End
Coefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M&Ms
datasets show that our framework achieves near-state-of-the-art Dice Similarity
Coefficient (DSC) and outperforms all models on end-slice performance,
improving patient-specific segmentation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Language <span class="highlight-title">Prompt</span>ing to Ease False Positives in Medical
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Myung Jin Kim, Hyeong Seok Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through
  Hierarchical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Zhang, Wei En Ng, Lixin Ma, Yuwen Wang, Jungqi Zhao, Allison Koenecke, Boyang Li, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision-language models may grasp basic spatial cues and simple
directions (e.g. left, right, front, back), but struggle with the
multi-dimensional spatial reasoning necessary for human-like understanding and
real-world applications. To address this gap, we develop SPHERE (Spatial
Perception and Hierarchical Evaluation of REasoning), a hierarchical evaluation
framework supported by a new human-annotated dataset. SPHERE systematically
probes models across increasing levels of complexity, from fundamental skills
to multi-skill integration and high-level reasoning that combines spatial,
visual, and logical understanding. Benchmark evaluation of state-of-the-art
models reveals significant deficiencies, especially in reasoning about distance
and proximity, understanding both egocentric and allocentric perspectives, and
applying spatial logic in physical contexts. These findings expose critical
blind spots in existing models and underscore the need for more advanced
spatial reasoning techniques, driving the development of vision-language models
that align more closely with human spatial cognition. The dataset will be
open-sourced upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion
  Models with Self-Augmented Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Zhang, Liang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of AI-generated images necessitates effective
watermarking techniques to protect intellectual property and detect fraudulent
content. While existing training-based watermarking methods show promise, they
often struggle with generalizing across diverse prompts and tend to introduce
visible artifacts. To this end, we propose a novel, provably generalizable
image watermarking approach for Latent Diffusion Models, termed Self-Augmented
Training (SAT-LDM). Our method aligns the training and testing phases through a
free generation distribution, thereby enhancing the watermarking module's
generalization capabilities. We theoretically consolidate SAT-LDM by proving
that the free generation distribution contributes to its tight generalization
bound, without the need for additional data collection. Extensive experiments
show that SAT-LDM not only achieves robust watermarking but also significantly
improves the quality of watermarked images across a wide range of prompts.
Moreover, our experimental analyses confirm the strong generalization abilities
of SAT-LDM. We hope that our method provides a practical and efficient solution
for securing high-fidelity AI-generated content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BoxMAC -- A Boxing <span class="highlight-title">Dataset</span> for Multi-label Action Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashikanta Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In competitive combat sports like boxing, analyzing a boxers's performance
statics is crucial for evaluating the quantity and variety of punches delivered
during bouts. These statistics provide valuable data and feedback, which are
routinely used for coaching and performance enhancement. We introduce BoxMAC, a
real-world boxing dataset featuring 15 professional boxers and encompassing 13
distinct action labels. Comprising over 60,000 frames, our dataset has been
meticulously annotated for multiple actions per frame with inputs from a boxing
coach. Since two boxers can execute different punches within a single
timestamp, this problem falls under the domain of multi-label action
classification. We propose a novel architecture for jointly recognizing
multiple actions in both individual images and videos. We investigate baselines
using deep neural network architectures to address both tasks. We believe that
BoxMAC will enable researchers and practitioners to develop and evaluate more
efficient models for performance analysis. With its realistic and diverse
nature, BoxMAC can serve as a valuable resource for the advancement of boxing
as a sport
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Significant modifications are required to improve the clarity and
  accuracy of the findings and This submission was made without the full
  agreement of all co-authors. To ensure proper authorship attribution and
  compliance with ethical guidelines, we are withdrawing this version. A
  revised and more complete version will be submitted soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cluster and Predict Latent Patches for Improved Masked Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothée Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, Piotr Bojanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Image Modeling (MIM) offers a promising approach to self-supervised
representation learning, however existing MIM models still lag behind the
state-of-the-art. In this paper, we systematically analyze target
representations, loss functions, and architectures, to introduce CAPI - a novel
pure-MIM framework that relies on the prediction of latent clusterings. Our
approach leverages a clustering-based loss, which is stable to train, and
exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%
accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,
substantially outperforming previous MIM methods and approaching the
performance of the current state-of-the-art, DINOv2. We release all our code
and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, submitted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Chen, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human sensing, which employs various sensors and advanced deep learning
technologies to accurately capture and interpret human body information, has
significantly impacted fields like public security and robotics. However,
current human sensing primarily depends on modalities such as cameras and
LiDAR, each of which has its own strengths and limitations. Furthermore,
existing multi-modal fusion solutions are typically designed for fixed modality
combinations, requiring extensive retraining when modalities are added or
removed for diverse scenarios. In this paper, we propose a modality-invariant
foundation model for all modalities, X-Fi, to address this issue. X-Fi enables
the independent or combinatory use of sensor modalities without additional
training by utilizing a transformer structure to accommodate variable input
sizes and incorporating a novel "X-fusion" mechanism to preserve
modality-specific features during multimodal integration. This approach not
only enhances adaptability but also facilitates the learning of complementary
features across modalities. Extensive experiments conducted on the MM-Fi and
XRF55 datasets, employing six distinct modalities, demonstrate that X-Fi
achieves state-of-the-art performance in human pose estimation (HPE) and human
activity recognition (HAR) tasks. The findings indicate that our proposed model
can efficiently support a wide range of human sensing applications, ultimately
contributing to the evolution of scalable, multimodal sensing technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by
  Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kengo Uchida, Takashi Shibuya, Yuhta Takida, Naoki Murata, Julian Tanke, Shusuke Takahashi, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In text-to-motion generation, controllability as well as generation quality
and speed has become increasingly critical. The controllability challenges
include generating a motion of a length that matches the given textual
description and editing the generated motions according to control signals,
such as the start-end positions and the pelvis trajectory. In this paper, we
propose MoLA, which provides fast, high-quality, variable-length motion
generation and can also deal with multiple editing tasks in a single framework.
Our approach revisits the motion representation used as inputs and outputs in
the model, incorporating an activation variable to enable variable-length
motion generation. Additionally, we integrate a variational autoencoder and a
latent diffusion model, further enhanced through adversarial training, to
achieve high-quality and fast generation. Moreover, we apply a training-free
guided generation framework to achieve various editing tasks with motion
control inputs. We quantitatively show the effectiveness of adversarial
learning in text-to-motion generation, and demonstrate the applicability of our
editing framework to multiple editing tasks in the motion domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-Video-T2V Technical Report: The Practice, Challenges, and Future of
  Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model
with 30B parameters and the ability to generate videos up to 204 frames in
length. A deep compression Variational Autoencoder, Video-VAE, is designed for
video generation tasks, achieving 16x16 spatial and 8x temporal compression
ratios, while maintaining exceptional video reconstruction quality. User
prompts are encoded using two bilingual text encoders to handle both English
and Chinese. A DiT with 3D full attention is trained using Flow Matching and is
employed to denoise input noise into latent frames. A video-based DPO approach,
Video-DPO, is applied to reduce artifacts and improve the visual quality of the
generated videos. We also detail our training strategies and share key
observations and insights. Step-Video-T2V's performance is evaluated on a novel
video generation benchmark, Step-Video-T2V-Eval, demonstrating its
state-of-the-art text-to-video quality when compared with both open-source and
commercial engines. Additionally, we discuss the limitations of current
diffusion-based model paradigm and outline future directions for video
foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval
available at https://github.com/stepfun-ai/Step-Video-T2V. The online version
can be accessed from https://yuewen.cn/videos as well. Our goal is to
accelerate the innovation of video foundation models and empower video content
creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-quality Unknown Object Instance Segmentation via Quadruple Boundary
  Error Refinement <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16132v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16132v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghyeok Back, Sangbeom Lee, Kangmin Kim, Joosoon Lee, Sungho Shin, Jemo Maeng, Kyoobin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient segmentation of unknown objects in unstructured
environments is essential for robotic manipulation. Unknown Object Instance
Segmentation (UOIS), which aims to identify all objects in unknown categories
and backgrounds, has become a key capability for various robotic tasks.
However, existing methods struggle with over-segmentation and
under-segmentation, leading to failures in manipulation tasks such as grasping.
To address these challenges, we propose QuBER (Quadruple Boundary Error
Refinement), a novel error-informed refinement approach for high-quality UOIS.
QuBER first estimates quadruple boundary errors-true positive, true negative,
false positive, and false negative pixels-at the instance boundaries of the
initial segmentation. It then refines the segmentation using an error-guided
fusion mechanism, effectively correcting both fine-grained and instance-level
segmentation errors. Extensive evaluations on three public benchmarks
demonstrate that QuBER outperforms state-of-the-art methods and consistently
improves various UOIS methods while maintaining a fast inference time of less
than 0.1 seconds. Furthermore, we show that QuBER improves the success rate of
grasping target objects in cluttered environments. Code and supplementary
materials are available at https://sites.google.com/view/uois-quber.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, accepted at ICRA 2025, project website:
  https://sites.google.com/view/uois-quber</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Growth Inhibitors for Suppressing Inappropriate Image Concepts in
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Die Chen, Zhiwen Li, Mingyuan Fan, Cen Chen, Wenmeng Zhou, Yanhao Wang, Yaliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their remarkable image generation capabilities, text-to-image
diffusion models inadvertently learn inappropriate concepts from vast and
unfiltered training data, which leads to various ethical and business risks.
Specifically, model-generated images may exhibit not safe for work (NSFW)
content and style copyright infringements. The prompts that result in these
problems often do not include explicit unsafe words; instead, they contain
obscure and associative terms, which are referred to as implicit unsafe
prompts. Existing approaches directly fine-tune models under textual guidance
to alter the cognition of the diffusion model, thereby erasing inappropriate
concepts. This not only requires concept-specific fine-tuning but may also
incur catastrophic forgetting. To address these issues, we explore the
representation of inappropriate concepts in the image space and guide them
towards more suitable ones by injecting growth inhibitors, which are tailored
based on the identified features related to inappropriate concepts during the
diffusion process. Additionally, due to the varying degrees and scopes of
inappropriate concepts, we train an adapter to infer the corresponding
suppression scale during the injection process. Our method effectively captures
the manifestation of subtle words at the image level, enabling direct and
efficient erasure of target concepts without the need for fine-tuning. Through
extensive experimentation, we demonstrate that our approach achieves superior
erasure results with little effect on other concepts while preserving image
quality and semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14317v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14317v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Xuanyu Yi, Haohan Weng, Qingshan Xu, Xiaokang Wei, Xianghui Yang, Chunchao Guo, Long Chen, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Triangle meshes are fundamental to 3D applications, enabling efficient
modification and rasterization while maintaining compatibility with standard
rendering pipelines. However, current automatic mesh generation methods
typically rely on intermediate representations that lack the continuous surface
quality inherent to meshes. Converting these representations into meshes
produces dense, suboptimal outputs. Although recent autoregressive approaches
demonstrate promise in directly modeling mesh vertices and faces, they are
constrained by the limitation in face count, scalability, and structural
fidelity. To address these challenges, we propose Nautilus, a locality-aware
autoencoder for artist-like mesh generation that leverages the local properties
of manifold meshes to achieve structural fidelity and efficient representation.
Our approach introduces a novel tokenization algorithm that preserves face
proximity relationships and compresses sequence length through locally shared
vertices and edges, enabling the generation of meshes with an unprecedented
scale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point
Conditioner that provides multi-scale geometric guidance, ensuring global
consistency and local structural fidelity by capturing fine-grained geometric
features. Extensive experiments demonstrate that Nautilus significantly
outperforms state-of-the-art methods in both fidelity and scalability. The
project page is at https://nautilusmeshgen.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingpeng Xie, Shiyu Tan, Yuanlei Wang, Tianle Du, Yifei Xue, Yizhen Lao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Neural Radiance Fields (NeRF) reconstructions are typically
hindered by the requirement for extensive image datasets and substantial
computational resources. This paper introduces IOVS4NeRF, a framework that
employs an uncertainty-guided incremental optimal view selection strategy
adaptable to various NeRF implementations. Specifically, by leveraging a hybrid
uncertainty model that combines rendering and positional uncertainties, the
proposed method calculates the most informative view from among the candidates,
thereby enabling incremental optimization of scene reconstruction. Our detailed
experiments demonstrate that IOVS4NeRF achieves high-fidelity NeRF
reconstruction with minimal computational resources, making it suitable for
large-scale scene applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variable Radiance Field for Real-World Category-Specific Reconstruction
  from Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Wang, Zhiqiang Yan, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing category-specific objects using Neural Radiance Field (NeRF)
from a single image is a promising yet challenging task. Existing approaches
predominantly rely on projection-based feature retrieval to associate 3D points
in the radiance field with local image features from the reference image.
However, this process is computationally expensive, dependent on known camera
intrinsics, and susceptible to occlusions. To address these limitations, we
propose Variable Radiance Field (VRF), a novel framework capable of efficiently
reconstructing category-specific objects without requiring known camera
intrinsics and demonstrating robustness against occlusions. First, we replace
the local feature retrieval with global latent representations, generated
through a single feed-forward pass, which improves efficiency and eliminates
reliance on camera intrinsics. Second, to tackle coordinate inconsistencies
inherent in real-world dataset, we define a canonical space by introducing a
learnable, category-specific shape template and explicitly aligning each
training object to this template using a learnable 3D transformation. This
approach also reduces the complexity of geometry prediction to modeling
deformations from the template to individual instances. Finally, we employ a
hyper-network-based method for efficient NeRF creation and enhance the
reconstruction performance through a contrastive learning-based pretraining
strategy. Evaluations on the CO3D dataset demonstrate that VRF achieves
state-of-the-art performance in both reconstruction quality and computational
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAR: Scale-wise Text-conditioned AutoRegressive image generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Biye Li, Huaian Chen, Yi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce STAR, a text-to-image model that employs a scale-wise
auto-regressive paradigm. Unlike VAR, which is constrained to class-conditioned
synthesis for images up to 256$\times$256, STAR enables text-driven image
generation up to 1024$\times$1024 through three key designs. First, we
introduce a pre-trained text encoder to extract and adopt representations for
textual constraints, enhancing details and generalizability. Second, given the
inherent structural correlation across different scales, we leverage 2D Rotary
Positional Encoding (RoPE) and tweak it into a normalized version, ensuring
consistent interpretation of relative positions across token maps and
stabilizing the training process. Third, we observe that simultaneously
sampling all tokens within a single scale can disrupt inter-token
relationships, leading to structural instability, particularly in
high-resolution generation. To address this, we propose a novel stable sampling
method that incorporates causal relationships into the sampling process,
ensuring both rich details and stable structures. Compared to previous
diffusion models and auto-regressive models, STAR surpasses existing benchmarks
in fidelity, text-image consistency, and aesthetic quality, requiring just
2.21s for 1024$\times$1024 images on A100. This highlights the potential of
auto-regressive methods in high-quality image synthesis, offering new
directions for the text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compress image to patches for Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Zhao, Yaoru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Vision Transformer (ViT) has made significant strides in the field of
computer vision. However, as the depth of the model and the resolution of the
input images increase, the computational cost associated with training and
running ViT models has surged dramatically. This paper proposes a hybrid model
based on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a
module called CI2P, which utilizes the CompressAI encoder to compress images
and subsequently generates a sequence of patches through a series of
convolutions. CI2P can replace the Patch Embedding component in the ViT model,
enabling seamless integration into existing ViT models. Compared to ViT-B/16,
CI2P-ViT has the number of patches input to the self-attention layer reduced to
a quarter of the original. This design not only significantly reduces the
computational cost of the ViT model but also effectively enhances the model's
accuracy by introducing the inductive bias properties of CNN. The ViT model's
precision is markedly enhanced. When trained from the ground up on the
Animals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing
a 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's
computational operations, measured in floating-point operations per second
(FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in
training velocity on identical hardware configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient-vDiT: Efficient Video Diffusion <span class="highlight-title">Transformer</span>s With Attention
  Tile 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promise of synthesizing high-fidelity videos, Diffusion
Transformers (DiTs) with 3D full attention suffer from expensive inference due
to the complexity of attention computation and numerous sampling steps. For
example, the popular Open-Sora-Plan model consumes more than 9 minutes for
generating a single video of 29 frames. This paper addresses the inefficiency
issue from two aspects: 1) Prune the 3D full attention based on the redundancy
within video data; We identify a prevalent tile-style repetitive pattern in the
3D attention maps for video data, and advocate a new family of sparse 3D
attention that holds a linear complexity w.r.t. the number of video frames. 2)
Shorten the sampling process by adopting existing multi-step consistency
distillation; We split the entire sampling trajectory into several segments and
perform consistency distillation within each one to activate few-step
generation capacities. We further devise a three-stage training pipeline to
conjoin the low-complexity attention and few-step generation capacities.
Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into
an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video
generation with a marginal performance trade-off in VBench. In addition, we
demonstrate that our approach is amenable to distributed inference, achieving
an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Vision-language Models for <span class="highlight-title">Self-supervised</span> Remote
  Physiological Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Yue, Miaojing Shi, Hanli Wang, Shuai Ding, Qijun Chen, Shanlin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial video-based remote physiological measurement is a promising research
area for detecting human vital signs (e.g., heart rate, respiration frequency)
in a non-contact way. Conventional approaches are mostly supervised learning,
requiring extensive collections of facial videos and synchronously recorded
photoplethysmography (PPG) signals. To tackle it, self-supervised learning has
recently gained attentions; due to the lack of ground truth PPG signals, its
performance is however limited. In this paper, we propose a novel
self-supervised framework that successfully integrates the popular
vision-language models (VLMs) into the remote physiological measurement task.
Given a facial video, we first augment its positive and negative video samples
with varying rPPG signal frequencies. Next, we introduce a frequency-oriented
vision-text pair generation method by carefully creating contrastive
spatio-temporal maps from positive and negative samples and designing proper
text prompts to describe their relative ratios of signal frequencies. A
pre-trained VLM is employed to extract features for these formed vision-text
pairs and estimate rPPG signals thereafter. We develop a series of generative
and contrastive learning mechanisms to optimize the VLM, including the
text-guided visual map reconstruction task, the vision-text contrastive
learning task, and the frequency contrastive and ranking task. Overall, our
method for the first time adapts VLMs to digest and align the frequency-related
knowledge in vision and text modalities. Extensive experiments on four
benchmark datasets demonstrate that it significantly outperforms state of the
art self-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Multi-modal Large Language Model to Concept Drift From
  <span class="highlight-title">Pre-train</span>ing Onwards <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Yang, Jie Lu, En Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) frequently face challenges from
concept drift when dealing with real-world streaming data, wherein
distributions change unpredictably. This mainly includes gradual drift due to
long-tailed data and sudden drift from Out-Of-Distribution (OOD) data, both of
which have increasingly drawn the attention of the research community. While
these issues have been extensively studied in the individual domain of vision
or language, their impacts on MLLMs in concept drift settings remain largely
underexplored. In this paper, we reveal the susceptibility and vulnerability of
Vision-Language (VL) models to significant biases arising from gradual drift
and sudden drift, particularly in the pre-training. To effectively address
these challenges, we propose a unified framework that extends concept drift
theory to the multi-modal domain, enhancing the adaptability of the VL model to
unpredictable distribution changes. Additionally, a T-distribution based drift
adapter is proposed to effectively mitigate the bias induced by the gradual
drift, which also facilitates the model in distinguishing sudden distribution
changes through explicit distribution modeling. Extensive experiments
demonstrate our method enhances the efficiency and accuracy of image-text
alignment in the pre-training of VL models, particularly in the concept drift
scenario. Moreover, various downstream tasks exhibit significant improvements
in our model's ability to adapt to the long-tailed open world. Furthermore, we
create a set of multi-modal datasets called OpenMMlo, specifically tailored for
the long-tailed open-world setting, to validate our findings. To foster the
development of the multi-modal community, we have made both OpenMMlo datasets
and our code publicly available at:
https://github.com/XiaoyuYoung/ConceptDriftMLLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object
  Detection and Motion Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahran Rahman Alve
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project aims to develop a robust video surveillance system, which can
segment videos into smaller clips based on the detection of activities. It uses
CCTV footage, for example, to record only major events-like the appearance of a
person or a thief-so that storage is optimized and digital searches are easier.
It utilizes the latest techniques in object detection and tracking, including
Convolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well
as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks
(LSTMs), to achieve high accuracy in detection and capture temporal
dependencies. The approach incorporates adaptive background modeling through
Gaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to
detect motions. Multi-scale and contextual analysis are used to improve
detection across different object sizes and environments. A hybrid motion
segmentation strategy combines statistical and deep learning models to manage
complex movements, while optimizations for real-time processing ensure
efficient computation. Tracking methods, such as Kalman Filters and Siamese
networks, are employed to maintain smooth tracking even in cases of occlusion.
Detection is improved on various-sized objects for multiple scenarios by
multi-scale and contextual analysis. Results demonstrate high precision and
recall in detecting and tracking objects, with significant improvements in
processing times and accuracy due to real-time optimizations and
illumination-invariant features. The impact of this research lies in its
potential to transform video surveillance, reducing storage requirements and
enhancing security through reliable and efficient object detection and
tracking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidSketch: Hand-drawn Sketch-Driven Video Generation with Diffusion
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Jiang, Shuang Chen, Boxi Wu, Xiaotong Guan, Jiahui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of generative artificial intelligence, previous studies
have achieved the task of generating aesthetic images from hand-drawn sketches,
fulfilling the public's needs for drawing. However, these methods are limited
to static images and lack the ability to control video animation generation
using hand-drawn sketches. To address this gap, we propose VidSketch, the first
method capable of generating high-quality video animations directly from any
number of hand-drawn sketches and simple text prompts, bridging the divide
between ordinary users and professional artists. Specifically, our method
introduces a Level-Based Sketch Control Strategy to automatically adjust the
guidance strength of sketches during the generation process, accommodating
users with varying drawing skills. Furthermore, a TempSpatial Attention
mechanism is designed to enhance the spatiotemporal consistency of generated
video animations, significantly improving the coherence across frames. You can
find more detailed cases on our official website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text4Seg: Reimagining Image Segmentation as Text Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu, Yiping Ke, Xinjiang Wang, Litong Feng, Wayne Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have shown exceptional capabilities
in vision-language tasks; however, effectively integrating image segmentation
into these models remains a significant challenge. In this paper, we introduce
Text4Seg, a novel text-as-mask paradigm that casts image segmentation as a text
generation problem, eliminating the need for additional decoders and
significantly simplifying the segmentation process. Our key innovation is
semantic descriptors, a new textual representation of segmentation masks where
each image patch is mapped to its corresponding text label. This unified
representation allows seamless integration into the auto-regressive training
pipeline of MLLMs for easier optimization. We demonstrate that representing an
image with $16\times16$ semantic descriptors yields competitive segmentation
performance. To enhance efficiency, we introduce the Row-wise Run-Length
Encoding (R-RLE), which compresses redundant text sequences, reducing the
length of semantic descriptors by 74% and accelerating inference by $3\times$,
without compromising performance. Extensive experiments across various vision
tasks, such as referring expression segmentation and comprehension, show that
Text4Seg achieves state-of-the-art performance on multiple datasets by
fine-tuning different MLLM backbones. Our approach provides an efficient,
scalable solution for vision-centric tasks within the MLLM framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://mc-lan.github.io/Text4Seg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Image-to-Video Diffusion Models for Large-Motion Frame
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17042v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17042v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoxu Jin, Hiroshi Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of video generation models has advanced significantly in
recent years, we adopt large-scale image-to-video diffusion models for video
frame interpolation. We present a conditional encoder designed to adapt an
image-to-video model for large-motion frame interpolation. To enhance
performance, we integrate a dual-branch feature extractor and propose a
cross-frame attention mechanism that effectively captures both spatial and
temporal information, enabling accurate interpolations of intermediate frames.
Our approach demonstrates superior performance on the Fr\'echet Video Distance
(FVD) metric when evaluated against other state-of-the-art approaches,
particularly in handling large motion scenarios, highlighting advancements in
generative-based methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Reconstruction of Shoes for Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik Shrestha, Sujan Kapali, Swikar Gautam, Vishal Pokharel, Santosh Giri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a mobile-based solution that enhances online shoe
shopping through 3D modeling and Augmented Reality (AR), leveraging the
efficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D
images, the framework generates realistic 3D shoe models from 2D images,
achieving an average Peak Signal-to-Noise Ratio (PSNR) of 32, and enables
immersive AR interactions via smartphones. A custom shoe segmentation dataset
of 3120 images was created, with the best-performing segmentation model
achieving an Intersection over Union (IoU) score of 0.95. This paper
demonstrates the potential of 3D modeling and AR to revolutionize online
shopping by offering realistic virtual interactions, with applicability across
broader fashion categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via
  LLM Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02795v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02795v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Yan, Yiming Zhang, Baoyi He, Yuhao Fu, Qi Zhou, Zhijie Sang, Chunlin Ji, Shengyu Zhang, Fei Wu, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce InfiFusion, an efficient training pipeline designed to integrate
multiple domain-specialized Large Language Models (LLMs) into a single pivot
model, effectively harnessing the strengths of each source model. Traditional
fusion methods either merge model parameters directly or rely on knowledge
distillation with rigid assumptions, limiting their flexibility and efficiency.
InfiFusion overcomes these limitations by enhancing Universal Logit
Distillation (ULD) with Top-K selection and Logits Standardization. We propose
two fusion strategies: Pairwise Fusion (InfiFusion$_p$), where each source
model knowledge is distilled individually into the pivot model followed by
merging and Unified Fusion (InfiFusion$_u$), where knowledge from all source
models is distilled simultaneously into the pivot model. InfiFusion outperforms
the state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11
widely applied benchmarks covering reasoning, coding, mathematics, and
instruction-following tasks. Notably, InfiFusion achieves this superior
performance while significantly reduces computational costs, completing full
training with only 160 H800 GPU hours compared to the millions typically
required for traditional LLM training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Significant performance improvements over the previous version; under
  review;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REP: Resource-Efficient <span class="highlight-title">Prompt</span>ing for Rehearsal-Free Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungho Jeon, Xinyue Ma, Kwang In Kim, Myeongjae Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent rehearsal-free methods, guided by prompts, excel in vision-related
continual learning (CL) with drifting data but lack resource efficiency, making
real-world deployment challenging. In this paper, we introduce
Resource-Efficient Prompting (REP), which improves the computational and memory
efficiency of prompt-based rehearsal-free methods while minimizing accuracy
trade-offs. Our approach employs swift prompt selection to refine input data
using a carefully provisioned model and introduces adaptive token merging
(AToM) and layer dropping (ALD) for efficient prompt updates. AToM and ALD
selectively skip data and model layers while preserving task-specific features
during new-task learning. Extensive experiments on multiple image
classification datasets demonstrates REP's superior resource efficiency over
state-of-the-art ViT- and CNN-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlexCAD: Unified and Versatile Controllable CAD Generation with
  Fine-tuned Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanwei Zhang, Shizhao Sun, Wenxiao Wang, Deng Cai, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there is a growing interest in creating computer-aided design (CAD)
models based on user intent, known as controllable CAD generation. Existing
work offers limited controllability and needs separate models for different
types of control, reducing efficiency and practicality. To achieve controllable
generation across all CAD construction hierarchies, such as sketch-extrusion,
extrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by
fine-tuning large language models (LLMs). First, to enhance comprehension by
LLMs, we represent a CAD model as a structured text by abstracting each
hierarchy as a sequence of text tokens. Second, to address various controllable
generation tasks in a unified model, we introduce a hierarchy-aware masking
strategy. Specifically, during training, we mask a hierarchy-aware field in the
CAD text with a mask token. This field, composed of a sequence of tokens, can
be set flexibly to represent various hierarchies. Subsequently, we ask LLMs to
predict this masked field. During inference, the user intent is converted into
a CAD text with a mask token replacing the part the user wants to modify, which
is then fed into FlexCAD to generate new CAD models. Comprehensive experiments
on public dataset demonstrate the effectiveness of FlexCAD in both generation
quality and controllability. Code will be available at
https://github.com/microsoft/FlexCAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounded Knowledge-Enhanced Medical Vision-Language <span class="highlight-title">Pre-train</span>ing for
  Chest X-Ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Deng, Zhongzhen Huang, Yunqi Wang, Zhichuan Wang, Zhao Wang, Xiaofan Zhang, Qi Dou, Yeung Yu Hui, Edward S. Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical foundation models have the potential to revolutionize healthcare by
providing robust and generalized representations of medical data. Medical
vision-language pre-training has emerged as a promising approach for learning
domain-general representations of medical image and text. Current algorithms
that exploit global and local alignment between medical image and text could
however be marred by redundant information in medical data. To address this
issue, we propose a grounded knowledge-enhanced medical vision-language
pre-training (GK-MVLP) framework for chest X-ray. In this framework, medical
knowledge was grounded to the appropriate anatomical regions by using a
transformer-based grounded knowledge-enhanced module for fine-grained alignment
between textural features of medical knowledge and the corresponding anatomical
region-level visual features. The performance of GK-MVLP was competitive with
or exceeded the state of the art on downstream image understanding tasks (chest
X-ray disease classification, disease localization), generative task (report
generation), and vision-language understanding task (medical visual
question-answering). Our results demonstrate the advantage of incorporating
grounding mechanism to remove biases and improve the alignment between chest
X-ray image and radiology report.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhishma Dedhia, Niraj K. Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several accounts of human cognition posit that our intelligence is rooted in
our ability to form abstract composable concepts, ground them in our
environment, and reason over these grounded entities. This trifecta of human
thought has remained elusive in modern intelligent machines. In this work, we
investigate whether slot representations extracted from visual scenes serve as
appropriate compositional abstractions for grounding and reasoning. We present
the Neural Slot Interpreter (NSI), which learns to ground object semantics in
slots. At the core of NSI is an XML-like schema that uses simple syntax rules
to organize the object semantics of a scene into object-centric schema
primitives. Then, the NSI metric learns to ground primitives into slots through
a structured contrastive learning objective that reasons over the intermodal
alignment. Experiments with a bi-modal object-property and scene retrieval task
demonstrate the grounding efficacy and interpretability of correspondences
learned by NSI. From a scene representation standpoint, we find that emergent
NSI slots that move beyond the image grid by binding to spatial objects
facilitate improved visual grounding compared to conventional
bounding-box-based approaches. From a data efficiency standpoint, we
empirically validate that NSI learns more generalizable representations from a
fixed amount of annotation data than the traditional approach. We also show
that the grounded slots surpass unsupervised slots in real-world object
discovery and scale with scene complexity. Finally, we investigate the
reasoning abilities of the grounded slots. Vision Transformers trained on
grounding-aware NSI tokenizers using as few as ten tokens outperform
patch-based tokens on challenging few-shot classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magic 1-For-1: Generating One Minute Video Clips within One Minute 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, Daquan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we present Magic 1-For-1 (Magic141), an efficient
video generation model with optimized memory consumption and inference latency.
The key idea is simple: factorize the text-to-video generation task into two
separate easier tasks for diffusion step distillation, namely text-to-image
generation and image-to-video generation. We verify that with the same
optimization algorithm, the image-to-video task is indeed easier to converge
over the text-to-video task. We also explore a bag of optimization tricks to
reduce the computational cost of training the image-to-video (I2V) models from
three aspects: 1) model convergence speedup by using a multi-modal prior
condition injection; 2) inference latency speed up by applying an adversarial
step distillation, and 3) inference memory cost optimization with parameter
sparsification. With those techniques, we are able to generate 5-second video
clips within 3 seconds. By applying a test time sliding window, we are able to
generate a minute-long video within one minute with significantly improved
visual quality and motion dynamics, spending less than 1 second for generating
1 second video clips on average. We conduct a series of preliminary
explorations to find out the optimal tradeoff between computational cost and
video quality during diffusion step distillation and hope this could be a good
foundation model for open-source explorations. The code and the model weights
are available at https://github.com/DA-Group-PKU/Magic-1-For-1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Serious updates are needed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFC-Bench: Benchmarking Multimodal Fact-Checking with Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11288v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11288v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have significantly improved multimodal
reasoning tasks, such as visual question answering and image captioning. These
models embed multimodal facts within their parameters, rather than relying on
external knowledge bases to store factual information explicitly. However, the
content discerned by LVLMs may deviate from factuality due to inherent bias or
incorrect inference. To address this issue, we introduce MFC-Bench, a rigorous
and comprehensive benchmark designed to evaluate the factual accuracy of LVLMs
across three stages of verdict prediction for MFC: Manipulation,
Out-of-Context, and Veracity Classification. Through our evaluation on
MFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering
that current models still fall short in multimodal fact-checking and
demonstrate insensitivity to various forms of manipulated content. We hope that
MFC-Bench could raise attention to the trustworthy AI potentially assisted by
LVLMs in the future. The MFC-Bench and accompanying resources are publicly
accessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing
research in the multimodal fact-checking field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing Multimodal Queries Representation via Fusion-Free
  Modality Interaction for Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet
  Transform Feature Modulation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Shinichiro Omachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared image super-resolution demands long-range dependency modeling and
multi-scale feature extraction to address challenges such as homogeneous
backgrounds, weak edges, and sparse textures. While Mamba-based state-space
models (SSMs) excel in global dependency modeling with linear complexity, their
block-wise processing disrupts spatial consistency, limiting their
effectiveness for IR image reconstruction. We propose IRSRMamba, a novel
framework integrating wavelet transform feature modulation for multi-scale
adaptation and an SSMs-based semantic consistency loss to restore fragmented
contextual information. This design enhances global-local feature fusion,
structural coherence, and fine-detail preservation while mitigating
block-induced artifacts. Experiments on benchmark datasets demonstrate that
IRSRMamba outperforms state-of-the-art methods in PSNR, SSIM, and perceptual
quality. This work establishes Mamba-based architectures as a promising
direction for high-fidelity IR image enhancement. Code are available at
https://github.com/yongsongH/IRSRMamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Text-<span class="highlight-title">Prompt</span>able Surgical Instrument Segmentation with Robust
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae-Min Choi, Juyoun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) is essential in computer-assisted
surgeries, with deep learning methods improving accuracy in complex
environments. Recently, text-promptable segmentation methods have been
introduced, generating masks based on textual descriptions. However, they
assume the text-described object is present and always generate an associated
mask even when the object is absent. Existing methods address this by using
prompts only for objects already known to exist in the scene, which relies on
inaccessible information. To address this, we rethink text-promptable SIS and
redefine it under robust conditions as Robust text-promptable SIS (R-SIS).
Unlike previous approaches, R-SIS is a process that analyzes text prompts for
all surgical instrument categories without relying on external knowledge,
identifies the instruments present in the scene, and segments them accordingly.
Building on this, we propose Robust Surgical Instrument Segmentation (RoSIS),
an optimized framework combining visual and language features for promptable
segmentation in the R-SIS setting. RoSIS employs an encoder-decoder
architecture with a Multi-Modal Fusion Block (MMFB) and a Selective Gate Block
(SGB) for balanced integration of vision and language features. Additionally,
an iterative refinement strategy enhances segmentation masks through a two-step
process: an initial pass with name-based prompts, followed by refinement with
location prompts. Experiments across multiple datasets and settings show that
RoSIS outperforms existing vision-based and promptable segmentation methods
under robust conditions. By rethinking text-promptable SIS, our work
establishes a fair and effective approach to surgical instrument segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 7 tables, submitted to IEEE Journal of
  Biomedical and Health Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Guide Dog: Egocentric Path Prediction on Smartphone <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Jadhav, Jeffery Cao, Abhishree Shetty, Urvashi Priyam Kumar, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Anirudh Koul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents AI Guide Dog (AIGD), a lightweight egocentric
(first-person) navigation system for visually impaired users, designed for
real-time deployment on smartphones. AIGD employs a vision-only multi-label
classification approach to predict directional commands, ensuring safe
navigation across diverse environments. We introduce a novel technique for
goal-based outdoor navigation by integrating GPS signals and high-level
directions, while also handling uncertain multi-path predictions for
destination-free indoor navigation. As the first navigation assistance system
to handle both goal-oriented and exploratory navigation across indoor and
outdoor settings, AIGD establishes a new benchmark in blind navigation. We
present methods, datasets, evaluations, and deployment insights to encourage
further innovations in assistive navigation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the AAAI 2025 Spring Symposium on Human-Compatible AI for
  Well-being: Harnessing Potential of GenAI for AI-Powered Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hiding and Recovering Knowledge in Text-to-Image Diffusion Models via
  Learnable <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12326v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12326v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Bui, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh Phung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable capability in generating
high-quality visual content from textual descriptions. However, since these
models are trained on large-scale internet data, they inevitably learn
undesirable concepts, such as sensitive content, copyrighted material, and
harmful or unethical elements. While previous works focus on permanently
removing such concepts, this approach is often impractical, as it can degrade
model performance and lead to irreversible loss of information. In this work,
we introduce a novel concept-hiding approach that makes unwanted concepts
inaccessible to public users while allowing controlled recovery when needed.
Instead of erasing knowledge from the model entirely, we incorporate a
learnable prompt into the cross-attention module, acting as a secure memory
that suppresses the generation of hidden concepts unless a secret key is
provided. This enables flexible access control -- ensuring that undesirable
content cannot be easily generated while preserving the option to reinstate it
under restricted conditions. Our method introduces a new paradigm where concept
suppression and controlled recovery coexist, which was not feasible in prior
works. We validate its effectiveness on the Stable Diffusion model,
demonstrating that hiding concepts mitigate the risks of permanent removal
while maintaining the model's overall capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Makes a Maze Look Like a Maze? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A unique aspect of human visual understanding is the ability to flexibly
interpret abstract concepts: acquiring lifted rules explaining what they
symbolize, grounding them across familiar and unfamiliar contexts, and making
predictions or reasoning about them. While off-the-shelf vision-language models
excel at making literal interpretations of images (e.g., recognizing object
categories such as tree branches), they still struggle to make sense of such
visual abstractions (e.g., how an arrangement of tree branches may form the
walls of a maze). To address this challenge, we introduce Deep Schema Grounding
(DSG), a framework that leverages explicit structured representations of visual
abstractions for grounding and reasoning. At the core of DSG are
schemas--dependency graph descriptions of abstract concepts that decompose them
into more primitive-level symbols. DSG uses large language models to extract
schemas, then hierarchically grounds concrete to abstract components of the
schema onto images with vision-language models. The grounded schema is used to
augment visual abstraction understanding. We systematically evaluate DSG and
different methods in reasoning on our new Visual Abstractions Dataset, which
consists of diverse, real-world images of abstract concepts and corresponding
question-answer pairs labeled by humans. We show that DSG significantly
improves the abstract visual reasoning performance of vision-language models,
and is a step toward human-aligned understanding of visual abstractions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Forrest Iandola, Stanislav Pidhorskyi, Igor Santesteban, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon, Shunsuke Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian-based human avatars have achieved an unprecedented level of visual
fidelity. However, existing approaches based on high-capacity neural networks
typically require a desktop GPU to achieve real-time performance for a single
avatar, and it remains non-trivial to animate and render such avatars on mobile
devices including a standalone VR headset due to substantially limited memory
and computational bandwidth. In this paper, we present SqueezeMe, a simple and
highly effective framework to convert high-fidelity 3D Gaussian full-body
avatars into a lightweight representation that supports both animation and
rendering with mobile-grade compute. Our key observation is that the decoding
of pose-dependent Gaussian attributes from a neural network creates
non-negligible memory and computational overhead. Inspired by blendshapes and
linear pose correctives widely used in Computer Graphics, we address this by
distilling the pose correctives learned with neural networks into linear
layers. Moreover, we further reduce the parameters by sharing the correctives
among nearby Gaussians. Combining them with a custom splatting pipeline based
on Vulkan, we achieve, for the first time, simultaneous animation and rendering
of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18295v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18295v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitai Kang, Mengxue Qu, Jyoti Kini, Yunchao Wei, Mubarak Shah, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-life scenarios, humans seek out objects in the 3D world to fulfill
their daily needs or intentions. This inspires us to introduce 3D intention
grounding, a new task in 3D object detection employing RGB-D, based on human
intention, such as "I want something to support my back". Closely related, 3D
visual grounding focuses on understanding human reference. To achieve detection
based on human intention, it relies on humans to observe the scene, reason out
the target that aligns with their intention ("pillow" in this case), and
finally provide a reference to the AI system, such as "A pillow on the couch".
Instead, 3D intention grounding challenges AI agents to automatically observe,
reason and detect the desired target solely based on human intention. To tackle
this challenge, we introduce the new Intent3D dataset, consisting of 44,990
intention texts associated with 209 fine-grained classes from 1,042 scenes of
the ScanNet dataset. We also establish several baselines based on different
language-based 3D object detection models on our benchmark. Finally, we propose
IntentNet, our unique approach, designed to tackle this intention-based
detection problem. It focuses on three key aspects: intention understanding,
reasoning to identify object candidates, and cascaded adaptive learning that
leverages the intrinsic priority logic of different losses for multiple
objective optimization. Project Page:
https://weitaikang.github.io/Intent3D-webpage/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Vision Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Bac Nguyen, Hugh Churchill, Khoa Luu, Samee U. Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised visual clustering has garnered significant attention in recent
times, aiming to characterize distributions of unlabeled visual images through
clustering based on a parameterized appearance approach. Alternatively,
clustering algorithms can be viewed as assignment problems, often characterized
as NP-hard, yet precisely solvable for small instances on contemporary
hardware. Adiabatic quantum computing (AQC) emerges as a promising solution,
poised to deliver substantial speedups for a range of NP-hard optimization
problems. However, existing clustering formulations face challenges in quantum
computing adoption due to scalability issues. In this study, we present the
first clustering formulation tailored for resolution using Adiabatic quantum
computing. An Ising model is introduced to represent the quantum mechanical
system implemented on AQC. The proposed approach demonstrates high
competitiveness compared to state-of-the-art optimization-based methods, even
when utilizing off-the-shelf integer programming solvers. Lastly, this work
showcases the solvability of the proposed clustering problem on
current-generation real quantum computers for small examples and analyzes the
properties of the obtained solutions
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2202.08837 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COBRA: A Continual Learning Approach to Vision-Brain Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Bac Nguyen, Arabinda Kumar Choudhary, Pawan Sinha, Xin Li, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Brain Understanding (VBU) aims to extract visual information perceived
by humans from brain activity recorded through functional Magnetic Resonance
Imaging (fMRI). Despite notable advancements in recent years, existing studies
in VBU continue to face the challenge of catastrophic forgetting, where models
lose knowledge from prior subjects as they adapt to new ones. Addressing
continual learning in this field is, therefore, essential. This paper
introduces a novel framework called Continual Learning for Vision-Brain (COBRA)
to address continual learning in VBU. Our approach includes three novel
modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific
(PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer
module. The SC module captures shared vision-brain patterns across subjects,
preserving this knowledge as the model encounters new subjects, thereby
reducing the impact of catastrophic forgetting. On the other hand, the PSS
module learns unique vision-brain patterns specific to each subject. Finally,
the MRIFormer module contains a transformer encoder and decoder that learns the
fMRI features for VBU from common and specific patterns. In a continual
learning setup, COBRA is trained in new PSS and MRIFormer modules for new
subjects, leaving the modules of previous subjects unaffected. As a result,
COBRA effectively addresses catastrophic forgetting and achieves
state-of-the-art performance in both continual learning and vision-brain
reconstruction tasks, surpassing previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Narnaware, Ashmal Vayani, Rohit Gupta, Sirnam Swetha, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful
societal prejudices, undermining the fairness and equity of AI applications. As
LMMs grow increasingly influential, addressing and mitigating inherent biases
related to stereotypes, harmful generations, and ambiguous assumptions in
real-world scenarios has become essential. However, existing datasets
evaluating stereotype biases in LMMs often lack diversity and rely on synthetic
images, leaving a gap in bias evaluation for real-world visual contexts. To
address this, we introduce the Stereotype Bias Benchmark (SB-bench), the most
comprehensive framework to date for assessing stereotype biases across nine
diverse categories with non-synthetic images. SB-bench rigorously evaluates
LMMs through carefully curated, visually grounded scenarios, challenging them
to reason accurately about visual stereotypes. It offers a robust evaluation
framework featuring real-world visual samples, image variations, and
multiple-choice question formats. By introducing visually grounded queries that
isolate visual biases from textual ones, SB-bench enables a precise and nuanced
assessment of a model's reasoning capabilities across varying levels of
difficulty. Through rigorous testing of state-of-the-art open-source and
closed-source LMMs, SB-bench provides a systematic approach to assessing
stereotype biases in LMMs across key social dimensions. This benchmark
represents a significant step toward fostering fairness in AI systems and
reducing harmful biases, laying the groundwork for more equitable and socially
responsible LMMs. Our code and dataset are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-preserving contrastive learning for spatial time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiru Jiao, Sander van Cranenburgh, Simeon Calvert, Hans van Lint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Informative representations enhance model performance and generalisability in
downstream tasks. However, learning self-supervised representations for
spatially characterised time series, like traffic interactions, poses
challenges as it requires maintaining fine-grained similarity relations in the
latent space. In this study, we incorporate two structure-preserving
regularisers for the contrastive learning of spatial time series: one
regulariser preserves the topology of similarities between instances, and the
other preserves the graph geometry of similarities across spatial and temporal
dimensions. To balance contrastive learning and structure preservation, we
propose a dynamic mechanism that adaptively weighs the trade-off and stabilises
training. We conduct experiments on multivariate time series classification, as
well as macroscopic and microscopic traffic prediction. For all three tasks,
our approach preserves the structures of similarity relations more effectively
and improves state-of-the-art task performances. The proposed approach can be
applied to an arbitrary encoder and is particularly beneficial for time series
with spatial or geographical features. Furthermore, this study suggests that
higher similarity structure preservation indicates more informative and useful
representations. This may help to understand the contribution of representation
learning in pattern recognition with neural networks. Our code is made openly
accessible with all resulting data at https://github.com/yiru-jiao/spclt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TL;DR: Preserving certain structures of similarity relations in
  spatio-temporal data can improve downstream task performance via contrastive
  learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypercone Assisted Contour Generation for Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annita Vapsi, Andrés Muñoz, Nancy Thomas, Keshav Ramani, Daniel Borrajo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the field of out-of-distribution (OOD) detection have
placed great emphasis on learning better representations suited to this task.
While there are distance-based approaches, distributional awareness has seldom
been exploited for better performance. We present HAC$_k$-OOD, a novel OOD
detection method that makes no distributional assumption about the data, but
automatically adapts to its distribution. Specifically, HAC$_k$-OOD constructs
a set of hypercones by maximizing the angular distance to neighbors in a given
data-point's vicinity to approximate the contour within which in-distribution
(ID) data-points lie. Experimental results show state-of-the-art FPR@95 and
AUROC performance on Near-OOD detection and on Far-OOD detection on the
challenging CIFAR-100 benchmark without explicitly training for OOD
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verification of Neural Networks against Convolutional Perturbations via
  Parameterised Kernels <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Brückner, Alessio Lomuscio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a method for the efficient verification of neural networks against
convolutional perturbations such as blurring or sharpening. To define input
perturbations we use well-known camera shake, box blur and sharpen kernels. We
demonstrate that these kernels can be linearly parameterised in a way that
allows for a variation of the perturbation strength while preserving desired
kernel properties. To facilitate their use in neural network verification, we
develop an efficient way of convolving a given input with these parameterised
kernels. The result of this convolution can be used to encode the perturbation
in a verification setting by prepending a linear layer to a given network. This
leads to tight bounds and a high effectiveness in the resulting verification
step. We add further precision by employing input splitting as a branch and
bound strategy. We demonstrate that we are able to verify robustness on a
number of standard benchmarks where the baseline is unable to provide any
safety certificates. To the best of our knowledge, this is the first solution
for verifying robustness against specific convolutional perturbations such as
camera shake.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with
  Multi-Modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Stephen F. Smith, Yu-Chiang Frank Wang, Min-Hung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current autonomous driving vehicles rely mainly on their individual sensors
to understand surrounding scenes and plan for future trajectories, which can be
unreliable when the sensors are malfunctioning or occluded. To address this
problem, cooperative perception methods via vehicle-to-vehicle (V2V)
communication have been proposed, but they have tended to focus on detection
and tracking. How those approaches contribute to overall cooperative planning
performance is still under-explored. Inspired by recent progress using Large
Language Models (LLMs) to build autonomous driving systems, we propose a novel
problem setting that integrates an LLM into cooperative autonomous driving,
with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and
benchmark. We also propose our baseline method Vehicle-to-Vehicle Large
Language Model (V2V-LLM), which uses an LLM to fuse perception information from
multiple connected autonomous vehicles (CAVs) and answer driving-related
questions: grounding, notable object identification, and planning. Experimental
results show that our proposed V2V-LLM can be a promising unified model
architecture for performing various tasks in cooperative autonomous driving,
and outperforms other baseline methods that use different fusion approaches.
Our work also creates a new research direction that can improve the safety of
future autonomous driving systems. Our project website:
https://eddyhkchiu.github.io/v2vllm.github.io/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphCompNet: A Position-Aware Model for Predicting and Compensating
  Shape Deviations in 3D Printing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Lei,  Chen, Juheon Lee, Juan Carlos Catana, Tsegai Yhdego, Nathan Moroney, Mohammad Amin Nabian, Hui Wang, Jun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a data-driven algorithm for modeling and compensating
shape deviations in additive manufacturing (AM), addressing challenges in
geometric accuracy and batch production. While traditional methods, such as
analytical models and metrology, laid the groundwork for geometric precision,
they are often impractical for large-scale production. Recent advancements in
machine learning (ML) have improved compensation precision, but issues remain
in generalizing across complex geometries and adapting to position-dependent
variations. We present a novel approach for powder bed fusion (PBF) processes,
using GraphCompNet, which is a computational framework combining graph-based
neural networks with a generative adversarial network (GAN)-inspired training
process. By leveraging point cloud data and dynamic graph convolutional neural
networks (DGCNNs), GraphCompNet models complex shapes and incorporates
position-specific thermal and mechanical factors. A two-stage adversarial
training procedure iteratively refines compensated designs via a
compensator-predictor architecture, offering real-time feedback and
optimization. Experimental validation across diverse shapes and positions shows
the framework significantly improves compensation accuracy (35 to 65 percent)
across the entire print space, adapting to position-dependent variations. This
work advances the development of Digital Twin technology for AM, enabling
scalable, real-time monitoring and compensation, and addressing critical gaps
in AM process control. The proposed method supports high-precision, automated
industrial-scale design and manufacturing systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Errors in the Paper: significant mathematical errors that were not
  noticed before submission, withdraw the paper for corrections</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">31</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented
  Generation with Flexible User Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to
mitigate large language model (LLM) hallucinations by incorporating external
knowledge retrieval. However, existing RAG frameworks often apply retrieval
indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or
failing to retrieve iteratively when required for complex reasoning. Recent
adaptive retrieval strategies, though adaptively navigates these retrieval
strategies, predict only based on query complexity and lacks user-driven
flexibility, making them infeasible for diverse user application needs. In this
paper, we introduce a novel user-controllable RAG framework that enables
dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two
classifiers: one trained to prioritize accuracy and another to prioritize
retrieval efficiency. Via an interpretable control parameter $\alpha$, users
can seamlessly navigate between minimal-cost retrieval and high-accuracy
retrieval based on their specific requirements. We empirically demonstrate that
our approach effectively balances accuracy, retrieval cost, and user
controllability, making it a practical and adaptable solution for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to
  Enhance Wikipedia Tail Biographies through Personal Narratives <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Adak, Pauras Mangesh Meher, Paramita Das, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wikipedia is an invaluable resource for factual information about a wide
range of entities. However, the quality of articles on less-known entities
often lags behind that of the well-known ones. This study proposes a novel
approach to enhancing Wikipedia's B and C category biography articles by
leveraging personal narratives such as autobiographies and biographies. By
utilizing a multi-staged retrieval-augmented generation technique -- REVerSum
-- we aim to enrich the informational content of these lesser-known articles.
Our study reveals that personal narratives can significantly improve the
quality of Wikipedia articles, providing a rich source of reliable information
that has been underutilized in previous studies. Based on crowd-based
evaluation, REVerSum generated content outperforms the best performing baseline
by 17% in terms of integrability to the original Wikipedia article and 28.5\%
in terms of informativeness. Code and Data are available at:
https://github.com/sayantan11995/wikipedia_enrichment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Evaluation of Fairness and Relevance in Recommender Systems with
  Pareto Frontier <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresia Veronika Rampisela, Tuukka Ruotsalo, Maria Maistro, Christina Lioma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness and relevance are two important aspects of recommender systems
(RSs). Typically, they are evaluated either (i) separately by individual
measures of fairness and relevance, or (ii) jointly using a single measure that
accounts for fairness with respect to relevance. However, approach (i) often
does not provide a reliable joint estimate of the goodness of the models, as it
has two different best models: one for fairness and another for relevance.
Approach (ii) is also problematic because these measures tend to be ad-hoc and
do not relate well to traditional relevance measures, like NDCG. Motivated by
this, we present a new approach for jointly evaluating fairness and relevance
in RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction
data, we compute their Pareto frontier for a pair of existing relevance and
fairness measures, and then use the distance from the frontier as a measure of
the jointly achievable fairness and relevance. Our approach is modular and
intuitive as it can be computed with existing measures. Experiments with 4 RS
models, 3 re-ranking strategies, and 6 datasets show that existing metrics have
inconsistent associations with our Pareto-optimal solution, making DPFR a more
robust and theoretically well-founded joint measure for assessing fairness and
relevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TheWebConf/WWW 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information
  Retrieval Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Zhirui Deng, Clara Rus, Xiaopeng Ye, Yuanna Liu, Jun Xu, Zhicheng Dou, Ji-Rong Wen, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern information retrieval (IR). achieving more than just accuracy is
essential to sustaining a healthy ecosystem, especially when addressing
fairness and diversity considerations. To meet these needs, various datasets,
algorithms, and evaluation frameworks have been introduced. However, these
algorithms are often tested across diverse metrics, datasets, and experimental
setups, leading to inconsistencies and difficulties in direct comparisons. This
highlights the need for a comprehensive IR toolkit that enables standardized
evaluation of fairness- and diversity-aware algorithms across different IR
tasks. To address this challenge, we present FairDiverse, an open-source and
standardized toolkit. FairDiverse offers a framework for integrating fair and
diverse methods, including pre-processing, in-processing, and post-processing
techniques, at different stages of the IR pipeline. The toolkit supports the
evaluation of 28 fairness and diversity algorithms across 16 base models,
covering two core IR tasks (search and recommendation) thereby establishing a
comprehensive benchmark. Moreover, FairDiverse is highly extensible, providing
multiple APIs that empower IR researchers to swiftly develop and evaluate their
own fairness and diversity aware models, while ensuring fair comparisons with
existing baselines. The project is open-sourced and available on
https://github.com/XuChen0427/FairDiverse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio
  Chord Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Waseem Akram, Stefano Dettori, Valentina Colla, Giorgio Carlo Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord recognition serves as a critical task in music information retrieval
due to the abstract and descriptive nature of chords in music analysis. While
audio chord recognition systems have achieved significant accuracy for small
vocabularies (e.g., major/minor chords), large-vocabulary chord recognition
remains a challenging problem. This complexity also arises from the inherent
long-tail distribution of chords, where rare chord types are underrepresented
in most datasets, leading to insufficient training samples. Effective chord
recognition requires leveraging contextual information from audio sequences,
yet existing models, such as combinations of convolutional neural networks,
bidirectional long short-term memory networks, and bidirectional transformers,
face limitations in capturing long-term dependencies and exhibit suboptimal
performance on large-vocabulary chord recognition tasks. This work proposes
ChordFormer, a novel conformer-based architecture designed to tackle structural
chord recognition (e.g., triads, bass, sevenths) for large vocabularies.
ChordFormer leverages conformer blocks that integrate convolutional neural
networks with transformers, thus enabling the model to capture both local
patterns and global dependencies effectively. By addressing challenges such as
class imbalance through a reweighted loss function and structured chord
representations, ChordFormer outperforms state-of-the-art models, achieving a
2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy
on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling
class imbalance, providing robust and balanced recognition across chord types.
This approach bridges the gap between theoretical music knowledge and practical
applications, advancing the field of large-vocabulary chord recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Retrieval Augmentation for Open-Ended and
  Knowledge-Intensive Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Zarif Ul Alam, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current video question answering systems perform well on some tasks
requiring only direct visual understanding, they struggle with questions
demanding knowledge beyond what is immediately observable in the video content.
We refer to this challenging scenario as knowledge-intensive video question
answering (KI-VideoQA), where models must retrieve and integrate external
information with visual understanding to generate accurate responses. This work
presents the first attempt to (1) study multi-modal retrieval-augmented
generation for KI-VideoQA, and (2) go beyond multi-choice questions by studying
open-ended questions in this task. Through an extensive empirical study of
state-of-the-art retrieval and vision language models in both zero-shot and
fine-tuned settings, we explore how different retrieval augmentation strategies
can enhance knowledge integration in KI-VideoQA. We analyze three key aspects:
(1) model's effectiveness across different information sources and modalities,
(2) the impact of heterogeneous multi-modal context integration, and (3)
model's effectiveness across different query formulation and retrieval result
consumption. Our results suggest that while retrieval augmentation generally
improves performance, its effectiveness varies significantly based on modality
choice and retrieval strategy. Additionally, we find that successful knowledge
integration often requires careful consideration of query formulation and
optimal retrieval depth. Our exploration advances state-of-the-art accuracy for
multiple choice questions by over 17.5% on the KnowIT VQA dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Recommendation Explanations through User-Centric Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingsen Zhang, Zihang Tian, Xueyang Feng, Xu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural language explanations for recommendations has become
increasingly important in recommender systems. Traditional approaches typically
treat user reviews as ground truth for explanations and focus on improving
review prediction accuracy by designing various model architectures. However,
due to limitations in data scale and model capability, these explanations often
fail to meet key user-centric aspects such as factuality, personalization, and
sentiment coherence, significantly reducing their overall helpfulness to users.
In this paper, we propose a novel paradigm that refines initial explanations
generated by existing explainable recommender models during the inference stage
to enhance their quality in multiple aspects. Specifically, we introduce a
multi-agent collaborative refinement framework based on large language models.
To ensure alignment between the refinement process and user demands, we employ
a plan-then-refine pattern to perform targeted modifications. To enable
continuous improvements, we design a hierarchical reflection mechanism that
provides feedback on the refinement process from both strategic and content
perspectives. Extensive experiments on three datasets demonstrate the
effectiveness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an
  LLM-Assisted Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyu Zhao, Yunxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In quantitative SciSci (science of science) studies, accurately identifying
individual scholars is paramount for scientific data analysis. However, the
variability in how names are represented-due to commonality, abbreviations, and
different spelling conventions-complicates this task. While identifier systems
like ORCID are being developed, many scholars remain unregistered, and numerous
publications are not included. Scholarly databases such as Clarivate and
OpenAlex have introduced their own ID systems as preliminary name
disambiguation solutions. This study evaluates the effectiveness of these
systems across different groups to determine their suitability for various
application scenarios. We sampled authors from the top quartile (Q1) of Web of
Science (WOS) journals based on country, discipline, and number of
corresponding author papers. For each group, we selected 100 scholars and
meticulously annotated all their papers using a Search-enhanced Large Language
Model method. Using these annotations, we identified the corresponding IDs in
OpenAlex and Clarivate, extracted all associated papers, filtered for Q1 WOS
journals, and calculated precision and recall by comparing against the
annotated dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaMTEB: Massive Text Embedding Benchmark in Persian Language <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)
text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our
benchmark includes 63 datasets spanning seven different tasks: classification,
clustering, pair classification, reranking, retrieval, summary retrieval, and
semantic textual similarity. The datasets are formed as a combination of
existing, translated, and newly generated data, offering a diverse evaluation
framework for Persian language models. Given the increasing use of text
embedding models in chatbots, evaluation datasets are becoming inseparable
ingredients in chatbot challenges and Retrieval-Augmented Generation systems.
As a contribution, we include chatbot evaluation datasets in the MTEB benchmark
for the first time. In addition, in this paper, we introduce the new task of
summary retrieval which is not part of the tasks included in standard MTEB.
Another contribution of this paper is the introduction of a substantial number
of new Persian language NLP datasets suitable for training and evaluation, some
of which have no previous counterparts in Persian. We evaluate the performance
of several Persian and multilingual embedding models in a range of tasks. This
work introduces an open-source benchmark with datasets, code and a public
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU-accelerated Multi-relational Parallel Graph Retrieval for Web-scale
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Guo, Guangxing Chen, Qian Gao, Xiaochao Liao, Jianjia Zheng, Lu Shen, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web recommendations provide personalized items from massive catalogs for
users, which rely heavily on retrieval stages to trade off the effectiveness
and efficiency of selecting a small relevant set from billion-scale candidates
in online digital platforms. As one of the largest Chinese search engine and
news feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based
Approximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance
estimation and efficient search for relevant items. However, current retrieval
at Baidu fails in comprehensive user-item relational understanding due to
dissected interaction modeling, and performs inefficiently in large-scale
graph-based ANNS because of suboptimal traversal navigation and the GPU
computational bottleneck under high concurrency. To this end, we propose a
GPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to
achieve effective yet efficient retrieval in web-scale recommendations. First,
we propose a multi-relational user-item relevance metric learning method that
unifies diverse user behaviors through multi-objective optimization and employs
a self-covariant loss to enhance pathfinding performance. Second, we develop a
hierarchical parallel graph-based ANNS to boost graph retrieval throughput,
which conducts breadth-depth-balanced searches on a large-scale item graph and
cost-effectively handles irregular neural computation via adaptive aggregation
on GPUs. In addition, we integrate system optimization strategies in the
deployment of GMP-GR in Baidu. Extensive experiments demonstrate the
superiority of GMP-GR in retrieval accuracy and efficiency. Deployed across
more than twenty applications at Baidu, GMP-GR serves hundreds of millions of
users with a throughput exceeding one hundred million requests per second.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLTW: Joint Improved Graph <span class="highlight-title">Transformer</span> and LLM via Three-Word Language
  for Knowledge Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Turn Multi-Modal Question Clarification for Enhanced
  Conversational Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kimia Ramezan, Alireza Amiri Bavandpour, Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational query clarification enables users to refine their search
queries through interactive dialogue, improving search effectiveness.
Traditional approaches rely on text-based clarifying questions, which often
fail to capture complex user preferences, particularly those involving visual
attributes. While recent work has explored single-turn multi-modal
clarification with images alongside text, such methods do not fully support the
progressive nature of user intent refinement over multiple turns. Motivated by
this, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,
which combines text and visual modalities to refine user queries in a
multi-turn conversation. To facilitate this task, we create a large-scale
dataset named ClariMM comprising over 13k multi-turn interactions and 33k
question-answer pairs containing multi-modal clarifying questions. We propose
Mario, a retrieval framework that employs a two-phase ranking strategy: initial
retrieval with BM25, followed by a multi-modal generative re-ranking model that
integrates textual and visual information from conversational history. Our
experiments show that multi-turn multi-modal clarification outperforms
uni-modal and single-turn approaches, improving MRR by 12.88%. The gains are
most significant in longer interactions, demonstrating the value of progressive
refinement for complex queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leave No One Behind: Enhancing Diversity While Maintaining Accuracy in
  Social Recommendation <span class="chip">DASFAA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Xiao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social recommendation, a branch of algorithms that utilizes social connection
information to construct recommender systems, has demonstrated its
effectiveness in enhancing recommendation accuracy. However, apart from
accuracy, the diversity of recommendations also plays a critical role in user
engagement. Unfortunately, the impact of social recommendation models on
recommendation diversity remains largely unexplored. In this study, we
investigate the dual performance of existing social recommendation algorithms
in terms of accuracy and diversity. Our empirical findings highlight a
concerning trend: social recommendation models tend to decrease diversity,
despite their accuracy improvements. To address this issue, we propose a novel
approach called Diversified Social Recommendation (DivSR), which leverages
relational knowledge distillation techniques to transfer high-diversity
structured knowledge from non-social recommendation models to social
recommendation models. DivSR is designed as a simple, model-agnostic framework
that integrates seamlessly with existing social recommendation architectures.
Experimental results on three benchmark datasets demonstrate that DivSR
significantly increases diversity without markedly compromising accuracy across
various social recommendation backbones, achieving a better accuracy-diversity
trade-off. Our code and data are publicly available at:
https://github.com/ll0ruc/DivSR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG vs. GraphRAG: A Systematic Evaluation and Key Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across
various tasks by retrieving relevant information from external sources,
particularly on text-based data. For structured data, such as knowledge graphs,
GraphRAG has been widely used to retrieve relevant information. However, recent
studies have revealed that structuring implicit knowledge from text into graphs
can benefit certain tasks, extending the application of GraphRAG from graph
data to general text-based data. Despite their successful extensions, most
applications of GraphRAG for text data have been designed for specific tasks
and datasets, lacking a systematic evaluation and comparison between RAG and
GraphRAG on widely used text-based benchmarks. In this paper, we systematically
evaluate RAG and GraphRAG on well-established benchmark tasks, such as Question
Answering and Query-based Summarization. Our results highlight the distinct
strengths of RAG and GraphRAG across different tasks and evaluation
perspectives. Inspired by these observations, we investigate strategies to
integrate their strengths to improve downstream tasks. Additionally, we provide
an in-depth discussion of the shortcomings of current GraphRAG approaches and
outline directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Ranking on Cascading Behavior Graphs for Accurate
  Multi-Behavior Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonwoo Ko, Minseo Jeon, Jinhong Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-behavior recommendation predicts items a user may purchase by analyzing
diverse behaviors like viewing, adding to a cart, and purchasing. Existing
methods fall into two categories: representation learning and graph ranking.
Representation learning generates user and item embeddings to capture latent
interaction patterns, leveraging multi-behavior properties for better
generalization. However, these methods often suffer from over-smoothing and
bias toward frequent interactions, limiting their expressiveness. Graph ranking
methods, on the other hand, directly compute personalized ranking scores,
capturing user preferences more effectively. Despite their potential, graph
ranking approaches have been primarily explored in single-behavior settings and
remain underutilized for multi-behavior recommendation. In this paper, we
propose CascadingRank, a novel graph ranking method for multi-behavior
recommendation. It models the natural sequence of user behaviors (e.g.,
viewing, adding to cart, and purchasing) through a cascading behavior graph. An
iterative algorithm computes ranking scores, ensuring smoothness, query
fitting, and cascading alignment. Experiments on three real-world datasets
demonstrate that CascadingRank outperforms state-of-the-art methods, with up to
9.56% and 7.16% improvements in HR@10 and NDCG@10, respectively. Furthermore,
we provide theoretical analysis highlighting its effectiveness, convergence,
and scalability, showcasing the advantages of graph ranking in multi-behavior
recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navve Wasserman, Roi Pony, Oshri Naparstek, Adi Raz Goldfarb, Eli Schwartz, Udi Barzelay, Leonid Karlinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate multi-modal document retrieval is crucial for Retrieval-Augmented
Generation (RAG), yet existing benchmarks do not fully capture real-world
challenges with their current design. We introduce REAL-MM-RAG, an
automatically generated benchmark designed to address four key properties
essential for real-world retrieval: (i) multi-modal documents, (ii) enhanced
difficulty, (iii) Realistic-RAG queries and (iv) accurate labeling.
Additionally, we propose a multi-difficulty-level scheme based on query
rephrasing to evaluate models' semantic understanding beyond keyword matching.
Our benchmark reveals significant model weaknesses, particularly in handling
table-heavy documents and robustness to query rephrasing. To mitigate these
shortcomings, we curate a rephrased training set and introduce a new
finance-focused, table-heavy dataset. Fine-tuning on these datasets enables
models to achieve state-of-the-art retrieval performance on REAL-MM-RAG
benchmark. Our work offers a better way to evaluate and improve retrieval in
multi-modal RAG systems while also providing training data and models that
address current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment <span class="chip">DASFAA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21745v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21745v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xiang, Li Fan, Tulika Saha, Xiaoying Pang, Yushan Pan, Haiyang Zhang, Chengtao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering is an essential aspect of network analysis that involves
grouping nodes into separate clusters. Recent developments in deep learning
have resulted in graph clustering, which has proven effective in many
applications. Nonetheless, these methods often encounter difficulties when
dealing with real-world graphs, particularly in the presence of noisy edges.
Additionally, many denoising graph clustering methods tend to suffer from lower
performance, training instability, and challenges in scaling to large datasets
compared to non-denoised models. To tackle these issues, we introduce a new
framework called the Robust Deep Graph Clustering Framework via Dual Soft
Assignment (RDSA). RDSA consists of three key components: (i) a node embedding
module that effectively integrates the graph's topological features and node
attributes; (ii) a structure-based soft assignment module that improves graph
modularity by utilizing an affinity matrix for node assignments; and (iii) a
node-based soft assignment module that identifies community landmarks and
refines node assignments to enhance the model's robustness. We assess RDSA on
various real-world datasets, demonstrating its superior performance relative to
existing state-of-the-art methods. Our findings indicate that RDSA provides
robust clustering across different graph types, excelling in clustering
effectiveness and robustness, including adaptability to noise, stability, and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA 2025; Complete version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Memory Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05558v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05558v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lu, Zheng Chai, Yuchao Zheng, Zhe Chen, Deping Xie, Peng Xu, Xun Zhou, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling user behavior sequences in recommender systems is essential for
understanding user preferences over time, enabling personalized and accurate
recommendations for improving user retention and enhancing business values.
Despite its significance, there are two challenges for current sequential
modeling approaches. From the spatial dimension, it is difficult to mutually
perceive similar users' interests for a generalized intention understanding;
from the temporal dimension, current methods are generally prone to forgetting
long-term interests due to the fixed-length input sequence. In this paper, we
present Large Memory Network (LMN), providing a novel idea by compressing and
storing user history behavior information in a large-scale memory block. With
the elaborated online deployment strategy, the memory block can be easily
scaled up to million-scale in the industry. Extensive offline comparison
experiments, memory scaling up experiments, and online A/B test on Douyin
E-Commerce Search (ECS) are performed, validating the superior performance of
LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of
users each day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reason4Rec: Large Language Models for Recommendation with Deliberative
  User Preference Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Fang, Wenjie Wang, Yang Zhang, Fengbin Zhu, Qifan Wang, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advancements in aligning Large Language Models (LLMs) with
recommendation tasks have shown great potential and promising performance
overall, these aligned recommendation LLMs still face challenges in complex
scenarios. This is primarily due to the current alignment approach focusing on
optimizing LLMs to generate user feedback directly, without incorporating
deliberation. To overcome this limitation and develop more reliable LLMs for
recommendations, we propose a new Deliberative Recommendation task, which
incorporates explicit reasoning about user preferences as an additional
alignment goal. We then introduce the Reasoning-powered Recommender framework
for deliberative user preference alignment, designed to enhance reasoning
capabilities by utilizing verbalized user feedback in a step-wise manner to
tackle this task. The framework employs collaborative step-wise experts and
tailored training strategies for each expert. Experimental results across three
real-world datasets demonstrate the rationality of the deliberative task
formulation and the superior performance of the proposed framework in improving
both prediction accuracy and reasoning quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Channel Multiplex Graph Neural Networks for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11624v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11624v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Yanwei Yu, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interactive relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant challenges: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations within behavior patterns on the target relation in recommender system
scenarios. In this work, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interactive relations, and includes a relation chain representation
learner and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our \model
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06% and 12.15% on average across all datasets in terms of
Recall@10 and NDCG@10 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciPIP: An LLM-based Scientific Paper Idea Proposer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has opened new
possibilities for automating the proposal of innovative scientific ideas. This
process involves two key phases: literature retrieval and idea generation.
However, existing approaches often fall short due to their reliance on
keyword-based search tools during the retrieval phase, which neglects crucial
semantic information and frequently results in incomplete retrieval outcomes.
Similarly, in the idea generation phase, current methodologies tend to depend
solely on the internal knowledge of LLMs or metadata from retrieved papers,
thereby overlooking significant valuable insights contained within the full
texts. To address these limitations, we introduce SciPIP, an innovative
framework designed to enhance the LLM-based proposal of scientific ideas
through improvements in both literature retrieval and idea generation. Our
approach begins with the construction of a comprehensive literature database
that supports advanced retrieval based not only on keywords but also on
semantics and citation relationships. This is complemented by the introduction
of a multi-granularity retrieval algorithm aimed at ensuring more thorough and
exhaustive retrieval results. For the idea generation phase, we propose a
dual-path framework that effectively integrates both the content of retrieved
papers and the extensive internal knowledge of LLMs. This integration
significantly boosts the novelty, feasibility, and practical value of proposed
ideas. Our experiments, conducted across various domains such as natural
language processing and computer vision, demonstrate SciPIP's capability to
generate a multitude of innovative and useful ideas. These findings underscore
SciPIP's potential as a valuable tool for researchers seeking to advance their
fields with groundbreaking concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 12 tables. The code has been availabel:
  https://github.com/cheerss/SciPIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Recommender Systems via Multimodal Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adamya Shyam, Ramya Kamani, Venkateswara Rao Kagita, Vikas Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative Filtering (CF) has emerged as one of the most prominent
implementation strategies for building recommender systems. The key idea is to
exploit the usage patterns of individuals to generate personalized
recommendations. CF techniques, especially for newly launched platforms, often
face a critical issue known as the data sparsity problem, which greatly limits
their performance. Cross-domain CF alleviates the problem of data sparsity by
finding a common set of entities (users or items) across the domains, which
then act as a conduit for knowledge transfer. Nevertheless, most real-world
datasets are collected from different domains, so they often lack information
about anchor points or reference information for entity alignment. This paper
introduces a domain adaptation technique to align the embeddings of entities
across domains. Our approach first exploits the available textual and visual
information to independently learn a multi-view latent representation for each
entity in the auxiliary and target domains. The different representations of
the entity are then fused to generate the corresponding unified representation.
A domain classifier is then trained to learn the embedding for the domain
alignment by fixing the unified features as the anchor points. Experiments on
\AS{four} publicly available benchmark datasets indicate the effectiveness of
our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal semantic retrieval for product search <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Esther Lopez Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval (also known as dense retrieval) based on textual data has
been extensively studied for both web search and product search application
fields, where the relevance of a query and a potential target document is
computed by their dense vector representation comparison. Product image is
crucial for e-commerce search interactions and is a key factor for customers at
product explorations. However, its impact on semantic retrieval has not been
well studied yet. In this research, we build a multimodal representation for
product items in e-commerce search in contrast to pure-text representation of
products, and investigate the impact of such representations. The models are
developed and evaluated on e-commerce datasets. We demonstrate that a
multimodal representation scheme for a product can show improvement either on
purchase recall or relevance accuracy in semantic retrieval. Additionally, we
provide numerical analysis for exclusive matches retrieved by a multimodal
semantic retrieval model versus a text-only semantic retrieval model, to
demonstrate the validation of multimodal solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EReL@MIR WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data and Decision Traceability for SDA TAP Lab's Prototype Battle
  Management System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Latha Pratti, Samya Bagchi, Yasir Latif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space Protocol is applying the principles derived from MITRE and NIST's
Supply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a
complex multi party system to achieve introspection, auditing, and replay of
data and decisions that ultimately lead to a end decision. The core goal of
decision traceability is to ensure transparency, accountability, and integrity
within the WA system. This is accomplished by providing a clear, auditable path
from the system's inputs all the way to the final decision. This traceability
enables the system to track the various algorithms and data flows that have
influenced a particular outcome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Factors Paper-<span class="highlight-title">Review</span>er Matching <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yanzhen Shen, SeongKu Kang, Xiusi Chen, Bowen Jin, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid increase in paper submissions to academic conferences, the
need for automated and accurate paper-reviewer matching is more critical than
ever. Previous efforts in this area have considered various factors to assess
the relevance of a reviewer's expertise to a paper, such as the semantic
similarity, shared topics, and citation connections between the paper and the
reviewer's previous works. However, most of these studies focus on only one
factor, resulting in an incomplete evaluation of the paper-reviewer relevance.
To address this issue, we propose a unified model for paper-reviewer matching
that jointly considers semantic, topic, and citation factors. To be specific,
during training, we instruction-tune a contextualized language model shared
across all factors to capture their commonalities and characteristics; during
inference, we chain the three factors to enable step-by-step, coarse-to-fine
search for qualified reviewers given a submission. Experiments on four datasets
(one of which is newly contributed by us) spanning various fields such as
machine learning, computer vision, information retrieval, and data mining
consistently demonstrate the effectiveness of our proposed Chain-of-Factors
model in comparison with state-of-the-art paper-reviewer matching methods and
scientific pre-trained language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; Accepted to WWW 2025 (Code:
  https://github.com/yuzhimanhua/CoF)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It
mainly consists of retrieval and generation. The retrieval modules (a.k.a.
retrievers) aim to find useful information used to facilitate the generation
modules (a.k.a. generators). As such, generators' performance largely depends
on the effectiveness and efficiency of retrievers. However, the widely used
retrieval paradigm remains flat. It treats retrieval procedures as a one-off
deal with constant granularity. Despite effectiveness, we argue that they
suffer from two limitations: (1) flat retrieval exerts a significant burden on
one retriever; (2) constant granularity limits the ceiling of retrieval
performance. In this work, we propose a progressive retrieval paradigm with
coarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance
effectiveness and efficiency. Specifically, FunnelRAG establishes a progressive
retrieval pipeline by collaborating coarse-to-fine granularity, large-to-small
quantity, and low-to-high capacity, which can relieve the burden on one
retriever and also promote the ceiling of retrieval performance. Extensive
experiments manifest that FunnelRAG achieves comparable retrieval performance
while the time overhead is reduced by nearly 40 percent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 13 tables. Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Foundation Models for Recommendation: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08346v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08346v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) serve as a fundamental tool for navigating the vast
expanse of online information, with deep learning advancements playing an
increasingly important role in improving ranking accuracy. Among these, graph
neural networks (GNNs) excel at extracting higher-order structural information,
while large language models (LLMs) are designed to process and comprehend
natural language, making both approaches highly effective and widely adopted.
Recent research has focused on graph foundation models (GFMs), which integrate
the strengths of GNNs and LLMs to model complex RS problems more efficiently by
leveraging the graph-based structure of user-item relationships alongside
textual understanding. In this survey, we provide a comprehensive overview of
GFM-based RS technologies by introducing a clear taxonomy of current
approaches, diving into methodological details, and highlighting key challenges
and future directions. By synthesizing recent advancements, we aim to offer
valuable insights into the evolving landscape of GFM-based recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-granularity Interest Retrieval and Refinement Network for
  Long-Term User Behavior Modeling in CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15005v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15005v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Hao Wang, Wei Guo, Luankang Zhang, Wanshan Yang, Runlong Yu, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through Rate (CTR) prediction is crucial for online personalization
platforms. Recent advancements have shown that modeling rich user behaviors can
significantly improve the performance of CTR prediction. Current long-term user
behavior modeling algorithms predominantly follow two cascading stages. The
first stage retrieves subsequence related to the target item from the long-term
behavior sequence, while the second stage models the relationship between the
subsequence and the target item. Despite significant progress, these methods
have two critical flaws. First, the retrieval query typically includes only
target item information, limiting the ability to capture the user's diverse
interests. Second, relational information, such as sequential and interactive
information within the subsequence, is frequently overlooked. Therefore, it
requires to be further mined to more accurately model user interests.
  To this end, we propose Multi-granularity Interest Retrieval and Refinement
Network (MIRRN). Specifically, we first construct queries based on behaviors
observed at different time scales to obtain subsequences, each capturing users'
interest at various granularities. We then introduce an noval multi-head
Fourier transformer to efficiently learn sequential and interactive information
within the subsequences, leading to more accurate modeling of user interests.
Finally, we employ multi-head target attention to adaptively assess the impact
of these multi-granularity interests on the target item. Extensive experiments
have demonstrated that MIRRN significantly outperforms state-of-the-art
baselines. Furthermore, an A/B test shows that MIRRN increases the average
number of listening songs by 1.32% and the average time of listening songs by
0.55% on the Huawei Music App. The implementation code is publicly available at
https://github.com/USTC-StarTeam/MIRRN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing Multimodal Queries Representation via Fusion-Free
  Modality Interaction for Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-EXR: Controllable <span class="highlight-title">Review</span> Generation for Explainable
  Recommendation via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15490v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15490v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Li, Shaohua Li, Winda Marantika, Alex C. Kot, Huijing Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in
image and audio generation tasks. However, there exist few attempts to employ
DDPM in the text generation, especially review generation under recommendation
systems. Fueled by the predicted reviews explainability that justifies
recommendations could assist users better understand the recommended items and
increase the transparency of recommendation system, we propose a Diffusion
Model-based Review Generation towards EXplainable Recommendation named
Diffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by
incrementally introducing varied levels of Gaussian noise to the sequence of
word embeddings and learns to reconstruct the original word representations in
the reverse process. The nature of DDPM enables our lightweight Transformer
backbone to perform excellently in the recommendation review generation task.
Extensive experimental results have demonstrated that Diffusion-EXR can achieve
state-of-the-art review generation for recommendation on two publicly available
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We request to withdraw our paper from the archive due to significant
  errors identified in the analysis and conclusions. Upon further review, we
  realized that these errors undermine the validity of our findings. We plan to
  conduct additional research to correct these issues and resubmit a revised
  version in the future</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ask in Any Modality: A Comprehensive <span class="highlight-title">Survey</span> on Multimodal
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08826v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08826v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) struggle with hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information enhancing factual and updated grounding. Recent advances in
multimodal learning have led to the development of Multimodal RAG,
incorporating multiple modalities such as text, images, audio, and video to
enhance the generated outputs. However, cross-modal alignment and reasoning
introduce unique challenges to Multimodal RAG, distinguishing it from
traditional unimodal RAG. This survey offers a structured and comprehensive
analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,
evaluation, methodologies, and innovations in retrieval, fusion, augmentation,
and generation. We precisely review training strategies, robustness
enhancements, and loss functions, while also exploring the diverse Multimodal
RAG scenarios. Furthermore, we discuss open challenges and future research
directions to support advancements in this evolving field. This survey lays the
foundation for developing more capable and reliable AI systems that effectively
leverage multimodal dynamic external knowledge bases. Resources are available
at https://github.com/llm-lab-org/Multimodal-RAG-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository:
  https://github.com/llm-lab-org/Multimodal-RAG-Survey</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Communications: A Unified Framework for Cross-modal Context-aware
  Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Rahim Tafazolli, Mehdi Bennis, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce token communications (TokCom), a unified
framework to leverage cross-modal context information in generative semantic
communications (GenSC). TokCom is a new paradigm, motivated by the recent
success of generative foundation models and multimodal large language models
(GFM/MLLMs), where the communication units are tokens, enabling efficient
transformer-based token processing at the transmitter and receiver. In this
paper, we introduce the potential opportunities and challenges of leveraging
context in GenSC, explore how to integrate GFM/MLLMs-based token processing
into semantic communication systems to leverage cross-modal context
effectively, present the key principles for efficient TokCom at various layers
in future wireless networks. We demonstrate the corresponding TokCom benefits
in a GenSC setup for image, leveraging cross-modal context information, which
increases the bandwidth efficiency by 70.8% with negligible loss of
semantic/perceptual quality. Finally, the potential research directions are
identified to facilitate adoption of TokCom in future wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting models' contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is quite effective and sensitive in identifying varying degrees
of contamination, and can highlight significant performance improvements due to
the leakage of multimodal benchmark training sets. Furthermore, we explore
whether the contamination originates from the base LLMs used by MLLMs or the
multimodal training phase, providing new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Compressed Image Latents and Multimodal Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever study of adapting compressed image latents
to suit the needs of downstream vision tasks that adopt Multimodal Large
Language Models (MLLMs). MLLMs have extended the success of large language
models to modalities (e.g. images) beyond text, but their billion scale hinders
deployment on resource-constrained end devices. While cloud-hosted MLLMs could
be available, transmitting raw, uncompressed images captured by end devices to
the cloud requires an efficient image compression system. To address this, we
focus on emerging neural image compression and propose a novel framework with a
lightweight transform-neck and a surrogate loss to adapt compressed image
latents for MLLM-based vision tasks. Given the huge scale of MLLMs, our
framework excludes the entire downstream MLLM except part of its visual encoder
from training our system. This stands out from most existing coding for machine
approaches that involve downstream networks in training and thus could be
impractical when the networks are MLLMs. The proposed framework is general in
that it is applicable to various MLLMs, neural image codecs, and multiple
application scenarios, where the neural image codec can be (1) pre-trained for
human perception without updating, (2) fully updated for joint human and
machine perception, or (3) fully updated for only machine perception. Extensive
experiments on different neural image codecs and various MLLMs show that our
method achieves great rate-accuracy performance with much less complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Attribute-Relation Representation Based Video Semantic
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Du, Yiping Duan, Qianqian Yang, Xiaoming Tao, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of multimedia data volume, there is an increasing need
for efficient video transmission in applications such as virtual reality and
future video streaming services. Semantic communication is emerging as a vital
technique for ensuring efficient and reliable transmission in low-bandwidth,
high-noise settings. However, most current approaches focus on joint
source-channel coding (JSCC) that depends on end-to-end training. These methods
often lack an interpretable semantic representation and struggle with
adaptability to various downstream tasks. In this paper, we introduce the use
of object-attribute-relation (OAR) as a semantic framework for videos to
facilitate low bit-rate coding and enhance the JSCC process for more effective
video transmission. We utilize OAR sequences for both low bit-rate
representation and generative video reconstruction. Additionally, we
incorporate OAR into the image JSCC model to prioritize communication resources
for areas more critical to downstream tasks. Our experiments on traffic
surveillance video datasets assess the effectiveness of our approach in terms
of video transmission performance. The empirical findings demonstrate that our
OAR-based video coding method not only outperforms H.265 coding at lower
bit-rates but also synergizes with JSCC to deliver robust and efficient video
transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing Multimodal Queries Representation via Fusion-Free
  Modality Interaction for Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-16T00:00:00Z">2025-02-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can't See the Forest for the Trees: Benchmarking Multimodal Safety
  Awareness for Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have expanded the capabilities of
traditional language models by enabling interaction through both text and
images. However, ensuring the safety of these models remains a significant
challenge, particularly in accurately identifying whether multimodal content is
safe or unsafe-a capability we term safety awareness. In this paper, we
introduce MMSafeAware, the first comprehensive multimodal safety awareness
benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500
carefully curated image-prompt pairs. MMSafeAware includes both unsafe and
over-safety subsets to assess models abilities to correctly identify unsafe
content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine
widely used MLLMs using MMSafeAware reveals that current models are not
sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies
36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further
explore three methods to improve safety awareness-prompting-based approaches,
visual contrastive decoding, and vision-centric reasoning fine-tuning-but find
that none achieve satisfactory performance. Our findings highlight the profound
challenges in developing MLLMs with robust safety awareness, underscoring the
need for further research in this area. All the code and data will be publicly
available to facilitate future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering
  without Font Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jiang, Yuan Yuan, Xinyi Bai, Zhuoqun Hao, Alyson Yin, Yaojie Hu, Wenyu Liao, Lyle Ungar, Camillo J. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work demonstrates that diffusion models can achieve font-controllable
multilingual text rendering using just raw images without font label
annotations. Visual text rendering remains a significant challenge. While
recent methods condition diffusion on glyphs, it is impossible to retrieve
exact font annotations from large-scale, real-world datasets, which prevents
user-specified font control. To address this, we propose a data-driven solution
that integrates the conditional diffusion model with a text segmentation model,
utilizing segmentation masks to capture and represent fonts in pixel space in a
self-supervised manner, thereby eliminating the need for any ground-truth
labels and enabling users to customize text rendering with any multilingual
font of their choice. The experiment provides a proof of concept of our
algorithm in zero-shot text and font editing across diverse fonts and
languages, providing valuable insights for the community and industry toward
achieving generalized visual text rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is preliminary work and code will be released at
  github.com/bowen-upenn/ControlText</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Advances in Discrete Speech Tokens: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of speech generation technologies in the era of large
language models (LLMs) has established discrete speech tokens as a foundational
paradigm for speech representation. These tokens, characterized by their
discrete, compact, and concise nature, are not only advantageous for efficient
transmission and storage, but also inherently compatible with the language
modeling framework, enabling seamless integration of speech into text-dominated
LLM architectures. Current research categorizes discrete speech tokens into two
principal classes: acoustic tokens and semantic tokens, each of which has
evolved into a rich research domain characterized by unique design philosophies
and methodological approaches. This survey systematically synthesizes the
existing taxonomy and recent innovations in discrete speech tokenization,
conducts a critical examination of the strengths and limitations of each
paradigm, and presents systematic experimental comparisons across token types.
Furthermore, we identify persistent challenges in the field and propose
potential research directions, aiming to offer actionable insights to inspire
future advancements in the development and application of discrete speech
tokens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures, 3 tables. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking Backward: Streaming Video-to-Video Translation with Feature
  Banks <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, Diana Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces StreamV2V, a diffusion model that achieves real-time
streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V
methods using batches to process limited frames, we opt to process frames in a
streaming fashion, to support unlimited frames. At the heart of StreamV2V lies
a backward-looking principle that relates the present to the past. This is
realized by maintaining a feature bank, which archives information from past
frames. For incoming frames, StreamV2V extends self-attention to include banked
keys and values and directly fuses similar past features into the output. The
feature bank is continually updated by merging stored and new features, making
it compact but informative. StreamV2V stands out for its adaptability and
efficiency, seamlessly integrating with image diffusion models without
fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x
faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative
metrics and user studies confirm StreamV2V's exceptional ability to maintain
temporal consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page:
  https://jeff-liangf.github.io/projects/streamv2v</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MemeSense: An Adaptive In-Context Framework for Social Commonsense
  Driven Meme Moderation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Adak, Somnath Banerjee, Rajarshi Mandal, Avik Halder, Sayan Layek, Rima Hazra, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memes present unique moderation challenges due to their subtle, multimodal
interplay of images, text, and social context. Standard systems relying
predominantly on explicit textual cues often overlook harmful content
camouflaged by irony, symbolism, or cultural references. To address this gap,
we introduce MemeSense, an adaptive in-context learning framework that fuses
social commonsense reasoning with visually and semantically related reference
examples. By encoding crucial task information into a learnable cognitive shift
vector, MemeSense effectively balances lexical, visual, and ethical
considerations, enabling precise yet context-aware meme intervention. Extensive
evaluations on a curated set of implicitly harmful memes demonstrate that
MemeSense substantially outperforms strong baselines, paving the way for safer
online communities. Code and data available at:
https://github.com/sayantan11995/MemeSense
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data available at:
  https://github.com/sayantan11995/MemeSense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSP: A Simulator For Multi-Agent Ranking Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommy Mordo, Tomer Kordonsky, Haya Nachimovsky, Moshe Tennenholtz, Oren Kurland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In ranking competitions, document authors compete for the highest rankings by
modifying their content in response to past rankings. Previous studies focused
on human participants, primarily students, in controlled settings. The rise of
generative AI, particularly Large Language Models (LLMs), introduces a new
paradigm: using LLMs as document authors. This approach addresses scalability
constraints in human-based competitions and reflects the growing role of
LLM-generated content on the web-a prime example of ranking competition. We
introduce a highly configurable ranking competition simulator that leverages
LLMs as document authors. It includes analytical tools to examine the resulting
datasets. We demonstrate its capabilities by generating multiple datasets and
conducting an extensive analysis. Our code and datasets are publicly available
for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Scientific Document Retrieval with Concept Coverage-based
  Query Set Generation <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeongKu Kang, Bowen Jin, Wonbin Kweon, Yu Zhang, Dongha Lee, Jiawei Han, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In specialized fields like the scientific domain, constructing large-scale
human-annotated datasets poses a significant challenge due to the need for
domain expertise. Recent methods have employed large language models to
generate synthetic queries, which serve as proxies for actual user queries.
However, they lack control over the content generated, often resulting in
incomplete coverage of academic concepts in documents. We introduce Concept
Coverage-based Query set Generation (CCQGen) framework, designed to generate a
set of queries with comprehensive coverage of the document's concepts. A key
distinction of CCQGen is that it adaptively adjusts the generation process
based on the previously generated queries. We identify concepts not
sufficiently covered by previous queries, and leverage them as conditions for
subsequent query generation. This approach guides each new query to complement
the previous ones, aiding in a thorough understanding of the document.
Extensive experiments demonstrate that CCQGen significantly enhances query
quality and retrieval performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gumbel Reranking: Differentiable End-to-End Reranker Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAG systems rely on rerankers to identify relevant documents. However,
fine-tuning these models remains challenging due to the scarcity of annotated
query-document pairs. Existing distillation-based approaches suffer from
training-inference misalignment and fail to capture interdependencies among
candidate documents. To overcome these limitations, we reframe the reranking
process as an attention-mask problem and propose Gumbel Reranking, an
end-to-end training framework for rerankers aimed at minimizing the
training-inference gap. In our approach, reranker optimization is reformulated
as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel
Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end
optimization by minimizing the overall language loss. Experiments across
various settings consistently demonstrate performance gains, including a 10.4\%
improvement in recall on HotpotQA for distinguishing indirectly relevant
documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graceful forgetting: Memory as a process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alain de Cheveigné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A rational theory of memory is proposed to explain how we can accommodate
unbounded sensory input within bounded storage space. Memory is stored as
statistics, organized into complex structures that are constantly summarized
and compressed to make room for new input. This process, driven by space
constraints, is guided by heuristics that optimize the memory for future needs.
Sensory input is rapidly encoded as simple statistics that are more slowly
elaborated into more abstract constructs. This theory differs from previous
accounts of memory by (a) its reliance on statistics, (b) its use of heuristics
to guide the choice of statistics, and (c) the emphasis on memory as a process
that is intensive, complex, and expensive. The theory is intended as an aid to
make sense of our extensive knowledge of memory, and bring us closer to an
understanding of memory in functional and mechanistic terms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinMTEB: Finance Massive Text Embedding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a crucial role in representing and retrieving
information across various NLP applications. Recent advances in large language
models (LLMs) have further enhanced the performance of embedding models. While
these models are often benchmarked on general-purpose datasets, real-world
applications demand domain-specific evaluation. In this work, we introduce the
Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart
to MTEB designed for the financial domain. FinMTEB comprises 64 financial
domain-specific embedding datasets across 7 tasks that cover diverse textual
types in both Chinese and English, such as financial news articles, corporate
annual reports, ESG reports, regulatory filings, and earnings call transcripts.
We also develop a finance-adapted model, FinPersona-E5, using a persona-based
data synthetic method to cover diverse financial embedding tasks for training.
Through extensive evaluation of 15 embedding models, including FinPersona-E5,
we show three key findings: (1) performance on general-purpose benchmarks shows
limited correlation with financial domain tasks; (2) domain-adapted models
consistently outperform their general-purpose counterparts; and (3)
surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated
dense embeddings in financial Semantic Textual Similarity (STS) tasks,
underscoring current limitations in dense embedding techniques. Our work
establishes a robust evaluation framework for financial NLP applications and
provides crucial insights for developing domain-specific embedding models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yixuantt/FinMTEB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuOTE: Question-Oriented Text Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Neeser, Kaylen Latimer, Aadyant Khatri, Chris Latimer, Naren Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to
retrieval-augmented generation (RAG) systems, aimed at improving document
representation for accurate and nuanced retrieval. Unlike traditional RAG
pipelines, which rely on embedding raw text chunks, QuOTE augments chunks with
hypothetical questions that the chunk can potentially answer, enriching the
representation space. This better aligns document embeddings with user query
semantics, and helps address issues such as ambiguity and context-dependent
relevance. Through extensive experiments across diverse benchmarks, we
demonstrate that QuOTE significantly enhances retrieval accuracy, including in
multi-hop question-answering tasks. Our findings highlight the versatility of
question generation as a fundamental indexing strategy, opening new avenues for
integrating question generation into retrieval-based AI pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashCheck: Exploration of Efficient Evidence Retrieval for Fast
  Fact-Checking <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Nanekhan, Venktesh V, Erik Martin, Henrik Vatndal, Vinay Setty, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advances in digital tools have led to the rampant spread of
misinformation. While fact-checking aims to combat this, manual fact-checking
is cumbersome and not scalable. It is essential for automated fact-checking to
be efficient for aiding in combating misinformation in real-time and at the
source. Fact-checking pipelines primarily comprise a knowledge retrieval
component which extracts relevant knowledge to fact-check a claim from large
knowledge sources like Wikipedia and a verification component. The existing
works primarily focus on the fact-verification part rather than evidence
retrieval from large data collections, which often face scalability issues for
practical applications such as live fact-checking. In this study, we address
this gap by exploring various methods for indexing a succinct set of factual
statements from large collections like Wikipedia to enhance the retrieval phase
of the fact-checking pipeline. We also explore the impact of vector
quantization to further improve the efficiency of pipelines that employ dense
retrieval approaches for first-stage retrieval. We study the efficiency and
effectiveness of the approaches on fact-checking datasets such as HoVer and
WiCE, leveraging Wikipedia as the knowledge source. We also evaluate the
real-world utility of the efficient retrieval approaches by fact-checking 2024
presidential debate and also open source the collection of claims with
corresponding labels identified in the debate. Through a combination of indexed
facts together with Dense retrieval and Index compression, we achieve up to a
10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the
classical fact-checking pipelines over large collections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECIR 2025, 15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion
  in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonduk Seo, Seunghyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query expansion is widely used in Information Retrieval (IR) to improve
search outcomes by enriching queries with additional contextual information.
Although recent Large Language Model (LLM) based methods generate
pseudo-relevant content and expanded terms via multiple prompts, they often
yield repetitive, narrow expansions that lack the diverse context needed to
retrieve all relevant information. In this paper, we introduce QA-Expand, a
novel and effective framework for query expansion. It first generates multiple
relevant questions from the initial query and subsequently produces
corresponding pseudo-answers as surrogate documents. A feedback model further
rewrites and filters these answers to ensure only the most informative
augmentations are incorporated. Extensive experiments on benchmarks such as
BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up
to 13% over state-of-the-art methods, offering a robust solution for modern
retrieval challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models
  for Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22353v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22353v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen Huang, Yong Dou, Xuhui Jiang, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has shown promising potential in
knowledge intensive question answering (QA). However, existing approaches only
consider the query itself, neither specifying the retrieval preferences for the
retrievers nor informing the generators of how to refer to the retrieved
documents for the answers, which poses a significant challenge to the QA
performance. To address these issues, we propose Rule-guided
Retrieval-Augmented Generation with LMs, which explicitly introduces rules for
in-context learning (RuleRAG-ICL) to guide retrievers to recall related
documents in the directions of rules and uniformly guide generators to reason
attributed by the same rules. Moreover, most existing RAG datasets were
constructed without considering rules and Knowledge Graphs (KGs) are recognized
as providing high-quality rules. Therefore, we construct five rule-aware RAG
benchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval
and reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL
improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of
+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,
experiments on four existing RAG datasets show RuleRAG is also effective by
offering rules in RuleQA to them, further proving the generalization of rule
guidance in RuleRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A
  Benchmark and Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13694v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13694v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yu, Mingyue Cheng, Jiqian Yang, Jie Ouyang, Yucong Luo, Chenyi Lei, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is increasingly recognized as an
effective approach to mitigating the hallucination of large language models
(LLMs) through the integration of external knowledge. While numerous efforts,
most studies focus on a single type of external knowledge source. In contrast,
most real-world applications involve diverse knowledge from various sources, a
scenario that has been relatively underexplored. The main dilemma is the lack
of a suitable dataset incorporating multiple knowledge sources and
pre-exploration of the associated issues. To address these challenges, we
standardize a benchmark dataset that combines structured and unstructured
knowledge across diverse and complementary domains. Building upon the dataset,
we identify the limitations of existing methods under such conditions.
Therefore, we develop PruningRAG, a plug-and-play RAG framework that uses
multi-granularity pruning strategies to more effectively incorporate relevant
context and mitigate the negative impact of misleading information. Extensive
experimental results demonstrate superior performance of PruningRAG and our
insightful findings are also reported. Our dataset and code are publicly
available\footnote{https://github.com/USTCAGI/PruningRAG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with
  Large Language Models for Multi-Behavior Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyi Ma, Xiaohan Li, Zezhong Fan, Kai Zhao, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sushant Kumar, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating diverse data modalities is crucial for enhancing the performance
of personalized recommendation systems. Traditional models, which often rely on
singular data sources, lack the depth needed to accurately capture the
multifaceted nature of item features and user behaviors. This paper introduces
a novel framework for multi-behavior recommendations, leveraging the fusion of
triple-modality, which is visual, textual, and graph data through alignment
with large language models (LLMs). By incorporating visual information, we
capture contextual and aesthetic item characteristics; textual data provides
insights into user interests and item features in detail; and graph data
elucidates relationships within the item-behavior heterogeneous graphs. Our
proposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs
to align and integrate these three modalities, achieving a comprehensive
representation of user behaviors. The LLM models the user's interactions
including behaviors and item features in natural languages. Initially, the LLM
is warmed up using only natural language-based prompts. We then devise the
modality fusion module based on cross-attention and self-attention mechanisms
to integrate different modalities from other models into the same embedding
space and incorporate them into an LLM. Extensive experiments demonstrate the
effectiveness of our approach in improving recommendation accuracy. Further
ablation studies validate the effectiveness of our model design and benefits of
the TMF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Data to Decisions: The Transformational Power of Machine Learning
  in Business Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kapilya Gangadharan, K. Malathi, Anoop Purandaran, Barathi Subramanian, Rathinaraja Jeyaraj, Soon Ki Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research aims to explore the impact of Machine Learning (ML) on the
evolution and efficacy of Recommendation Systems (RS), particularly in the
context of their growing significance in commercial business environments.
Methodologically, the study delves into the role of ML in crafting and refining
these systems, focusing on aspects such as data sourcing, feature engineering,
and the importance of evaluation metrics, thereby highlighting the iterative
nature of enhancing recommendation algorithms. The deployment of Recommendation
Engines (RE), driven by advanced algorithms and data analytics, is explored
across various domains, showcasing their significant impact on user experience
and decision-making processes. These engines not only streamline information
discovery and enhance collaboration but also accelerate knowledge acquisition,
proving vital in navigating the digital landscape for businesses. They
contribute significantly to sales, revenue, and the competitive edge of
enterprises by offering improved recommendations that align with individual
customer needs. The research identifies the increasing expectation of users for
a seamless, intuitive online experience, where content is personalized and
dynamically adapted to changing preferences. Future research directions include
exploring advancements in deep learning models, ethical considerations in the
deployment of RS, and addressing scalability challenges. This study emphasizes
the indispensability of comprehending and leveraging ML in RS for researchers
and practitioners, to tap into the full potential of personalized
recommendation in commercial business prospects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Embedding Spectrum for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowen Peng, Kazunari Sugiyama, Xin Liu, Tsunenori Mine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommender systems heavily rely on high-quality representations
learned from high-dimensional sparse data. While significant efforts have been
invested in designing powerful algorithms for extracting user preferences, the
factors contributing to good representations have remained relatively
unexplored. In this work, we shed light on an issue in the existing pair-wise
learning paradigm (i.e., the embedding collapse problem), that the
representations tend to span a subspace of the whole embedding space, leading
to a suboptimal solution and reducing the model capacity. Specifically,
optimization on observed interactions is equivalent to a low pass filter
causing users/items to have the same representations and resulting in a
complete collapse. While negative sampling acts as a high pass filter to
alleviate the collapse by balancing the embedding spectrum, its effectiveness
is only limited to certain losses, which still leads to an incomplete collapse.
To tackle this issue, we propose a novel method called DirectSpec, acting as a
reliable all pass filter to balance the spectrum distribution of the embeddings
during training, ensuring that users/items effectively span the entire
embedding space. Additionally, we provide a thorough analysis of DirectSpec
from a decorrelation perspective and propose an enhanced variant, DirectSpec+,
which employs self-paced gradients to optimize irrelevant samples more
effectively. Moreover, we establish a close connection between DirectSpec+ and
uniformity, demonstrating that contrastive learning (CL) can alleviate the
collapse issue by indirectly balancing the spectrum. Finally, we implement
DirectSpec and DirectSpec+ on two popular recommender models: MF and LightGCN.
Our experimental results demonstrate its effectiveness and efficiency over
competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Trans on Recommender Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-24T02:02:57.749006289Z">
            2025-02-24 02:02:57 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
