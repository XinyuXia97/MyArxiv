{"2025-02-16T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2502.06490v2","updated":"2025-02-16T08:11:09Z","published":"2025-02-10T14:08:25Z","title":"Recent Advances in Discrete Speech Tokens: A Review","summary":"  The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.\n","authors":["Yiwei Guo","Zhihan Li","Hankun Wang","Bohan Li","Chongtian Shao","Hanglei Zhang","Chenpeng Du","Xie Chen","Shujie Liu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06490v2.pdf","comment":"23 pages, 8 figures, 3 tables. Work in progress"},{"id":"http://arxiv.org/abs/2502.11184v1","updated":"2025-02-16T16:12:40Z","published":"2025-02-16T16:12:40Z","title":"Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs","summary":"  Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.\n","authors":["Wenxuan Wang","Xiaoyuan Liu","Kuiyi Gao","Jen-tse Huang","Youliang Yuan","Pinjia He","Shuai Wang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2502.11184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10999v1","updated":"2025-02-16T05:30:18Z","published":"2025-02-16T05:30:18Z","title":"ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering\n  without Font Annotations","summary":"  This work demonstrates that diffusion models can achieve font-controllable\nmultilingual text rendering using just raw images without font label\nannotations. Visual text rendering remains a significant challenge. While\nrecent methods condition diffusion on glyphs, it is impossible to retrieve\nexact font annotations from large-scale, real-world datasets, which prevents\nuser-specified font control. To address this, we propose a data-driven solution\nthat integrates the conditional diffusion model with a text segmentation model,\nutilizing segmentation masks to capture and represent fonts in pixel space in a\nself-supervised manner, thereby eliminating the need for any ground-truth\nlabels and enabling users to customize text rendering with any multilingual\nfont of their choice. The experiment provides a proof of concept of our\nalgorithm in zero-shot text and font editing across diverse fonts and\nlanguages, providing valuable insights for the community and industry toward\nachieving generalized visual text rendering.\n","authors":["Bowen Jiang","Yuan Yuan","Xinyi Bai","Zhuoqun Hao","Alyson Yin","Yaojie Hu","Wenyu Liao","Lyle Ungar","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.10999v1.pdf","comment":"This is preliminary work and code will be released at\n  github.com/bowen-upenn/ControlText"},{"id":"http://arxiv.org/abs/2405.15757v3","updated":"2025-02-16T03:01:55Z","published":"2024-05-24T17:53:06Z","title":"Looking Backward: Streaming Video-to-Video Translation with Feature\n  Banks","summary":"  This paper introduces StreamV2V, a diffusion model that achieves real-time\nstreaming video-to-video (V2V) translation with user prompts. Unlike prior V2V\nmethods using batches to process limited frames, we opt to process frames in a\nstreaming fashion, to support unlimited frames. At the heart of StreamV2V lies\na backward-looking principle that relates the present to the past. This is\nrealized by maintaining a feature bank, which archives information from past\nframes. For incoming frames, StreamV2V extends self-attention to include banked\nkeys and values and directly fuses similar past features into the output. The\nfeature bank is continually updated by merging stored and new features, making\nit compact but informative. StreamV2V stands out for its adaptability and\nefficiency, seamlessly integrating with image diffusion models without\nfine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x\nfaster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative\nmetrics and user studies confirm StreamV2V's exceptional ability to maintain\ntemporal consistency.\n","authors":["Feng Liang","Akio Kodaira","Chenfeng Xu","Masayoshi Tomizuka","Kurt Keutzer","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2405.15757v3.pdf","comment":"ICLR 2025. Project page:\n  https://jeff-liangf.github.io/projects/streamv2v"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.05803v2","updated":"2025-02-16T22:05:21Z","published":"2025-02-09T08:14:11Z","title":"FlashCheck: Exploration of Efficient Evidence Retrieval for Fast\n  Fact-Checking","summary":"  The advances in digital tools have led to the rampant spread of\nmisinformation. While fact-checking aims to combat this, manual fact-checking\nis cumbersome and not scalable. It is essential for automated fact-checking to\nbe efficient for aiding in combating misinformation in real-time and at the\nsource. Fact-checking pipelines primarily comprise a knowledge retrieval\ncomponent which extracts relevant knowledge to fact-check a claim from large\nknowledge sources like Wikipedia and a verification component. The existing\nworks primarily focus on the fact-verification part rather than evidence\nretrieval from large data collections, which often face scalability issues for\npractical applications such as live fact-checking. In this study, we address\nthis gap by exploring various methods for indexing a succinct set of factual\nstatements from large collections like Wikipedia to enhance the retrieval phase\nof the fact-checking pipeline. We also explore the impact of vector\nquantization to further improve the efficiency of pipelines that employ dense\nretrieval approaches for first-stage retrieval. We study the efficiency and\neffectiveness of the approaches on fact-checking datasets such as HoVer and\nWiCE, leveraging Wikipedia as the knowledge source. We also evaluate the\nreal-world utility of the efficient retrieval approaches by fact-checking 2024\npresidential debate and also open source the collection of claims with\ncorresponding labels identified in the debate. Through a combination of indexed\nfacts together with Dense retrieval and Index compression, we achieve up to a\n10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the\nclassical fact-checking pipelines over large collections.\n","authors":["Kevin Nanekhan","Venktesh V","Erik Martin","Henrik Vatndal","Vinay Setty","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2502.05803v2.pdf","comment":"Accepted to ECIR 2025, 15 pages"},{"id":"http://arxiv.org/abs/2502.11246v1","updated":"2025-02-16T19:46:24Z","published":"2025-02-16T19:46:24Z","title":"MemeSense: An Adaptive In-Context Framework for Social Commonsense\n  Driven Meme Moderation","summary":"  Memes present unique moderation challenges due to their subtle, multimodal\ninterplay of images, text, and social context. Standard systems relying\npredominantly on explicit textual cues often overlook harmful content\ncamouflaged by irony, symbolism, or cultural references. To address this gap,\nwe introduce MemeSense, an adaptive in-context learning framework that fuses\nsocial commonsense reasoning with visually and semantically related reference\nexamples. By encoding crucial task information into a learnable cognitive shift\nvector, MemeSense effectively balances lexical, visual, and ethical\nconsiderations, enabling precise yet context-aware meme intervention. Extensive\nevaluations on a curated set of implicitly harmful memes demonstrate that\nMemeSense substantially outperforms strong baselines, paving the way for safer\nonline communities. Code and data available at:\nhttps://github.com/sayantan11995/MemeSense\n","authors":["Sayantan Adak","Somnath Banerjee","Rajarshi Mandal","Avik Halder","Sayan Layek","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.11246v1.pdf","comment":"Code and data available at:\n  https://github.com/sayantan11995/MemeSense"},{"id":"http://arxiv.org/abs/2502.11197v1","updated":"2025-02-16T16:56:15Z","published":"2025-02-16T16:56:15Z","title":"CSP: A Simulator For Multi-Agent Ranking Competitions","summary":"  In ranking competitions, document authors compete for the highest rankings by\nmodifying their content in response to past rankings. Previous studies focused\non human participants, primarily students, in controlled settings. The rise of\ngenerative AI, particularly Large Language Models (LLMs), introduces a new\nparadigm: using LLMs as document authors. This approach addresses scalability\nconstraints in human-based competitions and reflects the growing role of\nLLM-generated content on the web-a prime example of ranking competition. We\nintroduce a highly configurable ranking competition simulator that leverages\nLLMs as document authors. It includes analytical tools to examine the resulting\ndatasets. We demonstrate its capabilities by generating multiple datasets and\nconducting an extensive analysis. Our code and datasets are publicly available\nfor research.\n","authors":["Tommy Mordo","Tomer Kordonsky","Haya Nachimovsky","Moshe Tennenholtz","Oren Kurland"],"pdf_url":"https://arxiv.org/pdf/2502.11197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11181v1","updated":"2025-02-16T15:59:50Z","published":"2025-02-16T15:59:50Z","title":"Improving Scientific Document Retrieval with Concept Coverage-based\n  Query Set Generation","summary":"  In specialized fields like the scientific domain, constructing large-scale\nhuman-annotated datasets poses a significant challenge due to the need for\ndomain expertise. Recent methods have employed large language models to\ngenerate synthetic queries, which serve as proxies for actual user queries.\nHowever, they lack control over the content generated, often resulting in\nincomplete coverage of academic concepts in documents. We introduce Concept\nCoverage-based Query set Generation (CCQGen) framework, designed to generate a\nset of queries with comprehensive coverage of the document's concepts. A key\ndistinction of CCQGen is that it adaptively adjusts the generation process\nbased on the previously generated queries. We identify concepts not\nsufficiently covered by previous queries, and leverage them as conditions for\nsubsequent query generation. This approach guides each new query to complement\nthe previous ones, aiding in a thorough understanding of the document.\nExtensive experiments demonstrate that CCQGen significantly enhances query\nquality and retrieval performance.\n","authors":["SeongKu Kang","Bowen Jin","Wonbin Kweon","Yu Zhang","Dongha Lee","Jiawei Han","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2502.11181v1.pdf","comment":"WSDM 2025"},{"id":"http://arxiv.org/abs/2502.08557v2","updated":"2025-02-16T14:24:44Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.11116v1","updated":"2025-02-16T13:23:39Z","published":"2025-02-16T13:23:39Z","title":"Gumbel Reranking: Differentiable End-to-End Reranker Optimization","summary":"  RAG systems rely on rerankers to identify relevant documents. However,\nfine-tuning these models remains challenging due to the scarcity of annotated\nquery-document pairs. Existing distillation-based approaches suffer from\ntraining-inference misalignment and fail to capture interdependencies among\ncandidate documents. To overcome these limitations, we reframe the reranking\nprocess as an attention-mask problem and propose Gumbel Reranking, an\nend-to-end training framework for rerankers aimed at minimizing the\ntraining-inference gap. In our approach, reranker optimization is reformulated\nas learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel\nTrick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end\noptimization by minimizing the overall language loss. Experiments across\nvarious settings consistently demonstrate performance gains, including a 10.4\\%\nimprovement in recall on HotpotQA for distinguishing indirectly relevant\ndocuments.\n","authors":["Siyuan Huang","Zhiyuan Ma","Jintao Du","Changhua Meng","Weiqiang Wang","Jingwen Leng","Minyi Guo","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2502.11116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11105v1","updated":"2025-02-16T12:46:34Z","published":"2025-02-16T12:46:34Z","title":"Graceful forgetting: Memory as a process","summary":"  A rational theory of memory is proposed to explain how we can accommodate\nunbounded sensory input within bounded storage space. Memory is stored as\nstatistics, organized into complex structures that are constantly summarized\nand compressed to make room for new input. This process, driven by space\nconstraints, is guided by heuristics that optimize the memory for future needs.\nSensory input is rapidly encoded as simple statistics that are more slowly\nelaborated into more abstract constructs. This theory differs from previous\naccounts of memory by (a) its reliance on statistics, (b) its use of heuristics\nto guide the choice of statistics, and (c) the emphasis on memory as a process\nthat is intensive, complex, and expensive. The theory is intended as an aid to\nmake sense of our extensive knowledge of memory, and bring us closer to an\nunderstanding of memory in functional and mechanistic terms.\n","authors":["Alain de Cheveigné"],"pdf_url":"https://arxiv.org/pdf/2502.11105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22353v3","updated":"2025-02-16T11:50:09Z","published":"2024-10-15T14:51:45Z","title":"RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models\n  for Question Answering","summary":"  Retrieval-augmented generation (RAG) has shown promising potential in\nknowledge intensive question answering (QA). However, existing approaches only\nconsider the query itself, neither specifying the retrieval preferences for the\nretrievers nor informing the generators of how to refer to the retrieved\ndocuments for the answers, which poses a significant challenge to the QA\nperformance. To address these issues, we propose Rule-guided\nRetrieval-Augmented Generation with LMs, which explicitly introduces rules for\nin-context learning (RuleRAG-ICL) to guide retrievers to recall related\ndocuments in the directions of rules and uniformly guide generators to reason\nattributed by the same rules. Moreover, most existing RAG datasets were\nconstructed without considering rules and Knowledge Graphs (KGs) are recognized\nas providing high-quality rules. Therefore, we construct five rule-aware RAG\nbenchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval\nand reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL\nimproves the retrieval quality of +89.2% in Recall@10 and answer accuracy of\n+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,\nexperiments on four existing RAG datasets show RuleRAG is also effective by\noffering rules in RuleQA to them, further proving the generalization of rule\nguidance in RuleRAG.\n","authors":["Zhongwu Chen","Chengjin Xu","Dingmin Wang","Zhen Huang","Yong Dou","Xuhui Jiang","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2410.22353v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13694v3","updated":"2025-02-16T11:07:13Z","published":"2024-09-03T03:31:37Z","title":"Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A\n  Benchmark and Empirical Study","summary":"  Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach to mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of external knowledge source. In contrast,\nmost real-world applications involve diverse knowledge from various sources, a\nscenario that has been relatively underexplored. The main dilemma is the lack\nof a suitable dataset incorporating multiple knowledge sources and\npre-exploration of the associated issues. To address these challenges, we\nstandardize a benchmark dataset that combines structured and unstructured\nknowledge across diverse and complementary domains. Building upon the dataset,\nwe identify the limitations of existing methods under such conditions.\nTherefore, we develop PruningRAG, a plug-and-play RAG framework that uses\nmulti-granularity pruning strategies to more effectively incorporate relevant\ncontext and mitigate the negative impact of misleading information. Extensive\nexperimental results demonstrate superior performance of PruningRAG and our\ninsightful findings are also reported. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}.\n","authors":["Shuo Yu","Mingyue Cheng","Jiqian Yang","Jie Ouyang","Yucong Luo","Chenyi Lei","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.13694v3.pdf","comment":"12 pages, 9 figures;"},{"id":"http://arxiv.org/abs/2410.12228v2","updated":"2025-02-16T09:41:32Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Kai Zhao","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08109v2","updated":"2025-02-16T08:19:12Z","published":"2024-02-12T22:56:18Z","title":"From Data to Decisions: The Transformational Power of Machine Learning\n  in Business Recommendations","summary":"  This research aims to explore the impact of Machine Learning (ML) on the\nevolution and efficacy of Recommendation Systems (RS), particularly in the\ncontext of their growing significance in commercial business environments.\nMethodologically, the study delves into the role of ML in crafting and refining\nthese systems, focusing on aspects such as data sourcing, feature engineering,\nand the importance of evaluation metrics, thereby highlighting the iterative\nnature of enhancing recommendation algorithms. The deployment of Recommendation\nEngines (RE), driven by advanced algorithms and data analytics, is explored\nacross various domains, showcasing their significant impact on user experience\nand decision-making processes. These engines not only streamline information\ndiscovery and enhance collaboration but also accelerate knowledge acquisition,\nproving vital in navigating the digital landscape for businesses. They\ncontribute significantly to sales, revenue, and the competitive edge of\nenterprises by offering improved recommendations that align with individual\ncustomer needs. The research identifies the increasing expectation of users for\na seamless, intuitive online experience, where content is personalized and\ndynamically adapted to changing preferences. Future research directions include\nexploring advancements in deep learning models, ethical considerations in the\ndeployment of RS, and addressing scalability challenges. This study emphasizes\nthe indispensability of comprehending and leveraging ML in RS for researchers\nand practitioners, to tap into the full potential of personalized\nrecommendation in commercial business prospects.\n","authors":["Kapilya Gangadharan","K. Malathi","Anoop Purandaran","Barathi Subramanian","Rathinaraja Jeyaraj","Soon Ki Jung"],"pdf_url":"https://arxiv.org/pdf/2402.08109v2.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.12032v2","updated":"2025-02-16T06:51:05Z","published":"2024-06-17T18:59:43Z","title":"Balancing Embedding Spectrum for Recommendation","summary":"  Modern recommender systems heavily rely on high-quality representations\nlearned from high-dimensional sparse data. While significant efforts have been\ninvested in designing powerful algorithms for extracting user preferences, the\nfactors contributing to good representations have remained relatively\nunexplored. In this work, we shed light on an issue in the existing pair-wise\nlearning paradigm (i.e., the embedding collapse problem), that the\nrepresentations tend to span a subspace of the whole embedding space, leading\nto a suboptimal solution and reducing the model capacity. Specifically,\noptimization on observed interactions is equivalent to a low pass filter\ncausing users/items to have the same representations and resulting in a\ncomplete collapse. While negative sampling acts as a high pass filter to\nalleviate the collapse by balancing the embedding spectrum, its effectiveness\nis only limited to certain losses, which still leads to an incomplete collapse.\nTo tackle this issue, we propose a novel method called DirectSpec, acting as a\nreliable all pass filter to balance the spectrum distribution of the embeddings\nduring training, ensuring that users/items effectively span the entire\nembedding space. Additionally, we provide a thorough analysis of DirectSpec\nfrom a decorrelation perspective and propose an enhanced variant, DirectSpec+,\nwhich employs self-paced gradients to optimize irrelevant samples more\neffectively. Moreover, we establish a close connection between DirectSpec+ and\nuniformity, demonstrating that contrastive learning (CL) can alleviate the\ncollapse issue by indirectly balancing the spectrum. Finally, we implement\nDirectSpec and DirectSpec+ on two popular recommender models: MF and LightGCN.\nOur experimental results demonstrate its effectiveness and efficiency over\ncompetitive baselines.\n","authors":["Shaowen Peng","Kazunari Sugiyama","Xin Liu","Tsunenori Mine"],"pdf_url":"https://arxiv.org/pdf/2406.12032v2.pdf","comment":"ACM Trans on Recommender Systems"},{"id":"http://arxiv.org/abs/2502.10990v1","updated":"2025-02-16T04:23:52Z","published":"2025-02-16T04:23:52Z","title":"FinMTEB: Finance Massive Text Embedding Benchmark","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, FinPersona-E5, using a persona-based\ndata synthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including FinPersona-E5,\nwe show three key findings: (1) performance on general-purpose benchmarks shows\nlimited correlation with financial domain tasks; (2) domain-adapted models\nconsistently outperform their general-purpose counterparts; and (3)\nsurprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated\ndense embeddings in financial Semantic Textual Similarity (STS) tasks,\nunderscoring current limitations in dense embedding techniques. Our work\nestablishes a robust evaluation framework for financial NLP applications and\nprovides crucial insights for developing domain-specific embedding models.\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10990v1.pdf","comment":"https://github.com/yixuantt/FinMTEB"},{"id":"http://arxiv.org/abs/2502.10976v1","updated":"2025-02-16T03:37:13Z","published":"2025-02-16T03:37:13Z","title":"QuOTE: Question-Oriented Text Embeddings","summary":"  We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to\nretrieval-augmented generation (RAG) systems, aimed at improving document\nrepresentation for accurate and nuanced retrieval. Unlike traditional RAG\npipelines, which rely on embedding raw text chunks, QuOTE augments chunks with\nhypothetical questions that the chunk can potentially answer, enriching the\nrepresentation space. This better aligns document embeddings with user query\nsemantics, and helps address issues such as ambiguity and context-dependent\nrelevance. Through extensive experiments across diverse benchmarks, we\ndemonstrate that QuOTE significantly enhances retrieval accuracy, including in\nmulti-hop question-answering tasks. Our findings highlight the versatility of\nquestion generation as a fundamental indexing strategy, opening new avenues for\nintegrating question generation into retrieval-based AI pipelines.\n","authors":["Andrew Neeser","Kaylen Latimer","Aadyant Khatri","Chris Latimer","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.10976v1.pdf","comment":null}]},"2025-02-17T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.16770v2","updated":"2025-02-17T18:59:56Z","published":"2024-08-29T17:59:54Z","title":"3D Whole-body Grasp Synthesis with Directional Controllability","summary":"  Synthesizing 3D whole bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Moreover, training\ndata for this task is really scarce, while capturing new data is expensive.\nRecent work goes beyond finite datasets via a divide-and-conquer approach; it\nfirst generates a \"guiding\" right-hand grasp, and then searches for bodies that\nmatch this. However, the guiding-hand synthesis lacks controllability and\nreceptacle awareness, so it likely has an implausible direction (i.e., a body\ncan't match this without penetrating the receptacle) and needs corrections\nthrough major post-processing. Moreover, the body search needs exhaustive\nsampling and is expensive. These are strong limitations. We tackle these with a\nnovel method called CWGrasp. Our key idea is that performing geometry-based\nreasoning \"early on,\" instead of \"too late,\" provides rich \"control\" signals\nfor inference. To this end, CWGrasp first samples a plausible\nreaching-direction vector (used later for both the arm and hand) from a\nprobabilistic model built via ray-casting from the object and collision\nchecking. Moreover, CWGrasp uniquely tackles both right and left-hand grasps.\nWe evaluate on the GRAB and ReplicaGrasp datasets. CWGrasp outperforms\nbaselines, at lower runtime and budget, while all components help performance.\nCode and models are available at https://gpaschalidis.github.io/cwgrasp.\n","authors":["Georgios Paschalidis","Romana Wilschut","Dimitrije Antić","Omid Taheri","Dimitrios Tzionas"],"pdf_url":"https://arxiv.org/pdf/2408.16770v2.pdf","comment":"3DV 2025"},{"id":"http://arxiv.org/abs/2502.12154v1","updated":"2025-02-17T18:59:50Z","published":"2025-02-17T18:59:50Z","title":"Diffusion Models without Classifier-free Guidance","summary":"  This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.\n","authors":["Zhicong Tang","Jianmin Bao","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2502.12154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12151v1","updated":"2025-02-17T18:59:03Z","published":"2025-02-17T18:59:03Z","title":"VoLUT: Efficient Volumetric streaming enhanced by LUT-based\n  super-resolution","summary":"  3D volumetric video provides immersive experience and is gaining traction in\ndigital media. Despite its rising popularity, the streaming of volumetric video\ncontent poses significant challenges due to the high data bandwidth\nrequirement. A natural approach to mitigate the bandwidth issue is to reduce\nthe volumetric video's data rate by downsampling the content prior to\ntransmission. The video can then be upsampled at the receiver's end using a\nsuper-resolution (SR) algorithm to reconstruct the high-resolution details.\nWhile super-resolution techniques have been extensively explored and advanced\nfor 2D video content, there is limited work on SR algorithms tailored for\nvolumetric videos.\n  To address this gap and the growing need for efficient volumetric video\nstreaming, we have developed VoLUT with a new SR algorithm specifically\ndesigned for volumetric content. Our algorithm uniquely harnesses the power of\nlookup tables (LUTs) to facilitate the efficient and accurate upscaling of\nlow-resolution volumetric data. The use of LUTs enables our algorithm to\nquickly reference precomputed high-resolution values, thereby significantly\nreducing the computational complexity and time required for upscaling. We\nfurther apply adaptive video bit rate algorithm (ABR) to dynamically determine\nthe downsampling rate according to the network condition and stream the\nselected video rate to the receiver. Compared to related work, VoLUT is the\nfirst to enable high-quality 3D SR on commodity mobile devices at line-rate.\nOur evaluation shows VoLUT can reduce bandwidth usage by 70% , boost QoE by\n36.7% for volumetric video streaming and achieve\n  3D SR speed-up with no quality compromise.\n","authors":["Chendong Wang","Anlan Zhang","Yifan Yang","Lili Qiu","Yuqing Yang","Xinyang Jiang","Feng Qian","Suman Banerjee"],"pdf_url":"https://arxiv.org/pdf/2502.12151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12148v1","updated":"2025-02-17T18:57:51Z","published":"2025-02-17T18:57:51Z","title":"HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  Generation","summary":"  The remarkable success of the autoregressive paradigm has made significant\nadvancement in Multimodal Large Language Models (MLLMs), with powerful models\nlike Show-o, Transfusion and Emu3 achieving notable progress in unified image\nunderstanding and generation. For the first time, we uncover a common\nphenomenon: the understanding capabilities of MLLMs are typically stronger than\ntheir generative capabilities, with a significant gap between the two. Building\non this insight, we propose HermesFlow, a simple yet general framework designed\nto seamlessly bridge the gap between understanding and generation in MLLMs.\nSpecifically, we take the homologous data as input to curate homologous\npreference data of both understanding and generation. Through Pair-DPO and\nself-play iterative optimization, HermesFlow effectively aligns multimodal\nunderstanding and generation using homologous preference data. Extensive\nexperiments demonstrate the significant superiority of our approach over prior\nmethods, particularly in narrowing the gap between multimodal understanding and\ngeneration. These findings highlight the potential of HermesFlow as a general\nalignment framework for next-generation multimodal foundation models. Code:\nhttps://github.com/Gen-Verse/HermesFlow\n","authors":["Ling Yang","Xinchen Zhang","Ye Tian","Chenming Shang","Minghao Xu","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2502.12148v1.pdf","comment":"Code: https://github.com/Gen-Verse/HermesFlow"},{"id":"http://arxiv.org/abs/2502.12146v1","updated":"2025-02-17T18:57:26Z","published":"2025-02-17T18:57:26Z","title":"Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising\n  Trajectory Sharpening","summary":"  We propose Diffusion-Sharpening, a fine-tuning approach that enhances\ndownstream alignment by optimizing sampling trajectories. Existing RL-based\nfine-tuning methods focus on single training timesteps and neglect\ntrajectory-level alignment, while recent sampling trajectory optimization\nmethods incur significant inference NFE costs. Diffusion-Sharpening overcomes\nthis by using a path integral framework to select optimal trajectories during\ntraining, leveraging reward feedback, and amortizing inference costs. Our\nmethod demonstrates superior training efficiency with faster convergence, and\nbest inference efficiency without requiring additional NFEs. Extensive\nexperiments show that Diffusion-Sharpening outperforms RL-based fine-tuning\nmethods (e.g., Diffusion-DPO) and sampling trajectory optimization methods\n(e.g., Inference Scaling) across diverse metrics including text alignment,\ncompositional capabilities, and human preferences, offering a scalable and\nefficient solution for future diffusion model fine-tuning. Code:\nhttps://github.com/Gen-Verse/Diffusion-Sharpening\n","authors":["Ye Tian","Ling Yang","Xinchen Zhang","Yunhai Tong","Mengdi Wang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2502.12146v1.pdf","comment":"Code: https://github.com/Gen-Verse/Diffusion-Sharpening"},{"id":"http://arxiv.org/abs/2502.12138v1","updated":"2025-02-17T18:54:05Z","published":"2025-02-17T18:54:05Z","title":"FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views","summary":"  We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/\n","authors":["Shangzhan Zhang","Jianyuan Wang","Yinghao Xu","Nan Xue","Christian Rupprecht","Xiaowei Zhou","Yujun Shen","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2502.12138v1.pdf","comment":"8 pages. Website: https://zhanghe3z.github.io/FLARE/"},{"id":"http://arxiv.org/abs/2502.12119v1","updated":"2025-02-17T18:43:41Z","published":"2025-02-17T18:43:41Z","title":"PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection","summary":"  Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.\n","authors":["Jinhe Bi","Yifan Wang","Danqi Yan","Xun Xiao","Artur Hecker","Volker Tresp","Yunpu Ma"],"pdf_url":"https://arxiv.org/pdf/2502.12119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12113v1","updated":"2025-02-17T18:38:27Z","published":"2025-02-17T18:38:27Z","title":"A Monocular Event-Camera Motion Capture System","summary":"  Motion capture systems are a widespread tool in research to record\nground-truth poses of objects. Commercial systems use reflective markers\nattached to the object and then triangulate pose of the object from multiple\ncamera views. Consequently, the object must be visible to multiple cameras\nwhich makes such multi-view motion capture systems unsuited for deployments in\nnarrow, confined spaces (e.g. ballast tanks of ships). In this technical report\nwe describe a monocular event-camera motion capture system which overcomes this\nlimitation and is ideally suited for narrow spaces. Instead of passive markers\nit relies on active, blinking LED markers such that each marker can be uniquely\nidentified from the blinking frequency. The markers are placed at known\nlocations on the tracking object. We then solve the PnP (perspective-n-points)\nproblem to obtain the position and orientation of the object. The developed\nsystem has millimeter accuracy, millisecond latency and we demonstrate that its\nstate estimate can be used to fly a small, agile quadrotor.\n","authors":["Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2502.12113v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2412.04453v2","updated":"2025-02-17T18:27:27Z","published":"2024-12-05T18:58:17Z","title":"NaVILA: Legged Robot Vision-Language-Action Model for Navigation","summary":"  This paper proposes to solve the problem of Vision-and-Language Navigation\nwith legged robots, which not only provides a flexible way for humans to\ncommand but also allows the robot to navigate through more challenging and\ncluttered scenes. However, it is non-trivial to translate human language\ninstructions all the way to low-level leg joint actions. We propose NaVILA, a\n2-level framework that unifies a Vision-Language-Action model (VLA) with\nlocomotion skills. Instead of directly predicting low-level actions from VLA,\nNaVILA first generates mid-level actions with spatial information in the form\nof language, (e.g., \"moving forward 75cm\"), which serves as an input for a\nvisual locomotion RL policy for execution. NaVILA substantially improves\nprevious approaches on existing benchmarks. The same advantages are\ndemonstrated in our newly developed benchmarks with IsaacLab, featuring more\nrealistic scenes, low-level controls, and real-world robot experiments. We show\nmore results at https://navila-bot.github.io/\n","authors":["An-Chieh Cheng","Yandong Ji","Zhaojing Yang","Zaitian Gongye","Xueyan Zou","Jan Kautz","Erdem Bıyık","Hongxu Yin","Sifei Liu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.04453v2.pdf","comment":"Website: https://navila-bot.github.io/"},{"id":"http://arxiv.org/abs/2404.01438v2","updated":"2025-02-17T18:22:03Z","published":"2024-04-01T19:22:43Z","title":"Generation and Detection of Sign Language Deepfakes - A Linguistic and\n  Visual Analysis","summary":"  This research explores the positive application of deepfake technology for\nupper body generation, specifically sign language for the Deaf and Hard of\nHearing (DHoH) community. Given the complexity of sign language and the\nscarcity of experts, the generated videos are vetted by a sign language expert\nfor accuracy. We construct a reliable deepfake dataset, evaluating its\ntechnical and visual credibility using computer vision and natural language\nprocessing models. The dataset, consisting of over 1200 videos featuring both\nseen and unseen individuals, is also used to detect deepfake videos targeting\nvulnerable individuals. Expert annotations confirm that the generated videos\nare comparable to real sign language content. Linguistic analysis, using\ntextual similarity scores and interpreter evaluations, shows that the\ninterpretation of generated videos is at least 90% similar to authentic sign\nlanguage. Visual analysis demonstrates that convincingly realistic deepfakes\ncan be produced, even for new subjects. Using a pose/style transfer model, we\npay close attention to detail, ensuring hand movements are accurate and align\nwith the driving video. We also apply machine learning algorithms to establish\na baseline for deepfake detection on this dataset, contributing to the\ndetection of fraudulent sign language videos.\n","authors":["Shahzeb Naeem","Muhammad Riyyan Khan","Usman Tariq","Abhinav Dhall","Carlos Ivan Colon","Hasan Al-Nashash"],"pdf_url":"https://arxiv.org/pdf/2404.01438v2.pdf","comment":"10 pages, 11 figures, IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL\n  SYSTEM"},{"id":"http://arxiv.org/abs/2502.12096v1","updated":"2025-02-17T18:14:18Z","published":"2025-02-17T18:14:18Z","title":"Token Communications: A Unified Framework for Cross-modal Context-aware\n  Semantic Communications","summary":"  In this paper, we introduce token communications (TokCom), a unified\nframework to leverage cross-modal context information in generative semantic\ncommunications (GenSC). TokCom is a new paradigm, motivated by the recent\nsuccess of generative foundation models and multimodal large language models\n(GFM/MLLMs), where the communication units are tokens, enabling efficient\ntransformer-based token processing at the transmitter and receiver. In this\npaper, we introduce the potential opportunities and challenges of leveraging\ncontext in GenSC, explore how to integrate GFM/MLLMs-based token processing\ninto semantic communication systems to leverage cross-modal context\neffectively, present the key principles for efficient TokCom at various layers\nin future wireless networks. We demonstrate the corresponding TokCom benefits\nin a GenSC setup for image, leveraging cross-modal context information, which\nincreases the bandwidth efficiency by 70.8% with negligible loss of\nsemantic/perceptual quality. Finally, the potential research directions are\nidentified to facilitate adoption of TokCom in future wireless networks.\n","authors":["Li Qiao","Mahdi Boloursaz Mashhadi","Zhen Gao","Rahim Tafazolli","Mehdi Bennis","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12095v1","updated":"2025-02-17T18:13:42Z","published":"2025-02-17T18:13:42Z","title":"Descriminative-Generative Custom Tokens for Vision-Language Models","summary":"  This paper explores the possibility of learning custom tokens for\nrepresenting new concepts in Vision-Language Models (VLMs). Our aim is to learn\ntokens that can be effective for both discriminative and generative tasks while\ncomposing well with words to form new input queries. The targeted concept is\nspecified in terms of a small set of images and a parent concept described\nusing text. We operate on CLIP text features and propose to use a combination\nof a textual inversion loss and a classification loss to ensure that text\nfeatures of the learned token are aligned with image features of the concept in\nthe CLIP embedding space. We restrict the learned token to a low-dimensional\nsubspace spanned by tokens for attributes that are appropriate for the given\nsuper-class. These modifications improve the quality of compositions of the\nlearned token with natural language for generating new scenes. Further, we show\nthat learned custom tokens can be used to form queries for text-to-image\nretrieval task, and also have the important benefit that composite queries can\nbe visualized to ensure that the desired concept is faithfully encoded. Based\non this, we introduce the method of Generation Aided Image Retrieval, where the\nquery is modified at inference time to better suit the search intent. On the\nDeepFashion2 dataset, our method improves Mean Reciprocal Retrieval (MRR) over\nrelevant baselines by 7%.\n","authors":["Pramuditha Perera","Matthew Trager","Luca Zancato","Alessandro Achille","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2502.12095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18057v3","updated":"2025-02-17T18:08:17Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for removing private or hazardous\ninformation from deep learning models. While MU has advanced significantly in\nunimodal (text or vision) settings, multimodal unlearning (MMU) remains\nunderexplored due to the lack of open benchmarks for evaluating cross-modal\ndata removal. To address this gap, we introduce CLEAR, the first open-source\nbenchmark designed specifically for MMU. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We conduct a comprehensive\nanalysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four\nevaluation sets, demonstrating that jointly unlearning both modalities\noutperforms single-modality approaches. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12081v1","updated":"2025-02-17T17:55:55Z","published":"2025-02-17T17:55:55Z","title":"Unhackable Temporal Rewarding for Scalable Video MLLMs","summary":"  In the pursuit of superior video-processing MLLMs, we have encountered a\nperplexing paradox: the \"anti-scaling law\", where more data and larger models\nlead to worse performance. This study unmasks the culprit: \"temporal hacking\",\na phenomenon where models shortcut by fixating on select frames, missing the\nfull video narrative. In this work, we systematically establish a comprehensive\ntheory of temporal hacking, defining it from a reinforcement learning\nperspective, introducing the Temporal Perplexity (TPL) score to assess this\nmisalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework\nto mitigate the temporal hacking. Both theoretically and empirically, TPL\nproves to be a reliable indicator of temporal modeling quality, correlating\nstrongly with frame activation patterns. Extensive experiments reveal that UTR\nnot only counters temporal hacking but significantly elevates video\ncomprehension capabilities. This work not only advances video-AI systems but\nalso illuminates the critical importance of aligning proxy rewards with true\nobjectives in MLLM development.\n","authors":["En Yu","Kangheng Lin","Liang Zhao","Yana Wei","Zining Zhu","Haoran Wei","Jianjian Sun","Zheng Ge","Xiangyu Zhang","Jingyu Wang","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2502.12081v1.pdf","comment":"Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/"},{"id":"http://arxiv.org/abs/2502.12080v1","updated":"2025-02-17T17:55:27Z","published":"2025-02-17T17:55:27Z","title":"HumanGif: Single-View Human Diffusion with Generative Prior","summary":"  While previous single-view-based 3D human reconstruction methods made\nsignificant progress in novel view synthesis, it remains a challenge to\nsynthesize both view-consistent and pose-consistent results for animatable\nhuman avatars from a single image input. Motivated by the success of 2D\ncharacter animation, we propose <strong>HumanGif</strong>, a single-view human\ndiffusion model with generative prior. Specifically, we formulate the\nsingle-view-based 3D human novel view and pose synthesis as a\nsingle-view-conditioned human diffusion process, utilizing generative priors\nfrom foundational diffusion models. To ensure fine-grained and consistent novel\nview and pose synthesis, we introduce a Human NeRF module in HumanGif to learn\nspatially aligned features from the input image, implicitly capturing the\nrelative camera and human pose transformation. Furthermore, we introduce an\nimage-level loss during optimization to bridge the gap between latent and image\nspaces in diffusion models. Extensive experiments on RenderPeople and\nDNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual\nperformance, with better generalizability for novel view and pose synthesis.\n","authors":["Shoukang Hu","Takuya Narihira","Kazumi Fukuda","Ryosuke Sawata","Takashi Shibuya","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2502.12080v1.pdf","comment":"Project page: https://skhu101.github.io/HumanGif/"},{"id":"http://arxiv.org/abs/2412.09115v2","updated":"2025-02-17T17:50:21Z","published":"2024-12-12T09:49:16Z","title":"Vision CNNs trained to estimate spatial latents learned similar\n  ventral-stream-aligned representations","summary":"  Studies of the functional role of the primate ventral visual stream have\ntraditionally focused on object categorization, often ignoring -- despite much\nprior evidence -- its role in estimating \"spatial\" latents such as object\nposition and pose. Most leading ventral stream models are derived by optimizing\nnetworks for object categorization, which seems to imply that the ventral\nstream is also derived under such an objective. Here, we explore an alternative\nhypothesis: Might the ventral stream be optimized for estimating spatial\nlatents? And a closely related question: How different -- if at all -- are\nrepresentations learned from spatial latent estimation compared to\ncategorization? To ask these questions, we leveraged synthetic image datasets\ngenerated by a 3D graphic engine and trained convolutional neural networks\n(CNNs) to estimate different combinations of spatial and category latents. We\nfound that models trained to estimate just a few spatial latents achieve neural\nalignment scores comparable to those trained on hundreds of categories, and the\nspatial latent performance of models strongly correlates with their neural\nalignment. Spatial latent and category-trained models have very similar -- but\nnot identical -- internal representations, especially in their early and middle\nlayers. We provide evidence that this convergence is partly driven by\nnon-target latent variability in the training data, which facilitates the\nimplicit learning of representations of those non-target latents. Taken\ntogether, these results suggest that many training objectives, such as spatial\nlatents, can lead to similar models aligned neurally with the ventral stream.\nThus, one should not assume that the ventral stream is optimized for object\ncategorization only. As a field, we need to continue to sharpen our measures of\ncomparing models to brains to better understand the functional roles of the\nventral stream.\n","authors":["Yudi Xie","Weichen Huang","Esther Alter","Jeremy Schwartz","Joshua B. Tenenbaum","James J. DiCarlo"],"pdf_url":"https://arxiv.org/pdf/2412.09115v2.pdf","comment":"30 pages, 21 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2405.01474v3","updated":"2025-02-17T17:24:42Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of the capabilities of these models when presented\nwith images and captions containing figurative meaning, such as metaphors or\nhumor. To close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present in the image, in the caption, or both. Using a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning (hallucination and incomplete or unsound reasoning) across classes of\nmodels via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v3.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.09838v2","updated":"2025-02-17T17:17:44Z","published":"2025-02-14T00:42:36Z","title":"HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation","summary":"  We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.\n","authors":["Tianwei Lin","Wenqiao Zhang","Sijing Li","Yuqian Yuan","Binhe Yu","Haoyuan Li","Wanggui He","Hao Jiang","Mengze Li","Xiaohui Song","Siliang Tang","Jun Xiao","Hui Lin","Yueting Zhuang","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2502.09838v2.pdf","comment":"Comments: added project page"},{"id":"http://arxiv.org/abs/2502.12027v1","updated":"2025-02-17T16:59:37Z","published":"2025-02-17T16:59:37Z","title":"Enhancing Transparent Object Pose Estimation: A Fusion of GDR-Net and\n  Edge Detection","summary":"  Object pose estimation of transparent objects remains a challenging task in\nthe field of robot vision due to the immense influence of lighting, background,\nand reflections. However, the edges of clear objects have the highest contrast,\nwhich leads to stable and prominent features. We propose a novel approach by\nincorporating edge detection in a pre-processing step for the tasks of object\ndetection and object pose estimation. We conducted experiments to investigate\nthe effect of edge detectors on transparent objects. We examine the performance\nof the state-of-the-art 6D object pose estimation pipeline GDR-Net and the\nobject detector YOLOX when applying different edge detectors as pre-processing\nsteps (i.e., Canny edge detection with and without color information, and\nholistically-nested edges (HED)). We evaluate the physically-based rendered\ndataset Trans6D-32 K of transparent objects with parameters proposed by the BOP\nChallenge. Our results indicate that applying edge detection as a\npre-processing enhances performance for certain objects.\n","authors":["Tessa Pulli","Peter Hönig","Stefan Thalhammer","Matthias Hirschmanner","Markus Vincze"],"pdf_url":"https://arxiv.org/pdf/2502.12027v1.pdf","comment":"accepted at First Austrian Symposium on AI, Robotics, and Vision\n  (AIROV 2024)"},{"id":"http://arxiv.org/abs/2501.18592v3","updated":"2025-02-17T16:54:39Z","published":"2025-01-30T18:59:36Z","title":"Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models","summary":"  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n","authors":["Hao Dong","Moru Liu","Kaiyang Zhou","Eleni Chatzi","Juho Kannala","Cyrill Stachniss","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.18592v3.pdf","comment":"Project page:\n  https://github.com/donghao51/Awesome-Multimodal-Adaptation"},{"id":"http://arxiv.org/abs/2502.12003v1","updated":"2025-02-17T16:41:46Z","published":"2025-02-17T16:41:46Z","title":"Predicting Next-Day Wildfire Spread with Time Series and Attention","summary":"  Recent research has demonstrated the potential of deep neural networks (DNNs)\nto accurately predict next-day wildfire spread, based upon the current extent\nof a fire and geospatial rasters of influential environmental covariates e.g.,\nvegetation, topography, climate, and weather. In this work, we investigate a\nrecent transformer-based model, termed the SwinUnet, for next-day wildfire\nprediction. We benchmark Swin-based models against several current\nstate-of-the-art models on WildfireSpreadTS (WFTS), a large public benchmark\ndataset of historical wildfire events. We consider two next-day fire prediction\nscenarios: when the model is given input of (i) a single previous day of data,\nor (ii) five previous days of data. We find that, with the proper\nmodifications, SwinUnet achieves state-of-the-art accuracy on next-day\nprediction for both the single-day and multi-day scenarios. SwinUnet's success\ndepends heavily upon utilizing pre-trained weights from ImageNet. Consistent\nwith prior work, we also found that models with multi-day-input always\noutperformed models with single-day input.\n","authors":["Saad Lahrichi","Jesse Johnson","Jordan Malof"],"pdf_url":"https://arxiv.org/pdf/2502.12003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12002v1","updated":"2025-02-17T16:40:23Z","published":"2025-02-17T16:40:23Z","title":"NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis\n  with Differential Digital Signal Processing","summary":"  Recent advancements in visual speech recognition (VSR) have promoted progress\nin lip-to-speech synthesis, where pre-trained VSR models enhance the\nintelligibility of synthesized speech by providing valuable semantic\ninformation. The success achieved by cascade frameworks, which combine\npseudo-VSR with pseudo-text-to-speech (TTS) or implicitly utilize the\ntranscribed text, highlights the benefits of leveraging VSR models. However,\nthese methods typically rely on mel-spectrograms as an intermediate\nrepresentation, which may introduce a key bottleneck: the domain gap between\nsynthetic mel-spectrograms, generated from inherently error-prone lip-to-speech\nmappings, and real mel-spectrograms used to train vocoders. This mismatch\ninevitably degrades synthesis quality. To bridge this gap, we propose Natural\nLip-to-Speech (NaturalL2S), an end-to-end framework integrating acoustic\ninductive biases with differentiable speech generation components.\nSpecifically, we introduce a fundamental frequency (F0) predictor to capture\nprosodic variations in synthesized speech. The predicted F0 then drives a\nDifferentiable Digital Signal Processing (DDSP) synthesizer to generate a\ncoarse signal which serves as prior information for subsequent speech\nsynthesis. Additionally, instead of relying on a reference speaker embedding as\nan auxiliary input, our approach achieves satisfactory performance on speaker\nsimilarity without explicitly modelling speaker characteristics. Both objective\nand subjective evaluation results demonstrate that NaturalL2S can effectively\nenhance the quality of the synthesized speech when compared to state-of-the-art\nmethods. Our demonstration page is accessible at\nhttps://yifan-liang.github.io/NaturalL2S/.\n","authors":["Yifan Liang","Fangkun Liu","Andong Li","Xiaodong Li","Chengshi Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.12002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09278v2","updated":"2025-02-17T16:37:49Z","published":"2025-02-13T12:49:25Z","title":"ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View\n  Gaussian Optimization","summary":"  Recent advances in diffusion models have significantly improved 3D\ngeneration, enabling the use of assets generated from an image for embodied AI\nsimulations. However, the one-to-many nature of the image-to-3D problem limits\ntheir use due to inconsistent content and quality across views. Previous models\noptimize a 3D model by sampling views from a view-conditioned diffusion prior,\nbut diffusion models cannot guarantee view consistency. Instead, we present\nConsistentDreamer, where we first generate a set of fixed multi-view prior\nimages and sample random views between them with another diffusion model\nthrough a score distillation sampling (SDS) loss. Thereby, we limit the\ndiscrepancies between the views guided by the SDS loss and ensure a consistent\nrough shape. In each iteration, we also use our generated multi-view prior\nimages for fine-detail reconstruction. To balance between the rough shape and\nthe fine-detail optimizations, we introduce dynamic task-dependent weights\nbased on homoscedastic uncertainty, updated automatically in each iteration.\nAdditionally, we employ opacity, depth distortion, and normal alignment losses\nto refine the surface for mesh extraction. Our method ensures better view\nconsistency and visual quality compared to the state-of-the-art.\n","authors":["Onat Şahin","Mohammad Altillawi","George Eskandar","Carlos Carbone","Ziyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09278v2.pdf","comment":"Manuscript accepted by Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2502.11993v1","updated":"2025-02-17T16:33:59Z","published":"2025-02-17T16:33:59Z","title":"MultiFlow: A unified deep learning framework for multi-vessel\n  classification, segmentation and clustering of phase-contrast MRI validated\n  on a multi-site single ventricle patient cohort","summary":"  This study presents a unified deep learning (DL) framework, MultiFlowSeg, for\nclassification and segmentation of velocity-encoded phase-contrast magnetic\nresonance imaging data, and MultiFlowDTC for temporal clustering of flow\nphenotypes. Applied to the FORCE registry of Fontan procedure patients,\nMultiFlowSeg achieved 100% classification accuracy for the aorta, SVC, and IVC,\nand 94% for the LPA and RPA. It demonstrated robust segmentation with a median\nDice score of 0.91 (IQR: 0.86-0.93). The automated pipeline processed registry\ndata, achieving high segmentation success despite challenges like poor image\nquality and dextrocardia. Temporal clustering identified five distinct patient\nsubgroups, with significant differences in clinical outcomes, including\nejection fraction, exercise tolerance, liver disease, and mortality. These\nresults demonstrate the potential of combining DL and time-varying flow data\nfor improved CHD prognosis and personalized care.\n","authors":["Tina Yao","Nicole St. Clair","Gabriel F. Miller","FORCE Investigators","Jennifer A. Steeden","Rahul H. Rathod","Vivek Muthurangu"],"pdf_url":"https://arxiv.org/pdf/2502.11993v1.pdf","comment":"6 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2502.11992v1","updated":"2025-02-17T16:33:33Z","published":"2025-02-17T16:33:33Z","title":"On the Logic Elements Associated with Round-Off Errors and Gaussian Blur\n  in Image Registration: A Simple Case of Commingling","summary":"  Discrete image registration can be a strategy to reconstruct signals from\nsamples corrupted by blur and noise. We examine superresolution and discrete\nimage registration for one-dimensional spatially-limited piecewise constant\nfunctions which are subject to blur which is Gaussian or a mixture of Gaussians\nas well as to round-off errors. Previous approaches address the signal recovery\nproblem as an optimization problem. We focus on a regime with low blur and\nsuggest that the operations of blur, sampling, and quantization are not unlike\nthe operation of a computer program and have an abstraction that can be studied\nwith a type of logic. When the minimum distance between discontinuity points is\nbetween $1.5$ and 2 times the sampling interval, we can encounter the simplest\nform of a type of interference between discontinuity points that we call\n``commingling.'' We describe a way to reason about two sets of samples of the\nsame signal that will often result in the correct recovery of signal\namplitudes. We also discuss ways to estimate bounds on the distances between\ndiscontinuity points.\n","authors":["Serap A. Savari"],"pdf_url":"https://arxiv.org/pdf/2502.11992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11989v1","updated":"2025-02-17T16:28:15Z","published":"2025-02-17T16:28:15Z","title":"Characterizing Photorealism and Artifacts in Diffusion Model-Generated\n  Images","summary":"  Diffusion model-generated images can appear indistinguishable from authentic\nphotographs, but these images often contain artifacts and implausibilities that\nreveal their AI-generated provenance. Given the challenge to public trust in\nmedia posed by photorealistic AI-generated images, we conducted a large-scale\nexperiment measuring human detection accuracy on 450 diffusion-model generated\nimages and 149 real images. Based on collecting 749,828 observations and 34,675\ncomments from 50,444 participants, we find that scene complexity of an image,\nartifact types within an image, display time of an image, and human curation of\nAI-generated images all play significant roles in how accurately people\ndistinguish real from AI-generated images. Additionally, we propose a taxonomy\ncharacterizing artifacts often appearing in images generated by diffusion\nmodels. Our empirical observations and taxonomy offer nuanced insights into the\ncapabilities and limitations of diffusion models to generate photorealistic\nimages in 2024.\n","authors":["Negar Kamali","Karyn Nakamura","Aakriti Kumar","Angelos Chatzimparmpas","Jessica Hullman","Matthew Groh"],"pdf_url":"https://arxiv.org/pdf/2502.11989v1.pdf","comment":"26 pages, 24 Figures, Accepted by ACM CHI 2025"},{"id":"http://arxiv.org/abs/2502.11974v1","updated":"2025-02-17T16:20:48Z","published":"2025-02-17T16:20:48Z","title":"Image Inversion: A Survey from GANs to Diffusion and Beyond","summary":"  Image inversion is a fundamental task in generative models, aiming to map\nimages back to their latent representations to enable downstream applications\nsuch as editing, restoration, and style transfer. This paper provides a\ncomprehensive review of the latest advancements in image inversion techniques,\nfocusing on two main paradigms: Generative Adversarial Network (GAN) inversion\nand diffusion model inversion. We categorize these techniques based on their\noptimization methods. For GAN inversion, we systematically classify existing\nmethods into encoder-based approaches, latent optimization approaches, and\nhybrid approaches, analyzing their theoretical foundations, technical\ninnovations, and practical trade-offs. For diffusion model inversion, we\nexplore training-free strategies, fine-tuning methods, and the design of\nadditional trainable modules, highlighting their unique advantages and\nlimitations. Additionally, we discuss several popular downstream applications\nand emerging applications beyond image tasks, identifying current challenges\nand future research directions. By synthesizing the latest developments, this\npaper aims to provide researchers and practitioners with a valuable reference\nresource, promoting further advancements in the field of image inversion. We\nkeep track of the latest works at https://github.com/RyanChenYN/ImageInversion\n","authors":["Yinan Chen","Jiangning Zhang","Yali Bi","Xiaobin Hu","Teng Hu","Zhucun Xue","Ran Yi","Yong Liu","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2502.11974v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.11971v1","updated":"2025-02-17T16:18:57Z","published":"2025-02-17T16:18:57Z","title":"Robust 6DoF Pose Tracking Considering Contour and Interior\n  Correspondence Uncertainty for AR Assembly Guidance","summary":"  Augmented reality assembly guidance is essential for intelligent\nmanufacturing and medical applications, requiring continuous measurement of the\n6DoF poses of manipulated objects. Although current tracking methods have made\nsignificant advancements in accuracy and efficiency, they still face challenges\nin robustness when dealing with cluttered backgrounds, rotationally symmetric\nobjects, and noisy sequences. In this paper, we first propose a robust\ncontour-based pose tracking method that addresses error-prone contour\ncorrespondences and improves noise tolerance. It utilizes a fan-shaped search\nstrategy to refine correspondences and models local contour shape and noise\nuncertainty as mixed probability distribution, resulting in a highly robust\ncontour energy function. Secondly, we introduce a CPU-only strategy to better\ntrack rotationally symmetric objects and assist the contour-based method in\novercoming local minima by exploring sparse interior correspondences. This is\nachieved by pre-sampling interior points from sparse viewpoint templates\noffline and using the DIS optical flow algorithm to compute their\ncorrespondences during tracking. Finally, we formulate a unified energy\nfunction to fuse contour and interior information, which is solvable using a\nre-weighted least squares algorithm. Experiments on public datasets and real\nscenarios demonstrate that our method significantly outperforms\nstate-of-the-art monocular tracking methods and can achieve more than 100 FPS\nusing only a CPU.\n","authors":["Jixiang Chen","Jing Chen","Kai Liu","Haochen Chang","Shanfeng Fu","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11971v1.pdf","comment":"Submitted to IEEE Transactions on Instrumentation and Measurement"},{"id":"http://arxiv.org/abs/2502.11969v1","updated":"2025-02-17T16:18:07Z","published":"2025-02-17T16:18:07Z","title":"Learning Generalizable Prompt for CLIP with Class Similarity Knowledge","summary":"  In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.\n","authors":["Sehun Jung","Hyang-won Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19353v2","updated":"2025-02-17T16:11:44Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023","summary":"  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v2.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.11955v1","updated":"2025-02-17T16:05:31Z","published":"2025-02-17T16:05:31Z","title":"pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM","summary":"  pySLAM is an open-source Python framework for Visual SLAM, supporting\nmonocular, stereo, and RGB-D cameras. It provides a flexible interface for\nintegrating both classical and modern local features, making it adaptable to\nvarious SLAM tasks. The framework includes different loop closure methods, a\nvolumetric reconstruction pipeline, and support for depth prediction models.\nAdditionally, it offers a suite of tools for visual odometry and SLAM\napplications. Designed for both beginners and experienced researchers, pySLAM\nencourages community contributions, fostering collaborative development in the\nfield of Visual SLAM.\n","authors":["Luigi Freda"],"pdf_url":"https://arxiv.org/pdf/2502.11955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11925v1","updated":"2025-02-17T15:35:36Z","published":"2025-02-17T15:35:36Z","title":"GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs","summary":"  The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.\n","authors":["Yi Fang","Bowen Jin","Jiacheng Shen","Sirui Ding","Qiaoyu Tan","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2502.11925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19651v2","updated":"2025-02-17T15:29:40Z","published":"2024-07-29T02:32:44Z","title":"Bridging Compressed Image Latents and Multimodal Large Language Models","summary":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n","authors":["Chia-Hao Kao","Cheng Chien","Yu-Jen Tseng","Yi-Hsin Chen","Alessandro Gnutti","Shao-Yuan Lo","Wen-Hsiao Peng","Riccardo Leonardi"],"pdf_url":"https://arxiv.org/pdf/2407.19651v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11897v1","updated":"2025-02-17T15:22:31Z","published":"2025-02-17T15:22:31Z","title":"DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation","summary":"  In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a\ntraining-free paradigm that can make use of adaptive temporal compression in\nlatent space. While existing video generative models apply fixed compression\nrates via pretrained VAE, we observe that real-world video content exhibits\nsubstantial temporal non-uniformity, with high-motion segments containing more\ninformation than static scenes. Based on this insight, DLFR-VAE dynamically\nadjusts the latent frame rate according to the content complexity.\nSpecifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent\nFrame Rate Scheduler that partitions videos into temporal chunks and adaptively\ndetermines optimal frame rates based on information-theoretic content\ncomplexity, and (2) A training-free adaptation mechanism that transforms\npretrained VAE architectures into a dynamic VAE that can process features with\nvariable frame rates. Our simple but effective DLFR-VAE can function as a\nplug-and-play module, seamlessly integrating with existing video generation\nmodels and accelerating the video generation process.\n","authors":["Zhihang Yuan","Siyuan Wang","Rui Xie","Hanling Zhang","Tongcheng Fang","Yuzhang Shang","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11891v1","updated":"2025-02-17T15:17:08Z","published":"2025-02-17T15:17:08Z","title":"From Open-Vocabulary to Vocabulary-Free Semantic Segmentation","summary":"  Open-vocabulary semantic segmentation enables models to identify novel object\ncategories beyond their training data. While this flexibility represents a\nsignificant advancement, current approaches still rely on manually specified\nclass names as input, creating an inherent bottleneck in real-world\napplications. This work proposes a Vocabulary-Free Semantic Segmentation\npipeline, eliminating the need for predefined class vocabularies. Specifically,\nwe address the chicken-and-egg problem where users need knowledge of all\npotential objects within a scene to identify them, yet the purpose of\nsegmentation is often to discover these objects. The proposed approach\nleverages Vision-Language Models to automatically recognize objects and\ngenerate appropriate class names, aiming to solve the challenge of class\nspecification and naming quality. Through extensive experiments on several\npublic datasets, we highlight the crucial role of the text encoder in model\nperformance, particularly when the image text classes are paired with generated\ndescriptions. Despite the challenges introduced by the sensitivity of the\nsegmentation text encoder to false negatives within the class tagging process,\nwhich adds complexity to the task, we demonstrate that our fully automated\npipeline significantly enhances vocabulary-free segmentation accuracy across\ndiverse real-world scenarios.\n","authors":["Klara Reichard","Giulia Rizzoli","Stefano Gasperini","Lukas Hoyer","Pietro Zanuttigh","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2502.11891v1.pdf","comment":"Submitted to: Pattern Recognition Letters, Klara Reichard and Giulia\n  Rizzoli equally contributed to this work"},{"id":"http://arxiv.org/abs/2501.15369v2","updated":"2025-02-17T15:09:31Z","published":"2025-01-26T02:34:58Z","title":"iFormer: Integrating ConvNet and Transformer for Mobile Application","summary":"  We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\n\\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.\n","authors":["Chuanyang Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.15369v2.pdf","comment":"Accepted to ICLR 2025. Code:\n  https://github.com/ChuanyangZheng/iFormer"},{"id":"http://arxiv.org/abs/2403.16998v3","updated":"2025-02-17T14:58:31Z","published":"2024-03-25T17:59:09Z","title":"Understanding Long Videos with Multimodal Language Models","summary":"  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n","authors":["Kanchana Ranasinghe","Xiang Li","Kumara Kahatapitiya","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2403.16998v3.pdf","comment":"Code available at https://github.com/kahnchana/mvu"},{"id":"http://arxiv.org/abs/2502.11864v1","updated":"2025-02-17T14:56:25Z","published":"2025-02-17T14:56:25Z","title":"Does Knowledge About Perceptual Uncertainty Help an Agent in Automated\n  Driving?","summary":"  Agents in real-world scenarios like automated driving deal with uncertainty\nin their environment, in particular due to perceptual uncertainty. Although,\nreinforcement learning is dedicated to autonomous decision-making under\nuncertainty these algorithms are typically not informed about the uncertainty\ncurrently contained in their environment. On the other hand, uncertainty\nestimation for perception itself is typically directly evaluated in the\nperception domain, e.g., in terms of false positive detection rates or\ncalibration errors based on camera images. Its use for deciding on\ngoal-oriented actions remains largely unstudied. In this paper, we investigate\nhow an agent's behavior is influenced by an uncertain perception and how this\nbehavior changes if information about this uncertainty is available. Therefore,\nwe consider a proxy task, where the agent is rewarded for driving a route as\nfast as possible without colliding with other road users. For controlled\nexperiments, we introduce uncertainty in the observation space by perturbing\nthe perception of the given agent while informing the latter. Our experiments\nshow that an unreliable observation space modeled by a perturbed perception\nleads to a defensive driving behavior of the agent. Furthermore, when adding\nthe information about the current uncertainty directly to the observation\nspace, the agent adapts to the specific situation and in general accomplishes\nits task faster while, at the same time, accounting for risks.\n","authors":["Natalie Grabowsky","Annika Mütze","Joshua Wendland","Nils Jansen","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2502.11864v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.11859v1","updated":"2025-02-17T14:50:53Z","published":"2025-02-17T14:50:53Z","title":"Defining and Evaluating Visual Language Models' Basic Spatial Abilities:\n  A Perspective from Psychometrics","summary":"  The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.\n","authors":["Wenrui Xu","Dalin Lyu","Weihang Wang","Jie Feng","Chen Gao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.11859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11858v1","updated":"2025-02-17T14:50:34Z","published":"2025-02-17T14:50:34Z","title":"Rethinking Audio-Visual Adversarial Vulnerability from Temporal and\n  Modality Perspectives","summary":"  While audio-visual learning equips models with a richer understanding of the\nreal world by leveraging multiple sensory modalities, this integration also\nintroduces new vulnerabilities to adversarial attacks.\n  In this paper, we present a comprehensive study of the adversarial robustness\nof audio-visual models, considering both temporal and modality-specific\nvulnerabilities. We propose two powerful adversarial attacks: 1) a temporal\ninvariance attack that exploits the inherent temporal redundancy across\nconsecutive time segments and 2) a modality misalignment attack that introduces\nincongruence between the audio and visual modalities. These attacks are\ndesigned to thoroughly assess the robustness of audio-visual models against\ndiverse threats. Furthermore, to defend against such attacks, we introduce a\nnovel audio-visual adversarial training framework. This framework addresses key\nchallenges in vanilla adversarial training by incorporating efficient\nadversarial perturbation crafting tailored to multi-modal data and an\nadversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds\ndataset demonstrate that our proposed temporal and modality-based attacks in\ndegrading model performance can achieve state-of-the-art performance, while our\nadversarial training defense largely improves the adversarial robustness as\nwell as the adversarial training efficiency.\n","authors":["Zeliang Zhang","Susan Liang","Daiki Shimada","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11858v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11850v1","updated":"2025-02-17T14:44:12Z","published":"2025-02-17T14:44:12Z","title":"Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif\n  Discovery","summary":"  Time Series Motif Discovery (TSMD) identifies repeating patterns in time\nseries data, but its unsupervised nature might result in motifs that are not\ninteresting to the user. To address this, we propose a framework that allows\nthe user to impose constraints on the motifs to be discovered, where\nconstraints can easily be defined according to the properties of the desired\nmotifs in the application domain. We also propose an efficient implementation\nof the framework, the LoCoMotif-DoK algorithm. We demonstrate that\nLoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic\ndata, outperforming other TSMD techniques which only support a limited form of\ndomain knowledge.\n","authors":["Aras Yurtman","Daan Van Wesenbeeck","Wannes Meert","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2502.11850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00464v2","updated":"2025-02-17T14:44:05Z","published":"2025-02-01T15:48:20Z","title":"Evaluation of End-to-End Continuous Spanish Lipreading in Different Data\n  Conditions","summary":"  Visual speech recognition remains an open research problem where different\nchallenges must be considered by dispensing with the auditory sense, such as\nvisual ambiguities, the inter-personal variability among speakers, and the\ncomplex modeling of silence. Nonetheless, recent remarkable results have been\nachieved in the field thanks to the availability of large-scale databases and\nthe use of powerful attention mechanisms. Besides, multiple languages apart\nfrom English are nowadays a focus of interest. This paper presents noticeable\nadvances in automatic continuous lipreading for Spanish. First, an end-to-end\nsystem based on the hybrid CTC/Attention architecture is presented. Experiments\nare conducted on two corpora of disparate nature, reaching state-of-the-art\nresults that significantly improve the best performance obtained to date for\nboth databases. In addition, a thorough ablation study is carried out, where it\nis studied how the different components that form the architecture influence\nthe quality of speech recognition. Then, a rigorous error analysis is carried\nout to investigate the different factors that could affect the learning of the\nautomatic system. Finally, a new Spanish lipreading benchmark is consolidated.\nCode and trained models are available at\nhttps://github.com/david-gimeno/evaluating-end2end-spanish-lipreading.\n","authors":["David Gimeno-Gómez","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2502.00464v2.pdf","comment":"Accepted in the \"Language Resources and Evaluation\" journal, Springer\n  Nature"},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11831v1","updated":"2025-02-17T14:27:14Z","published":"2025-02-17T14:27:14Z","title":"Intuitive physics understanding emerges from self-supervised pretraining\n  on natural videos","summary":"  We investigate the emergence of intuitive physics understanding in\ngeneral-purpose deep neural network models trained to predict masked regions in\nnatural videos. Leveraging the violation-of-expectation framework, we find that\nvideo prediction models trained to predict outcomes in a learned representation\nspace demonstrate an understanding of various intuitive physics properties,\nsuch as object permanence and shape consistency. In contrast, video prediction\nin pixel space and multimodal large language models, which reason through text,\nachieve performance closer to chance. Our comparisons of these architectures\nreveal that jointly learning an abstract representation space while predicting\nmissing parts of sensory input, akin to predictive coding, is sufficient to\nacquire an understanding of intuitive physics, and that even models trained on\none week of unique video achieve above chance performance. This challenges the\nidea that core knowledge -- a set of innate systems to help understand the\nworld -- needs to be hardwired to develop an understanding of intuitive\nphysics.\n","authors":["Quentin Garrido","Nicolas Ballas","Mahmoud Assran","Adrien Bardes","Laurent Najman","Michael Rabbat","Emmanuel Dupoux","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2502.11831v1.pdf","comment":"24 pages,14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2411.14467v2","updated":"2025-02-17T14:21:20Z","published":"2024-11-18T15:46:39Z","title":"Towards Scalable Insect Monitoring: Ultra-Lightweight CNNs as On-Device\n  Triggers for Insect Camera Traps","summary":"  Camera traps, combined with AI, have emerged as a way to achieve automated,\nscalable biodiversity monitoring. However, the passive infrared (PIR) sensors\nthat trigger camera traps are poorly suited for detecting small, fast-moving\nectotherms such as insects. Insects comprise over half of all animal species\nand are key components of ecosystems and agriculture. The need for an\nappropriate and scalable insect camera trap is critical in the wake of\nconcerning reports of declines in insect populations. This study proposes an\nalternative to the PIR trigger: ultra-lightweight convolutional neural networks\nrunning on low-powered hardware to detect insects in a continuous stream of\ncaptured images. We train a suite of models to distinguish insect images from\nbackgrounds. Our design achieves zero latency between trigger and image\ncapture. Our models are rigorously tested and achieve high accuracy ranging\nfrom 91.8% to 96.4% AUC on validation data and >87% AUC on data from\ndistributions unseen during training. The high specificity of our models\nensures minimal saving of false positive images, maximising deployment storage\nefficiency. High recall scores indicate a minimal false negative rate,\nmaximising insect detection. Further analysis with saliency maps shows the\nlearned representation of our models to be robust, with low reliance on\nspurious background features. Our system is also shown to operate deployed on\noff-the-shelf, low-powered microcontroller units, consuming a maximum power\ndraw of less than 300mW. This enables longer deployment times using cheap and\nreadily available battery components. Overall we offer a step change in the\ncost, efficiency and scope of insect monitoring. Solving the challenging\ntrigger problem, we demonstrate a system which can be deployed for far longer\nthan existing designs and budgets power and bandwidth effectively, moving\ntowards a generic insect camera trap.\n","authors":["Ross Gardiner","Sareh Rowands","Benno I. Simmons"],"pdf_url":"https://arxiv.org/pdf/2411.14467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11809v1","updated":"2025-02-17T13:54:02Z","published":"2025-02-17T13:54:02Z","title":"Revealing Bias Formation in Deep Neural Networks Through the Geometric\n  Mechanisms of Human Visual Decoupling","summary":"  Deep neural networks (DNNs) often exhibit biases toward certain categories\nduring object recognition, even under balanced training data conditions. The\nintrinsic mechanisms underlying these biases remain unclear. Inspired by the\nhuman visual system, which decouples object manifolds through hierarchical\nprocessing to achieve object recognition, we propose a geometric analysis\nframework linking the geometric complexity of class-specific perceptual\nmanifolds in DNNs to model bias. Our findings reveal that differences in\ngeometric complexity can lead to varying recognition capabilities across\ncategories, introducing biases. To support this analysis, we present the\nPerceptual-Manifold-Geometry library, designed for calculating the geometric\nproperties of perceptual manifolds.\n","authors":["Yanbiao Ma","Bowei Liu","Wei Dai","Jiayi Chen","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2502.11809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v3","updated":"2025-02-17T13:50:17Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11801v1","updated":"2025-02-17T13:46:47Z","published":"2025-02-17T13:46:47Z","title":"3D Gaussian Inpainting with Depth-Guided Cross-View Consistency","summary":"  When performing 3D inpainting using novel-view rendering methods like Neural\nRadiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture\nand geometry consistency across camera views has been a challenge. In this\npaper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided\nCross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided\nby the rendered depth information from each training view, our 3DGIC exploits\nbackground pixels visible across different views for updating the inpainting\nmask, allowing us to refine the 3DGS for inpainting purposes.Through extensive\nexperiments on benchmark datasets, we confirm that our 3DGIC outperforms\ncurrent state-of-the-art 3D inpainting methods quantitatively and\nqualitatively.\n","authors":["Sheng-Yu Huang","Zi-Ting Chou","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07758v2","updated":"2025-02-17T13:44:46Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Dragoş Duşe","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander Rölle","Christina C. Westhoff","Bénédicte Lenoir","Niels Halama","Inka Zörnig","Dirk Jäger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v2.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2410.07173v2","updated":"2025-02-17T13:25:17Z","published":"2024-10-09T17:59:33Z","title":"Better Language Models Exhibit Higher Visual Alignment","summary":"  How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08474v2","updated":"2025-02-17T13:22:54Z","published":"2024-09-13T02:00:16Z","title":"Rethinking Meta-Learning from a Learning Lens","summary":"  Meta-learning has emerged as a powerful approach for leveraging knowledge\nfrom previous tasks to solve new tasks. The mainstream methods focus on\ntraining a well-generalized model initialization, which is then adapted to\ndifferent tasks with limited data and updates. However, it pushes the model\noverfitting on the training tasks. Previous methods mainly attributed this to\nthe lack of data and used augmentations to address this issue, but they were\nlimited by sufficient training and effective augmentation strategies. In this\nwork, we focus on the more fundamental learning to learn strategy of\nmeta-learning to explore what causes errors and how to eliminate these errors\nwithout changing the environment. Specifically, we first rethink the\nalgorithmic procedure of meta-learning from a learning lens. Through\ntheoretical and empirical analyses, we find that (i) this paradigm faces the\nrisk of both overfitting and underfitting and (ii) the model adapted to\ndifferent tasks promote each other where the effect is stronger if the tasks\nare more similar. Based on this insight, we propose using task relations to\ncalibrate the optimization process of meta-learning and propose a plug-and-play\nmethod called Task Relation Learner (TRLearner) to achieve this goal.\nSpecifically, it first obtains task relation matrices from the extracted\ntask-specific meta-data. Then, it uses the obtained matrices with\nrelation-aware consistency regularization to guide optimization. Extensive\ntheoretical and empirical analyses demonstrate the effectiveness of TRLearner.\n","authors":["Jingyao Wang","Wenwen Qiang","Chuxiong Sun","Changwen Zheng","Jiangmeng Li"],"pdf_url":"https://arxiv.org/pdf/2409.08474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11777v1","updated":"2025-02-17T13:11:35Z","published":"2025-02-17T13:11:35Z","title":"Deep Neural Networks for Accurate Depth Estimation with Latent Space\n  Features","summary":"  Depth estimation plays a pivotal role in advancing human-robot interactions,\nespecially in indoor environments where accurate 3D scene reconstruction is\nessential for tasks like navigation and object handling. Monocular depth\nestimation, which relies on a single RGB camera, offers a more affordable\nsolution compared to traditional methods that use stereo cameras or LiDAR.\nHowever, despite recent progress, many monocular approaches struggle with\naccurately defining depth boundaries, leading to less precise reconstructions.\nIn response to these challenges, this study introduces a novel depth estimation\nframework that leverages latent space features within a deep convolutional\nneural network to enhance the precision of monocular depth maps. The proposed\nmodel features dual encoder-decoder architecture, enabling both color-to-depth\nand depth-to-depth transformations. This structure allows for refined depth\nestimation through latent space encoding. To further improve the accuracy of\ndepth boundaries and local features, a new loss function is introduced. This\nfunction combines latent loss with gradient loss, helping the model maintain\nthe integrity of depth boundaries. The framework is thoroughly tested using the\nNYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in\ncomplex indoor scenarios. The results clearly show that this approach\neffectively reduces depth ambiguities and blurring, making it a promising\nsolution for applications in human-robot interaction and 3D scene\nreconstruction.\n","authors":["Siddiqui Muhammad Yasir","Hyunsik Ahn"],"pdf_url":"https://arxiv.org/pdf/2502.11777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11775v1","updated":"2025-02-17T13:07:40Z","published":"2025-02-17T13:07:40Z","title":"video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model","summary":"  While recent advancements in reasoning optimization have significantly\nenhanced the capabilities of large language models (LLMs), existing efforts to\nimprove reasoning have been limited to solving mathematical problems and\nfocusing on visual graphical inputs, neglecting broader applications in general\nvideo understanding.This paper proposes video-SALMONN-o1, the first open-source\nreasoning-enhanced audio-visual LLM designed for general video understanding\ntasks. To enhance its reasoning abilities, we develop a reasoning-intensive\ndataset featuring challenging audio-visual questions with step-by-step\nsolutions. We also propose process direct preference optimization (pDPO), which\nleverages contrastive step selection to achieve efficient step-level reward\nmodelling tailored for multimodal inputs. Additionally, we introduce RivaBench,\nthe first reasoning-intensive video understanding benchmark, featuring over\n4,000 high-quality, expert-curated question-answer pairs across scenarios such\nas standup comedy, academic presentations, and synthetic video detection.\nvideo-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision\nbaseline across different video reasoning benchmarks. Besides, pDPO achieves\n6-8% improvements compared to the supervised fine-tuning model on RivaBench.\nEnhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection\ncapabilities.\n","authors":["Guangzhi Sun","Yudong Yang","Jimin Zhuang","Changli Tang","Yixuan Li","Wei Li","Zejun MA","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07560v2","updated":"2025-02-17T12:57:42Z","published":"2025-02-11T13:57:30Z","title":"Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning","summary":"  Class-incremental learning (CIL) seeks to enable a model to sequentially\nlearn new classes while retaining knowledge of previously learned ones.\nBalancing flexibility and stability remains a significant challenge,\nparticularly when the task ID is unknown. To address this, our study reveals\nthat the gap in feature distribution between novel and existing tasks is\nprimarily driven by differences in mean and covariance moments. Building on\nthis insight, we propose a novel semantic drift calibration method that\nincorporates mean shift compensation and covariance calibration. Specifically,\nwe calculate each class's mean by averaging its sample embeddings and estimate\ntask shifts using weighted embedding changes based on their proximity to the\nprevious mean, effectively capturing mean shifts for all learned classes with\neach new task. We also apply Mahalanobis distance constraint for covariance\ncalibration, aligning class-specific embedding covariances between old and\ncurrent networks to mitigate the covariance shift. Additionally, we integrate a\nfeature-level self-distillation approach to enhance generalization.\nComprehensive experiments on commonly used datasets demonstrate the\neffectiveness of our approach. The source code is available at\n\\href{https://github.com/fwu11/MACIL.git}{https://github.com/fwu11/MACIL.git}.\n","authors":["Fangwen Wu","Lechao Cheng","Shengeng Tang","Xiaofeng Zhu","Chaowei Fang","Dingwen Zhang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07560v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.11763v1","updated":"2025-02-17T12:55:41Z","published":"2025-02-17T12:55:41Z","title":"Lightweight Deepfake Detection Based on Multi-Feature Fusion","summary":"  Deepfake technology utilizes deep learning based face manipulation techniques\nto seamlessly replace faces in videos creating highly realistic but\nartificially generated content. Although this technology has beneficial\napplications in media and entertainment misuse of its capabilities may lead to\nserious risks including identity theft cyberbullying and false information. The\nintegration of DL with visual cognition has resulted in important technological\nimprovements particularly in addressing privacy risks caused by artificially\ngenerated deepfake images on digital media platforms. In this study we propose\nan efficient and lightweight method for detecting deepfake images and videos\nmaking it suitable for devices with limited computational resources. In order\nto reduce the computational burden usually associated with DL models our method\nintegrates machine learning classifiers in combination with keyframing\napproaches and texture analysis. Moreover the features extracted with a\nhistogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands\nwere integrated to evaluate using random forest extreme gradient boosting extra\ntrees and support vector classifier algorithms. Our findings show a\nfeature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and\n96% on FaceForensics++ and Celeb-DFv2 respectively.\n","authors":["Siddiqui Muhammad Yasir","Hyun Kim"],"pdf_url":"https://arxiv.org/pdf/2502.11763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08075v2","updated":"2025-02-17T12:53:00Z","published":"2025-02-12T02:37:16Z","title":"Knowledge Swapping via Learning and Unlearning","summary":"  We introduce \\textbf{Knowledge Swapping}, a novel task designed to\nselectively regulate knowledge of a pretrained model by enabling the forgetting\nof user\\-specified information, retaining essential knowledge, and acquiring\nnew knowledge simultaneously. By delving into the analysis of knock-on feature\nhierarchy, we find that incremental learning typically progresses from\nlow\\-level representations to higher\\-level semantics, whereas forgetting tends\nto occur in the opposite direction\\-starting from high-level semantics and\nmoving down to low-level features. Building upon this, we propose to benchmark\nthe knowledge swapping task with the strategy of \\textit{Learning Before\nForgetting}. Comprehensive experiments on various tasks like image\nclassification, object detection, and semantic segmentation validate the\neffectiveness of the proposed strategy. The source code is available at\n\\href{https://github.com/xingmingyu123456/KnowledgeSwapping}{https://github.com/xingmingyu123456/KnowledgeSwapping}.\n","authors":["Mingyu Xing","Lechao Cheng","Shengeng Tang","Yaxiong Wang","Zhun Zhong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08075v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.11756v1","updated":"2025-02-17T12:52:10Z","published":"2025-02-17T12:52:10Z","title":"On the Computation of the Fisher Information in Continual Learning","summary":"  One of the most popular methods for continual learning with deep neural\nnetworks is Elastic Weight Consolidation (EWC), which involves computing the\nFisher Information. The exact way in which the Fisher Information is computed\nis however rarely described, and multiple different implementations for it can\nbe found online. This blog post discusses and empirically compares several\noften-used implementations, which highlights that many currently reported\nresults for EWC could likely be improved by changing the way the Fisher\nInformation is computed.\n","authors":["Gido M. van de Ven"],"pdf_url":"https://arxiv.org/pdf/2502.11756v1.pdf","comment":"To appear in the blogpost track at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11751v1","updated":"2025-02-17T12:47:00Z","published":"2025-02-17T12:47:00Z","title":"Language Models Can See Better: Visual Contrastive Decoding For LLM\n  Multimodal Reasoning","summary":"  Although Large Language Models (LLMs) excel in reasoning and generation for\nlanguage tasks, they are not specifically designed for multimodal challenges.\nTraining Multimodal Large Language Models (MLLMs), however, is\nresource-intensive and constrained by various training limitations. In this\npaper, we propose the Modular-based Visual Contrastive Decoding (MVCD)\nframework to move this obstacle. Our framework leverages LLMs' In-Context\nLearning (ICL) capability and the proposed visual contrastive-example decoding\n(CED), specifically tailored for this framework, without requiring any\nadditional training. By converting visual signals into text and focusing on\ncontrastive output distributions during decoding, we can highlight the new\ninformation introduced by contextual examples, explore their connections, and\navoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual\nperception to make it see and reason over the input visuals. To demonstrate\nMVCD's effectiveness, we conduct experiments with four LLMs across five\nquestion answering datasets. Our results not only show consistent improvement\nin model accuracy but well explain the effective components inside our decoding\nstrategy. Our code will be available at https://github.com/Pbhgit/MVCD.\n","authors":["Yuqi Pang","Bowen Yang","Haoqin Tu","Yun Cao","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11751v1.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.11749v1","updated":"2025-02-17T12:43:04Z","published":"2025-02-17T12:43:04Z","title":"JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse Unrolling\n  Network for Accelerating Dynamic MRI","summary":"  Joint low-rank and sparse unrolling networks have shown superior performance\nin dynamic MRI reconstruction. However, existing works mainly utilized matrix\nlow-rank priors, neglecting the tensor characteristics of dynamic MRI images,\nand only a global threshold is applied for the sparse constraint to the\nmulti-channel data, limiting the flexibility of the network. Additionally, most\nof them have inherently complex network structure, with intricate interactions\namong variables. In this paper, we propose a novel deep unrolling network,\nJotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank\nand attention-based sparse priors. Specifically, we utilize tensor low-rank\nprior to exploit the structural correlations in high-dimensional data.\nConvolutional neural networks are used to adaptively learn the low-rank and\nsparse transform domains. A novel attention-based soft thresholding operator is\nproposed to assign a unique learnable threshold to each channel of the data in\nthe CNN-learned sparse domain. The network is unrolled from the elaborately\ndesigned composite splitting algorithm and thus features a simple yet efficient\nparallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon)\ndemonstrate the superior performance of JotlasNet in dynamic MRI\nreconstruction.\n","authors":["Yinghao Zhang","Haiyan Gui","Ningdi Yang","Yue Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11749v1.pdf","comment":"13 pages, 7 figures, accepted by Magnetic Resonance Imaging"},{"id":"http://arxiv.org/abs/2502.11748v1","updated":"2025-02-17T12:42:38Z","published":"2025-02-17T12:42:38Z","title":"ILIAS: Instance-Level Image retrieval At Scale","summary":"  This work introduces ILIAS, a new test dataset for Instance-Level Image\nretrieval At Scale. It is designed to evaluate the ability of current and\nfuture foundation models and retrieval techniques to recognize particular\nobjects. The key benefits over existing datasets include large scale, domain\ndiversity, accurate ground truth, and a performance that is far from saturated.\nILIAS includes query and positive images for 1,000 object instances, manually\ncollected to capture challenging conditions and diverse domains. Large-scale\nretrieval is conducted against 100 million distractor images from YFCC100M. To\navoid false negatives without extra annotation effort, we include only query\nobjects confirmed to have emerged after 2014, i.e. the compilation date of\nYFCC100M. An extensive benchmarking is performed with the following\nobservations: i) models fine-tuned on specific domains, such as landmarks or\nproducts, excel in that domain but fail on ILIAS ii) learning a linear\nadaptation layer using multi-domain class supervision results in performance\nimprovements, especially for vision-language models iii) local descriptors in\nretrieval re-ranking are still a key ingredient, especially in the presence of\nsevere background clutter iv) the text-to-image performance of the\nvision-language foundation models is surprisingly close to the corresponding\nimage-to-image case. website: https://vrg.fel.cvut.cz/ilias/\n","authors":["Giorgos Kordopatis-Zilos","Vladan Stojnić","Anna Manko","Pavel Šuma","Nikolaos-Antonios Ypsilantis","Nikos Efthymiadis","Zakaria Laskar","Jiří Matas","Ondřej Chum","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2502.11748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02401v7","updated":"2025-02-17T12:37:09Z","published":"2024-10-03T11:29:09Z","title":"SynCo: Synthetic Hard Negatives for Contrastive Visual Representation\n  Learning","summary":"  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning, but efficiently leveraging hard negatives, which are\nsamples closely resembling the anchor, remains challenging. We introduce SynCo\n(Synthetic negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and strong representation learning,\nsurpassing MoCo-v2 by +0.4% and MoCHI by +1.0% on ImageNet ILSVRC-2012 linear\nevaluation. It also transfers more effectively to detection tasks achieving\nstrong results on PASCAL VOC detection (57.2% AP) and significantly improving\nover MoCo-v2 on COCO detection (+1.0% AP) and instance segmentation (+0.8% AP).\nOur synthetic hard negative generation approach significantly enhances visual\nrepresentations learned through self-supervised contrastive learning.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.02401v7.pdf","comment":"Preprint. Code: https://github.com/giakoumoglou/synco, Supplementary:\n  https://giakoumoglou.com/src/synco_suppl.pdf"},{"id":"http://arxiv.org/abs/2502.05540v2","updated":"2025-02-17T12:36:11Z","published":"2025-02-08T12:10:02Z","title":"Demystifying Catastrophic Forgetting in Two-Stage Incremental Object\n  Detector","summary":"  Catastrophic forgetting is a critical chanllenge for incremental object\ndetection (IOD). Most existing methods treat the detector monolithically,\nrelying on instance replay or knowledge distillation without analyzing\ncomponent-specific forgetting. Through dissection of Faster R-CNN, we reveal a\nkey insight: Catastrophic forgetting is predominantly localized to the RoI Head\nclassifier, while regressors retain robustness across incremental stages. This\nfinding challenges conventional assumptions, motivating us to develop a\nframework termed NSGP-RePRE. Regional Prototype Replay (RePRE) mitigates\nclassifier forgetting via replay of two types of prototypes: coarse prototypes\nrepresent class-wise semantic centers of RoI features, while fine-grained\nprototypes model intra-class variations. Null Space Gradient Projection (NSGP)\nis further introduced to eliminate prototype-feature misalignment by updating\nthe feature extractor in directions orthogonal to subspace of old inputs via\ngradient projection, aligning RePRE with incremental learning dynamics. Our\nsimple yet effective design allows NSGP-RePRE to achieve state-of-the-art\nperformance on the Pascal VOC and MS COCO datasets under various settings. Our\nwork not only advances IOD methodology but also provide pivotal insights for\ncatastrophic forgetting mitigation in IOD. Code will be available soon.\n","authors":["Qirui Wu","Shizhou Zhang","De Cheng","Yinghui Xing","Di Xu","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05540v2.pdf","comment":"14 pages, 7 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.11744v1","updated":"2025-02-17T12:34:42Z","published":"2025-02-17T12:34:42Z","title":"FUNCTO: Function-Centric One-Shot Imitation Learning for Tool\n  Manipulation","summary":"  Learning tool use from a single human demonstration video offers a highly\nintuitive and efficient approach to robot teaching. While humans can\neffortlessly generalize a demonstrated tool manipulation skill to diverse tools\nthat support the same function (e.g., pouring with a mug versus a teapot),\ncurrent one-shot imitation learning (OSIL) methods struggle to achieve this. A\nkey challenge lies in establishing functional correspondences between\ndemonstration and test tools, considering significant geometric variations\namong tools with the same function (i.e., intra-function variations). To\naddress this challenge, we propose FUNCTO (Function-Centric OSIL for Tool\nManipulation), an OSIL method that establishes function-centric correspondences\nwith a 3D functional keypoint representation, enabling robots to generalize\ntool manipulation skills from a single human demonstration video to novel tools\nwith the same function despite significant intra-function variations. With this\nformulation, we factorize FUNCTO into three stages: (1) functional keypoint\nextraction, (2) function-centric correspondence establishment, and (3)\nfunctional keypoint-based action planning. We evaluate FUNCTO against exiting\nmodular OSIL methods and end-to-end behavioral cloning methods through\nreal-robot experiments on diverse tool manipulation tasks. The results\ndemonstrate the superiority of FUNCTO when generalizing to novel tools with\nintra-function geometric variations. More details are available at\nhttps://sites.google.com/view/functo.\n","authors":["Chao Tang","Anxing Xiao","Yuhong Deng","Tianrun Hu","Wenlong Dong","Hanbo Zhang","David Hsu","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11742v1","updated":"2025-02-17T12:29:26Z","published":"2025-02-17T12:29:26Z","title":"Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition","summary":"  Image-to-point cloud cross-modal Visual Place Recognition (VPR) is a\nchallenging task where the query is an RGB image, and the database samples are\nLiDAR point clouds. Compared to single-modal VPR, this approach benefits from\nthe widespread availability of RGB cameras and the robustness of point clouds\nin providing accurate spatial geometry and distance information. However,\ncurrent methods rely on intermediate modalities that capture either the\nvertical or horizontal field of view, limiting their ability to fully exploit\nthe complementary information from both sensors. In this work, we propose an\ninnovative initial retrieval + re-rank method that effectively combines\ninformation from range (or RGB) images and Bird's Eye View (BEV) images. Our\napproach relies solely on a computationally efficient global descriptor\nsimilarity search process to achieve re-ranking. Additionally, we introduce a\nnovel similarity label supervision technique to maximize the utility of limited\ntraining data. Specifically, we employ points average distance to approximate\nappearance similarity and incorporate an adaptive margin, based on similarity\ndifferences, into the vanilla triplet loss. Experimental results on the KITTI\ndataset demonstrate that our method significantly outperforms state-of-the-art\napproaches.\n","authors":["Jianyi Peng","Fan Lu","Bin Li","Yuan Huang","Sanqing Qu","Guang Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11742v1.pdf","comment":"Submmitted to IEEE IV 2025"},{"id":"http://arxiv.org/abs/2502.11740v1","updated":"2025-02-17T12:26:34Z","published":"2025-02-17T12:26:34Z","title":"Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via\n  Modality-decoupled Gradient Descent","summary":"  Recent MLLMs have shown emerging visual understanding and reasoning abilities\nafter being pre-trained on large-scale multimodal datasets. Unlike\npre-training, where MLLMs receive rich visual-text alignment,\ninstruction-tuning is often text-driven with weaker visual supervision, leading\nto the degradation of pre-trained visual understanding and causing visual\nforgetting. Existing approaches, such as direct fine-tuning and continual\nlearning methods, fail to explicitly address this issue, often compressing\nvisual representations and prioritizing task alignment over visual retention,\nwhich further worsens visual forgetting. To overcome this limitation, we\nintroduce a novel perspective leveraging effective rank to quantify the\ndegradation of visual representation richness, interpreting this degradation\nthrough the information bottleneck principle as excessive compression that\nleads to the degradation of crucial pre-trained visual knowledge. Building on\nthis view, we propose a modality-decoupled gradient descent (MDGD) method that\nregulates gradient updates to maintain the effective rank of visual\nrepresentations while mitigating the over-compression effects described by the\ninformation bottleneck. By explicitly disentangling the optimization of visual\nunderstanding from task-specific alignment, MDGD preserves pre-trained visual\nknowledge while enabling efficient task adaptation. To enable lightweight\ninstruction-tuning, we further develop a memory-efficient fine-tuning approach\nusing gradient masking, which selectively updates a subset of model parameters\nto enable parameter-efficient fine-tuning (PEFT), reducing computational\noverhead while preserving rich visual representations. Extensive experiments\nacross various downstream tasks and backbone MLLMs demonstrate that MDGD\neffectively mitigates visual forgetting from pre-trained tasks while enabling\nstrong adaptation to new tasks.\n","authors":["Junda Wu","Yuxin Xiong","Xintong Li","Yu Xia","Ruoyu Wang","Yu Wang","Tong Yu","Sungchul Kim","Ryan A. Rossi","Lina Yao","Jingbo Shang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2502.11740v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.11731v1","updated":"2025-02-17T12:18:24Z","published":"2025-02-17T12:18:24Z","title":"GraphMorph: Tubular Structure Extraction by Morphing Predicted Graphs","summary":"  Accurately restoring topology is both challenging and crucial in tubular\nstructure extraction tasks, such as blood vessel segmentation and road network\nextraction. Diverging from traditional approaches based on pixel-level\nclassification, our proposed method, named GraphMorph, focuses on branch-level\nfeatures of tubular structures to achieve more topologically accurate\npredictions. GraphMorph comprises two main components: a Graph Decoder and a\nMorph Module. Utilizing multi-scale features extracted from an image patch by\nthe segmentation network, the Graph Decoder facilitates the learning of\nbranch-level features and generates a graph that accurately represents the\ntubular structure in this patch. The Morph Module processes two primary inputs:\nthe graph and the centerline probability map, provided by the Graph Decoder and\nthe segmentation network, respectively. Employing a novel SkeletonDijkstra\nalgorithm, the Morph Module produces a centerline mask that aligns with the\npredicted graph. Furthermore, we observe that employing centerline masks\npredicted by GraphMorph significantly reduces false positives in the\nsegmentation task, which is achieved by a simple yet effective post-processing\nstrategy. The efficacy of our method in the centerline extraction and\nsegmentation tasks has been substantiated through experimental evaluations\nacross various datasets. Source code will be released soon.\n","authors":["Zhao Zhang","Ziwei Zhao","Dong Wang","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11731v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.03517v2","updated":"2025-02-17T12:13:15Z","published":"2023-10-05T12:56:34Z","title":"PrototypeFormer: Learning to Explore Prototype Relationships for\n  Few-shot Image Classification","summary":"  Few-shot image classification has received considerable attention for\novercoming the challenge of limited classification performance with limited\nsamples in novel classes. Most existing works employ sophisticated learning\nstrategies and feature learning modules to alleviate this challenge. In this\npaper, we propose a novel method called PrototypeFormer, exploring the\nrelationships among category prototypes in the few-shot scenario. Specifically,\nwe utilize a transformer architecture to build a prototype extraction module,\naiming to extract class representations that are more discriminative for\nfew-shot classification. Besides, during the model training process, we propose\na contrastive learning-based optimization approach to optimize prototype\nfeatures in few-shot learning scenarios. Despite its simplicity, our method\nperforms remarkably well, with no bells and whistles. We have experimented with\nour approach on several popular few-shot image classification benchmark\ndatasets, which shows that our method outperforms all current state-of-the-art\nmethods. In particular, our method achieves 97.07\\% and 90.88\\% on 5-way 5-shot\nand 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art\nresults with accuracy of 0.57\\% and 6.84\\%, respectively. The code will be\nreleased later.\n","authors":["Meijuan Su","Feihong He","Fanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2310.03517v2.pdf","comment":"Submitted to Neurocomputing"},{"id":"http://arxiv.org/abs/2406.10469v2","updated":"2025-02-17T12:12:25Z","published":"2024-06-15T02:19:31Z","title":"Object-Attribute-Relation Representation Based Video Semantic\n  Communication","summary":"  With the rapid growth of multimedia data volume, there is an increasing need\nfor efficient video transmission in applications such as virtual reality and\nfuture video streaming services. Semantic communication is emerging as a vital\ntechnique for ensuring efficient and reliable transmission in low-bandwidth,\nhigh-noise settings. However, most current approaches focus on joint\nsource-channel coding (JSCC) that depends on end-to-end training. These methods\noften lack an interpretable semantic representation and struggle with\nadaptability to various downstream tasks. In this paper, we introduce the use\nof object-attribute-relation (OAR) as a semantic framework for videos to\nfacilitate low bit-rate coding and enhance the JSCC process for more effective\nvideo transmission. We utilize OAR sequences for both low bit-rate\nrepresentation and generative video reconstruction. Additionally, we\nincorporate OAR into the image JSCC model to prioritize communication resources\nfor areas more critical to downstream tasks. Our experiments on traffic\nsurveillance video datasets assess the effectiveness of our approach in terms\nof video transmission performance. The empirical findings demonstrate that our\nOAR-based video coding method not only outperforms H.265 coding at lower\nbit-rates but also synergizes with JSCC to deliver robust and efficient video\ntransmission.\n","authors":["Qiyuan Du","Yiping Duan","Qianqian Yang","Xiaoming Tao","Mérouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2406.10469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11726v1","updated":"2025-02-17T12:11:56Z","published":"2025-02-17T12:11:56Z","title":"No-reference geometry quality assessment for colorless point clouds via\n  list-wise rank learning","summary":"  Geometry quality assessment (GQA) of colorless point clouds is crucial for\nevaluating the performance of emerging point cloud-based solutions (e.g.,\nwatermarking, compression, and 3-Dimensional (3D) reconstruction).\nUnfortunately, existing objective GQA approaches are traditional full-reference\nmetrics, whereas state-of-the-art learning-based point cloud quality assessment\n(PCQA) methods target both color and geometry distortions, neither of which are\nqualified for the no-reference GQA task. In addition, the lack of large-scale\nGQA datasets with subjective scores, which are always imprecise, biased, and\ninconsistent, also hinders the development of learning-based GQA metrics.\nDriven by these limitations, this paper proposes a no-reference geometry-only\nquality assessment approach based on list-wise rank learning, termed LRL-GQA,\nwhich comprises of a geometry quality assessment network (GQANet) and a\nlist-wise rank learning network (LRLNet). The proposed LRL-GQA formulates the\nno-reference GQA as a list-wise rank problem, with the objective of directly\noptimizing the entire quality ordering. Specifically, a large dataset\ncontaining a variety of geometry-only distortions is constructed first, named\nLRL dataset, in which each sample is label-free but coupled with quality\nranking information. Then, the GQANet is designed to capture intrinsic\nmulti-scale patch-wise geometric features in order to predict a quality index\nfor each point cloud. After that, the LRLNet leverages the LRL dataset and a\nlikelihood loss to train the GQANet and ranks the input list of degraded point\nclouds according to their distortion levels. In addition, the pre-trained\nGQANet can be fine-tuned further to obtain absolute quality scores.\nExperimental results demonstrate the superior performance of the proposed\nno-reference LRL-GQA method compared with existing full-reference GQA metrics.\n","authors":["Zheng Li","Bingxu Xie","Chao Chu","Weiqing Li","Zhiyong Su"],"pdf_url":"https://arxiv.org/pdf/2502.11726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11725v1","updated":"2025-02-17T12:11:01Z","published":"2025-02-17T12:11:01Z","title":"Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual\n  Metrics","summary":"  Measuring perceptual similarity is a key tool in computer vision. In recent\nyears perceptual metrics based on features extracted from neural networks with\nlarge and diverse training sets, e.g. CLIP, have become popular. At the same\ntime, the metrics extracted from features of neural networks are not\nadversarially robust. In this paper we show that adversarially robust CLIP\nmodels, called R-CLIP$_\\textrm{F}$, obtained by unsupervised adversarial\nfine-tuning induce a better and adversarially robust perceptual metric that\noutperforms existing metrics in a zero-shot setting, and further matches the\nperformance of state-of-the-art metrics while being robust after fine-tuning.\nMoreover, our perceptual metric achieves strong performance on related tasks\nsuch as robust image-to-image retrieval, which becomes especially relevant when\napplied to \"Not Safe for Work\" (NSFW) content detection and dataset filtering.\nWhile standard perceptual metrics can be easily attacked by a small\nperturbation completely degrading NSFW detection, our robust perceptual metric\nmaintains high accuracy under an attack while having similar performance for\nunperturbed images. Finally, perceptual metrics induced by robust CLIP models\nhave higher interpretability: feature inversion can show which images are\nconsidered similar, while text inversion can find what images are associated to\na given prompt. This also allows us to visualize the very rich visual concepts\nlearned by a CLIP model, including memorized persons, paintings and complex\nqueries.\n","authors":["Francesco Croce","Christian Schlarmann","Naman Deep Singh","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2502.11725v1.pdf","comment":"This work has been accepted for publication in the IEEE Conference on\n  Secure and Trustworthy Machine Learning (SaTML). The final version will be\n  available on IEEE Xplore"},{"id":"http://arxiv.org/abs/2502.11724v1","updated":"2025-02-17T12:10:35Z","published":"2025-02-17T12:10:35Z","title":"Incomplete Modality Disentangled Representation for Ophthalmic Disease\n  Grading and Diagnosis","summary":"  Ophthalmologists typically require multimodal data sources to improve\ndiagnostic accuracy in clinical decisions. However, due to medical device\nshortages, low-quality data and data privacy concerns, missing data modalities\nare common in real-world scenarios. Existing deep learning methods tend to\naddress it by learning an implicit latent subspace representation for different\nmodality combinations. We identify two significant limitations of these\nmethods: (1) implicit representation constraints that hinder the model's\nability to capture modality-specific information and (2) modality\nheterogeneity, causing distribution gaps and redundancy in feature\nrepresentations. To address these, we propose an Incomplete Modality\nDisentangled Representation (IMDR) strategy, which disentangles features into\nexplicit independent modal-common and modal-specific features by guidance of\nmutual information, distilling informative knowledge and enabling it to\nreconstruct valuable missing semantics and produce robust multimodal\nrepresentations. Furthermore, we introduce a joint proxy learning module that\nassists IMDR in eliminating intra-modality redundancy by exploiting the\nextracted proxies from each class. Experiments on four ophthalmology multimodal\ndatasets demonstrate that the proposed IMDR outperforms the state-of-the-art\nmethods significantly.\n","authors":["Chengzhi Liu","Zile Huang","Zhe Chen","Feilong Tang","Yu Tian","Zhongxing Xu","Zihong Luo","Yalin Zheng","Yanda Meng"],"pdf_url":"https://arxiv.org/pdf/2502.11724v1.pdf","comment":"7 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.05952v2","updated":"2025-02-17T12:04:53Z","published":"2025-01-10T13:27:04Z","title":"Scalable Vision Language Model Training via High Quality Data Curation","summary":"  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning\nvia High QuaLity Data Curation), an open-source vision language model (VLM)\nseries achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.\nThe following three key improvements contribute to SAIL-VL's leading\nperformance: (1) Scalable high-quality visual understanding data construction:\nWe implement a data construction pipeline to enable hundred-million-scale\nhigh-quality recaption data annotation, and the resulted dataset SAIL-Caption\nis validated to be of the highest data quality compared with opensource\nalternatives. (2) Scalable Pretraining with High-Quality Visual Understanding\nData: We scale SAIL-VL's pretraining budget up to 655B tokens and show that\neven a 2B VLM benefits from scaled up training data sizes, exhibiting expected\ndata size scaling laws in visual understanding and instruction following\nperformance. (3) Scalable SFT via data quantity and complexity scaling: We\ncurate a high-quality SFT dataset collection which outperforms opensource\nalternatives in data quantity scaling effectiveness. We also demonstrate that\ntraining with progressively higher-complexity data surpasses baseline one-stage\ntraining by a large margin. SAIL-VL series models achieve the highest average\nscore in 18 widely used VLM benchmarks in our evaluation, with the 2B model\ntakes the top position over VLMs of comparable sizes on OpenCompass 2024\n(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust\nvisual comprehension abilities. SAIL-VL series models are released at\nHuggingFace (https://huggingface.co/BytedanceDouyinContent).\n","authors":["Hongyuan Dong","Zijian Kang","Weijie Yin","Xiao Liang","Chao Feng","Jiao Ran"],"pdf_url":"https://arxiv.org/pdf/2501.05952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11718v1","updated":"2025-02-17T12:02:23Z","published":"2025-02-17T12:02:23Z","title":"\"See the World, Discover Knowledge\": A Chinese Factuality Evaluation for\n  Large Vision Language Models","summary":"  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n","authors":["Jihao Gu","Yingyao Wang","Pi Bu","Chen Wang","Ziming Wang","Tengtao Song","Donglai Wei","Jiale Yuan","Yingxiu Zhao","Yancheng He","Shilong Li","Jiaheng Liu","Meng Cao","Jun Song","Yingshui Tan","Xiang Li","Wenbo Su","Zhicheng Zheng","Xiaoyong Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11718v1.pdf","comment":"24 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.08279v2","updated":"2025-02-17T12:01:02Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14585v2","updated":"2025-02-17T11:59:26Z","published":"2024-08-26T19:09:21Z","title":"Global-Local Distillation Network-Based Audio-Visual Speaker Tracking\n  with Incomplete Modalities","summary":"  In speaker tracking research, integrating and complementing multi-modal data\nis a crucial strategy for improving the accuracy and robustness of tracking\nsystems. However, tracking with incomplete modalities remains a challenging\nissue due to noisy observations caused by occlusion, acoustic noise, and sensor\nfailures. Especially when there is missing data in multiple modalities, the\nperformance of existing multi-modal fusion methods tends to decrease. To this\nend, we propose a Global-Local Distillation-based Tracker (GLDTracker) for\nrobust audio-visual speaker tracking. GLDTracker is driven by a teacher-student\ndistillation model, enabling the flexible fusion of incomplete information from\neach modality. The teacher network processes global signals captured by camera\nand microphone arrays, and the student network handles local information\nsubject to visual occlusion and missing audio channels. By transferring\nknowledge from teacher to student, the student network can better adapt to\ncomplex dynamic scenes with incomplete observations. In the student network, a\nglobal feature reconstruction module based on the generative adversarial\nnetwork is constructed to reconstruct global features from feature embedding\nwith missing local information. Furthermore, a multi-modal multi-level fusion\nattention is introduced to integrate the incomplete feature and the\nreconstructed feature, leveraging the complementarity and consistency of\naudio-visual and global-local features. Experimental results on the AV16.3\ndataset demonstrate that the proposed GLDTracker outperforms existing\nstate-of-the-art audio-visual trackers and achieves leading performance on both\nstandard and incomplete modalities datasets, highlighting its superiority and\nrobustness in complex conditions. The code and models will be available.\n","authors":["Yidi Li","Yihan Li","Yixin Guo","Bin Ren","Zhenhuan Xu","Hao Guo","Hong Liu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2408.14585v2.pdf","comment":"We request to withdraw our paper from arXiv due to unresolved author\n  disagreements about the data interpretation and study conclusions. To\n  maintain scientific integrity, we believe withdrawing the paper is necessary.\n  We regret any confusion caused"},{"id":"http://arxiv.org/abs/2502.11712v1","updated":"2025-02-17T11:54:43Z","published":"2025-02-17T11:54:43Z","title":"Component-aware Unsupervised Logical Anomaly Generation for Industrial\n  Anomaly Detection","summary":"  Anomaly detection is critical in industrial manufacturing for ensuring\nproduct quality and improving efficiency in automated processes. The scarcity\nof anomalous samples limits traditional detection methods, making anomaly\ngeneration essential for expanding the data repository. However, recent\ngenerative models often produce unrealistic anomalies increasing false\npositives, or require real-world anomaly samples for training. In this work, we\ntreat anomaly generation as a compositional problem and propose ComGEN, a\ncomponent-aware and unsupervised framework that addresses the gap in logical\nanomaly generation. Our method comprises a multi-component learning strategy to\ndisentangle visual components, followed by subsequent generation editing\nprocedures. Disentangled text-to-component pairs, revealing intrinsic logical\nconstraints, conduct attention-guided residual mapping and model training with\niteratively matched references across multiple scales. Experiments on the\nMVTecLOCO dataset confirm the efficacy of ComGEN, achieving the best AUROC\nscore of 91.2%. Additional experiments on the real-world scenario of Diesel\nEngine and widely-used MVTecAD dataset demonstrate significant performance\nimprovements when integrating simulated anomalies generated by ComGEN into\nautomated production workflows.\n","authors":["Xuan Tong","Yang Chang","Qing Zhao","Jiawen Yu","Boyang Wang","Junxiong Lin","Yuxuan Lin","Xinji Mai","Haoran Wang","Zeng Tao","Yan Wang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03210v2","updated":"2025-02-17T11:53:44Z","published":"2024-12-04T10:55:44Z","title":"Parametric PerceptNet: A bio-inspired deep-net trained for Image Quality\n  Assessment","summary":"  Human vision models are at the core of image processing. For instance,\nclassical approaches to the problem of image quality are based on models that\ninclude knowledge about human vision. However, nowadays, deep learning\napproaches have obtained competitive results by simply approaching this problem\nas regression of human decisions, and training an standard network on\nhuman-rated datasets. These approaches have the advantages of being easily\nadaptable to a particular problem and they fit very efficiently when data is\navailable. However, mainly due to the excess of parameters, they have the\nproblems of lack of interpretability, and over-fitting. Here we propose a\nvision model that combines the best of both worlds by using a parametric neural\nnetwork architecture. We parameterize the layers to have bioplausible\nfunctionality, and provide a set of bioplausible parameters. We analyzed\ndifferent versions of the model and compared it with the non-parametric\nversion. The parametric models achieve a three orders of magnitude reduction in\nthe number of parameters without suffering in regression performance.\nFurthermore, we show that the parametric models behave better during training\nand are easier to interpret as vision models. Interestingly, we find that, even\ninitialized with bioplausible trained for regression using human rated\ndatasets, which we call the feature-spreading problem. This suggests that the\ndeep learning approach is inherently flawed, and emphasizes the need to\nevaluate and train models beyond regression.\n","authors":["Jorge Vila-Tomás","Pablo Hernández-Cámara","Valero Laparra","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2412.03210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11710v1","updated":"2025-02-17T11:50:42Z","published":"2025-02-17T11:50:42Z","title":"The Worse The Better: Content-Aware Viewpoint Generation Network for\n  Projection-related Point Cloud Quality Assessment","summary":"  Through experimental studies, however, we observed the instability of final\npredicted quality scores, which change significantly over different viewpoint\nsettings. Inspired by the \"wooden barrel theory\", given the default\ncontent-independent viewpoints of existing projection-related PCQA approaches,\nthis paper presents a novel content-aware viewpoint generation network (CAVGN)\nto learn better viewpoints by taking the distribution of geometric and\nattribute features of degraded point clouds into consideration. Firstly, the\nproposed CAVGN extracts multi-scale geometric and texture features of the\nentire input point cloud, respectively. Then, for each default\ncontent-independent viewpoint, the extracted geometric and texture features are\nrefined to focus on its corresponding visible part of the input point cloud.\nFinally, the refined geometric and texture features are concatenated to\ngenerate an optimized viewpoint. To train the proposed CAVGN, we present a\nself-supervised viewpoint ranking network (SSVRN) to select the viewpoint with\nthe worst quality projected image to construct a default-optimized viewpoint\ndataset, which consists of thousands of paired default viewpoints and\ncorresponding optimized viewpoints. Experimental results show that the\nprojection-related PCQA methods can achieve higher performance using the\nviewpoints generated by the proposed CAVGN.\n","authors":["Zhiyong Su","Bingxu Xie","Zheng Li","Jincan Wu","Weiqing Li"],"pdf_url":"https://arxiv.org/pdf/2502.11710v1.pdf","comment":"To be published in IEEE Transactions on Circuits and Systems for\n  Video Technology"},{"id":"http://arxiv.org/abs/2502.11697v1","updated":"2025-02-17T11:34:58Z","published":"2025-02-17T11:34:58Z","title":"MVTokenFlow: High-quality 4D Content Generation using Multiview Token\n  Flow","summary":"  In this paper, we present MVTokenFlow for high-quality 4D content creation\nfrom monocular videos. Recent advancements in generative models such as video\ndiffusion models and multiview diffusion models enable us to create videos or\n3D models. However, extending these generative models for dynamic 4D content\ncreation is still a challenging task that requires the generated content to be\nconsistent spatially and temporally. To address this challenge, MVTokenFlow\nutilizes the multiview diffusion model to generate multiview images on\ndifferent timesteps, which attains spatial consistency across different\nviewpoints and allows us to reconstruct a reasonable coarse 4D field. Then,\nMVTokenFlow further regenerates all the multiview images using the rendered 2D\nflows as guidance. The 2D flows effectively associate pixels from different\ntimesteps and improve the temporal consistency by reusing tokens in the\nregeneration process. Finally, the regenerated images are spatiotemporally\nconsistent and utilized to refine the coarse 4D field to get a high-quality 4D\nfield. Experiments demonstrate the effectiveness of our design and show\nsignificantly improved quality than baseline methods.\n","authors":["Hanzhuo Huang","Yuan Liu","Ge Zheng","Jiepeng Wang","Zhiyang Dou","Sibei Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11697v1.pdf","comment":"ICLR 2025. Project page: https://soolab.github.io/MVTokenFlow"},{"id":"http://arxiv.org/abs/2406.01395v4","updated":"2025-02-17T11:24:49Z","published":"2024-06-03T14:58:49Z","title":"TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for\n  Traversability Estimation","summary":"  This paper presents TE-NeXt, a novel and efficient architecture for\nTraversability Estimation (TE) from sparse LiDAR point clouds based on a\nresidual convolution block. TE-NeXt block fuses notions of current trends such\nas attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate\nhigh capacity for generalisation in a variety of urban and natural\nenvironments, using well-known and accessible datasets such as SemanticKITTI,\nRellis-3D and SemanticUSL. Thus, the designed architecture ouperforms\nstate-of-the-art methods in the problem of semantic segmentation, demonstrating\nbetter results in unstructured environments and maintaining high reliability\nand robustness in urbans environments, which leads to better abstraction.\nImplementation is available in a open repository to the scientific community\nwith the aim of ensuring the reproducibility of results.\n","authors":["Antonio Santo","Juan J. Cabrera","David Valiente","Carlos Viegas","Arturo Gil"],"pdf_url":"https://arxiv.org/pdf/2406.01395v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22078v2","updated":"2025-02-17T11:08:04Z","published":"2024-10-29T14:36:03Z","title":"DINeuro: Distilling Knowledge from 2D Natural Images via Deformable\n  Tubular Transferring Strategy for 3D Neuron Reconstruction","summary":"  Reconstructing neuron morphology from 3D light microscope imaging data is\ncritical to aid neuroscientists in analyzing brain networks and neuroanatomy.\nWith the boost from deep learning techniques, a variety of learning-based\nsegmentation models have been developed to enhance the signal-to-noise ratio of\nraw neuron images as a pre-processing step in the reconstruction workflow.\nHowever, most existing models directly encode the latent representative\nfeatures of volumetric neuron data but neglect their intrinsic morphological\nknowledge. To address this limitation, we design a novel framework that\ndistills the prior knowledge from a 2D Vision Transformer pre-trained on\nextensive 2D natural images to facilitate neuronal morphological learning of\nour 3D Vision Transformer. To bridge the knowledge gap between the 2D natural\nimage and 3D microscopic morphologic domains, we propose a deformable tubular\ntransferring strategy that adapts the pre-trained 2D natural knowledge to the\ninherent tubular characteristics of neuronal structure in the latent embedding\nspace. The experimental results on the Janelia dataset of the BigNeuron project\ndemonstrate that our method achieves a segmentation performance improvement of\n4.53% in mean Dice and 3.56% in mean 95% Hausdorff distance.\n","authors":["Yik San Cheng","Runkai Zhao","Heng Wang","Hanchuan Peng","Yui Lo","Yuqian Chen","Lauren J. O'Donnell","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2410.22078v2.pdf","comment":"9 pages, 3 figures, and 2 tables. This work has been accepted to 2025\n  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)"},{"id":"http://arxiv.org/abs/2502.11663v1","updated":"2025-02-17T10:53:56Z","published":"2025-02-17T10:53:56Z","title":"MaskGWM: A Generalizable Driving World Model with Video Mask\n  Reconstruction","summary":"  World models that forecast environmental changes from actions are vital for\nautonomous driving models with strong generalization. The prevailing driving\nworld model mainly build on video prediction model. Although these models can\nproduce high-fidelity video sequences with advanced diffusion-based generator,\nthey are constrained by their predictive duration and overall generalization\ncapabilities. In this paper, we explore to solve this problem by combining\ngeneration loss with MAE-style feature-level context learning. In particular,\nwe instantiate this target with three key design: (1) A more scalable Diffusion\nTransformer (DiT) structure trained with extra mask construction task. (2) we\ndevise diffusion-related mask tokens to deal with the fuzzy relations between\nmask reconstruction and generative diffusion process. (3) we extend mask\nconstruction task to spatial-temporal domain by utilizing row-wise mask for\nshifted self-attention rather than masked self-attention in MAE. Then, we adopt\na row-wise cross-view module to align with this mask design. Based on above\nimprovement, we propose MaskGWM: a Generalizable driving World Model embodied\nwith Video Mask reconstruction. Our model contains two variants: MaskGWM-long,\nfocusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view\ngeneration. Comprehensive experiments on standard benchmarks validate the\neffectiveness of the proposed method, which contain normal validation of\nNuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot\nvalidation of Waymo dataset. Quantitative metrics on these datasets show our\nmethod notably improving state-of-the-art driving world model.\n","authors":["Jingcheng Ni","Yuxin Guo","Yichen Liu","Rui Chen","Lewei Lu","Zehuan Wu"],"pdf_url":"https://arxiv.org/pdf/2502.11663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11655v1","updated":"2025-02-17T10:46:47Z","published":"2025-02-17T10:46:47Z","title":"Object-Centric Image to Video Generation with Language Guidance","summary":"  Accurate and flexible world models are crucial for autonomous systems to\nunderstand their environment and predict future events. Object-centric models,\nwith structured latent spaces, have shown promise in modeling object dynamics\nand interactions, but often face challenges in scaling to complex datasets and\nincorporating external guidance, limiting their applicability in robotics. To\naddress these limitations, we propose TextOCVP, an object-centric model for\nimage-to-video generation guided by textual descriptions. TextOCVP parses an\nobserved scene into object representations, called slots, and utilizes a\ntext-conditioned transformer predictor to forecast future object states and\nvideo frames. Our approach jointly models object dynamics and interactions\nwhile incorporating textual guidance, thus leading to accurate and controllable\npredictions. Our method's structured latent space offers enhanced control over\nthe prediction process, outperforming several image-to-video generative\nbaselines. Additionally, we demonstrate that structured object-centric\nrepresentations provide superior controllability and interpretability,\nfacilitating the modeling of object dynamics and enabling more precise and\nunderstandable predictions. Videos and code are available at\nhttps://play-slot.github.io/TextOCVP/.\n","authors":["Angel Villar-Corrales","Gjergj Plepi","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2502.11655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11651v1","updated":"2025-02-17T10:43:38Z","published":"2025-02-17T10:43:38Z","title":"MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease\n  Progression","summary":"  Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at\n\\href{https://github.com/linjiemu/MMXU}{https://github.com/linjiemu/MMXU}.\n","authors":["Linjie Mu","Zhongzhen Huang","Shengqian Qin","Yakun Zhu","Shaoting Zhang","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09269v2","updated":"2025-02-17T10:42:24Z","published":"2025-02-13T12:31:09Z","title":"Memory-based Ensemble Learning in CMR Semantic Segmentation","summary":"  Existing models typically segment either the entire 3D frame or 2D slices\nindependently to derive clinical functional metrics from ventricular\nsegmentation in cardiac cine sequences. While performing well overall, they\nstruggle at the end slices. To address this, we leverage spatial continuity to\nextract global uncertainty from segmentation variance and use it as memory in\nour ensemble learning method, Streaming, for classifier weighting, balancing\noverall and end-slice performance. Additionally, we introduce the End\nCoefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M&Ms\ndatasets show that our framework achieves near-state-of-the-art Dice Similarity\nCoefficient (DSC) and outperforms all models on end-slice performance,\nimproving patient-specific segmentation accuracy.\n","authors":["Yiwei Liu","Ziyi Wu","Liang Zhong","Lingyi Wen","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07546v2","updated":"2025-02-17T10:41:01Z","published":"2024-11-12T04:50:10Z","title":"Contrastive Language Prompting to Ease False Positives in Medical\n  Anomaly Detection","summary":"  A pre-trained visual-language model, contrastive language-image pre-training\n(CLIP), successfully accomplishes various downstream tasks with text prompts,\nsuch as finding images or localizing regions within the image. Despite CLIP's\nstrong multi-modal data capabilities, it remains limited in specialized\nenvironments, such as medical applications. For this purpose, many CLIP\nvariants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives\nrelated to normal regions persist. Thus, we aim to present a simple yet\nimportant goal of reducing false positives in medical anomaly detection. We\nintroduce a Contrastive LAnguage Prompting (CLAP) method that leverages both\npositive and negative text prompts. This straightforward approach identifies\npotential lesion regions by visual attention to the positive prompts in the\ngiven image. To reduce false positives, we attenuate attention on normal\nregions using negative prompts. Extensive experiments with the BMAD dataset,\nincluding six biomedical benchmarks, demonstrate that CLAP method enhances\nanomaly detection performance. Our future plans include developing an automated\nfine prompting method for more practical usage.\n","authors":["YeongHyeon Park","Myung Jin Kim","Hyeong Seok Kim"],"pdf_url":"https://arxiv.org/pdf/2411.07546v2.pdf","comment":"4 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.11642v1","updated":"2025-02-17T10:36:36Z","published":"2025-02-17T10:36:36Z","title":"GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with\n  Pose Guidance from Text","summary":"  In this paper, we introduce GaussianMotion, a novel human rendering model\nthat generates fully animatable scenes aligned with textual descriptions using\nGaussian Splatting. Although existing methods achieve reasonable text-to-3D\ngeneration of human bodies using various 3D representations, they often face\nlimitations in fidelity and efficiency, or primarily focus on static models\nwith limited pose control. In contrast, our method generates fully animatable\n3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score\ndistillation, achieving high fidelity and efficient rendering for arbitrary\nposes. By densely generating diverse random poses during optimization, our\ndeformable 3D human model learns to capture a wide range of natural motions\ndistilled from a pose-conditioned diffusion model in an end-to-end manner.\nFurthermore, we propose Adaptive Score Distillation that effectively balances\nrealistic detail and smoothness to achieve optimal 3D results. Experimental\nresults demonstrate that our approach outperforms existing baselines by\nproducing high-quality textures in both static and animated results, and by\ngenerating diverse 3D human models from various textual inputs.\n","authors":["Gyumin Shim","Sangmin Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2502.11642v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.11638v1","updated":"2025-02-17T10:31:24Z","published":"2025-02-17T10:31:24Z","title":"Enhancing Out-of-Distribution Detection in Medical Imaging with\n  Normalizing Flows","summary":"  Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging\nto ensure reliability and safety by identifying inputs outside a model's\ntraining distribution. Existing methods often require retraining or\nmodifications to pre-trained models, which is impractical for clinical\napplications. This study introduces a post-hoc normalizing flow-based approach\nthat seamlessly integrates with pre-trained models. By leveraging normalizing\nflows, it estimates the likelihood of feature vectors extracted from\npre-trained models, capturing semantically meaningful representations without\nrelying on pixel-level statistics. The method was evaluated using the MedMNIST\nbenchmark and a newly curated MedOOD dataset simulating clinically relevant\ndistributional shifts. Performance was measured using standard OOD detection\nmetrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses\ncomparing it against ten baseline methods. On MedMNIST, the proposed model\nachieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD,\nit achieved an AUROC of 84.61%, demonstrating superior performance against\nother methods. Its post-hoc nature ensures compatibility with existing clinical\nworkflows, addressing the limitations of previous approaches. The model and\ncode to build OOD datasets are available at\nhttps://github.com/dlotfi/MedOODFlow.\n","authors":["Dariush Lotfi","Mohammad-Ali Nikouei Mahani","Mohamad Koohi-Moghadam","Kyongtae Ty Bae"],"pdf_url":"https://arxiv.org/pdf/2502.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12693v2","updated":"2025-02-17T10:28:00Z","published":"2024-12-17T09:10:55Z","title":"SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through\n  Hierarchical Evaluation","summary":"  Current vision-language models may grasp basic spatial cues and simple\ndirections (e.g. left, right, front, back), but struggle with the\nmulti-dimensional spatial reasoning necessary for human-like understanding and\nreal-world applications. To address this gap, we develop SPHERE (Spatial\nPerception and Hierarchical Evaluation of REasoning), a hierarchical evaluation\nframework supported by a new human-annotated dataset. SPHERE systematically\nprobes models across increasing levels of complexity, from fundamental skills\nto multi-skill integration and high-level reasoning that combines spatial,\nvisual, and logical understanding. Benchmark evaluation of state-of-the-art\nmodels reveals significant deficiencies, especially in reasoning about distance\nand proximity, understanding both egocentric and allocentric perspectives, and\napplying spatial logic in physical contexts. These findings expose critical\nblind spots in existing models and underscore the need for more advanced\nspatial reasoning techniques, driving the development of vision-language models\nthat align more closely with human spatial cognition. The dataset will be\nopen-sourced upon publication.\n","authors":["Wenyu Zhang","Wei En Ng","Lixin Ma","Yuwen Wang","Jungqi Zhao","Allison Koenecke","Boyang Li","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00463v2","updated":"2025-02-17T10:13:59Z","published":"2024-12-31T14:22:53Z","title":"SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion\n  Models with Self-Augmented Training","summary":"  The rapid proliferation of AI-generated images necessitates effective\nwatermarking techniques to protect intellectual property and detect fraudulent\ncontent. While existing training-based watermarking methods show promise, they\noften struggle with generalizing across diverse prompts and tend to introduce\nvisible artifacts. To this end, we propose a novel, provably generalizable\nimage watermarking approach for Latent Diffusion Models, termed Self-Augmented\nTraining (SAT-LDM). Our method aligns the training and testing phases through a\nfree generation distribution, thereby enhancing the watermarking module's\ngeneralization capabilities. We theoretically consolidate SAT-LDM by proving\nthat the free generation distribution contributes to its tight generalization\nbound, without the need for additional data collection. Extensive experiments\nshow that SAT-LDM not only achieves robust watermarking but also significantly\nimproves the quality of watermarked images across a wide range of prompts.\nMoreover, our experimental analyses confirm the strong generalization abilities\nof SAT-LDM. We hope that our method provides a practical and efficient solution\nfor securing high-fidelity AI-generated content.\n","authors":["Lu Zhang","Liang Zeng"],"pdf_url":"https://arxiv.org/pdf/2501.00463v2.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.18204v2","updated":"2025-02-17T10:01:36Z","published":"2024-12-24T06:20:01Z","title":"BoxMAC -- A Boxing Dataset for Multi-label Action Classification","summary":"  In competitive combat sports like boxing, analyzing a boxers's performance\nstatics is crucial for evaluating the quantity and variety of punches delivered\nduring bouts. These statistics provide valuable data and feedback, which are\nroutinely used for coaching and performance enhancement. We introduce BoxMAC, a\nreal-world boxing dataset featuring 15 professional boxers and encompassing 13\ndistinct action labels. Comprising over 60,000 frames, our dataset has been\nmeticulously annotated for multiple actions per frame with inputs from a boxing\ncoach. Since two boxers can execute different punches within a single\ntimestamp, this problem falls under the domain of multi-label action\nclassification. We propose a novel architecture for jointly recognizing\nmultiple actions in both individual images and videos. We investigate baselines\nusing deep neural network architectures to address both tasks. We believe that\nBoxMAC will enable researchers and practitioners to develop and evaluate more\nefficient models for performance analysis. With its realistic and diverse\nnature, BoxMAC can serve as a valuable resource for the advancement of boxing\nas a sport\n","authors":["Shashikanta Sahoo"],"pdf_url":"https://arxiv.org/pdf/2412.18204v2.pdf","comment":"Significant modifications are required to improve the clarity and\n  accuracy of the findings and This submission was made without the full\n  agreement of all co-authors. To ensure proper authorship attribution and\n  compliance with ethical guidelines, we are withdrawing this version. A\n  revised and more complete version will be submitted soon"},{"id":"http://arxiv.org/abs/2502.11619v1","updated":"2025-02-17T10:01:24Z","published":"2025-02-17T10:01:24Z","title":"Membership Inference Attacks for Face Images Against Fine-Tuned Latent\n  Diffusion Models","summary":"  The rise of generative image models leads to privacy concerns when it comes\nto the huge datasets used to train such models. This paper investigates the\npossibility of inferring if a set of face images was used for fine-tuning a\nLatent Diffusion Model (LDM). A Membership Inference Attack (MIA) method is\npresented for this task. Using generated auxiliary data for the training of the\nattack model leads to significantly better performance, and so does the use of\nwatermarks. The guidance scale used for inference was found to have a\nsignificant influence. If a LDM is fine-tuned for long enough, the text prompt\nused for inference has no significant influence. The proposed MIA is found to\nbe viable in a realistic black-box setup against LDMs fine-tuned on\nface-images.\n","authors":["Lauritz Christian Holme","Anton Mosquera Storgaard","Siavash Arjomand Bigdeli"],"pdf_url":"https://arxiv.org/pdf/2502.11619v1.pdf","comment":"In Proceedings of the 20th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP\n  2025) - Volume 2: VISAPP, pages 439-446"},{"id":"http://arxiv.org/abs/2502.11618v1","updated":"2025-02-17T10:01:13Z","published":"2025-02-17T10:01:13Z","title":"Real-time Neural Rendering of LiDAR Point Clouds","summary":"  Static LiDAR scanners produce accurate, dense, colored point clouds, but\noften contain obtrusive artifacts which makes them ill-suited for direct\ndisplay. We propose an efficient method to render photorealistic images of such\nscans without any expensive preprocessing or training of a scene-specific\nmodel. A naive projection of the point cloud to the output view using 1x1\npixels is fast and retains the available detail, but also results in\nunintelligible renderings as background points leak in between the foreground\npixels. The key insight is that these projections can be transformed into a\nrealistic result using a deep convolutional model in the form of a U-Net, and a\ndepth-based heuristic that prefilters the data. The U-Net also handles\nLiDAR-specific problems such as missing parts due to occlusion, color\ninconsistencies and varying point densities. We also describe a method to\ngenerate synthetic training data to deal with imperfectly-aligned ground truth\nimages. Our method achieves real-time rendering rates using an off-the-shelf\nGPU and outperforms the state-of-the-art in both speed and quality.\n","authors":["Joni Vanherck","Brent Zoomers","Tom Mertens","Lode Jorissen","Nick Michiels"],"pdf_url":"https://arxiv.org/pdf/2502.11618v1.pdf","comment":"6 pages, 3 figures, 1 table,"},{"id":"http://arxiv.org/abs/2502.08769v2","updated":"2025-02-17T09:54:11Z","published":"2025-02-12T20:17:10Z","title":"Cluster and Predict Latent Patches for Improved Masked Image Modeling","summary":"  Masked Image Modeling (MIM) offers a promising approach to self-supervised\nrepresentation learning, however existing MIM models still lag behind the\nstate-of-the-art. In this paper, we systematically analyze target\nrepresentations, loss functions, and architectures, to introduce CAPI - a novel\npure-MIM framework that relies on the prediction of latent clusterings. Our\napproach leverages a clustering-based loss, which is stable to train, and\nexhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%\naccuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,\nsubstantially outperforming previous MIM methods and approaching the\nperformance of the current state-of-the-art, DINOv2. We release all our code\nand models.\n","authors":["Timothée Darcet","Federico Baldassarre","Maxime Oquab","Julien Mairal","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2502.08769v2.pdf","comment":"13 pages, 7 figures, submitted to TMLR"},{"id":"http://arxiv.org/abs/2502.11586v1","updated":"2025-02-17T09:18:06Z","published":"2025-02-17T09:18:06Z","title":"Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis\n  from Japanese Haiku","summary":"  In the era of the metaverse, where immersive technologies redefine human\nexperiences, translating abstract literary concepts into navigable 3D\nenvironments presents a fundamental challenge in preserving semantic and\nemotional fidelity. This research introduces HaikuVerse, a novel framework for\ntransforming poetic abstraction into spatial representation, with Japanese\nHaiku serving as an ideal test case due to its sophisticated encapsulation of\nprofound emotions and imagery within minimal text. While existing text-to-3D\nmethods struggle with nuanced interpretations, we present a literary-guided\napproach that synergizes traditional poetry analysis with advanced generative\ntechnologies. Our framework centers on two key innovations: (1) Hierarchical\nLiterary-Criticism Theory Grounded Parsing (H-LCTGP), which captures both\nexplicit imagery and implicit emotional resonance through structured semantic\ndecomposition, and (2) Progressive Dimensional Synthesis (PDS), a multi-stage\npipeline that systematically transforms poetic elements into coherent 3D scenes\nthrough sequential diffusion processes, geometric optimization, and real-time\nenhancement. Extensive experiments demonstrate that HaikuVerse significantly\noutperforms conventional text-to-3D approaches in both literary fidelity and\nvisual quality, establishing a new paradigm for preserving cultural heritage in\nimmersive digital spaces. Project website at:\nhttps://syllables-to-scenes.github.io/\n","authors":["Chunan Yu","Yidong Han","Chaotao Ding","Ying Zang","Lanyun Zhu","Xinhao Chen","Zejian Li","Renjun Xu","Tianrun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11586v1.pdf","comment":"16 pages, 11 figures, submitted to IJCAI"},{"id":"http://arxiv.org/abs/2410.10167v3","updated":"2025-02-17T09:11:44Z","published":"2024-10-14T05:23:12Z","title":"X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing","summary":"  Human sensing, which employs various sensors and advanced deep learning\ntechnologies to accurately capture and interpret human body information, has\nsignificantly impacted fields like public security and robotics. However,\ncurrent human sensing primarily depends on modalities such as cameras and\nLiDAR, each of which has its own strengths and limitations. Furthermore,\nexisting multi-modal fusion solutions are typically designed for fixed modality\ncombinations, requiring extensive retraining when modalities are added or\nremoved for diverse scenarios. In this paper, we propose a modality-invariant\nfoundation model for all modalities, X-Fi, to address this issue. X-Fi enables\nthe independent or combinatory use of sensor modalities without additional\ntraining by utilizing a transformer structure to accommodate variable input\nsizes and incorporating a novel \"X-fusion\" mechanism to preserve\nmodality-specific features during multimodal integration. This approach not\nonly enhances adaptability but also facilitates the learning of complementary\nfeatures across modalities. Extensive experiments conducted on the MM-Fi and\nXRF55 datasets, employing six distinct modalities, demonstrate that X-Fi\nachieves state-of-the-art performance in human pose estimation (HPE) and human\nactivity recognition (HAR) tasks. The findings indicate that our proposed model\ncan efficiently support a wide range of human sensing applications, ultimately\ncontributing to the evolution of scalable, multimodal sensing technologies.\n","authors":["Xinyan Chen","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01867v3","updated":"2025-02-17T09:00:41Z","published":"2024-06-04T00:38:44Z","title":"MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by\n  Adversarial Training","summary":"  In text-to-motion generation, controllability as well as generation quality\nand speed has become increasingly critical. The controllability challenges\ninclude generating a motion of a length that matches the given textual\ndescription and editing the generated motions according to control signals,\nsuch as the start-end positions and the pelvis trajectory. In this paper, we\npropose MoLA, which provides fast, high-quality, variable-length motion\ngeneration and can also deal with multiple editing tasks in a single framework.\nOur approach revisits the motion representation used as inputs and outputs in\nthe model, incorporating an activation variable to enable variable-length\nmotion generation. Additionally, we integrate a variational autoencoder and a\nlatent diffusion model, further enhanced through adversarial training, to\nachieve high-quality and fast generation. Moreover, we apply a training-free\nguided generation framework to achieve various editing tasks with motion\ncontrol inputs. We quantitatively show the effectiveness of adversarial\nlearning in text-to-motion generation, and demonstrate the applicability of our\nediting framework to multiple editing tasks in the motion domain.\n","authors":["Kengo Uchida","Takashi Shibuya","Yuhta Takida","Naoki Murata","Julian Tanke","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2406.01867v3.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.11570v1","updated":"2025-02-17T08:59:59Z","published":"2025-02-17T08:59:59Z","title":"Towards a Trustworthy Anomaly Detection for Critical Applications\n  through Approximated Partial AUC Loss","summary":"  Anomaly Detection is a crucial step for critical applications such in the\nindustrial, medical or cybersecurity domains. These sectors share the same\nrequirement of handling differently the different types of classification\nerrors. Indeed, even if false positives are acceptable, false negatives are\nnot, because it would reflect a missed detection of a quality issue, a disease\nor a cyber threat. To fulfill this requirement, we propose a method that\ndynamically applies a trustworthy approximated partial AUC ROC loss (tapAUC). A\nbinary classifier is trained to optimize the specific range of the AUC ROC\ncurve that prevents the True Positive Rate (TPR) to reach 100% while minimizing\nthe False Positive Rate (FPR). The optimal threshold that does not trigger any\nfalse negative is then kept and used at the test step. The results show a TPR\nof 92.52% at a 20.43% FPR for an average across 6 datasets, representing a TPR\nimprovement of 4.3% for a FPR cost of 12.2% against other state-of-the-art\nmethods. The code is available at https://github.com/ArnaudBougaham/tapAUC.\n","authors":["Arnaud Bougaham","Benoît Frénay"],"pdf_url":"https://arxiv.org/pdf/2502.11570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10248v2","updated":"2025-02-17T08:58:33Z","published":"2025-02-14T15:58:10Z","title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model","summary":"  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n","authors":["Guoqing Ma","Haoyang Huang","Kun Yan","Liangyu Chen","Nan Duan","Shengming Yin","Changyi Wan","Ranchen Ming","Xiaoniu Song","Xing Chen","Yu Zhou","Deshan Sun","Deyu Zhou","Jian Zhou","Kaijun Tan","Kang An","Mei Chen","Wei Ji","Qiling Wu","Wen Sun","Xin Han","Yanan Wei","Zheng Ge","Aojie Li","Bin Wang","Bizhu Huang","Bo Wang","Brian Li","Changxing Miao","Chen Xu","Chenfei Wu","Chenguang Yu","Dapeng Shi","Dingyuan Hu","Enle Liu","Gang Yu","Ge Yang","Guanzhe Huang","Gulin Yan","Haiyang Feng","Hao Nie","Haonan Jia","Hanpeng Hu","Hanqi Chen","Haolong Yan","Heng Wang","Hongcheng Guo","Huilin Xiong","Huixin Xiong","Jiahao Gong","Jianchang Wu","Jiaoren Wu","Jie Wu","Jie Yang","Jiashuai Liu","Jiashuo Li","Jingyang Zhang","Junjing Guo","Junzhe Lin","Kaixiang Li","Lei Liu","Lei Xia","Liang Zhao","Liguo Tan","Liwen Huang","Liying Shi","Ming Li","Mingliang Li","Muhua Cheng","Na Wang","Qiaohui Chen","Qinglin He","Qiuyan Liang","Quan Sun","Ran Sun","Rui Wang","Shaoliang Pang","Shiliang Yang","Sitong Liu","Siqi Liu","Shuli Gao","Tiancheng Cao","Tianyu Wang","Weipeng Ming","Wenqing He","Xu Zhao","Xuelin Zhang","Xianfang Zeng","Xiaojia Liu","Xuan Yang","Yaqi Dai","Yanbo Yu","Yang Li","Yineng Deng","Yingming Wang","Yilei Wang","Yuanwei Lu","Yu Chen","Yu Luo","Yuchu Luo","Yuhe Yin","Yuheng Feng","Yuxiang Yang","Zecheng Tang","Zekai Zhang","Zidong Yang","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.10248v2.pdf","comment":"36 pages, 14 figures"},{"id":"http://arxiv.org/abs/2306.16132v4","updated":"2025-02-17T08:43:25Z","published":"2023-06-28T12:01:51Z","title":"High-quality Unknown Object Instance Segmentation via Quadruple Boundary\n  Error Refinement","summary":"  Accurate and efficient segmentation of unknown objects in unstructured\nenvironments is essential for robotic manipulation. Unknown Object Instance\nSegmentation (UOIS), which aims to identify all objects in unknown categories\nand backgrounds, has become a key capability for various robotic tasks.\nHowever, existing methods struggle with over-segmentation and\nunder-segmentation, leading to failures in manipulation tasks such as grasping.\nTo address these challenges, we propose QuBER (Quadruple Boundary Error\nRefinement), a novel error-informed refinement approach for high-quality UOIS.\nQuBER first estimates quadruple boundary errors-true positive, true negative,\nfalse positive, and false negative pixels-at the instance boundaries of the\ninitial segmentation. It then refines the segmentation using an error-guided\nfusion mechanism, effectively correcting both fine-grained and instance-level\nsegmentation errors. Extensive evaluations on three public benchmarks\ndemonstrate that QuBER outperforms state-of-the-art methods and consistently\nimproves various UOIS methods while maintaining a fast inference time of less\nthan 0.1 seconds. Furthermore, we show that QuBER improves the success rate of\ngrasping target objects in cluttered environments. Code and supplementary\nmaterials are available at https://sites.google.com/view/uois-quber.\n","authors":["Seunghyeok Back","Sangbeom Lee","Kangmin Kim","Joosoon Lee","Sungho Shin","Jemo Maeng","Kyoobin Lee"],"pdf_url":"https://arxiv.org/pdf/2306.16132v4.pdf","comment":"8 pages, 7 figures, accepted at ICRA 2025, project website:\n  https://sites.google.com/view/uois-quber"},{"id":"http://arxiv.org/abs/2408.01014v2","updated":"2025-02-17T08:34:38Z","published":"2024-08-02T05:17:14Z","title":"Growth Inhibitors for Suppressing Inappropriate Image Concepts in\n  Diffusion Models","summary":"  Despite their remarkable image generation capabilities, text-to-image\ndiffusion models inadvertently learn inappropriate concepts from vast and\nunfiltered training data, which leads to various ethical and business risks.\nSpecifically, model-generated images may exhibit not safe for work (NSFW)\ncontent and style copyright infringements. The prompts that result in these\nproblems often do not include explicit unsafe words; instead, they contain\nobscure and associative terms, which are referred to as implicit unsafe\nprompts. Existing approaches directly fine-tune models under textual guidance\nto alter the cognition of the diffusion model, thereby erasing inappropriate\nconcepts. This not only requires concept-specific fine-tuning but may also\nincur catastrophic forgetting. To address these issues, we explore the\nrepresentation of inappropriate concepts in the image space and guide them\ntowards more suitable ones by injecting growth inhibitors, which are tailored\nbased on the identified features related to inappropriate concepts during the\ndiffusion process. Additionally, due to the varying degrees and scopes of\ninappropriate concepts, we train an adapter to infer the corresponding\nsuppression scale during the injection process. Our method effectively captures\nthe manifestation of subtle words at the image level, enabling direct and\nefficient erasure of target concepts without the need for fine-tuning. Through\nextensive experimentation, we demonstrate that our approach achieves superior\nerasure results with little effect on other concepts while preserving image\nquality and semantics.\n","authors":["Die Chen","Zhiwen Li","Mingyuan Fan","Cen Chen","Wenmeng Zhou","Yanhao Wang","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2408.01014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14317v4","updated":"2025-02-17T08:18:07Z","published":"2025-01-24T08:22:02Z","title":"Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation","summary":"  Triangle meshes are fundamental to 3D applications, enabling efficient\nmodification and rasterization while maintaining compatibility with standard\nrendering pipelines. However, current automatic mesh generation methods\ntypically rely on intermediate representations that lack the continuous surface\nquality inherent to meshes. Converting these representations into meshes\nproduces dense, suboptimal outputs. Although recent autoregressive approaches\ndemonstrate promise in directly modeling mesh vertices and faces, they are\nconstrained by the limitation in face count, scalability, and structural\nfidelity. To address these challenges, we propose Nautilus, a locality-aware\nautoencoder for artist-like mesh generation that leverages the local properties\nof manifold meshes to achieve structural fidelity and efficient representation.\nOur approach introduces a novel tokenization algorithm that preserves face\nproximity relationships and compresses sequence length through locally shared\nvertices and edges, enabling the generation of meshes with an unprecedented\nscale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point\nConditioner that provides multi-scale geometric guidance, ensuring global\nconsistency and local structural fidelity by capturing fine-grained geometric\nfeatures. Extensive experiments demonstrate that Nautilus significantly\noutperforms state-of-the-art methods in both fidelity and scalability. The\nproject page is at https://nautilusmeshgen.github.io.\n","authors":["Yuxuan Wang","Xuanyu Yi","Haohan Weng","Qingshan Xu","Xiaokang Wei","Xianghui Yang","Chunchao Guo","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.14317v4.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2407.18611v3","updated":"2025-02-17T08:15:19Z","published":"2024-07-26T09:11:25Z","title":"IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs","summary":"  Large-scale Neural Radiance Fields (NeRF) reconstructions are typically\nhindered by the requirement for extensive image datasets and substantial\ncomputational resources. This paper introduces IOVS4NeRF, a framework that\nemploys an uncertainty-guided incremental optimal view selection strategy\nadaptable to various NeRF implementations. Specifically, by leveraging a hybrid\nuncertainty model that combines rendering and positional uncertainties, the\nproposed method calculates the most informative view from among the candidates,\nthereby enabling incremental optimization of scene reconstruction. Our detailed\nexperiments demonstrate that IOVS4NeRF achieves high-fidelity NeRF\nreconstruction with minimal computational resources, making it suitable for\nlarge-scale scene applications.\n","authors":["Jingpeng Xie","Shiyu Tan","Yuanlei Wang","Tianle Du","Yifei Xue","Yizhen Lao"],"pdf_url":"https://arxiv.org/pdf/2407.18611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11534v1","updated":"2025-02-17T08:04:53Z","published":"2025-02-17T08:04:53Z","title":"SurgPose: a Dataset for Articulated Robotic Surgical Tool Pose\n  Estimation and Tracking","summary":"  Accurate and efficient surgical robotic tool pose estimation is of\nfundamental significance to downstream applications such as augmented reality\n(AR) in surgical training and learning-based autonomous manipulation. While\nsignificant advancements have been made in pose estimation for humans and\nanimals, it is still a challenge in surgical robotics due to the scarcity of\npublished data. The relatively large absolute error of the da Vinci end\neffector kinematics and arduous calibration procedure make calibrated\nkinematics data collection expensive. Driven by this limitation, we collected a\ndataset, dubbed SurgPose, providing instance-aware semantic keypoints and\nskeletons for visual surgical tool pose estimation and tracking. By marking\nkeypoints using ultraviolet (UV) reactive paint, which is invisible under white\nlight and fluorescent under UV light, we execute the same trajectory under\ndifferent lighting conditions to collect raw videos and keypoint annotations,\nrespectively. The SurgPose dataset consists of approximately 120k surgical\ninstrument instances (80k for training and 40k for validation) of 6 categories.\nEach instrument instance is labeled with 7 semantic keypoints. Since the videos\nare collected in stereo pairs, the 2D pose can be lifted to 3D based on\nstereo-matching depth. In addition to releasing the dataset, we test a few\nbaseline approaches to surgical instrument tracking to demonstrate the utility\nof SurgPose. More details can be found at surgpose.github.io.\n","authors":["Zijian Wu","Adam Schmidt","Randy Moore","Haoying Zhou","Alexandre Banks","Peter Kazanzides","Septimiu E. Salcudean"],"pdf_url":"https://arxiv.org/pdf/2502.11534v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2502.11532v1","updated":"2025-02-17T08:03:55Z","published":"2025-02-17T08:03:55Z","title":"Control-CLIP: Decoupling Category and Style Guidance in CLIP for\n  Specific-Domain Generation","summary":"  Text-to-image diffusion models have shown remarkable capabilities of\ngenerating high-quality images closely aligned with textual inputs. However,\nthe effectiveness of text guidance heavily relies on the CLIP text encoder,\nwhich is trained to pay more attention to general content but struggles to\ncapture semantics in specific domains like styles. As a result, generation\nmodels tend to fail on prompts like \"a photo of a cat in Pokemon style\" in\nterms of simply producing images depicting \"a photo of a cat\". To fill this\ngap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that\nenables the CLIP model to learn the meaning of category and style in a\ncomplement manner. With specially designed fine-tuning tasks on minimal data\nand a modified cross-attention mechanism, Control-CLIP can precisely guide the\ndiffusion model to a specific domain. Moreover, the parameters of the diffusion\nmodel remain unchanged at all, preserving the original generation performance\nand diversity. Experiments across multiple domains confirm the effectiveness of\nour approach, particularly highlighting its robust plug-and-play capability in\ngenerating content with various specific styles.\n","authors":["Zexi Jia","Chuanwei Huang","Hongyan Fei","Yeshuang Zhu","Zhiqiang Yuan","Jinchao Zhang","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05145v2","updated":"2025-02-17T07:54:08Z","published":"2023-06-08T12:12:02Z","title":"Variable Radiance Field for Real-World Category-Specific Reconstruction\n  from Single Image","summary":"  Reconstructing category-specific objects using Neural Radiance Field (NeRF)\nfrom a single image is a promising yet challenging task. Existing approaches\npredominantly rely on projection-based feature retrieval to associate 3D points\nin the radiance field with local image features from the reference image.\nHowever, this process is computationally expensive, dependent on known camera\nintrinsics, and susceptible to occlusions. To address these limitations, we\npropose Variable Radiance Field (VRF), a novel framework capable of efficiently\nreconstructing category-specific objects without requiring known camera\nintrinsics and demonstrating robustness against occlusions. First, we replace\nthe local feature retrieval with global latent representations, generated\nthrough a single feed-forward pass, which improves efficiency and eliminates\nreliance on camera intrinsics. Second, to tackle coordinate inconsistencies\ninherent in real-world dataset, we define a canonical space by introducing a\nlearnable, category-specific shape template and explicitly aligning each\ntraining object to this template using a learnable 3D transformation. This\napproach also reduces the complexity of geometry prediction to modeling\ndeformations from the template to individual instances. Finally, we employ a\nhyper-network-based method for efficient NeRF creation and enhance the\nreconstruction performance through a contrastive learning-based pretraining\nstrategy. Evaluations on the CO3D dataset demonstrate that VRF achieves\nstate-of-the-art performance in both reconstruction quality and computational\nefficiency.\n","authors":["Kun Wang","Zhiqiang Yan","Zhenyu Zhang","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2306.05145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10797v2","updated":"2025-02-17T07:48:07Z","published":"2024-06-16T03:45:45Z","title":"STAR: Scale-wise Text-conditioned AutoRegressive image generation","summary":"  We introduce STAR, a text-to-image model that employs a scale-wise\nauto-regressive paradigm. Unlike VAR, which is constrained to class-conditioned\nsynthesis for images up to 256$\\times$256, STAR enables text-driven image\ngeneration up to 1024$\\times$1024 through three key designs. First, we\nintroduce a pre-trained text encoder to extract and adopt representations for\ntextual constraints, enhancing details and generalizability. Second, given the\ninherent structural correlation across different scales, we leverage 2D Rotary\nPositional Encoding (RoPE) and tweak it into a normalized version, ensuring\nconsistent interpretation of relative positions across token maps and\nstabilizing the training process. Third, we observe that simultaneously\nsampling all tokens within a single scale can disrupt inter-token\nrelationships, leading to structural instability, particularly in\nhigh-resolution generation. To address this, we propose a novel stable sampling\nmethod that incorporates causal relationships into the sampling process,\nensuring both rich details and stable structures. Compared to previous\ndiffusion models and auto-regressive models, STAR surpasses existing benchmarks\nin fidelity, text-image consistency, and aesthetic quality, requiring just\n2.21s for 1024$\\times$1024 images on A100. This highlights the potential of\nauto-regressive methods in high-quality image synthesis, offering new\ndirections for the text-to-image generation.\n","authors":["Xiaoxiao Ma","Mohan Zhou","Tao Liang","Yalong Bai","Tiejun Zhao","Biye Li","Huaian Chen","Yi Jin"],"pdf_url":"https://arxiv.org/pdf/2406.10797v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.10120v2","updated":"2025-02-17T07:35:28Z","published":"2025-02-14T12:40:37Z","title":"Compress image to patches for Vision Transformer","summary":"  The Vision Transformer (ViT) has made significant strides in the field of\ncomputer vision. However, as the depth of the model and the resolution of the\ninput images increase, the computational cost associated with training and\nrunning ViT models has surged dramatically. This paper proposes a hybrid model\nbased on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a\nmodule called CI2P, which utilizes the CompressAI encoder to compress images\nand subsequently generates a sequence of patches through a series of\nconvolutions. CI2P can replace the Patch Embedding component in the ViT model,\nenabling seamless integration into existing ViT models. Compared to ViT-B/16,\nCI2P-ViT has the number of patches input to the self-attention layer reduced to\na quarter of the original. This design not only significantly reduces the\ncomputational cost of the ViT model but also effectively enhances the model's\naccuracy by introducing the inductive bias properties of CNN. The ViT model's\nprecision is markedly enhanced. When trained from the ground up on the\nAnimals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing\na 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's\ncomputational operations, measured in floating-point operations per second\n(FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in\ntraining velocity on identical hardware configurations.\n","authors":["Xinfeng Zhao","Yaoru Sun"],"pdf_url":"https://arxiv.org/pdf/2502.10120v2.pdf","comment":"15 pages,5 figures"},{"id":"http://arxiv.org/abs/2502.11515v1","updated":"2025-02-17T07:29:36Z","published":"2025-02-17T07:29:36Z","title":"SayAnything: Audio-Driven Lip Synchronization with Conditional Video\n  Diffusion","summary":"  Recent advances in diffusion models have led to significant progress in\naudio-driven lip synchronization. However, existing methods typically rely on\nconstrained audio-visual alignment priors or multi-stage learning of\nintermediate representations to force lip motion synthesis. This leads to\ncomplex training pipelines and limited motion naturalness. In this paper, we\npresent SayAnything, a conditional video diffusion framework that directly\nsynthesizes lip movements from audio input while preserving speaker identity.\nSpecifically, we propose three specialized modules including identity\npreservation module, audio guidance module, and editing control module. Our\nnovel design effectively balances different condition signals in the latent\nspace, enabling precise control over appearance, motion, and region-specific\ngeneration without requiring additional supervision signals or intermediate\nrepresentations. Extensive experiments demonstrate that SayAnything generates\nhighly realistic videos with improved lip-teeth coherence, enabling unseen\ncharacters to say anything, while effectively generalizing to animated\ncharacters.\n","authors":["Junxian Ma","Shiwen Wang","Jian Yang","Junyi Hu","Jian Liang","Guosheng Lin","Jingbo chen","Kai Li","Yu Meng"],"pdf_url":"https://arxiv.org/pdf/2502.11515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06155v2","updated":"2025-02-17T07:08:23Z","published":"2025-02-10T05:00:56Z","title":"Efficient-vDiT: Efficient Video Diffusion Transformers With Attention\n  Tile","summary":"  Despite the promise of synthesizing high-fidelity videos, Diffusion\nTransformers (DiTs) with 3D full attention suffer from expensive inference due\nto the complexity of attention computation and numerous sampling steps. For\nexample, the popular Open-Sora-Plan model consumes more than 9 minutes for\ngenerating a single video of 29 frames. This paper addresses the inefficiency\nissue from two aspects: 1) Prune the 3D full attention based on the redundancy\nwithin video data; We identify a prevalent tile-style repetitive pattern in the\n3D attention maps for video data, and advocate a new family of sparse 3D\nattention that holds a linear complexity w.r.t. the number of video frames. 2)\nShorten the sampling process by adopting existing multi-step consistency\ndistillation; We split the entire sampling trajectory into several segments and\nperform consistency distillation within each one to activate few-step\ngeneration capacities. We further devise a three-stage training pipeline to\nconjoin the low-complexity attention and few-step generation capacities.\nNotably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into\nan efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video\ngeneration with a marginal performance trade-off in VBench. In addition, we\ndemonstrate that our approach is amenable to distributed inference, achieving\nan additional 3.91x speedup when running on 4 GPUs with sequence parallelism.\n","authors":["Hangliang Ding","Dacheng Li","Runlong Su","Peiyuan Zhang","Zhijie Deng","Ion Stoica","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11501v1","updated":"2025-02-17T07:05:36Z","published":"2025-02-17T07:05:36Z","title":"Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?","summary":"  Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.\n","authors":["Zichen Wen","Yifeng Gao","Weijia Li","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11501v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.11494v1","updated":"2025-02-17T06:56:28Z","published":"2025-02-17T06:56:28Z","title":"Stop Looking for Important Tokens in Multimodal Language Models:\n  Duplication Matters More","summary":"  Vision tokens in multimodal large language models often dominate huge\ncomputational overhead due to their excessive length compared to linguistic\nmodality. Abundant recent methods aim to solve this problem with token pruning,\nwhich first defines an importance criterion for tokens and then prunes the\nunimportant vision tokens during inference. However, in this paper, we show\nthat the importance is not an ideal indicator to decide whether a token should\nbe pruned. Surprisingly, it usually results in inferior performance than random\ntoken pruning and leading to incompatibility to efficient attention computation\noperators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),\nwhich prunes tokens based on its duplication with other tokens, leading to\nsignificant and training-free acceleration. Concretely, DART selects a small\nsubset of pivot tokens and then retains the tokens with low duplication to the\npivots, ensuring minimal information loss during token pruning. Experiments\ndemonstrate that DART can prune 88.9% vision tokens while maintaining\ncomparable performance, leading to a 1.99$\\times$ and 2.99$\\times$ speed-up in\ntotal time and prefilling stage, respectively, with good compatibility to\nefficient attention operators. Our codes are available at\nhttps://github.com/ZichenWen1/DART.\n","authors":["Zichen Wen","Yifeng Gao","Shaobo Wang","Junyuan Zhang","Qintong Zhang","Weijia Li","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11494v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.11492v1","updated":"2025-02-17T06:54:49Z","published":"2025-02-17T06:54:49Z","title":"Why Vision Language Models Struggle with Visual Arithmetic? Towards\n  Enhanced Chart and Geometry Understanding","summary":"  Vision Language Models (VLMs) have achieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arithmetic, seemingly simple\ncapabilities like object counting or length comparison, which are essential for\nrelevant complex tasks like chart understanding and geometric reasoning. In\nthis work, we first investigate the root causes of this deficiency through a\nsuite of probing tasks focusing on basic visual arithmetic. Our analysis\nreveals that while pre-trained vision encoders typically capture sufficient\ninformation, the text decoder often fails to decode it correctly for arithmetic\nreasoning. To address this, we propose CogAlign, a novel post-training strategy\ninspired by Piaget's theory of cognitive development. CogAlign trains VLMs to\nrecognize invariant properties under visual transformations. We demonstrate\nthat this approach significantly improves the performance of three diverse VLMs\non our proposed probing tasks. Furthermore, CogAlign enhances performance by an\naverage of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching\nsupervised fine-tuning methods while requiring only 60% less training data.\nThese results highlight the effectiveness and generalizability of CogAlign in\nimproving fundamental visual arithmetic capabilities and their transfer to\ndownstream tasks.\n","authors":["Kung-Hsiang Huang","Can Qin","Haoyi Qiu","Philippe Laban","Shafiq Joty","Caiming Xiong","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2502.11492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08507v2","updated":"2025-02-17T06:52:15Z","published":"2024-07-11T13:45:50Z","title":"Bootstrapping Vision-language Models for Self-supervised Remote\n  Physiological Measurement","summary":"  Facial video-based remote physiological measurement is a promising research\narea for detecting human vital signs (e.g., heart rate, respiration frequency)\nin a non-contact way. Conventional approaches are mostly supervised learning,\nrequiring extensive collections of facial videos and synchronously recorded\nphotoplethysmography (PPG) signals. To tackle it, self-supervised learning has\nrecently gained attentions; due to the lack of ground truth PPG signals, its\nperformance is however limited. In this paper, we propose a novel\nself-supervised framework that successfully integrates the popular\nvision-language models (VLMs) into the remote physiological measurement task.\nGiven a facial video, we first augment its positive and negative video samples\nwith varying rPPG signal frequencies. Next, we introduce a frequency-oriented\nvision-text pair generation method by carefully creating contrastive\nspatio-temporal maps from positive and negative samples and designing proper\ntext prompts to describe their relative ratios of signal frequencies. A\npre-trained VLM is employed to extract features for these formed vision-text\npairs and estimate rPPG signals thereafter. We develop a series of generative\nand contrastive learning mechanisms to optimize the VLM, including the\ntext-guided visual map reconstruction task, the vision-text contrastive\nlearning task, and the frequency contrastive and ranking task. Overall, our\nmethod for the first time adapts VLMs to digest and align the frequency-related\nknowledge in vision and text modalities. Extensive experiments on four\nbenchmark datasets demonstrate that it significantly outperforms state of the\nart self-supervised methods.\n","authors":["Zijie Yue","Miaojing Shi","Hanli Wang","Shuai Ding","Qijun Chen","Shanlin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.08507v2.pdf","comment":"International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2405.13459v3","updated":"2025-02-17T06:46:11Z","published":"2024-05-22T09:01:56Z","title":"Adapting Multi-modal Large Language Model to Concept Drift From\n  Pre-training Onwards","summary":"  Multi-modal Large Language Models (MLLMs) frequently face challenges from\nconcept drift when dealing with real-world streaming data, wherein\ndistributions change unpredictably. This mainly includes gradual drift due to\nlong-tailed data and sudden drift from Out-Of-Distribution (OOD) data, both of\nwhich have increasingly drawn the attention of the research community. While\nthese issues have been extensively studied in the individual domain of vision\nor language, their impacts on MLLMs in concept drift settings remain largely\nunderexplored. In this paper, we reveal the susceptibility and vulnerability of\nVision-Language (VL) models to significant biases arising from gradual drift\nand sudden drift, particularly in the pre-training. To effectively address\nthese challenges, we propose a unified framework that extends concept drift\ntheory to the multi-modal domain, enhancing the adaptability of the VL model to\nunpredictable distribution changes. Additionally, a T-distribution based drift\nadapter is proposed to effectively mitigate the bias induced by the gradual\ndrift, which also facilitates the model in distinguishing sudden distribution\nchanges through explicit distribution modeling. Extensive experiments\ndemonstrate our method enhances the efficiency and accuracy of image-text\nalignment in the pre-training of VL models, particularly in the concept drift\nscenario. Moreover, various downstream tasks exhibit significant improvements\nin our model's ability to adapt to the long-tailed open world. Furthermore, we\ncreate a set of multi-modal datasets called OpenMMlo, specifically tailored for\nthe long-tailed open-world setting, to validate our findings. To foster the\ndevelopment of the multi-modal community, we have made both OpenMMlo datasets\nand our code publicly available at:\nhttps://github.com/XiaoyuYoung/ConceptDriftMLLMs.\n","authors":["Xiaoyu Yang","Jie Lu","En Yu"],"pdf_url":"https://arxiv.org/pdf/2405.13459v3.pdf","comment":"ICLR 2025 Poster"},{"id":"http://arxiv.org/abs/2502.11481v1","updated":"2025-02-17T06:35:37Z","published":"2025-02-17T06:35:37Z","title":"Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound\n  Videos","summary":"  The intersection of medical imaging and artificial intelligence has become an\nimportant research direction in intelligent medical treatment, particularly in\nthe analysis of medical images using deep learning for clinical diagnosis.\nDespite the advances, existing keyframe classification methods lack extraction\nof time series features, while ultrasonic video classification based on\nthree-dimensional convolution requires uniform frame numbers across patients,\nresulting in poor feature extraction efficiency and model classification\nperformance. This study proposes a novel video classification method based on\nCNN and LSTM, introducing NLP's long and short sentence processing scheme into\nvideo classification for the first time. The method reduces CNN-extracted image\nfeatures to 1x512 dimension, followed by sorting and compressing feature\nvectors for LSTM training. Specifically, feature vectors are sorted by patient\nvideo frame numbers and populated with padding value 0 to form variable\nbatches, with invalid padding values compressed before LSTM training to\nconserve computing resources. Experimental results demonstrate that our\nvariable-frame CNNLSTM method outperforms other approaches across all metrics,\nshowing improvements of 3-6% in F1 score and 1.5% in specificity compared to\nkeyframe methods. The variable-frame CNNLSTM also achieves better accuracy and\nprecision than equal-frame CNNLSTM. These findings validate the effectiveness\nof our approach in classifying variable-frame ultrasound videos and suggest\npotential applications in other medical imaging modalities.\n","authors":["Xiangxiang Cui","Zhongyu Li","Xiayue Fan","Peng Huang","Ying Wang","Meng Yang","Shi Chang","Jihua Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11477v1","updated":"2025-02-17T06:28:53Z","published":"2025-02-17T06:28:53Z","title":"Learning to Sample Effective and Diverse Prompts for Text-to-Image\n  Generation","summary":"  Recent advances in text-to-image diffusion models have achieved impressive\nimage generation capabilities. However, it remains challenging to control the\ngeneration process with desired properties (e.g., aesthetic quality, user\nintention), which can be expressed as black-box reward functions. In this\npaper, we focus on prompt adaptation, which refines the original prompt into\nmodel-preferred prompts to generate desired images. While prior work uses\nreinforcement learning (RL) to optimize prompts, we observe that applying RL\noften results in generating similar postfixes and deterministic behaviors. To\nthis end, we introduce \\textbf{P}rompt \\textbf{A}daptation with\n\\textbf{G}FlowNets (\\textbf{PAG}), a novel approach that frames prompt\nadaptation as a probabilistic inference problem. Our key insight is that\nleveraging Generative Flow Networks (GFlowNets) allows us to shift from reward\nmaximization to sampling from an unnormalized density function, enabling both\nhigh-quality and diverse prompt generation. However, we identify that a naive\napplication of GFlowNets suffers from mode collapse and uncovers a previously\noverlooked phenomenon: the progressive loss of neural plasticity in the model,\nwhich is compounded by inefficient credit assignment in sequential prompt\ngeneration. To address this critical challenge, we develop a systematic\napproach in PAG with flow reactivation, reward-prioritized sampling, and reward\ndecomposition for prompt adaptation. Extensive experiments validate that PAG\nsuccessfully learns to sample effective and diverse prompts for text-to-image\ngeneration. We also show that PAG exhibits strong robustness across various\nreward functions and transferability to different text-to-image models.\n","authors":["Taeyoung Yun","Dinghuai Zhang","Jinkyoo Park","Ling Pan"],"pdf_url":"https://arxiv.org/pdf/2502.11477v1.pdf","comment":"18 pages, 14 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.11468v1","updated":"2025-02-17T05:57:57Z","published":"2025-02-17T05:57:57Z","title":"Semantically Robust Unsupervised Image Translation for Paired Remote\n  Sensing Images","summary":"  Image translation for change detection or classification in bi-temporal\nremote sensing images is unique. Although it can acquire paired images, it is\nstill unsupervised. Moreover, strict semantic preservation in translation is\nalways needed instead of multimodal outputs. In response to these problems,\nthis paper proposes a new method, SRUIT (Semantically Robust Unsupervised\nImage-to-image Translation), which ensures semantically robust translation and\nproduces deterministic output. Inspired by previous works, the method explores\nthe underlying characteristics of bi-temporal Remote Sensing images and designs\nthe corresponding networks. Firstly, we assume that bi-temporal Remote Sensing\nimages share the same latent space, for they are always acquired from the same\nland location. So SRUIT makes the generators share their high-level layers, and\nthis constraint will compel two domain mapping to fall into the same latent\nspace. Secondly, considering land covers of bi-temporal images could evolve\ninto each other, SRUIT exploits the cross-cycle-consistent adversarial networks\nto translate from one to the other and recover them. Experimental results show\nthat constraints of sharing weights and cross-cycle consistency enable\ntranslated images with both good perceptual image quality and semantic\npreservation for significant differences.\n","authors":["Sheng Fang","Kaiyu Li","Zhe Li","Jianli Zhao","Xingli Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05331v3","updated":"2025-02-17T05:54:20Z","published":"2024-12-05T07:44:40Z","title":"Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object\n  Detection and Motion Tracking","summary":"  This project aims to develop a robust video surveillance system, which can\nsegment videos into smaller clips based on the detection of activities. It uses\nCCTV footage, for example, to record only major events-like the appearance of a\nperson or a thief-so that storage is optimized and digital searches are easier.\nIt utilizes the latest techniques in object detection and tracking, including\nConvolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well\nas Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks\n(LSTMs), to achieve high accuracy in detection and capture temporal\ndependencies. The approach incorporates adaptive background modeling through\nGaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to\ndetect motions. Multi-scale and contextual analysis are used to improve\ndetection across different object sizes and environments. A hybrid motion\nsegmentation strategy combines statistical and deep learning models to manage\ncomplex movements, while optimizations for real-time processing ensure\nefficient computation. Tracking methods, such as Kalman Filters and Siamese\nnetworks, are employed to maintain smooth tracking even in cases of occlusion.\nDetection is improved on various-sized objects for multiple scenarios by\nmulti-scale and contextual analysis. Results demonstrate high precision and\nrecall in detecting and tracking objects, with significant improvements in\nprocessing times and accuracy due to real-time optimizations and\nillumination-invariant features. The impact of this research lies in its\npotential to transform video surveillance, reducing storage requirements and\nenhancing security through reliable and efficient object detection and\ntracking.\n","authors":["Shahran Rahman Alve"],"pdf_url":"https://arxiv.org/pdf/2412.05331v3.pdf","comment":"15 Pages, 7 Figures"},{"id":"http://arxiv.org/abs/2502.01101v2","updated":"2025-02-17T05:49:03Z","published":"2025-02-03T06:45:00Z","title":"VidSketch: Hand-drawn Sketch-Driven Video Generation with Diffusion\n  Control","summary":"  With the advancement of generative artificial intelligence, previous studies\nhave achieved the task of generating aesthetic images from hand-drawn sketches,\nfulfilling the public's needs for drawing. However, these methods are limited\nto static images and lack the ability to control video animation generation\nusing hand-drawn sketches. To address this gap, we propose VidSketch, the first\nmethod capable of generating high-quality video animations directly from any\nnumber of hand-drawn sketches and simple text prompts, bridging the divide\nbetween ordinary users and professional artists. Specifically, our method\nintroduces a Level-Based Sketch Control Strategy to automatically adjust the\nguidance strength of sketches during the generation process, accommodating\nusers with varying drawing skills. Furthermore, a TempSpatial Attention\nmechanism is designed to enhance the spatiotemporal consistency of generated\nvideo animations, significantly improving the coherence across frames. You can\nfind more detailed cases on our official website.\n","authors":["Lifan Jiang","Shuang Chen","Boxi Wu","Xiaotong Guan","Jiahui Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.01101v2.pdf","comment":"17pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.09855v2","updated":"2025-02-17T05:35:12Z","published":"2024-10-13T14:28:16Z","title":"Text4Seg: Reimagining Image Segmentation as Text Generation","summary":"  Multimodal Large Language Models (MLLMs) have shown exceptional capabilities\nin vision-language tasks; however, effectively integrating image segmentation\ninto these models remains a significant challenge. In this paper, we introduce\nText4Seg, a novel text-as-mask paradigm that casts image segmentation as a text\ngeneration problem, eliminating the need for additional decoders and\nsignificantly simplifying the segmentation process. Our key innovation is\nsemantic descriptors, a new textual representation of segmentation masks where\neach image patch is mapped to its corresponding text label. This unified\nrepresentation allows seamless integration into the auto-regressive training\npipeline of MLLMs for easier optimization. We demonstrate that representing an\nimage with $16\\times16$ semantic descriptors yields competitive segmentation\nperformance. To enhance efficiency, we introduce the Row-wise Run-Length\nEncoding (R-RLE), which compresses redundant text sequences, reducing the\nlength of semantic descriptors by 74% and accelerating inference by $3\\times$,\nwithout compromising performance. Extensive experiments across various vision\ntasks, such as referring expression segmentation and comprehension, show that\nText4Seg achieves state-of-the-art performance on multiple datasets by\nfine-tuning different MLLM backbones. Our approach provides an efficient,\nscalable solution for vision-centric tasks within the MLLM framework.\n","authors":["Mengcheng Lan","Chaofeng Chen","Yue Zhou","Jiaxing Xu","Yiping Ke","Xinjiang Wang","Litong Feng","Wayne Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09855v2.pdf","comment":"ICLR 2025. Project page: https://mc-lan.github.io/Text4Seg/"},{"id":"http://arxiv.org/abs/2502.11456v1","updated":"2025-02-17T05:29:50Z","published":"2025-02-17T05:29:50Z","title":"Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning\n  Network for Semi-supervised 3D Medical Image Segmentation","summary":"  Semi-supervised 3D medical image segmentation aims to achieve accurate\nsegmentation using few labelled data and numerous unlabelled data. The main\nchallenge in the design of semi-supervised learning methods consists in the\neffective use of the unlabelled data for training. A promising solution\nconsists of ensuring consistent predictions across different views of the data,\nwhere the efficacy of this strategy depends on the accuracy of the\npseudo-labels generated by the model for this consistency learning strategy. In\nthis paper, we introduce a new methodology to produce high-quality\npseudo-labels for a consistency learning strategy to address semi-supervised 3D\nmedical image segmentation. The methodology has three important contributions.\nThe first contribution is the Cooperative Rectification Learning Network (CRLN)\nthat learns multiple prototypes per class to be used as external knowledge\npriors to adaptively rectify pseudo-labels at the voxel level. The second\ncontribution consists of the Dynamic Interaction Module (DIM) to facilitate\npairwise and cross-class interactions between prototypes and multi-resolution\nimage features, enabling the production of accurate voxel-level clues for\npseudo-label rectification. The third contribution is the Cooperative Positive\nSupervision (CPS), which optimises uncertain representations to align with\nunassertive representations of their class distributions, improving the model's\naccuracy in classifying uncertain regions. Extensive experiments on three\npublic 3D medical segmentation datasets demonstrate the effectiveness and\nsuperiority of our semi-supervised learning method.\n","authors":["Yanyan Wang","Kechen Song","Yuyuan Liu","Shuai Ma","Yunhui Yan","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2502.11456v1.pdf","comment":"Medical Image Analysis"},{"id":"http://arxiv.org/abs/2412.17042v3","updated":"2025-02-17T05:13:09Z","published":"2024-12-22T14:49:55Z","title":"Adapting Image-to-Video Diffusion Models for Large-Motion Frame\n  Interpolation","summary":"  With the development of video generation models has advanced significantly in\nrecent years, we adopt large-scale image-to-video diffusion models for video\nframe interpolation. We present a conditional encoder designed to adapt an\nimage-to-video model for large-motion frame interpolation. To enhance\nperformance, we integrate a dual-branch feature extractor and propose a\ncross-frame attention mechanism that effectively captures both spatial and\ntemporal information, enabling accurate interpolations of intermediate frames.\nOur approach demonstrates superior performance on the Fr\\'echet Video Distance\n(FVD) metric when evaluated against other state-of-the-art approaches,\nparticularly in handling large motion scenarios, highlighting advancements in\ngenerative-based methodologies.\n","authors":["Luoxu Jin","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2412.17042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18643v2","updated":"2025-02-17T05:11:30Z","published":"2025-01-29T04:18:51Z","title":"3D Reconstruction of Shoes for Augmented Reality","summary":"  This paper introduces a mobile-based solution that enhances online shoe\nshopping through 3D modeling and Augmented Reality (AR), leveraging the\nefficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D\nimages, the framework generates realistic 3D shoe models from 2D images,\nachieving an average Peak Signal-to-Noise Ratio (PSNR) of 32, and enables\nimmersive AR interactions via smartphones. A custom shoe segmentation dataset\nof 3120 images was created, with the best-performing segmentation model\nachieving an Intersection over Union (IoU) score of 0.95. This paper\ndemonstrates the potential of 3D modeling and AR to revolutionize online\nshopping by offering realistic virtual interactions, with applicability across\nbroader fashion categories.\n","authors":["Pratik Shrestha","Sujan Kapali","Swikar Gautam","Vishal Pokharel","Santosh Giri"],"pdf_url":"https://arxiv.org/pdf/2501.18643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11440v1","updated":"2025-02-17T04:54:47Z","published":"2025-02-17T04:54:47Z","title":"Medical Image Registration Meets Vision Foundation Model: Prototype\n  Learning and Contour Awareness","summary":"  Medical image registration is a fundamental task in medical image analysis,\naiming to establish spatial correspondences between paired images. However,\nexisting unsupervised deformable registration methods rely solely on\nintensity-based similarity metrics, lacking explicit anatomical knowledge,\nwhich limits their accuracy and robustness. Vision foundation models, such as\nthe Segment Anything Model (SAM), can generate high-quality segmentation masks\nthat provide explicit anatomical structure knowledge, addressing the\nlimitations of traditional methods that depend only on intensity similarity.\nBased on this, we propose a novel SAM-assisted registration framework\nincorporating prototype learning and contour awareness. The framework includes:\n(1) Explicit anatomical information injection, where SAM-generated segmentation\nmasks are used as auxiliary inputs throughout training and testing to ensure\nthe consistency of anatomical information; (2) Prototype learning, which\nleverages segmentation masks to extract prototype features and aligns\nprototypes to optimize semantic correspondences between images; and (3)\nContour-aware loss, a contour-aware loss is designed that leverages the edges\nof segmentation masks to improve the model's performance in fine-grained\ndeformation fields. Extensive experiments demonstrate that the proposed\nframework significantly outperforms existing methods across multiple datasets,\nparticularly in challenging scenarios with complex anatomical structures and\nambiguous boundaries. Our code is available at\nhttps://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registration.\n","authors":["Hao Xu","Tengfei Xue","Jianan Fan","Dongnan Liu","Yuqian Chen","Fan Zhang","Carl-Fredrik Westin","Ron Kikinis","Lauren J. O'Donnell","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2502.11440v1.pdf","comment":"Accepted by Information Processing in Medical Imaging (IPMI) 2025"},{"id":"http://arxiv.org/abs/2502.11427v1","updated":"2025-02-17T04:38:12Z","published":"2025-02-17T04:38:12Z","title":"Do we Really Need Visual Instructions? Towards Visual Instruction-Free\n  Fine-tuning for Large Vision-Language Models","summary":"  Visual instruction tuning has become the predominant technology in eliciting\nthe multimodal task-solving capabilities of large vision-language models\n(LVLMs). Despite the success, as visual instructions require images as the\ninput, it would leave the gap in inheriting the task-solving capabilities from\nthe backbone LLMs, and make it costly to collect a large-scale dataset. To\naddress it, we propose ViFT, a visual instruction-free fine-tuning framework\nfor LVLMs. In ViFT, we only require the text-only instructions and image\ncaption data during training, to separately learn the task-solving and visual\nperception abilities. During inference, we extract and combine the\nrepresentations of the text and image inputs, for fusing the two abilities to\nfulfill multimodal tasks. Experimental results demonstrate that ViFT can\nachieve state-of-the-art performance on several visual reasoning and visual\ninstruction following benchmarks, with rather less training data. Our code and\ndata will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11427v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2502.11408v1","updated":"2025-02-17T03:49:18Z","published":"2025-02-17T03:49:18Z","title":"Precise GPS-Denied UAV Self-Positioning via Context-Enhanced Cross-View\n  Geo-Localization","summary":"  Image retrieval has been employed as a robust complementary technique to\naddress the challenge of Unmanned Aerial Vehicles (UAVs) self-positioning.\nHowever, most existing methods primarily focus on localizing objects captured\nby UAVs through complex part-based representations, often overlooking the\nunique challenges associated with UAV self-positioning, such as fine-grained\nspatial discrimination requirements and dynamic scene variations. To address\nthe above issues, we propose the Context-Enhanced method for precise UAV\nSelf-Positioning (CEUSP), specifically designed for UAV self-positioning tasks.\nCEUSP integrates a Dynamic Sampling Strategy (DSS) to efficiently select\noptimal negative samples, while the Rubik's Cube Attention (RCA) module,\ncombined with the Context-Aware Channel Integration (CACI) module, enhances\nfeature representation and discrimination by exploiting interdimensional\ninteractions, inspired by the rotational mechanics of a Rubik's Cube. Extensive\nexperimental validate the effectiveness of the proposed method, demonstrating\nnotable improvements in feature representation and UAV self-positioning\naccuracy within complex urban environments. Our approach achieves\nstate-of-the-art performance on the DenseUAV dataset, which is specifically\ndesigned for dense urban contexts, and also delivers competitive results on the\nwidely recognized University-1652 benchmark.\n","authors":["Yuanze Xu","Ming Dai","Wenxiao Cai","Wankou Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11408v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2501.02795v3","updated":"2025-02-17T03:49:14Z","published":"2025-01-06T06:29:55Z","title":"InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion","summary":"  We introduce InfiFusion, an efficient training pipeline designed to integrate\nmultiple domain-specialized Large Language Models (LLMs) into a single pivot\nmodel, effectively harnessing the strengths of each source model. Traditional\nfusion methods either merge model parameters directly or rely on knowledge\ndistillation with rigid assumptions, limiting their flexibility and efficiency.\nInfiFusion overcomes these limitations by enhancing Universal Logit\nDistillation (ULD) with Top-K selection and Logits Standardization. We propose\ntwo fusion strategies: Pairwise Fusion (InfiFusion$_p$), where each source\nmodel knowledge is distilled individually into the pivot model followed by\nmerging and Unified Fusion (InfiFusion$_u$), where knowledge from all source\nmodels is distilled simultaneously into the pivot model. InfiFusion outperforms\nthe state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11\nwidely applied benchmarks covering reasoning, coding, mathematics, and\ninstruction-following tasks. Notably, InfiFusion achieves this superior\nperformance while significantly reduces computational costs, completing full\ntraining with only 160 H800 GPU hours compared to the millions typically\nrequired for traditional LLM training.\n","authors":["Zhaoyi Yan","Yiming Zhang","Baoyi He","Yuhao Fu","Qi Zhou","Zhijie Sang","Chunlin Ji","Shengyu Zhang","Fei Wu","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2501.02795v3.pdf","comment":"Significant performance improvements over the previous version; under\n  review;"},{"id":"http://arxiv.org/abs/2502.11390v1","updated":"2025-02-17T03:12:16Z","published":"2025-02-17T03:12:16Z","title":"MARS: Mesh AutoRegressive Model for 3D Shape Detailization","summary":"  State-of-the-art methods for mesh detailization predominantly utilize\nGenerative Adversarial Networks (GANs) to generate detailed meshes from coarse\nones. These methods typically learn a specific style code for each category or\nsimilar categories without enforcing geometry supervision across different\nLevels of Detail (LODs). Consequently, such methods often fail to generalize\nacross a broader range of categories and cannot ensure shape consistency\nthroughout the detailization process. In this paper, we introduce MARS, a novel\napproach for 3D shape detailization. Our method capitalizes on a novel\nmulti-LOD, multi-category mesh representation to learn shape-consistent mesh\nrepresentations in latent space across different LODs. We further propose a\nmesh autoregressive model capable of generating such latent representations\nthrough next-LOD token prediction. This approach significantly enhances the\nrealism of the generated shapes. Extensive experiments conducted on the\nchallenging 3D Shape Detailization benchmark demonstrate that our proposed MARS\nmodel achieves state-of-the-art performance, surpassing existing methods in\nboth qualitative and quantitative assessments. Notably, the model's capability\nto generate fine-grained details while preserving the overall shape integrity\nis particularly commendable.\n","authors":["Jingnan Gao","Weizhe Liu","Weixuan Sun","Senbo Wang","Xibin Song","Taizhang Shang","Shenzhou Chen","Hongdong Li","Xiaokang Yang","Yichao Yan","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2502.11390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04772v3","updated":"2025-02-17T03:10:52Z","published":"2024-06-07T09:17:33Z","title":"REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning","summary":"  Recent rehearsal-free methods, guided by prompts, excel in vision-related\ncontinual learning (CL) with drifting data but lack resource efficiency, making\nreal-world deployment challenging. In this paper, we introduce\nResource-Efficient Prompting (REP), which improves the computational and memory\nefficiency of prompt-based rehearsal-free methods while minimizing accuracy\ntrade-offs. Our approach employs swift prompt selection to refine input data\nusing a carefully provisioned model and introduces adaptive token merging\n(AToM) and layer dropping (ALD) for efficient prompt updates. AToM and ALD\nselectively skip data and model layers while preserving task-specific features\nduring new-task learning. Extensive experiments on multiple image\nclassification datasets demonstrates REP's superior resource efficiency over\nstate-of-the-art ViT- and CNN-based methods.\n","authors":["Sungho Jeon","Xinyue Ma","Kwang In Kim","Myeongjae Jeon"],"pdf_url":"https://arxiv.org/pdf/2406.04772v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05823v2","updated":"2025-02-17T03:04:57Z","published":"2024-11-05T05:45:26Z","title":"FlexCAD: Unified and Versatile Controllable CAD Generation with\n  Fine-tuned Large Language Models","summary":"  Recently, there is a growing interest in creating computer-aided design (CAD)\nmodels based on user intent, known as controllable CAD generation. Existing\nwork offers limited controllability and needs separate models for different\ntypes of control, reducing efficiency and practicality. To achieve controllable\ngeneration across all CAD construction hierarchies, such as sketch-extrusion,\nextrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by\nfine-tuning large language models (LLMs). First, to enhance comprehension by\nLLMs, we represent a CAD model as a structured text by abstracting each\nhierarchy as a sequence of text tokens. Second, to address various controllable\ngeneration tasks in a unified model, we introduce a hierarchy-aware masking\nstrategy. Specifically, during training, we mask a hierarchy-aware field in the\nCAD text with a mask token. This field, composed of a sequence of tokens, can\nbe set flexibly to represent various hierarchies. Subsequently, we ask LLMs to\npredict this masked field. During inference, the user intent is converted into\na CAD text with a mask token replacing the part the user wants to modify, which\nis then fed into FlexCAD to generate new CAD models. Comprehensive experiments\non public dataset demonstrate the effectiveness of FlexCAD in both generation\nquality and controllability. Code will be available at\nhttps://github.com/microsoft/FlexCAD.\n","authors":["Zhanwei Zhang","Shizhao Sun","Wenxiao Wang","Deng Cai","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2411.05823v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11382v1","updated":"2025-02-17T02:54:14Z","published":"2025-02-17T02:54:14Z","title":"A Physics-Informed Blur Learning Framework for Imaging Systems","summary":"  Accurate blur estimation is essential for high-performance imaging across\nvarious applications. Blur is typically represented by the point spread\nfunction (PSF). In this paper, we propose a physics-informed PSF learning\nframework for imaging systems, consisting of a simple calibration followed by a\nlearning process. Our framework could achieve both high accuracy and universal\napplicability. Inspired by the Seidel PSF model for representing spatially\nvarying PSF, we identify its limitations in optimization and introduce a novel\nwavefront-based PSF model accompanied by an optimization strategy, both\nreducing optimization complexity and improving estimation accuracy. Moreover,\nour wavefront-based PSF model is independent of lens parameters, eliminate the\nneed for prior knowledge of the lens. To validate our approach, we compare it\nwith recent PSF estimation methods (Degradation Transfer and Fast Two-step)\nthrough a deblurring task, where all the estimated PSFs are used to train\nstate-of-the-art deblurring algorithms. Our approach demonstrates improvements\nin image quality in simulation and also showcases noticeable visual quality\nimprovements on real captured images.\n","authors":["Liqun Chen","Yuxuan Li","Jun Dai","Jinwei Gu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2502.11382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11381v1","updated":"2025-02-17T02:53:08Z","published":"2025-02-17T02:53:08Z","title":"Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for\n  UAV-View Geo-Localization","summary":"  UAV-View Geo-Localization (UVGL) aims to ascertain the precise location of a\nUAV by retrieving the most similar GPS-tagged satellite image. However,\nexisting methods predominantly rely on supervised learning paradigms that\nnecessitate annotated paired data for training, which incurs substantial\nannotation costs and impedes large-scale deployment. To overcome this\nlimitation, we propose the Dynamic Memory-Driven and Neighborhood Information\nLearning (DMNIL) network, a lightweight end-to-end self-supervised framework\nfor UAV-view geo-localization. The DMNIL framework utilizes a dual-path\nclustering-based contrastive learning architecture as its baseline to model\nintra-view structural relationships, enhancing feature consistency and\ndiscriminability. Additionally, a dynamic memory-driven hierarchical learning\nmodule is proposed to progressively mine local and global information,\nreinforcing multi-level feature associations to improve model robustness. To\nbridge the domain gap between UAV and satellite views, we design an\ninformation-consistent evolutionary learning mechanism that systematically\nexplores latent correlations within intra-view neighborhoods and across\ncross-view domains, ultimately constructing a unified cross-view feature\nrepresentation space. Extensive experiments on three benchmarks\n(University-1652, SUES-200, and DenseUAV) demonstrate that DMNIL achieves\ncompetitive performance against state-of-the-art supervised methods while\nmaintaining computational efficiency. Notably, this superiority is attained\nwithout relying on paired training data, underscoring the framework's\npracticality for real-world deployment. Codes will be released soon.\n","authors":["Zhongwei Chen","Zhao-Xu Yang","Hai-Jun Rong"],"pdf_url":"https://arxiv.org/pdf/2502.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14750v2","updated":"2025-02-17T02:49:16Z","published":"2024-04-23T05:16:24Z","title":"Grounded Knowledge-Enhanced Medical Vision-Language Pre-training for\n  Chest X-Ray","summary":"  Medical foundation models have the potential to revolutionize healthcare by\nproviding robust and generalized representations of medical data. Medical\nvision-language pre-training has emerged as a promising approach for learning\ndomain-general representations of medical image and text. Current algorithms\nthat exploit global and local alignment between medical image and text could\nhowever be marred by redundant information in medical data. To address this\nissue, we propose a grounded knowledge-enhanced medical vision-language\npre-training (GK-MVLP) framework for chest X-ray. In this framework, medical\nknowledge was grounded to the appropriate anatomical regions by using a\ntransformer-based grounded knowledge-enhanced module for fine-grained alignment\nbetween textural features of medical knowledge and the corresponding anatomical\nregion-level visual features. The performance of GK-MVLP was competitive with\nor exceeded the state of the art on downstream image understanding tasks (chest\nX-ray disease classification, disease localization), generative task (report\ngeneration), and vision-language understanding task (medical visual\nquestion-answering). Our results demonstrate the advantage of incorporating\ngrounding mechanism to remove biases and improve the alignment between chest\nX-ray image and radiology report.\n","authors":["Qiao Deng","Zhongzhen Huang","Yunqi Wang","Zhichuan Wang","Zhao Wang","Xiaofan Zhang","Qi Dou","Yeung Yu Hui","Edward S. Hui"],"pdf_url":"https://arxiv.org/pdf/2404.14750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07887v3","updated":"2025-02-17T02:18:38Z","published":"2024-02-02T12:37:23Z","title":"Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot\n  Representations","summary":"  Several accounts of human cognition posit that our intelligence is rooted in\nour ability to form abstract composable concepts, ground them in our\nenvironment, and reason over these grounded entities. This trifecta of human\nthought has remained elusive in modern intelligent machines. In this work, we\ninvestigate whether slot representations extracted from visual scenes serve as\nappropriate compositional abstractions for grounding and reasoning. We present\nthe Neural Slot Interpreter (NSI), which learns to ground object semantics in\nslots. At the core of NSI is an XML-like schema that uses simple syntax rules\nto organize the object semantics of a scene into object-centric schema\nprimitives. Then, the NSI metric learns to ground primitives into slots through\na structured contrastive learning objective that reasons over the intermodal\nalignment. Experiments with a bi-modal object-property and scene retrieval task\ndemonstrate the grounding efficacy and interpretability of correspondences\nlearned by NSI. From a scene representation standpoint, we find that emergent\nNSI slots that move beyond the image grid by binding to spatial objects\nfacilitate improved visual grounding compared to conventional\nbounding-box-based approaches. From a data efficiency standpoint, we\nempirically validate that NSI learns more generalizable representations from a\nfixed amount of annotation data than the traditional approach. We also show\nthat the grounded slots surpass unsupervised slots in real-world object\ndiscovery and scale with scene complexity. Finally, we investigate the\nreasoning abilities of the grounded slots. Vision Transformers trained on\ngrounding-aware NSI tokenizers using as few as ten tokens outperform\npatch-based tokens on challenging few-shot classification tasks.\n","authors":["Bhishma Dedhia","Niraj K. Jha"],"pdf_url":"https://arxiv.org/pdf/2403.07887v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11360v1","updated":"2025-02-17T02:18:33Z","published":"2025-02-17T02:18:33Z","title":"GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder","summary":"  We introduce GeoDANO, a geometric vision-language model (VLM) with a\ndomain-agnostic vision encoder, for solving plane geometry problems. Although\nVLMs have been employed for solving geometry problems, their ability to\nrecognize geometric features remains insufficiently analyzed. To address this\ngap, we propose a benchmark that evaluates the recognition of visual geometric\nfeatures, including primitives such as dots and lines, and relations such as\northogonality. Our preliminary study shows that vision encoders often used in\ngeneral-purpose VLMs, e.g., OpenCLIP, fail to detect these features and\nstruggle to generalize across domains. We develop GeoCLIP, a CLIP based model\ntrained on synthetic geometric diagram-caption pairs to overcome the\nlimitation. Benchmark results show that GeoCLIP outperforms existing vision\nencoders in recognizing geometric features. We then propose our VLM, GeoDANO,\nwhich augments GeoCLIP with a domain adaptation strategy for unseen diagram\nstyles. GeoDANO outperforms specialized methods for plane geometry problems and\nGPT-4o on MathVerse.\n","authors":["Seunghyuk Cho","Zhenyue Qin","Yang Liu","Youngbin Choi","Seungbeom Lee","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2502.11360v1.pdf","comment":"14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.07701v3","updated":"2025-02-17T02:02:08Z","published":"2025-02-11T16:58:15Z","title":"Magic 1-For-1: Generating One Minute Video Clips within One Minute","summary":"  In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.\n","authors":["Hongwei Yi","Shitong Shao","Tian Ye","Jiantong Zhao","Qingyu Yin","Michael Lingelbach","Li Yuan","Yonghong Tian","Enze Xie","Daquan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07701v3.pdf","comment":"Serious updates are needed"},{"id":"http://arxiv.org/abs/2406.11288v3","updated":"2025-02-17T02:00:52Z","published":"2024-06-17T07:51:44Z","title":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from factuality due to inherent bias or\nincorrect inference. To address this issue, we introduce MFC-Bench, a rigorous\nand comprehensive benchmark designed to evaluate the factual accuracy of LVLMs\nacross three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n","authors":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2406.11288v3.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2502.11338v1","updated":"2025-02-17T01:31:36Z","published":"2025-02-17T01:31:36Z","title":"WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld\n  Radiographic Testing","summary":"  Radiographic testing is a fundamental non-destructive evaluation technique\nfor identifying weld defects and assessing quality in industrial applications\ndue to its high-resolution imaging capabilities. Over the past decade, deep\nlearning techniques have significantly advanced weld defect identification in\nradiographic images. However, conventional approaches, which rely on training\nsmall-scale, task-specific models on single-scenario datasets, exhibit poor\ncross-scenario generalization. Recently, the Segment Anything Model (SAM), a\npre-trained visual foundation model trained on large-scale datasets, has\ndemonstrated exceptional zero-shot generalization capabilities. Fine-tuning SAM\nwith limited domain-specific data has yielded promising results in fields such\nas medical image segmentation and anomaly detection. To the best of our\nknowledge, this work is the first to introduce SAM-based segmentation for\ngeneral weld radiographic testing images. We propose WRT-SAM, a novel weld\nradiographic defect segmentation model that leverages SAM through an\nadapter-based integration with a specialized prompt generator architecture. To\nimprove adaptability to grayscale weld radiographic images, we introduce a\nfrequency prompt generator module, which enhances the model's sensitivity to\nfrequency-domain information. Furthermore, to address the multi-scale nature of\nweld defects, we incorporate a multi-scale prompt generator module, enabling\nthe model to effectively extract and encode defect information across varying\nscales. Extensive experimental evaluations demonstrate that WRT-SAM achieves a\nrecall of 78.87%, a precision of 84.04%, and an AUC of 0.9746, setting a new\nstate-of-the-art (SOTA) benchmark. Moreover, the model exhibits superior\nzero-shot generalization performance, highlighting its potential for practical\ndeployment in diverse radiographic testing scenarios.\n","authors":["Yunyi Zhou","Kun Shi","Gang Hao"],"pdf_url":"https://arxiv.org/pdf/2502.11338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11337v1","updated":"2025-02-17T01:27:35Z","published":"2025-02-17T01:27:35Z","title":"A Comparison of Human and Machine Learning Errors in Face Recognition","summary":"  Machine learning applications in high-stakes scenarios should always operate\nunder human oversight. Developing an optimal combination of human and machine\nintelligence requires an understanding of their complementarities, particularly\nregarding the similarities and differences in the way they make mistakes. We\nperform extensive experiments in the area of face recognition and compare two\nautomated face recognition systems against human annotators through a\ndemographically balanced user study. Our research uncovers important ways in\nwhich machine learning errors and human errors differ from each other, and\nsuggests potential strategies in which human-machine collaboration can improve\naccuracy in face recognition.\n","authors":["Marina Estévez-Almenzar","Ricardo Baeza-Yates","Carlos Castillo"],"pdf_url":"https://arxiv.org/pdf/2502.11337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09873v2","updated":"2025-02-17T01:24:21Z","published":"2024-05-16T07:49:24Z","title":"IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet\n  Transform Feature Modulation Model","summary":"  Infrared image super-resolution demands long-range dependency modeling and\nmulti-scale feature extraction to address challenges such as homogeneous\nbackgrounds, weak edges, and sparse textures. While Mamba-based state-space\nmodels (SSMs) excel in global dependency modeling with linear complexity, their\nblock-wise processing disrupts spatial consistency, limiting their\neffectiveness for IR image reconstruction. We propose IRSRMamba, a novel\nframework integrating wavelet transform feature modulation for multi-scale\nadaptation and an SSMs-based semantic consistency loss to restore fragmented\ncontextual information. This design enhances global-local feature fusion,\nstructural coherence, and fine-detail preservation while mitigating\nblock-induced artifacts. Experiments on benchmark datasets demonstrate that\nIRSRMamba outperforms state-of-the-art methods in PSNR, SSIM, and perceptual\nquality. This work establishes Mamba-based architectures as a promising\ndirection for high-fidelity IR image enhancement. Code are available at\nhttps://github.com/yongsongH/IRSRMamba.\n","authors":["Yongsong Huang","Tomo Miyazaki","Xiaofeng Liu","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2405.09873v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2411.12199v2","updated":"2025-02-17T01:10:00Z","published":"2024-11-19T03:30:44Z","title":"Rethinking Text-Promptable Surgical Instrument Segmentation with Robust\n  Framework","summary":"  Surgical instrument segmentation (SIS) is essential in computer-assisted\nsurgeries, with deep learning methods improving accuracy in complex\nenvironments. Recently, text-promptable segmentation methods have been\nintroduced, generating masks based on textual descriptions. However, they\nassume the text-described object is present and always generate an associated\nmask even when the object is absent. Existing methods address this by using\nprompts only for objects already known to exist in the scene, which relies on\ninaccessible information. To address this, we rethink text-promptable SIS and\nredefine it under robust conditions as Robust text-promptable SIS (R-SIS).\nUnlike previous approaches, R-SIS is a process that analyzes text prompts for\nall surgical instrument categories without relying on external knowledge,\nidentifies the instruments present in the scene, and segments them accordingly.\nBuilding on this, we propose Robust Surgical Instrument Segmentation (RoSIS),\nan optimized framework combining visual and language features for promptable\nsegmentation in the R-SIS setting. RoSIS employs an encoder-decoder\narchitecture with a Multi-Modal Fusion Block (MMFB) and a Selective Gate Block\n(SGB) for balanced integration of vision and language features. Additionally,\nan iterative refinement strategy enhances segmentation masks through a two-step\nprocess: an initial pass with name-based prompts, followed by refinement with\nlocation prompts. Experiments across multiple datasets and settings show that\nRoSIS outperforms existing vision-based and promptable segmentation methods\nunder robust conditions. By rethinking text-promptable SIS, our work\nestablishes a fair and effective approach to surgical instrument segmentation.\n","authors":["Tae-Min Choi","Juyoun Park"],"pdf_url":"https://arxiv.org/pdf/2411.12199v2.pdf","comment":"11 pages, 6 figures, 7 tables, submitted to IEEE Journal of\n  Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2502.11329v1","updated":"2025-02-17T01:04:47Z","published":"2025-02-17T01:04:47Z","title":"Differentially private fine-tuned NF-Net to predict GI cancer type","summary":"  Based on global genomic status, the cancer tumor is classified as\nMicrosatellite Instable (MSI) and Microsatellite Stable (MSS). Immunotherapy is\nused to diagnose MSI, whereas radiation and chemotherapy are used for MSS.\nTherefore, it is significant to classify a gastro-intestinal (GI) cancer tumor\ninto MSI vs. MSS to provide appropriate treatment. The existing literature\nshowed that deep learning could directly predict the class of GI cancer tumors\nfrom histological images. However, deep learning (DL) models are susceptible to\nvarious threats, including membership inference attacks, model extraction\nattacks, etc. These attacks render the use of DL models impractical in\nreal-world scenarios. To make the DL models useful and maintain privacy, we\nintegrate differential privacy (DP) with DL. In particular, this paper aims to\npredict the state of GI cancer while preserving the privacy of sensitive data.\nWe fine-tuned the Normalizer Free Net (NF-Net) model. We obtained an accuracy\nof 88.98\\% without DP to predict (GI) cancer status. When we fine-tuned the\nNF-Net using DP-AdamW and adaptive DP-AdamW, we got accuracies of 74.58% and\n76.48%, respectively. Moreover, we investigate the Weighted Random Sampler\n(WRS) and Class weighting (CW) to solve the data imbalance. We also evaluated\nand analyzed the DP algorithms in different settings.\n","authors":["Sai Venkatesh Chilukoti","Imran Hossen Md","Liqun Shan","Vijay Srinivas Tida","Xiali Hei"],"pdf_url":"https://arxiv.org/pdf/2502.11329v1.pdf","comment":"10 pages, 8 tables, 2 figures"},{"id":"http://arxiv.org/abs/2501.07957v2","updated":"2025-02-17T00:40:03Z","published":"2025-01-14T09:21:17Z","title":"AI Guide Dog: Egocentric Path Prediction on Smartphone","summary":"  This paper presents AI Guide Dog (AIGD), a lightweight egocentric\n(first-person) navigation system for visually impaired users, designed for\nreal-time deployment on smartphones. AIGD employs a vision-only multi-label\nclassification approach to predict directional commands, ensuring safe\nnavigation across diverse environments. We introduce a novel technique for\ngoal-based outdoor navigation by integrating GPS signals and high-level\ndirections, while also handling uncertain multi-path predictions for\ndestination-free indoor navigation. As the first navigation assistance system\nto handle both goal-oriented and exploratory navigation across indoor and\noutdoor settings, AIGD establishes a new benchmark in blind navigation. We\npresent methods, datasets, evaluations, and deployment insights to encourage\nfurther innovations in assistive navigation systems.\n","authors":["Aishwarya Jadhav","Jeffery Cao","Abhishree Shetty","Urvashi Priyam Kumar","Aditi Sharma","Ben Sukboontip","Jayant Sravan Tamarapalli","Jingyi Zhang","Anirudh Koul"],"pdf_url":"https://arxiv.org/pdf/2501.07957v2.pdf","comment":"Accepted at the AAAI 2025 Spring Symposium on Human-Compatible AI for\n  Well-being: Harnessing Potential of GenAI for AI-Powered Science"},{"id":"http://arxiv.org/abs/2403.12326v3","updated":"2025-02-17T00:34:04Z","published":"2024-03-18T23:42:04Z","title":"Hiding and Recovering Knowledge in Text-to-Image Diffusion Models via\n  Learnable Prompts","summary":"  Diffusion models have demonstrated remarkable capability in generating\nhigh-quality visual content from textual descriptions. However, since these\nmodels are trained on large-scale internet data, they inevitably learn\nundesirable concepts, such as sensitive content, copyrighted material, and\nharmful or unethical elements. While previous works focus on permanently\nremoving such concepts, this approach is often impractical, as it can degrade\nmodel performance and lead to irreversible loss of information. In this work,\nwe introduce a novel concept-hiding approach that makes unwanted concepts\ninaccessible to public users while allowing controlled recovery when needed.\nInstead of erasing knowledge from the model entirely, we incorporate a\nlearnable prompt into the cross-attention module, acting as a secure memory\nthat suppresses the generation of hidden concepts unless a secret key is\nprovided. This enables flexible access control -- ensuring that undesirable\ncontent cannot be easily generated while preserving the option to reinstate it\nunder restricted conditions. Our method introduces a new paradigm where concept\nsuppression and controlled recovery coexist, which was not feasible in prior\nworks. We validate its effectiveness on the Stable Diffusion model,\ndemonstrating that hiding concepts mitigate the risks of permanent removal\nwhile maintaining the model's overall capability.\n","authors":["Anh Bui","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2403.12326v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08202v2","updated":"2025-02-17T23:45:08Z","published":"2024-09-12T16:41:47Z","title":"What Makes a Maze Look Like a Maze?","summary":"  A unique aspect of human visual understanding is the ability to flexibly\ninterpret abstract concepts: acquiring lifted rules explaining what they\nsymbolize, grounding them across familiar and unfamiliar contexts, and making\npredictions or reasoning about them. While off-the-shelf vision-language models\nexcel at making literal interpretations of images (e.g., recognizing object\ncategories such as tree branches), they still struggle to make sense of such\nvisual abstractions (e.g., how an arrangement of tree branches may form the\nwalls of a maze). To address this challenge, we introduce Deep Schema Grounding\n(DSG), a framework that leverages explicit structured representations of visual\nabstractions for grounding and reasoning. At the core of DSG are\nschemas--dependency graph descriptions of abstract concepts that decompose them\ninto more primitive-level symbols. DSG uses large language models to extract\nschemas, then hierarchically grounds concrete to abstract components of the\nschema onto images with vision-language models. The grounded schema is used to\naugment visual abstraction understanding. We systematically evaluate DSG and\ndifferent methods in reasoning on our new Visual Abstractions Dataset, which\nconsists of diverse, real-world images of abstract concepts and corresponding\nquestion-answer pairs labeled by humans. We show that DSG significantly\nimproves the abstract visual reasoning performance of vision-language models,\nand is a step toward human-aligned understanding of visual abstractions.\n","authors":["Joy Hsu","Jiayuan Mao","Joshua B. Tenenbaum","Noah D. Goodman","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2409.08202v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12379v1","updated":"2025-02-17T23:31:57Z","published":"2025-02-17T23:31:57Z","title":"OCT Data is All You Need: How Vision Transformers with and without\n  Pre-training Benefit Imaging","summary":"  Optical Coherence Tomography (OCT) provides high-resolution cross-sectional\nimages useful for diagnosing various diseases, but their distinct\ncharacteristics from natural images raise questions about whether large-scale\npre-training on datasets like ImageNet is always beneficial. In this paper, we\ninvestigate the impact of ImageNet-based pre-training on Vision Transformer\n(ViT) performance for OCT image classification across different dataset sizes.\nOur experiments cover four-category retinal pathologies (CNV, DME, Drusen,\nNormal). Results suggest that while pre-training can accelerate convergence and\npotentially offer better performance in smaller datasets, training from scratch\nmay achieve comparable or even superior accuracy when sufficient OCT data is\navailable. Our findings highlight the importance of matching domain\ncharacteristics in pre-training and call for further study on large-scale\nOCT-specific pre-training.\n","authors":["Zihao Han","Philippe De Wilde"],"pdf_url":"https://arxiv.org/pdf/2502.12379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12377v1","updated":"2025-02-17T23:30:50Z","published":"2025-02-17T23:30:50Z","title":"Alignment and Adversarial Robustness: Are More Human-Like Models More\n  Secure?","summary":"  Representational alignment refers to the extent to which a model's internal\nrepresentations mirror biological vision, offering insights into both neural\nsimilarity and functional correspondence. Recently, some more aligned models\nhave demonstrated higher resiliency to adversarial examples, raising the\nquestion of whether more human-aligned models are inherently more secure. In\nthis work, we conduct a large-scale empirical analysis to systematically\ninvestigate the relationship between representational alignment and adversarial\nrobustness. We evaluate 118 models spanning diverse architectures and training\nparadigms, measuring their neural and behavioral alignment and engineering task\nperformance across 106 benchmarks as well as their adversarial robustness via\nAutoAttack. Our findings reveal that while average alignment and robustness\nexhibit a weak overall correlation, specific alignment benchmarks serve as\nstrong predictors of adversarial robustness, particularly those that measure\nselectivity towards texture or shape. These results suggest that different\nforms of alignment play distinct roles in model robustness, motivating further\ninvestigation into how alignment-driven approaches can be leveraged to build\nmore secure and perceptually-grounded vision models.\n","authors":["Blaine Hoak","Kunyang Li","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2502.12377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15171v3","updated":"2025-02-17T23:29:25Z","published":"2024-12-19T18:46:55Z","title":"SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars","summary":"  Gaussian-based human avatars have achieved an unprecedented level of visual\nfidelity. However, existing approaches based on high-capacity neural networks\ntypically require a desktop GPU to achieve real-time performance for a single\navatar, and it remains non-trivial to animate and render such avatars on mobile\ndevices including a standalone VR headset due to substantially limited memory\nand computational bandwidth. In this paper, we present SqueezeMe, a simple and\nhighly effective framework to convert high-fidelity 3D Gaussian full-body\navatars into a lightweight representation that supports both animation and\nrendering with mobile-grade compute. Our key observation is that the decoding\nof pose-dependent Gaussian attributes from a neural network creates\nnon-negligible memory and computational overhead. Inspired by blendshapes and\nlinear pose correctives widely used in Computer Graphics, we address this by\ndistilling the pose correctives learned with neural networks into linear\nlayers. Moreover, we further reduce the parameters by sharing the correctives\namong nearby Gaussians. Combining them with a custom splatting pipeline based\non Vulkan, we achieve, for the first time, simultaneous animation and rendering\nof 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset.\n","authors":["Forrest Iandola","Stanislav Pidhorskyi","Igor Santesteban","Divam Gupta","Anuj Pahuja","Nemanja Bartolovic","Frank Yu","Emanuel Garbin","Tomas Simon","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2412.15171v3.pdf","comment":"v3"},{"id":"http://arxiv.org/abs/2405.18295v3","updated":"2025-02-17T23:10:49Z","published":"2024-05-28T15:48:39Z","title":"Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention","summary":"  In real-life scenarios, humans seek out objects in the 3D world to fulfill\ntheir daily needs or intentions. This inspires us to introduce 3D intention\ngrounding, a new task in 3D object detection employing RGB-D, based on human\nintention, such as \"I want something to support my back\". Closely related, 3D\nvisual grounding focuses on understanding human reference. To achieve detection\nbased on human intention, it relies on humans to observe the scene, reason out\nthe target that aligns with their intention (\"pillow\" in this case), and\nfinally provide a reference to the AI system, such as \"A pillow on the couch\".\nInstead, 3D intention grounding challenges AI agents to automatically observe,\nreason and detect the desired target solely based on human intention. To tackle\nthis challenge, we introduce the new Intent3D dataset, consisting of 44,990\nintention texts associated with 209 fine-grained classes from 1,042 scenes of\nthe ScanNet dataset. We also establish several baselines based on different\nlanguage-based 3D object detection models on our benchmark. Finally, we propose\nIntentNet, our unique approach, designed to tackle this intention-based\ndetection problem. It focuses on three key aspects: intention understanding,\nreasoning to identify object candidates, and cascaded adaptive learning that\nleverages the intrinsic priority logic of different losses for multiple\nobjective optimization. Project Page:\nhttps://weitaikang.github.io/Intent3D-webpage/\n","authors":["Weitai Kang","Mengxue Qu","Jyoti Kini","Yunchao Wei","Mubarak Shah","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2405.18295v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12360v1","updated":"2025-02-17T22:50:45Z","published":"2025-02-17T22:50:45Z","title":"Detecting Systematic Weaknesses in Vision Models along Predefined\n  Human-Understandable Dimensions","summary":"  Studying systematic weaknesses of DNNs has gained prominence in the last few\nyears with the rising focus on building safe AI systems. Slice discovery\nmethods (SDMs) are prominent algorithmic approaches for finding such systematic\nweaknesses. They identify top-k semantically coherent slices/subsets of data\nwhere a DNN-under-test has low performance. For being directly useful, e.g., as\nevidences in a safety argumentation, slices should be aligned with\nhuman-understandable (safety-relevant) dimensions, which, for example, are\ndefined by safety and domain experts as parts of the operational design domain\n(ODD). While straightforward for structured data, the lack of semantic metadata\nmakes these investigations challenging for unstructured data. Therefore, we\npropose a complete workflow which combines contemporary foundation models with\nalgorithms for combinatorial search that consider structured data and DNN\nerrors for finding systematic weaknesses in images. In contrast to existing\napproaches, ours identifies weak slices that are in line with predefined\nhuman-understandable dimensions. As the workflow includes foundation models,\nits intermediate and final results may not always be exact. Therefore, we build\ninto our workflow an approach to address the impact of noisy metadata. We\nevaluate our approach w.r.t. its quality on four popular computer vision\ndatasets, including autonomous driving datasets like Cityscapes, BDD100k, and\nRailSem19, while using multiple state-of-the-art models as DNNs-under-test.\n","authors":["Sujan Sai Gannamaneni","Rohil Prakash Rao","Michael Mock","Maram Akila","Stefan Wrobel"],"pdf_url":"https://arxiv.org/pdf/2502.12360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12359v1","updated":"2025-02-17T22:48:34Z","published":"2025-02-17T22:48:34Z","title":"LanP: Rethinking the Impact of Language Priors in Large Vision-Language\n  Models","summary":"  Large Vision-Language Models (LVLMs) have shown impressive performance in\nvarious tasks. However, LVLMs suffer from hallucination, which hinders their\nadoption in the real world. Existing studies emphasized that the strong\nlanguage priors of LVLMs can overpower visual information, causing\nhallucinations. However, the positive role of language priors is the key to a\npowerful LVLM. If the language priors are too weak, LVLMs will struggle to\nleverage rich parameter knowledge and instruction understanding abilities to\ncomplete tasks in challenging visual scenarios where visual information alone\nis insufficient. Therefore, we propose a benchmark called LanP to rethink the\nimpact of Language Priors in LVLMs. It is designed to investigate how strong\nlanguage priors are in current LVLMs. LanP consists of 170 images and 340\ncorresponding well-designed questions. Extensive experiments on 25 popular\nLVLMs reveal that many LVLMs' language priors are not strong enough to\neffectively aid question answering when objects are partially hidden. Many\nmodels, including GPT-4 Turbo, exhibit an accuracy below 0.5 in such a\nscenario.\n","authors":["Zongyu Wu","Yuwei Niu","Hongcheng Gao","Minhua Lin","Zhiwei Zhang","Zhifang Zhang","Qi Shi","Yilong Wang","Sike Fu","Junjie Xu","Junjie Ao","Enyan Dai","Lei Feng","Xiang Zhang","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12359v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2309.09907v3","updated":"2025-02-17T22:17:19Z","published":"2023-09-18T16:15:16Z","title":"Quantum Vision Clustering","summary":"  Unsupervised visual clustering has garnered significant attention in recent\ntimes, aiming to characterize distributions of unlabeled visual images through\nclustering based on a parameterized appearance approach. Alternatively,\nclustering algorithms can be viewed as assignment problems, often characterized\nas NP-hard, yet precisely solvable for small instances on contemporary\nhardware. Adiabatic quantum computing (AQC) emerges as a promising solution,\npoised to deliver substantial speedups for a range of NP-hard optimization\nproblems. However, existing clustering formulations face challenges in quantum\ncomputing adoption due to scalability issues. In this study, we present the\nfirst clustering formulation tailored for resolution using Adiabatic quantum\ncomputing. An Ising model is introduced to represent the quantum mechanical\nsystem implemented on AQC. The proposed approach demonstrates high\ncompetitiveness compared to state-of-the-art optimization-based methods, even\nwhen utilizing off-the-shelf integer programming solvers. Lastly, this work\nshowcases the solvability of the proposed clustering problem on\ncurrent-generation real quantum computers for small examples and analyzes the\nproperties of the obtained solutions\n","authors":["Xuan Bac Nguyen","Hugh Churchill","Khoa Luu","Samee U. Khan"],"pdf_url":"https://arxiv.org/pdf/2309.09907v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.08837 by other authors"},{"id":"http://arxiv.org/abs/2502.12342v1","updated":"2025-02-17T22:10:47Z","published":"2025-02-17T22:10:47Z","title":"REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark","summary":"  Accurate multi-modal document retrieval is crucial for Retrieval-Augmented\nGeneration (RAG), yet existing benchmarks do not fully capture real-world\nchallenges with their current design. We introduce REAL-MM-RAG, an\nautomatically generated benchmark designed to address four key properties\nessential for real-world retrieval: (i) multi-modal documents, (ii) enhanced\ndifficulty, (iii) Realistic-RAG queries and (iv) accurate labeling.\nAdditionally, we propose a multi-difficulty-level scheme based on query\nrephrasing to evaluate models' semantic understanding beyond keyword matching.\nOur benchmark reveals significant model weaknesses, particularly in handling\ntable-heavy documents and robustness to query rephrasing. To mitigate these\nshortcomings, we curate a rephrased training set and introduce a new\nfinance-focused, table-heavy dataset. Fine-tuning on these datasets enables\nmodels to achieve state-of-the-art retrieval performance on REAL-MM-RAG\nbenchmark. Our work offers a better way to evaluate and improve retrieval in\nmulti-modal RAG systems while also providing training data and models that\naddress current limitations.\n","authors":["Navve Wasserman","Roi Pony","Oshri Naparstek","Adi Raz Goldfarb","Eli Schwartz","Udi Barzelay","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2502.12342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17475v2","updated":"2025-02-17T22:01:31Z","published":"2024-11-25T18:59:54Z","title":"COBRA: A Continual Learning Approach to Vision-Brain Understanding","summary":"  Vision-Brain Understanding (VBU) aims to extract visual information perceived\nby humans from brain activity recorded through functional Magnetic Resonance\nImaging (fMRI). Despite notable advancements in recent years, existing studies\nin VBU continue to face the challenge of catastrophic forgetting, where models\nlose knowledge from prior subjects as they adapt to new ones. Addressing\ncontinual learning in this field is, therefore, essential. This paper\nintroduces a novel framework called Continual Learning for Vision-Brain (COBRA)\nto address continual learning in VBU. Our approach includes three novel\nmodules: a Subject Commonality (SC) module, a Prompt-based Subject Specific\n(PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer\nmodule. The SC module captures shared vision-brain patterns across subjects,\npreserving this knowledge as the model encounters new subjects, thereby\nreducing the impact of catastrophic forgetting. On the other hand, the PSS\nmodule learns unique vision-brain patterns specific to each subject. Finally,\nthe MRIFormer module contains a transformer encoder and decoder that learns the\nfMRI features for VBU from common and specific patterns. In a continual\nlearning setup, COBRA is trained in new PSS and MRIFormer modules for new\nsubjects, leaving the modules of previous subjects unaffected. As a result,\nCOBRA effectively addresses catastrophic forgetting and achieves\nstate-of-the-art performance in both continual learning and vision-brain\nreconstruction tasks, surpassing previous methods.\n","authors":["Xuan-Bac Nguyen","Arabinda Kumar Choudhary","Pawan Sinha","Xin Li","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2411.17475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08779v2","updated":"2025-02-17T20:45:50Z","published":"2025-02-12T20:41:53Z","title":"SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models","summary":"  Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful\nsocietal prejudices, undermining the fairness and equity of AI applications. As\nLMMs grow increasingly influential, addressing and mitigating inherent biases\nrelated to stereotypes, harmful generations, and ambiguous assumptions in\nreal-world scenarios has become essential. However, existing datasets\nevaluating stereotype biases in LMMs often lack diversity and rely on synthetic\nimages, leaving a gap in bias evaluation for real-world visual contexts. To\naddress this, we introduce the Stereotype Bias Benchmark (SB-bench), the most\ncomprehensive framework to date for assessing stereotype biases across nine\ndiverse categories with non-synthetic images. SB-bench rigorously evaluates\nLMMs through carefully curated, visually grounded scenarios, challenging them\nto reason accurately about visual stereotypes. It offers a robust evaluation\nframework featuring real-world visual samples, image variations, and\nmultiple-choice question formats. By introducing visually grounded queries that\nisolate visual biases from textual ones, SB-bench enables a precise and nuanced\nassessment of a model's reasoning capabilities across varying levels of\ndifficulty. Through rigorous testing of state-of-the-art open-source and\nclosed-source LMMs, SB-bench provides a systematic approach to assessing\nstereotype biases in LMMs across key social dimensions. This benchmark\nrepresents a significant step toward fostering fairness in AI systems and\nreducing harmful biases, laying the groundwork for more equitable and socially\nresponsible LMMs. Our code and dataset are publicly available.\n","authors":["Vishal Narnaware","Ashmal Vayani","Rohit Gupta","Sirnam Swetha","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2502.08779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12303v1","updated":"2025-02-17T20:22:52Z","published":"2025-02-17T20:22:52Z","title":"From Gaming to Research: GTA V for Synthetic Data Generation for\n  Robotics and Navigations","summary":"  In computer vision, the development of robust algorithms capable of\ngeneralizing effectively in real-world scenarios more and more often requires\nlarge-scale datasets collected under diverse environmental conditions. However,\nacquiring such datasets is time-consuming, costly, and sometimes unfeasible. To\naddress these limitations, the use of synthetic data has gained attention as a\nviable alternative, allowing researchers to generate vast amounts of data while\nsimulating various environmental contexts in a controlled setting. In this\nstudy, we investigate the use of synthetic data in robotics and navigation,\nspecifically focusing on Simultaneous Localization and Mapping (SLAM) and\nVisual Place Recognition (VPR). In particular, we introduce a synthetic dataset\ncreated using the virtual environment of the video game Grand Theft Auto V (GTA\nV), along with an algorithm designed to generate a VPR dataset, without human\nsupervision. Through a series of experiments centered on SLAM and VPR, we\ndemonstrate that synthetic data derived from GTA V are qualitatively comparable\nto real-world data. Furthermore, these synthetic data can complement or even\nsubstitute real-world data in these applications. This study sets the stage for\nthe creation of large-scale synthetic datasets, offering a cost-effective and\nscalable solution for future research and development.\n","authors":["Matteo Scucchia","Matteo Ferrara","Davide Maltoni"],"pdf_url":"https://arxiv.org/pdf/2502.12303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12300v1","updated":"2025-02-17T20:21:33Z","published":"2025-02-17T20:21:33Z","title":"Per-channel autoregressive linear prediction padding in tiled CNN\n  processing of 2D spatial data","summary":"  We present linear prediction as a differentiable padding method. For each\nchannel, a stochastic autoregressive linear model is fitted to the padding\ninput by minimizing its noise terms in the least-squares sense. The padding is\nformed from the expected values of the autoregressive model given the known\npixels. We trained the convolutional RVSR super-resolution model from scratch\non satellite image data, using different padding methods. Linear prediction\npadding slightly reduced the mean square super-resolution error compared to\nzero and replication padding, with a moderate increase in time cost. Linear\nprediction padding better approximated satellite image data and RVSR feature\nmap data. With zero padding, RVSR appeared to use more of its capacity to\ncompensate for the high approximation error. Cropping the network output by a\nfew pixels reduced the super-resolution error and the effect of the choice of\npadding method on the error, favoring output cropping with the faster\nreplication and zero padding methods, for the studied workload.\n","authors":["Olli Niemitalo","Otto Rosenberg","Nathaniel Narra","Olli Koskela","Iivari Kunttu"],"pdf_url":"https://arxiv.org/pdf/2502.12300v1.pdf","comment":"18 pages, 20 figures including appendix; to be submitted for review;\n  for source code, see https://doi.org/10.5281/zenodo.14871260"},{"id":"http://arxiv.org/abs/2502.12297v1","updated":"2025-02-17T20:13:43Z","published":"2025-02-17T20:13:43Z","title":"Duo Streamers: A Streaming Gesture Recognition Framework","summary":"  Gesture recognition in resource-constrained scenarios faces significant\nchallenges in achieving high accuracy and low latency. The streaming gesture\nrecognition framework, Duo Streamers, proposed in this paper, addresses these\nchallenges through a three-stage sparse recognition mechanism, an RNN-lite\nmodel with an external hidden state, and specialized training and\npost-processing pipelines, thereby making innovative progress in real-time\nperformance and lightweight design. Experimental results show that Duo\nStreamers matches mainstream methods in accuracy metrics, while reducing the\nreal-time factor by approximately 92.3%, i.e., delivering a nearly 13-fold\nspeedup. In addition, the framework shrinks parameter counts to 1/38 (idle\nstate) and 1/9 (busy state) compared to mainstream models. In summary, Duo\nStreamers not only offers an efficient and practical solution for streaming\ngesture recognition in resource-constrained devices but also lays a solid\nfoundation for extended applications in multimodal and diverse scenarios.\n","authors":["Boxuan Zhu","Sicheng Yang","Zhuo Wang","Haining Liang","Junxiao Shen"],"pdf_url":"https://arxiv.org/pdf/2502.12297v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06380v2","updated":"2025-02-17T20:09:02Z","published":"2025-02-10T12:01:05Z","title":"Structure-preserving contrastive learning for spatial time series","summary":"  Informative representations enhance model performance and generalisability in\ndownstream tasks. However, learning self-supervised representations for\nspatially characterised time series, like traffic interactions, poses\nchallenges as it requires maintaining fine-grained similarity relations in the\nlatent space. In this study, we incorporate two structure-preserving\nregularisers for the contrastive learning of spatial time series: one\nregulariser preserves the topology of similarities between instances, and the\nother preserves the graph geometry of similarities across spatial and temporal\ndimensions. To balance contrastive learning and structure preservation, we\npropose a dynamic mechanism that adaptively weighs the trade-off and stabilises\ntraining. We conduct experiments on multivariate time series classification, as\nwell as macroscopic and microscopic traffic prediction. For all three tasks,\nour approach preserves the structures of similarity relations more effectively\nand improves state-of-the-art task performances. The proposed approach can be\napplied to an arbitrary encoder and is particularly beneficial for time series\nwith spatial or geographical features. Furthermore, this study suggests that\nhigher similarity structure preservation indicates more informative and useful\nrepresentations. This may help to understand the contribution of representation\nlearning in pattern recognition with neural networks. Our code is made openly\naccessible with all resulting data at https://github.com/yiru-jiao/spclt.\n","authors":["Yiru Jiao","Sander van Cranenburgh","Simeon Calvert","Hans van Lint"],"pdf_url":"https://arxiv.org/pdf/2502.06380v2.pdf","comment":"TL;DR: Preserving certain structures of similarity relations in\n  spatio-temporal data can improve downstream task performance via contrastive\n  learning"},{"id":"http://arxiv.org/abs/2501.10209v2","updated":"2025-02-17T19:55:38Z","published":"2025-01-17T14:08:32Z","title":"Hypercone Assisted Contour Generation for Out-of-Distribution Detection","summary":"  Recent advances in the field of out-of-distribution (OOD) detection have\nplaced great emphasis on learning better representations suited to this task.\nWhile there are distance-based approaches, distributional awareness has seldom\nbeen exploited for better performance. We present HAC$_k$-OOD, a novel OOD\ndetection method that makes no distributional assumption about the data, but\nautomatically adapts to its distribution. Specifically, HAC$_k$-OOD constructs\na set of hypercones by maximizing the angular distance to neighbors in a given\ndata-point's vicinity to approximate the contour within which in-distribution\n(ID) data-points lie. Experimental results show state-of-the-art FPR@95 and\nAUROC performance on Near-OOD detection and on Far-OOD detection on the\nchallenging CIFAR-100 benchmark without explicitly training for OOD\nperformance.\n","authors":["Annita Vapsi","Andrés Muñoz","Nancy Thomas","Keshav Ramani","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2501.10209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04594v2","updated":"2025-02-17T19:37:58Z","published":"2024-11-07T10:25:20Z","title":"Verification of Neural Networks against Convolutional Perturbations via\n  Parameterised Kernels","summary":"  We develop a method for the efficient verification of neural networks against\nconvolutional perturbations such as blurring or sharpening. To define input\nperturbations we use well-known camera shake, box blur and sharpen kernels. We\ndemonstrate that these kernels can be linearly parameterised in a way that\nallows for a variation of the perturbation strength while preserving desired\nkernel properties. To facilitate their use in neural network verification, we\ndevelop an efficient way of convolving a given input with these parameterised\nkernels. The result of this convolution can be used to encode the perturbation\nin a verification setting by prepending a linear layer to a given network. This\nleads to tight bounds and a high effectiveness in the resulting verification\nstep. We add further precision by employing input splitting as a branch and\nbound strategy. We demonstrate that we are able to verify robustness on a\nnumber of standard benchmarks where the baseline is unable to provide any\nsafety certificates. To the best of our knowledge, this is the first solution\nfor verifying robustness against specific convolutional perturbations such as\ncamera shake.\n","authors":["Benedikt Brückner","Alessio Lomuscio"],"pdf_url":"https://arxiv.org/pdf/2411.04594v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09980v2","updated":"2025-02-17T19:34:15Z","published":"2025-02-14T08:05:41Z","title":"V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multi-Modal Large Language Models","summary":"  Current autonomous driving vehicles rely mainly on their individual sensors\nto understand surrounding scenes and plan for future trajectories, which can be\nunreliable when the sensors are malfunctioning or occluded. To address this\nproblem, cooperative perception methods via vehicle-to-vehicle (V2V)\ncommunication have been proposed, but they have tended to focus on detection\nand tracking. How those approaches contribute to overall cooperative planning\nperformance is still under-explored. Inspired by recent progress using Large\nLanguage Models (LLMs) to build autonomous driving systems, we propose a novel\nproblem setting that integrates an LLM into cooperative autonomous driving,\nwith the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and\nbenchmark. We also propose our baseline method Vehicle-to-Vehicle Large\nLanguage Model (V2V-LLM), which uses an LLM to fuse perception information from\nmultiple connected autonomous vehicles (CAVs) and answer driving-related\nquestions: grounding, notable object identification, and planning. Experimental\nresults show that our proposed V2V-LLM can be a promising unified model\narchitecture for performing various tasks in cooperative autonomous driving,\nand outperforms other baseline methods that use different fusion approaches.\nOur work also creates a new research direction that can improve the safety of\nfuture autonomous driving systems. Our project website:\nhttps://eddyhkchiu.github.io/v2vllm.github.io/ .\n","authors":["Hsu-kuang Chiu","Ryo Hachiuma","Chien-Yi Wang","Stephen F. Smith","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09980v2.pdf","comment":"Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/"},{"id":"http://arxiv.org/abs/2502.09652v2","updated":"2025-02-17T19:05:43Z","published":"2025-02-11T20:22:00Z","title":"GraphCompNet: A Position-Aware Model for Predicting and Compensating\n  Shape Deviations in 3D Printing","summary":"  This paper introduces a data-driven algorithm for modeling and compensating\nshape deviations in additive manufacturing (AM), addressing challenges in\ngeometric accuracy and batch production. While traditional methods, such as\nanalytical models and metrology, laid the groundwork for geometric precision,\nthey are often impractical for large-scale production. Recent advancements in\nmachine learning (ML) have improved compensation precision, but issues remain\nin generalizing across complex geometries and adapting to position-dependent\nvariations. We present a novel approach for powder bed fusion (PBF) processes,\nusing GraphCompNet, which is a computational framework combining graph-based\nneural networks with a generative adversarial network (GAN)-inspired training\nprocess. By leveraging point cloud data and dynamic graph convolutional neural\nnetworks (DGCNNs), GraphCompNet models complex shapes and incorporates\nposition-specific thermal and mechanical factors. A two-stage adversarial\ntraining procedure iteratively refines compensated designs via a\ncompensator-predictor architecture, offering real-time feedback and\noptimization. Experimental validation across diverse shapes and positions shows\nthe framework significantly improves compensation accuracy (35 to 65 percent)\nacross the entire print space, adapting to position-dependent variations. This\nwork advances the development of Digital Twin technology for AM, enabling\nscalable, real-time monitoring and compensation, and addressing critical gaps\nin AM process control. The proposed method supports high-precision, automated\nindustrial-scale design and manufacturing systems.\n","authors":[" Lei"," Chen","Juheon Lee","Juan Carlos Catana","Tsegai Yhdego","Nathan Moroney","Mohammad Amin Nabian","Hui Wang","Jun Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.09652v2.pdf","comment":"Errors in the Paper: significant mathematical errors that were not\n  noticed before submission, withdraw the paper for corrections"},{"id":"http://arxiv.org/abs/2502.12258v1","updated":"2025-02-17T19:01:27Z","published":"2025-02-17T19:01:27Z","title":"SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale\n  Convolutions and Multiview Attention Mechanisms","summary":"  Efficient segmentation of smoke plumes is crucial for environmental\nmonitoring and industrial safety, enabling the detection and mitigation of\nharmful emissions from activities like quarry blasts and wildfires. Accurate\nsegmentation facilitates environmental impact assessments, timely\ninterventions, and compliance with safety standards. However, existing models\noften face high computational demands and limited adaptability to diverse smoke\nappearances, restricting their deployment in resource-constrained environments.\nTo address these issues, we introduce SmokeNet, a novel deep learning\narchitecture that leverages multiscale convolutions and multiview linear\nattention mechanisms combined with layer-specific loss functions to handle the\ncomplex dynamics of diverse smoke plumes, ensuring efficient and accurate\nsegmentation across varied environments. Additionally, we evaluate SmokeNet's\nperformance and versatility using four datasets, including our quarry blast\nsmoke dataset made available to the community. The results demonstrate that\nSmokeNet maintains a favorable balance between computational efficiency and\nsegmentation accuracy, making it suitable for deployment in environmental\nmonitoring and safety management systems. By contributing a new dataset and\noffering an efficient segmentation model, SmokeNet advances smoke segmentation\ncapabilities in diverse and challenging environments.\n","authors":["Xuesong Liu","Emmett J. Ientilucci"],"pdf_url":"https://arxiv.org/pdf/2502.12258v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.12145v1","updated":"2025-02-17T18:56:20Z","published":"2025-02-17T18:56:20Z","title":"Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented\n  Generation with Flexible User Control","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nmitigate large language model (LLM) hallucinations by incorporating external\nknowledge retrieval. However, existing RAG frameworks often apply retrieval\nindiscriminately,leading to inefficiencies-over-retrieving when unnecessary or\nfailing to retrieve iteratively when required for complex reasoning. Recent\nadaptive retrieval strategies, though adaptively navigates these retrieval\nstrategies, predict only based on query complexity and lacks user-driven\nflexibility, making them infeasible for diverse user application needs. In this\npaper, we introduce a novel user-controllable RAG framework that enables\ndynamic adjustment of the accuracy-cost trade-off. Our approach leverages two\nclassifiers: one trained to prioritize accuracy and another to prioritize\nretrieval efficiency. Via an interpretable control parameter $\\alpha$, users\ncan seamlessly navigate between minimal-cost retrieval and high-accuracy\nretrieval based on their specific requirements. We empirically demonstrate that\nour approach effectively balances accuracy, retrieval cost, and user\ncontrollability, making it a practical and adaptable solution for real-world\napplications.\n","authors":["Jinyan Su","Jennifer Healey","Preslav Nakov","Claire Cardie"],"pdf_url":"https://arxiv.org/pdf/2502.12145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12137v1","updated":"2025-02-17T18:53:42Z","published":"2025-02-17T18:53:42Z","title":"REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to\n  Enhance Wikipedia Tail Biographies through Personal Narratives","summary":"  Wikipedia is an invaluable resource for factual information about a wide\nrange of entities. However, the quality of articles on less-known entities\noften lags behind that of the well-known ones. This study proposes a novel\napproach to enhancing Wikipedia's B and C category biography articles by\nleveraging personal narratives such as autobiographies and biographies. By\nutilizing a multi-staged retrieval-augmented generation technique -- REVerSum\n-- we aim to enrich the informational content of these lesser-known articles.\nOur study reveals that personal narratives can significantly improve the\nquality of Wikipedia articles, providing a rich source of reliable information\nthat has been underutilized in previous studies. Based on crowd-based\nevaluation, REVerSum generated content outperforms the best performing baseline\nby 17% in terms of integrability to the original Wikipedia article and 28.5\\%\nin terms of informativeness. Code and Data are available at:\nhttps://github.com/sayantan11995/wikipedia_enrichment\n","authors":["Sayantan Adak","Pauras Mangesh Meher","Paramita Das","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.12137v1.pdf","comment":"Accepted at COLING2025 Industry Track"},{"id":"http://arxiv.org/abs/2410.21745v4","updated":"2025-02-17T16:26:20Z","published":"2024-10-29T05:18:34Z","title":"RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment","summary":"  Graph clustering is an essential aspect of network analysis that involves\ngrouping nodes into separate clusters. Recent developments in deep learning\nhave resulted in graph clustering, which has proven effective in many\napplications. Nonetheless, these methods often encounter difficulties when\ndealing with real-world graphs, particularly in the presence of noisy edges.\nAdditionally, many denoising graph clustering methods tend to suffer from lower\nperformance, training instability, and challenges in scaling to large datasets\ncompared to non-denoised models. To tackle these issues, we introduce a new\nframework called the Robust Deep Graph Clustering Framework via Dual Soft\nAssignment (RDSA). RDSA consists of three key components: (i) a node embedding\nmodule that effectively integrates the graph's topological features and node\nattributes; (ii) a structure-based soft assignment module that improves graph\nmodularity by utilizing an affinity matrix for node assignments; and (iii) a\nnode-based soft assignment module that identifies community landmarks and\nrefines node assignments to enhance the model's robustness. We assess RDSA on\nvarious real-world datasets, demonstrating its superior performance relative to\nexisting state-of-the-art methods. Our findings indicate that RDSA provides\nrobust clustering across different graph types, excelling in clustering\neffectiveness and robustness, including adaptability to noise, stability, and\nscalability.\n","authors":["Yang Xiang","Li Fan","Tulika Saha","Xiaoying Pang","Yushan Pan","Haiyang Zhang","Chengtao Ji"],"pdf_url":"https://arxiv.org/pdf/2410.21745v4.pdf","comment":"Accepted by DASFAA 2025; Complete version"},{"id":"http://arxiv.org/abs/2502.11921v1","updated":"2025-02-17T15:33:28Z","published":"2025-02-17T15:33:28Z","title":"Joint Evaluation of Fairness and Relevance in Recommender Systems with\n  Pareto Frontier","summary":"  Fairness and relevance are two important aspects of recommender systems\n(RSs). Typically, they are evaluated either (i) separately by individual\nmeasures of fairness and relevance, or (ii) jointly using a single measure that\naccounts for fairness with respect to relevance. However, approach (i) often\ndoes not provide a reliable joint estimate of the goodness of the models, as it\nhas two different best models: one for fairness and another for relevance.\nApproach (ii) is also problematic because these measures tend to be ad-hoc and\ndo not relate well to traditional relevance measures, like NDCG. Motivated by\nthis, we present a new approach for jointly evaluating fairness and relevance\nin RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction\ndata, we compute their Pareto frontier for a pair of existing relevance and\nfairness measures, and then use the distance from the frontier as a measure of\nthe jointly achievable fairness and relevance. Our approach is modular and\nintuitive as it can be computed with existing measures. Experiments with 4 RS\nmodels, 3 re-ranking strategies, and 6 datasets show that existing metrics have\ninconsistent associations with our Pareto-optimal solution, making DPFR a more\nrobust and theoretically well-founded joint measure for assessing fairness and\nrelevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation\n","authors":["Theresia Veronika Rampisela","Tuukka Ruotsalo","Maria Maistro","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2502.11921v1.pdf","comment":"Accepted to TheWebConf/WWW 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.11883v1","updated":"2025-02-17T15:11:09Z","published":"2025-02-17T15:11:09Z","title":"FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information\n  Retrieval Algorithms","summary":"  In modern information retrieval (IR). achieving more than just accuracy is\nessential to sustaining a healthy ecosystem, especially when addressing\nfairness and diversity considerations. To meet these needs, various datasets,\nalgorithms, and evaluation frameworks have been introduced. However, these\nalgorithms are often tested across diverse metrics, datasets, and experimental\nsetups, leading to inconsistencies and difficulties in direct comparisons. This\nhighlights the need for a comprehensive IR toolkit that enables standardized\nevaluation of fairness- and diversity-aware algorithms across different IR\ntasks. To address this challenge, we present FairDiverse, an open-source and\nstandardized toolkit. FairDiverse offers a framework for integrating fair and\ndiverse methods, including pre-processing, in-processing, and post-processing\ntechniques, at different stages of the IR pipeline. The toolkit supports the\nevaluation of 28 fairness and diversity algorithms across 16 base models,\ncovering two core IR tasks (search and recommendation) thereby establishing a\ncomprehensive benchmark. Moreover, FairDiverse is highly extensible, providing\nmultiple APIs that empower IR researchers to swiftly develop and evaluate their\nown fairness and diversity aware models, while ensuring fair comparisons with\nexisting baselines. The project is open-sourced and available on\nhttps://github.com/XuChen0427/FairDiverse.\n","authors":["Chen Xu","Zhirui Deng","Clara Rus","Xiaopeng Ye","Yuanna Liu","Jun Xu","Zhicheng Dou","Ji-Rong Wen","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2502.11883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11747v1","updated":"2025-02-17T12:40:35Z","published":"2025-02-17T12:40:35Z","title":"Multi-Modal Retrieval Augmentation for Open-Ended and\n  Knowledge-Intensive Video Question Answering","summary":"  While current video question answering systems perform well on some tasks\nrequiring only direct visual understanding, they struggle with questions\ndemanding knowledge beyond what is immediately observable in the video content.\nWe refer to this challenging scenario as knowledge-intensive video question\nanswering (KI-VideoQA), where models must retrieve and integrate external\ninformation with visual understanding to generate accurate responses. This work\npresents the first attempt to (1) study multi-modal retrieval-augmented\ngeneration for KI-VideoQA, and (2) go beyond multi-choice questions by studying\nopen-ended questions in this task. Through an extensive empirical study of\nstate-of-the-art retrieval and vision language models in both zero-shot and\nfine-tuned settings, we explore how different retrieval augmentation strategies\ncan enhance knowledge integration in KI-VideoQA. We analyze three key aspects:\n(1) model's effectiveness across different information sources and modalities,\n(2) the impact of heterogeneous multi-modal context integration, and (3)\nmodel's effectiveness across different query formulation and retrieval result\nconsumption. Our results suggest that while retrieval augmentation generally\nimproves performance, its effectiveness varies significantly based on modality\nchoice and retrieval strategy. Additionally, we find that successful knowledge\nintegration often requires careful consideration of query formulation and\noptimal retrieval depth. Our exploration advances state-of-the-art accuracy for\nmultiple choice questions by over 17.5% on the KnowIT VQA dataset.\n","authors":["Md Zarif Ul Alam","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2502.11747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11721v1","updated":"2025-02-17T12:08:18Z","published":"2025-02-17T12:08:18Z","title":"Enhancing Recommendation Explanations through User-Centric Refinement","summary":"  Generating natural language explanations for recommendations has become\nincreasingly important in recommender systems. Traditional approaches typically\ntreat user reviews as ground truth for explanations and focus on improving\nreview prediction accuracy by designing various model architectures. However,\ndue to limitations in data scale and model capability, these explanations often\nfail to meet key user-centric aspects such as factuality, personalization, and\nsentiment coherence, significantly reducing their overall helpfulness to users.\nIn this paper, we propose a novel paradigm that refines initial explanations\ngenerated by existing explainable recommender models during the inference stage\nto enhance their quality in multiple aspects. Specifically, we introduce a\nmulti-agent collaborative refinement framework based on large language models.\nTo ensure alignment between the refinement process and user demands, we employ\na plan-then-refine pattern to perform targeted modifications. To enable\ncontinuous improvements, we design a hierarchical reflection mechanism that\nprovides feedback on the refinement process from both strategic and content\nperspectives. Extensive experiments on three datasets demonstrate the\neffectiveness of our framework.\n","authors":["Jingsen Zhang","Zihang Tian","Xueyang Feng","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05558v3","updated":"2025-02-17T11:34:41Z","published":"2025-02-08T13:08:11Z","title":"Large Memory Network for Recommendation","summary":"  Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.\n","authors":["Hui Lu","Zheng Chai","Yuchao Zheng","Zhe Chen","Deping Xie","Peng Xu","Xun Zhou","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2502.05558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02061v2","updated":"2025-02-17T11:22:58Z","published":"2025-02-04T07:17:54Z","title":"Reason4Rec: Large Language Models for Recommendation with Deliberative\n  User Preference Alignment","summary":"  While recent advancements in aligning Large Language Models (LLMs) with\nrecommendation tasks have shown great potential and promising performance\noverall, these aligned recommendation LLMs still face challenges in complex\nscenarios. This is primarily due to the current alignment approach focusing on\noptimizing LLMs to generate user feedback directly, without incorporating\ndeliberation. To overcome this limitation and develop more reliable LLMs for\nrecommendations, we propose a new Deliberative Recommendation task, which\nincorporates explicit reasoning about user preferences as an additional\nalignment goal. We then introduce the Reasoning-powered Recommender framework\nfor deliberative user preference alignment, designed to enhance reasoning\ncapabilities by utilizing verbalized user feedback in a step-wise manner to\ntackle this task. The framework employs collaborative step-wise experts and\ntailored training strategies for each expert. Experimental results across three\nreal-world datasets demonstrate the rationality of the deliberative task\nformulation and the superior performance of the proposed framework in improving\nboth prediction accuracy and reasoning quality.\n","authors":["Yi Fang","Wenjie Wang","Yang Zhang","Fengbin Zhu","Qifan Wang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2502.02061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11610v1","updated":"2025-02-17T09:54:46Z","published":"2025-02-17T09:54:46Z","title":"Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an\n  LLM-Assisted Benchmark","summary":"  In quantitative SciSci (science of science) studies, accurately identifying\nindividual scholars is paramount for scientific data analysis. However, the\nvariability in how names are represented-due to commonality, abbreviations, and\ndifferent spelling conventions-complicates this task. While identifier systems\nlike ORCID are being developed, many scholars remain unregistered, and numerous\npublications are not included. Scholarly databases such as Clarivate and\nOpenAlex have introduced their own ID systems as preliminary name\ndisambiguation solutions. This study evaluates the effectiveness of these\nsystems across different groups to determine their suitability for various\napplication scenarios. We sampled authors from the top quartile (Q1) of Web of\nScience (WOS) journals based on country, discipline, and number of\ncorresponding author papers. For each group, we selected 100 scholars and\nmeticulously annotated all their papers using a Search-enhanced Large Language\nModel method. Using these annotations, we identified the corresponding IDs in\nOpenAlex and Clarivate, extracted all associated papers, filtered for Q1 WOS\njournals, and calculated precision and recall by comparing against the\nannotated dataset.\n","authors":["Renyu Zhao","Yunxin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11624v4","updated":"2025-02-17T09:26:15Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Effective recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interactive relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant challenges: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations within behavior patterns on the target relation in recommender system\nscenarios. In this work, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interactive relations, and includes a relation chain representation\nlearner and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06% and 12.15% on average across all datasets in terms of\nRecall@10 and NDCG@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Yanwei Yu","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.11624v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11571v1","updated":"2025-02-17T09:05:21Z","published":"2025-02-17T09:05:21Z","title":"FaMTEB: Massive Text Embedding Benchmark in Persian Language","summary":"  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.\n","authors":["Erfan Zinvandi","Morteza Alikhani","Mehran Sarmadi","Zahra Pourbahman","Sepehr Arvin","Reza Kazemi","Arash Amini"],"pdf_url":"https://arxiv.org/pdf/2502.11571v1.pdf","comment":"to appear in ACL 2025"},{"id":"http://arxiv.org/abs/2410.23166v2","updated":"2025-02-17T08:59:45Z","published":"2024-10-30T16:18:22Z","title":"SciPIP: An LLM-based Scientific Paper Idea Proposer","summary":"  The rapid advancement of large language models (LLMs) has opened new\npossibilities for automating the proposal of innovative scientific ideas. This\nprocess involves two key phases: literature retrieval and idea generation.\nHowever, existing approaches often fall short due to their reliance on\nkeyword-based search tools during the retrieval phase, which neglects crucial\nsemantic information and frequently results in incomplete retrieval outcomes.\nSimilarly, in the idea generation phase, current methodologies tend to depend\nsolely on the internal knowledge of LLMs or metadata from retrieved papers,\nthereby overlooking significant valuable insights contained within the full\ntexts. To address these limitations, we introduce SciPIP, an innovative\nframework designed to enhance the LLM-based proposal of scientific ideas\nthrough improvements in both literature retrieval and idea generation. Our\napproach begins with the construction of a comprehensive literature database\nthat supports advanced retrieval based not only on keywords but also on\nsemantics and citation relationships. This is complemented by the introduction\nof a multi-granularity retrieval algorithm aimed at ensuring more thorough and\nexhaustive retrieval results. For the idea generation phase, we propose a\ndual-path framework that effectively integrates both the content of retrieved\npapers and the extensive internal knowledge of LLMs. This integration\nsignificantly boosts the novelty, feasibility, and practical value of proposed\nideas. Our experiments, conducted across various domains such as natural\nlanguage processing and computer vision, demonstrate SciPIP's capability to\ngenerate a multitude of innovative and useful ideas. These findings underscore\nSciPIP's potential as a valuable tool for researchers seeking to advance their\nfields with groundbreaking concepts.\n","authors":["Wenxiao Wang","Lihui Gu","Liye Zhang","Yunxiang Luo","Yi Dai","Chen Shen","Liang Xie","Binbin Lin","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2410.23166v2.pdf","comment":"20 pages, 5 figures, 12 tables. The code has been availabel:\n  https://github.com/cheerss/SciPIP"},{"id":"http://arxiv.org/abs/2306.13887v3","updated":"2025-02-17T08:44:49Z","published":"2023-06-24T07:27:43Z","title":"Cross-domain Recommender Systems via Multimodal Domain Adaptation","summary":"  Collaborative Filtering (CF) has emerged as one of the most prominent\nimplementation strategies for building recommender systems. The key idea is to\nexploit the usage patterns of individuals to generate personalized\nrecommendations. CF techniques, especially for newly launched platforms, often\nface a critical issue known as the data sparsity problem, which greatly limits\ntheir performance. Cross-domain CF alleviates the problem of data sparsity by\nfinding a common set of entities (users or items) across the domains, which\nthen act as a conduit for knowledge transfer. Nevertheless, most real-world\ndatasets are collected from different domains, so they often lack information\nabout anchor points or reference information for entity alignment. This paper\nintroduces a domain adaptation technique to align the embeddings of entities\nacross domains. Our approach first exploits the available textual and visual\ninformation to independently learn a multi-view latent representation for each\nentity in the auxiliary and target domains. The different representations of\nthe entity are then fused to generate the corresponding unified representation.\nA domain classifier is then trained to learn the embedding for the domain\nalignment by fixing the unified features as the anchor points. Experiments on\n\\AS{four} publicly available benchmark datasets indicate the effectiveness of\nour proposed approach.\n","authors":["Adamya Shyam","Ramya Kamani","Venkateswara Rao Kagita","Vikas Kumar"],"pdf_url":"https://arxiv.org/pdf/2306.13887v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2501.07365v3","updated":"2025-02-17T08:40:10Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commerce search interactions and is a key factor for customers at\nproduct explorations. However, its impact on semantic retrieval has not been\nwell studied yet. In this research, we build a multimodal representation for\nproduct items in e-commerce search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v3.pdf","comment":"Accepted at EReL@MIR WWW 2025"},{"id":"http://arxiv.org/abs/2502.09827v2","updated":"2025-02-17T08:34:43Z","published":"2025-02-13T23:53:40Z","title":"Data and Decision Traceability for SDA TAP Lab's Prototype Battle\n  Management System","summary":"  Space Protocol is applying the principles derived from MITRE and NIST's\nSupply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a\ncomplex multi party system to achieve introspection, auditing, and replay of\ndata and decisions that ultimately lead to a end decision. The core goal of\ndecision traceability is to ensure transparency, accountability, and integrity\nwithin the WA system. This is accomplished by providing a clear, auditable path\nfrom the system's inputs all the way to the final decision. This traceability\nenables the system to track the various algorithms and data flows that have\ninfluenced a particular outcome.\n","authors":["Latha Pratti","Samya Bagchi","Yasir Latif"],"pdf_url":"https://arxiv.org/pdf/2502.09827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11490v1","updated":"2025-02-17T06:49:34Z","published":"2025-02-17T06:49:34Z","title":"GPU-accelerated Multi-relational Parallel Graph Retrieval for Web-scale\n  Recommendations","summary":"  Web recommendations provide personalized items from massive catalogs for\nusers, which rely heavily on retrieval stages to trade off the effectiveness\nand efficiency of selecting a small relevant set from billion-scale candidates\nin online digital platforms. As one of the largest Chinese search engine and\nnews feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based\nApproximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance\nestimation and efficient search for relevant items. However, current retrieval\nat Baidu fails in comprehensive user-item relational understanding due to\ndissected interaction modeling, and performs inefficiently in large-scale\ngraph-based ANNS because of suboptimal traversal navigation and the GPU\ncomputational bottleneck under high concurrency. To this end, we propose a\nGPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to\nachieve effective yet efficient retrieval in web-scale recommendations. First,\nwe propose a multi-relational user-item relevance metric learning method that\nunifies diverse user behaviors through multi-objective optimization and employs\na self-covariant loss to enhance pathfinding performance. Second, we develop a\nhierarchical parallel graph-based ANNS to boost graph retrieval throughput,\nwhich conducts breadth-depth-balanced searches on a large-scale item graph and\ncost-effectively handles irregular neural computation via adaptive aggregation\non GPUs. In addition, we integrate system optimization strategies in the\ndeployment of GMP-GR in Baidu. Extensive experiments demonstrate the\nsuperiority of GMP-GR in retrieval accuracy and efficiency. Deployed across\nmore than twenty applications at Baidu, GMP-GR serves hundreds of millions of\nusers with a throughput exceeding one hundred million requests per second.\n","authors":["Zhuoning Guo","Guangxing Chen","Qian Gao","Xiaochao Liao","Jianjia Zheng","Lu Shen","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11471v1","updated":"2025-02-17T06:02:59Z","published":"2025-02-17T06:02:59Z","title":"GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language\n  for Knowledge Graph Completion","summary":"  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines.\n","authors":["Kangyang Luo","Yuzhuo Bai","Cheng Gao","Shuzheng Si","Yingli Shen","Zhu Liu","Zhitong Wang","Cunliang Kong","Wenhao Li","Yufei Huang","Ye Tian","Xuantang Xiong","Lei Han","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11442v1","updated":"2025-02-17T04:58:14Z","published":"2025-02-17T04:58:14Z","title":"Multi-Turn Multi-Modal Question Clarification for Enhanced\n  Conversational Understanding","summary":"  Conversational query clarification enables users to refine their search\nqueries through interactive dialogue, improving search effectiveness.\nTraditional approaches rely on text-based clarifying questions, which often\nfail to capture complex user preferences, particularly those involving visual\nattributes. While recent work has explored single-turn multi-modal\nclarification with images alongside text, such methods do not fully support the\nprogressive nature of user intent refinement over multiple turns. Motivated by\nthis, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,\nwhich combines text and visual modalities to refine user queries in a\nmulti-turn conversation. To facilitate this task, we create a large-scale\ndataset named ClariMM comprising over 13k multi-turn interactions and 33k\nquestion-answer pairs containing multi-modal clarifying questions. We propose\nMario, a retrieval framework that employs a two-phase ranking strategy: initial\nretrieval with BM25, followed by a multi-modal generative re-ranking model that\nintegrates textual and visual information from conversational history. Our\nexperiments show that multi-turn multi-modal clarification outperforms\nuni-modal and single-turn approaches, improving MRR by 12.88%. The gains are\nmost significant in longer interactions, demonstrating the value of progressive\nrefinement for complex queries.\n","authors":["Kimia Ramezan","Alireza Amiri Bavandpour","Yifei Yuan","Clemencia Siro","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2502.11442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14483v4","updated":"2025-02-17T03:10:44Z","published":"2023-10-23T01:29:18Z","title":"Chain-of-Factors Paper-Reviewer Matching","summary":"  With the rapid increase in paper submissions to academic conferences, the\nneed for automated and accurate paper-reviewer matching is more critical than\never. Previous efforts in this area have considered various factors to assess\nthe relevance of a reviewer's expertise to a paper, such as the semantic\nsimilarity, shared topics, and citation connections between the paper and the\nreviewer's previous works. However, most of these studies focus on only one\nfactor, resulting in an incomplete evaluation of the paper-reviewer relevance.\nTo address this issue, we propose a unified model for paper-reviewer matching\nthat jointly considers semantic, topic, and citation factors. To be specific,\nduring training, we instruction-tune a contextualized language model shared\nacross all factors to capture their commonalities and characteristics; during\ninference, we chain the three factors to enable step-by-step, coarse-to-fine\nsearch for qualified reviewers given a submission. Experiments on four datasets\n(one of which is newly contributed by us) spanning various fields such as\nmachine learning, computer vision, information retrieval, and data mining\nconsistently demonstrate the effectiveness of our proposed Chain-of-Factors\nmodel in comparison with state-of-the-art paper-reviewer matching methods and\nscientific pre-trained language models.\n","authors":["Yu Zhang","Yanzhen Shen","SeongKu Kang","Xiusi Chen","Bowen Jin","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2310.14483v4.pdf","comment":"10 pages; Accepted to WWW 2025 (Code:\n  https://github.com/yuzhimanhua/CoF)"},{"id":"http://arxiv.org/abs/2410.10293v3","updated":"2025-02-17T02:51:14Z","published":"2024-10-14T08:47:21Z","title":"FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG","summary":"  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It\nmainly consists of retrieval and generation. The retrieval modules (a.k.a.\nretrievers) aim to find useful information used to facilitate the generation\nmodules (a.k.a. generators). As such, generators' performance largely depends\non the effectiveness and efficiency of retrievers. However, the widely used\nretrieval paradigm remains flat. It treats retrieval procedures as a one-off\ndeal with constant granularity. Despite effectiveness, we argue that they\nsuffer from two limitations: (1) flat retrieval exerts a significant burden on\none retriever; (2) constant granularity limits the ceiling of retrieval\nperformance. In this work, we propose a progressive retrieval paradigm with\ncoarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance\neffectiveness and efficiency. Specifically, FunnelRAG establishes a progressive\nretrieval pipeline by collaborating coarse-to-fine granularity, large-to-small\nquantity, and low-to-high capacity, which can relieve the burden on one\nretriever and also promote the ceiling of retrieval performance. Extensive\nexperiments manifest that FunnelRAG achieves comparable retrieval performance\nwhile the time overhead is reduced by nearly 40 percent.\n","authors":["Xinping Zhao","Yan Zhong","Zetian Sun","Xinshuo Hu","Zhenyu Liu","Dongfang Li","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10293v3.pdf","comment":"18 pages, 6 figures, 13 tables. Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2502.08346v3","updated":"2025-02-17T02:47:18Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11374v1","updated":"2025-02-17T02:41:11Z","published":"2025-02-17T02:41:11Z","title":"Leave No One Behind: Enhancing Diversity While Maintaining Accuracy in\n  Social Recommendation","summary":"  Social recommendation, a branch of algorithms that utilizes social connection\ninformation to construct recommender systems, has demonstrated its\neffectiveness in enhancing recommendation accuracy. However, apart from\naccuracy, the diversity of recommendations also plays a critical role in user\nengagement. Unfortunately, the impact of social recommendation models on\nrecommendation diversity remains largely unexplored. In this study, we\ninvestigate the dual performance of existing social recommendation algorithms\nin terms of accuracy and diversity. Our empirical findings highlight a\nconcerning trend: social recommendation models tend to decrease diversity,\ndespite their accuracy improvements. To address this issue, we propose a novel\napproach called Diversified Social Recommendation (DivSR), which leverages\nrelational knowledge distillation techniques to transfer high-diversity\nstructured knowledge from non-social recommendation models to social\nrecommendation models. DivSR is designed as a simple, model-agnostic framework\nthat integrates seamlessly with existing social recommendation architectures.\nExperimental results on three benchmark datasets demonstrate that DivSR\nsignificantly increases diversity without markedly compromising accuracy across\nvarious social recommendation backbones, achieving a better accuracy-diversity\ntrade-off. Our code and data are publicly available at:\nhttps://github.com/ll0ruc/DivSR\n","authors":["Lei Li","Xiao Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11374v1.pdf","comment":"Accepted by DASFAA2025"},{"id":"http://arxiv.org/abs/2502.11371v1","updated":"2025-02-17T02:36:30Z","published":"2025-02-17T02:36:30Z","title":"RAG vs. GraphRAG: A Systematic Evaluation and Key Insights","summary":"  Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across\nvarious tasks by retrieving relevant information from external sources,\nparticularly on text-based data. For structured data, such as knowledge graphs,\nGraphRAG has been widely used to retrieve relevant information. However, recent\nstudies have revealed that structuring implicit knowledge from text into graphs\ncan benefit certain tasks, extending the application of GraphRAG from graph\ndata to general text-based data. Despite their successful extensions, most\napplications of GraphRAG for text data have been designed for specific tasks\nand datasets, lacking a systematic evaluation and comparison between RAG and\nGraphRAG on widely used text-based benchmarks. In this paper, we systematically\nevaluate RAG and GraphRAG on well-established benchmark tasks, such as Question\nAnswering and Query-based Summarization. Our results highlight the distinct\nstrengths of RAG and GraphRAG across different tasks and evaluation\nperspectives. Inspired by these observations, we investigate strategies to\nintegrate their strengths to improve downstream tasks. Additionally, we provide\nan in-depth discussion of the shortcomings of current GraphRAG approaches and\noutline directions for future research.\n","authors":["Haoyu Han","Harry Shomer","Yu Wang","Yongjia Lei","Kai Guo","Zhigang Hua","Bo Long","Hui Liu","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2502.11371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15005v5","updated":"2025-02-17T01:53:43Z","published":"2024-11-22T15:29:05Z","title":"Multi-granularity Interest Retrieval and Refinement Network for\n  Long-Term User Behavior Modeling in CTR Prediction","summary":"  Click-through Rate (CTR) prediction is crucial for online personalization\nplatforms. Recent advancements have shown that modeling rich user behaviors can\nsignificantly improve the performance of CTR prediction. Current long-term user\nbehavior modeling algorithms predominantly follow two cascading stages. The\nfirst stage retrieves subsequence related to the target item from the long-term\nbehavior sequence, while the second stage models the relationship between the\nsubsequence and the target item. Despite significant progress, these methods\nhave two critical flaws. First, the retrieval query typically includes only\ntarget item information, limiting the ability to capture the user's diverse\ninterests. Second, relational information, such as sequential and interactive\ninformation within the subsequence, is frequently overlooked. Therefore, it\nrequires to be further mined to more accurately model user interests.\n  To this end, we propose Multi-granularity Interest Retrieval and Refinement\nNetwork (MIRRN). Specifically, we first construct queries based on behaviors\nobserved at different time scales to obtain subsequences, each capturing users'\ninterest at various granularities. We then introduce an noval multi-head\nFourier transformer to efficiently learn sequential and interactive information\nwithin the subsequences, leading to more accurate modeling of user interests.\nFinally, we employ multi-head target attention to adaptively assess the impact\nof these multi-granularity interests on the target item. Extensive experiments\nhave demonstrated that MIRRN significantly outperforms state-of-the-art\nbaselines. Furthermore, an A/B test shows that MIRRN increases the average\nnumber of listening songs by 1.32% and the average time of listening songs by\n0.55% on the Huawei Music App. The implementation code is publicly available at\nhttps://github.com/USTC-StarTeam/MIRRN.\n","authors":["Xiang Xu","Hao Wang","Wei Guo","Luankang Zhang","Wanshan Yang","Runlong Yu","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15005v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2312.15490v4","updated":"2025-02-17T01:36:07Z","published":"2023-12-24T14:23:15Z","title":"Diffusion-EXR: Controllable Review Generation for Explainable\n  Recommendation via Diffusion Models","summary":"  Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in\nimage and audio generation tasks. However, there exist few attempts to employ\nDDPM in the text generation, especially review generation under recommendation\nsystems. Fueled by the predicted reviews explainability that justifies\nrecommendations could assist users better understand the recommended items and\nincrease the transparency of recommendation system, we propose a Diffusion\nModel-based Review Generation towards EXplainable Recommendation named\nDiffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by\nincrementally introducing varied levels of Gaussian noise to the sequence of\nword embeddings and learns to reconstruct the original word representations in\nthe reverse process. The nature of DDPM enables our lightweight Transformer\nbackbone to perform excellently in the recommendation review generation task.\nExtensive experimental results have demonstrated that Diffusion-EXR can achieve\nstate-of-the-art review generation for recommendation on two publicly available\nbenchmark datasets.\n","authors":["Ling Li","Shaohua Li","Winda Marantika","Alex C. Kot","Huijing Zhan"],"pdf_url":"https://arxiv.org/pdf/2312.15490v4.pdf","comment":"We request to withdraw our paper from the archive due to significant\n  errors identified in the analysis and conclusions. Upon further review, we\n  realized that these errors undermine the validity of our findings. We plan to\n  conduct additional research to correct these issues and resubmit a revised\n  version in the future"},{"id":"http://arxiv.org/abs/2502.11335v1","updated":"2025-02-17T01:13:45Z","published":"2025-02-17T01:13:45Z","title":"Personalized Ranking on Cascading Behavior Graphs for Accurate\n  Multi-Behavior Recommendation","summary":"  Multi-behavior recommendation predicts items a user may purchase by analyzing\ndiverse behaviors like viewing, adding to a cart, and purchasing. Existing\nmethods fall into two categories: representation learning and graph ranking.\nRepresentation learning generates user and item embeddings to capture latent\ninteraction patterns, leveraging multi-behavior properties for better\ngeneralization. However, these methods often suffer from over-smoothing and\nbias toward frequent interactions, limiting their expressiveness. Graph ranking\nmethods, on the other hand, directly compute personalized ranking scores,\ncapturing user preferences more effectively. Despite their potential, graph\nranking approaches have been primarily explored in single-behavior settings and\nremain underutilized for multi-behavior recommendation. In this paper, we\npropose CascadingRank, a novel graph ranking method for multi-behavior\nrecommendation. It models the natural sequence of user behaviors (e.g.,\nviewing, adding to cart, and purchasing) through a cascading behavior graph. An\niterative algorithm computes ranking scores, ensuring smoothness, query\nfitting, and cascading alignment. Experiments on three real-world datasets\ndemonstrate that CascadingRank outperforms state-of-the-art methods, with up to\n9.56% and 7.16% improvements in HR@10 and NDCG@10, respectively. Furthermore,\nwe provide theoretical analysis highlighting its effectiveness, convergence,\nand scalability, showcasing the advantages of graph ranking in multi-behavior\nrecommendation.\n","authors":["Geonwoo Ko","Minseo Jeon","Jinhong Jung"],"pdf_url":"https://arxiv.org/pdf/2502.11335v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2502.08826v2","updated":"2025-02-17T23:26:44Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v2.pdf","comment":"GitHub repository:\n  https://github.com/llm-lab-org/Multimodal-RAG-Survey"},{"id":"http://arxiv.org/abs/2502.12342v1","updated":"2025-02-17T22:10:47Z","published":"2025-02-17T22:10:47Z","title":"REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark","summary":"  Accurate multi-modal document retrieval is crucial for Retrieval-Augmented\nGeneration (RAG), yet existing benchmarks do not fully capture real-world\nchallenges with their current design. We introduce REAL-MM-RAG, an\nautomatically generated benchmark designed to address four key properties\nessential for real-world retrieval: (i) multi-modal documents, (ii) enhanced\ndifficulty, (iii) Realistic-RAG queries and (iv) accurate labeling.\nAdditionally, we propose a multi-difficulty-level scheme based on query\nrephrasing to evaluate models' semantic understanding beyond keyword matching.\nOur benchmark reveals significant model weaknesses, particularly in handling\ntable-heavy documents and robustness to query rephrasing. To mitigate these\nshortcomings, we curate a rephrased training set and introduce a new\nfinance-focused, table-heavy dataset. Fine-tuning on these datasets enables\nmodels to achieve state-of-the-art retrieval performance on REAL-MM-RAG\nbenchmark. Our work offers a better way to evaluate and improve retrieval in\nmulti-modal RAG systems while also providing training data and models that\naddress current limitations.\n","authors":["Navve Wasserman","Roi Pony","Oshri Naparstek","Adi Raz Goldfarb","Eli Schwartz","Udi Barzelay","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2502.12342v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2502.12096v1","updated":"2025-02-17T18:14:18Z","published":"2025-02-17T18:14:18Z","title":"Token Communications: A Unified Framework for Cross-modal Context-aware\n  Semantic Communications","summary":"  In this paper, we introduce token communications (TokCom), a unified\nframework to leverage cross-modal context information in generative semantic\ncommunications (GenSC). TokCom is a new paradigm, motivated by the recent\nsuccess of generative foundation models and multimodal large language models\n(GFM/MLLMs), where the communication units are tokens, enabling efficient\ntransformer-based token processing at the transmitter and receiver. In this\npaper, we introduce the potential opportunities and challenges of leveraging\ncontext in GenSC, explore how to integrate GFM/MLLMs-based token processing\ninto semantic communication systems to leverage cross-modal context\neffectively, present the key principles for efficient TokCom at various layers\nin future wireless networks. We demonstrate the corresponding TokCom benefits\nin a GenSC setup for image, leveraging cross-modal context information, which\nincreases the bandwidth efficiency by 70.8% with negligible loss of\nsemantic/perceptual quality. Finally, the potential research directions are\nidentified to facilitate adoption of TokCom in future wireless networks.\n","authors":["Li Qiao","Mahdi Boloursaz Mashhadi","Zhen Gao","Rahim Tafazolli","Mehdi Bennis","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19651v2","updated":"2025-02-17T15:29:40Z","published":"2024-07-29T02:32:44Z","title":"Bridging Compressed Image Latents and Multimodal Large Language Models","summary":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n","authors":["Chia-Hao Kao","Cheng Chien","Yu-Jen Tseng","Yi-Hsin Chen","Alessandro Gnutti","Shao-Yuan Lo","Wen-Hsiao Peng","Riccardo Leonardi"],"pdf_url":"https://arxiv.org/pdf/2407.19651v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2406.10469v2","updated":"2025-02-17T12:12:25Z","published":"2024-06-15T02:19:31Z","title":"Object-Attribute-Relation Representation Based Video Semantic\n  Communication","summary":"  With the rapid growth of multimedia data volume, there is an increasing need\nfor efficient video transmission in applications such as virtual reality and\nfuture video streaming services. Semantic communication is emerging as a vital\ntechnique for ensuring efficient and reliable transmission in low-bandwidth,\nhigh-noise settings. However, most current approaches focus on joint\nsource-channel coding (JSCC) that depends on end-to-end training. These methods\noften lack an interpretable semantic representation and struggle with\nadaptability to various downstream tasks. In this paper, we introduce the use\nof object-attribute-relation (OAR) as a semantic framework for videos to\nfacilitate low bit-rate coding and enhance the JSCC process for more effective\nvideo transmission. We utilize OAR sequences for both low bit-rate\nrepresentation and generative video reconstruction. Additionally, we\nincorporate OAR into the image JSCC model to prioritize communication resources\nfor areas more critical to downstream tasks. Our experiments on traffic\nsurveillance video datasets assess the effectiveness of our approach in terms\nof video transmission performance. The empirical findings demonstrate that our\nOAR-based video coding method not only outperforms H.265 coding at lower\nbit-rates but also synergizes with JSCC to deliver robust and efficient video\ntransmission.\n","authors":["Qiyuan Du","Yiping Duan","Qianqian Yang","Xiaoming Tao","Mérouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2406.10469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"}]},"2025-02-18T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.12135v2","updated":"2025-02-18T05:21:59Z","published":"2025-02-17T18:53:27Z","title":"MagicArticulate: Make Your 3D Models Articulation-Ready","summary":"  With the explosive growth of 3D content creation, there is an increasing\ndemand for automatically converting static 3D models into articulation-ready\nversions that support realistic animation. Traditional approaches rely heavily\non manual annotation, which is both time-consuming and labor-intensive.\nMoreover, the lack of large-scale benchmarks has hindered the development of\nlearning-based solutions. In this work, we present MagicArticulate, an\neffective framework that automatically transforms static 3D models into\narticulation-ready assets. Our key contributions are threefold. First, we\nintroduce Articulation-XL, a large-scale benchmark containing over 33k 3D\nmodels with high-quality articulation annotations, carefully curated from\nObjaverse-XL. Second, we propose a novel skeleton generation method that\nformulates the task as a sequence modeling problem, leveraging an\nauto-regressive transformer to naturally handle varying numbers of bones or\njoints within skeletons and their inherent dependencies across different 3D\nmodels. Third, we predict skinning weights using a functional diffusion process\nthat incorporates volumetric geodesic distance priors between vertices and\njoints. Extensive experiments demonstrate that MagicArticulate significantly\noutperforms existing methods across diverse object categories, achieving\nhigh-quality articulation that enables realistic animation. Project page:\nhttps://chaoyuesong.github.io/MagicArticulate.\n","authors":["Chaoyue Song","Jianfeng Zhang","Xiu Li","Fan Yang","Yiwen Chen","Zhongcong Xu","Jun Hao Liew","Xiaoyang Guo","Fayao Liu","Jiashi Feng","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2502.12135v2.pdf","comment":"Project: https://chaoyuesong.github.io/MagicArticulate"},{"id":"http://arxiv.org/abs/2501.08545v4","updated":"2025-02-18T12:58:49Z","published":"2025-01-15T03:11:33Z","title":"T2VEval: T2V-generated Videos Benchmark Dataset and Objective Evaluation\n  Method","summary":"  Recent advances in text-to-video (T2V) technology, as demonstrated by models\nsuch as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the\napplicability and popularity of the technology. This progress has created a\ngrowing demand for accurate quality assessment metrics to evaluate the\nperceptual quality of T2V-generated videos and optimize video generation\nmodels. However, assessing the quality of text-to-video outputs remain\nchallenging due to the presence of highly complex distortions, such as\nunnatural actions and phenomena that defy human cognition. To address these\nchallenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset\nfor text-to-video quality evaluation, which contains 148 textual prompts and\n1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation,\nwe scored each video on four dimensions in the subjective experiment, which are\noverall impression, text-video consistency, realness, and technical quality.\nBased on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for\nT2V quality evaluation. T2VEval assesses videos across three branches:\ntext-video consistency, realness, and technical quality. Using an\nattention-based fusion module, T2VEval effectively integrates features from\neach branch and predicts scores with the aid of a large language model.\nAdditionally, we implemented a divide-and-conquer training strategy, enabling\neach branch to learn targeted knowledge while maintaining synergy with the\nothers. Experimental results demonstrate that T2VEval achieves state-of-the-art\nperformance across multiple metrics.\n","authors":["Zelu Qi","Ping Shi","Shuqi Wang","Zhaoyang Zhang","Fei Zhao","Zefeng Ying","Da Pan"],"pdf_url":"https://arxiv.org/pdf/2501.08545v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11594v2","updated":"2025-02-18T03:59:44Z","published":"2025-02-17T09:28:31Z","title":"iMOVE: Instance-Motion-Aware Video Understanding","summary":"  Enhancing the fine-grained instance spatiotemporal motion perception\ncapabilities of Video Large Language Models is crucial for improving their\ntemporal and general video understanding. However, current models struggle to\nperceive detailed and complex instance motions. To address these challenges, we\nhave made improvements from both data and model perspectives. In terms of data,\nwe have meticulously curated iMOVE-IT, the first large-scale\ninstance-motion-aware video instruction-tuning dataset. This dataset is\nenriched with comprehensive instance motion annotations and spatiotemporal\nmutual-supervision tasks, providing extensive training for the model's\ninstance-motion-awareness. Building on this foundation, we introduce iMOVE, an\ninstance-motion-aware video foundation model that utilizes Event-aware\nSpatiotemporal Efficient Modeling to retain informative instance spatiotemporal\nmotion details while maintaining computational efficiency. It also incorporates\nRelative Spatiotemporal Position Tokens to ensure awareness of instance\nspatiotemporal positions. Evaluations indicate that iMOVE excels not only in\nvideo temporal understanding and general video understanding but also\ndemonstrates significant advantages in long-term video understanding.\n","authors":["Jiaze Li","Yaya Shi","Zongyang Ma","Haoran Xu","Feng Cheng","Huihui Xiao","Ruiwen Kang","Fan Yang","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13145v1","updated":"2025-02-18T18:59:57Z","published":"2025-02-18T18:59:57Z","title":"Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation","summary":"  Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba\n","authors":["Bencheng Liao","Hongyuan Tao","Qian Zhang","Tianheng Cheng","Yingyue Li","Haoran Yin","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.13145v1.pdf","comment":"Code and model are available at https://github.com/hustvl/mmMamba"},{"id":"http://arxiv.org/abs/2502.13146v1","updated":"2025-02-18T18:59:57Z","published":"2025-02-18T18:59:57Z","title":"Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization","summary":"  The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https://github.com/taco-group/Re-Align.\n","authors":["Shuo Xing","Yuping Wang","Peiran Li","Ruizheng Bai","Yueqi Wang","Chengxuan Qian","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2502.13146v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.13144v1","updated":"2025-02-18T18:59:21Z","published":"2025-02-18T18:59:21Z","title":"RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based\n  Reinforcement Learning","summary":"  Existing end-to-end autonomous driving (AD) algorithms typically follow the\nImitation Learning (IL) paradigm, which faces challenges such as causal\nconfusion and the open-loop gap. In this work, we establish a 3DGS-based\nclosed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS\ntechniques, we construct a photorealistic digital replica of the real physical\nworld, enabling the AD policy to extensively explore the state space and learn\nto handle out-of-distribution scenarios through large-scale trial and error. To\nenhance safety, we design specialized rewards that guide the policy to\neffectively respond to safety-critical events and understand real-world causal\nrelationships. For better alignment with human driving behavior, IL is\nincorporated into RL training as a regularization term. We introduce a\nclosed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS\nenvironments. Compared to IL-based methods, RAD achieves stronger performance\nin most closed-loop metrics, especially 3x lower collision rate. Abundant\nclosed-loop results are presented at https://hgao-cv.github.io/RAD.\n","authors":["Hao Gao","Shaoyu Chen","Bo Jiang","Bencheng Liao","Yiang Shi","Xiaoyang Guo","Yuechuan Pu","Haoran Yin","Xiangyu Li","Xinbang Zhang","Ying Zhang","Wenyu Liu","Qian Zhang","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.13144v1.pdf","comment":"Project Page: https://hgao-cv.github.io/RAD"},{"id":"http://arxiv.org/abs/2502.13143v1","updated":"2025-02-18T18:59:02Z","published":"2025-02-18T18:59:02Z","title":"SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and\n  Object Manipulation","summary":"  Spatial intelligence is a critical component of embodied AI, promoting robots\nto understand and interact with their environments. While recent advances have\nenhanced the ability of VLMs to perceive object locations and positional\nrelationships, they still lack the capability to precisely understand object\norientations-a key requirement for tasks involving fine-grained manipulations.\nAddressing this limitation not only requires geometric reasoning but also an\nexpressive and intuitive way to represent orientation. In this context, we\npropose that natural language offers a more flexible representation space than\ncanonical frames, making it particularly suitable for instruction-following\nrobotic systems. In this paper, we introduce the concept of semantic\norientation, which defines object orientations using natural language in a\nreference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the\n''handle'' direction of a knife). To support this, we construct OrienText300K,\na large-scale dataset of 3D models annotated with semantic orientations that\nlink geometric understanding to functional semantics. By integrating semantic\norientation into a VLM system, we enable robots to generate manipulation\nactions with both positional and orientational constraints. Extensive\nexperiments in simulation and real world demonstrate that our approach\nsignificantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy\non Open6DOR and 74.9% accuracy on SIMPLER.\n","authors":["Zekun Qi","Wenyao Zhang","Yufei Ding","Runpei Dong","Xinqiang Yu","Jingwen Li","Lingyun Xu","Baoyu Li","Xialin He","Guofan Fan","Jiazhao Zhang","Jiawei He","Jiayuan Gu","Xin Jin","Kaisheng Ma","Zhizheng Zhang","He Wang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.13143v1.pdf","comment":"Project page: https://qizekun.github.io/sofar/"},{"id":"http://arxiv.org/abs/2502.13133v1","updated":"2025-02-18T18:56:18Z","published":"2025-02-18T18:56:18Z","title":"AV-Flow: Transforming Text to Audio-Visual Human-like Interactions","summary":"  We introduce AV-Flow, an audio-visual generative model that animates\nphoto-realistic 4D talking avatars given only text input. In contrast to prior\nwork that assumes an existing speech signal, we synthesize speech and vision\njointly. We demonstrate human-like speech synthesis, synchronized lip motion,\nlively facial expressions and head pose; all generated from just text\ncharacters. The core premise of our approach lies in the architecture of our\ntwo parallel diffusion transformers. Intermediate highway connections ensure\ncommunication between the audio and visual modalities, and thus, synchronized\nspeech intonation and facial dynamics (e.g., eyebrow motion). Our model is\ntrained with flow matching, leading to expressive results and fast inference.\nIn case of dyadic conversations, AV-Flow produces an always-on avatar, that\nactively listens and reacts to the audio-visual input of a user. Through\nextensive experiments, we show that our method outperforms prior work,\nsynthesizing natural-looking 4D talking avatars. Project page:\nhttps://aggelinacha.github.io/AV-Flow/\n","authors":["Aggelina Chatziagapi","Louis-Philippe Morency","Hongyu Gong","Michael Zollhoefer","Dimitris Samaras","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2502.13133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13130v1","updated":"2025-02-18T18:55:21Z","published":"2025-02-18T18:55:21Z","title":"Magma: A Foundation Model for Multimodal AI Agents","summary":"  We present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging from UI navigation to robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where the actionable visual objects (e.g., clickable buttons in GUI) in\nimages are labeled by Set-of-Mark (SoM) for action grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition of\nspatial-temporal intelligence for our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results on UI navigation and robotic manipulation tasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models that are trained on much larger datasets. We\nmake our model and code public for reproducibility at\nhttps://microsoft.github.io/Magma.\n","authors":["Jianwei Yang","Reuben Tan","Qianhui Wu","Ruijie Zheng","Baolin Peng","Yongyuan Liang","Yu Gu","Mu Cai","Seonghyeon Ye","Joel Jang","Yuquan Deng","Lars Liden","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.13130v1.pdf","comment":"29 pages, 16 figures, technical report from MSR"},{"id":"http://arxiv.org/abs/2502.13129v1","updated":"2025-02-18T18:53:24Z","published":"2025-02-18T18:53:24Z","title":"Is Noise Conditioning Necessary for Denoising Generative Models?","summary":"  It is widely believed that noise conditioning is indispensable for denoising\ndiffusion models to work successfully. This work challenges this belief.\nMotivated by research on blind image denoising, we investigate a variety of\ndenoising-based generative models in the absence of noise conditioning. To our\nsurprise, most models exhibit graceful degradation, and in some cases, they\neven perform better without noise conditioning. We provide a theoretical\nanalysis of the error caused by removing noise conditioning and demonstrate\nthat our analysis aligns with empirical observations. We further introduce a\nnoise-unconditional model that achieves a competitive FID of 2.23 on CIFAR-10,\nsignificantly narrowing the gap to leading noise-conditional models. We hope\nour findings will inspire the community to revisit the foundations and\nformulations of denoising generative models.\n","authors":["Qiao Sun","Zhicheng Jiang","Hanhong Zhao","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2502.13129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13103v1","updated":"2025-02-18T18:13:19Z","published":"2025-02-18T18:13:19Z","title":"WeedsGalore: A Multispectral and Multitemporal UAV-based Dataset for\n  Crop and Weed Segmentation in Agricultural Maize Fields","summary":"  Weeds are one of the major reasons for crop yield loss but current weeding\npractices fail to manage weeds in an efficient and targeted manner. Effective\nweed management is especially important for crops with high worldwide\nproduction such as maize, to maximize crop yield for meeting increasing global\ndemands. Advances in near-sensing and computer vision enable the development of\nnew tools for weed management. Specifically, state-of-the-art segmentation\nmodels, coupled with novel sensing technologies, can facilitate timely and\naccurate weeding and monitoring systems. However, learning-based approaches\nrequire annotated data and show a lack of generalization to aerial imaging for\ndifferent crops. We present a novel dataset for semantic and instance\nsegmentation of crops and weeds in agricultural maize fields. The multispectral\nUAV-based dataset contains images with RGB, red-edge, and near-infrared bands,\na large number of plant instances, dense annotations for maize and four weed\nclasses, and is multitemporal. We provide extensive baseline results for both\ntasks, including probabilistic methods to quantify prediction uncertainty,\nimprove model calibration, and demonstrate the approach's applicability to\nout-of-distribution data. The results show the effectiveness of the two\nadditional bands compared to RGB only, and better performance in our target\ndomain than models trained on existing datasets. We hope our dataset advances\nresearch on methods and operational systems for fine-grained weed\nidentification, enhancing the robustness and applicability of UAV-based weed\nmanagement. The dataset and code are available at\nhttps://github.com/GFZ/weedsgalore\n","authors":["Ekin Celikkan","Timo Kunzmann","Yertay Yeskaliyev","Sibylle Itzerott","Nadja Klein","Martin Herold"],"pdf_url":"https://arxiv.org/pdf/2502.13103v1.pdf","comment":"11 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2501.19353v3","updated":"2025-02-18T18:07:34Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SciCap Challenge 2023","summary":"  Since the SciCap datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SciCap Challenge took place, inviting global teams\nto use an expanded SciCap dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SciCap\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v3.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.13095v1","updated":"2025-02-18T18:06:48Z","published":"2025-02-18T18:06:48Z","title":"Understanding and Rectifying Safety Perception Distortion in VLMs","summary":"  Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.\n","authors":["Xiaohan Zou","Jian Kang","George Kesidis","Lu Lin"],"pdf_url":"https://arxiv.org/pdf/2502.13095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14728v2","updated":"2025-02-18T17:48:45Z","published":"2023-02-28T16:34:55Z","title":"Semantically Consistent Person Image Generation","summary":"  We propose a data-driven approach for context-aware person image generation.\nSpecifically, we attempt to generate a person image such that the synthesized\ninstance can blend into a complex scene. In our method, the position, scale,\nand appearance of the generated person are semantically conditioned on the\nexisting persons in the scene. The proposed technique is divided into three\nsequential steps. At first, we employ a Pix2PixHD model to infer a coarse\nsemantic mask that represents the new person's spatial location, scale, and\npotential pose. Next, we use a data-centric approach to select the closest\nrepresentation from a precomputed cluster of fine semantic masks. Finally, we\nadopt a multi-scale, attention-guided architecture to transfer the appearance\nattributes from an exemplar image. The proposed strategy enables us to\nsynthesize semantically coherent realistic persons that can blend into an\nexisting scene without altering the global context. We conclude our findings\nwith relevant qualitative and quantitative evaluations.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2302.14728v2.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2024"},{"id":"http://arxiv.org/abs/2410.11236v2","updated":"2025-02-18T17:41:03Z","published":"2024-10-15T03:43:51Z","title":"Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward\n  Modeling","summary":"  In this paper, we focus on the task of conditional image generation, where an\nimage is synthesized according to user instructions. The critical challenge\nunderpinning this task is ensuring both the fidelity of the generated images\nand their semantic alignment with the provided conditions. To tackle this\nissue, previous studies have employed supervised perceptual losses derived from\npre-trained models, i.e., reward models, to enforce alignment between the\ncondition and the generated result. However, we observe one inherent\nshortcoming: considering the diversity of synthesized images, the reward model\nusually provides inaccurate feedback when encountering newly generated data,\nwhich can undermine the training process. To address this limitation, we\npropose an uncertainty-aware reward modeling, called Ctrl-U, including\nuncertainty estimation and uncertainty-aware regularization, designed to reduce\nthe adverse effects of imprecise feedback from the reward model. Given the\ninherent cognitive uncertainty within reward models, even images generated\nunder identical conditions often result in a relatively large discrepancy in\nreward loss. Inspired by the observation, we explicitly leverage such\nprediction variance as an uncertainty indicator. Based on the uncertainty\nestimation, we regularize the model training by adaptively rectifying the\nreward. In particular, rewards with lower uncertainty receive higher loss\nweights, while those with higher uncertainty are given reduced weights to allow\nfor larger variability. The proposed uncertainty regularization facilitates\nreward fine-tuning through consistency construction. Extensive experiments\nvalidate the effectiveness of our methodology in improving the controllability\nand generation quality, as well as its scalability across diverse conditional\nscenarios. Codes are publicly available at\nhttps://grenoble-zhang.github.io/Ctrl-U-Page/.\n","authors":["Guiyu Zhang","Huan-ang Gao","Zijian Jiang","Hao Zhao","Zhedong Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.11236v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2206.02717v2","updated":"2025-02-18T17:40:45Z","published":"2022-06-06T16:18:15Z","title":"Scene Aware Person Image Generation through Global Contextual\n  Conditioning","summary":"  Person image generation is an intriguing yet challenging problem. However,\nthis task becomes even more difficult under constrained situations. In this\nwork, we propose a novel pipeline to generate and insert contextually relevant\nperson images into an existing scene while preserving the global semantics.\nMore specifically, we aim to insert a person such that the location, pose, and\nscale of the person being inserted blends in with the existing persons in the\nscene. Our method uses three individual networks in a sequential pipeline. At\nfirst, we predict the potential location and the skeletal structure of the new\nperson by conditioning a Wasserstein Generative Adversarial Network (WGAN) on\nthe existing human skeletons present in the scene. Next, the predicted skeleton\nis refined through a shallow linear network to achieve higher structural\naccuracy in the generated image. Finally, the target image is generated from\nthe refined skeleton using another generative network conditioned on a given\nimage of the target person. In our experiments, we achieve high-resolution\nphoto-realistic generation results while preserving the general context of the\nscene. We conclude our paper with multiple qualitative and quantitative\nbenchmarks on the results.\n","authors":["Prasun Roy","Subhankar Ghosh","Saumik Bhattacharya","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2206.02717v2.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2022"},{"id":"http://arxiv.org/abs/2502.13081v1","updated":"2025-02-18T17:34:04Z","published":"2025-02-18T17:34:04Z","title":"Personalized Image Generation with Deep Generative Models: A Decade\n  Survey","summary":"  Recent advancements in generative models have significantly facilitated the\ndevelopment of personalized content creation. Given a small set of images with\nuser-specific concept, personalized image generation allows to create images\nthat incorporate the specified concept and adhere to provided text\ndescriptions. Due to its wide applications in content creation, significant\neffort has been devoted to this field in recent years. Nonetheless, the\ntechnologies used for personalization have evolved alongside the development of\ngenerative models, with their distinct and interrelated components. In this\nsurvey, we present a comprehensive review of generalized personalized image\ngeneration across various generative models, including traditional GANs,\ncontemporary text-to-image diffusion models, and emerging multi-model\nautoregressive models. We first define a unified framework that standardizes\nthe personalization process across different generative models, encompassing\nthree key components, i.e., inversion spaces, inversion methods, and\npersonalization schemes. This unified framework offers a structured approach to\ndissecting and comparing personalization techniques across different generative\narchitectures. Building upon this unified framework, we further provide an\nin-depth analysis of personalization techniques within each generative model,\nhighlighting their unique contributions and innovations. Through comparative\nanalysis, this survey elucidates the current landscape of personalized image\ngeneration, identifying commonalities and distinguishing features among\nexisting methods. Finally, we discuss the open challenges in the field and\npropose potential directions for future research. We keep tracing related works\nat https://github.com/csyxwei/Awesome-Personalized-Image-Generation.\n","authors":["Yuxiang Wei","Yiheng Zheng","Yabo Zhang","Ming Liu","Zhilong Ji","Lei Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2502.13081v1.pdf","comment":"39 pages; under submission; more information:\n  https://github.com/csyxwei/Awesome-Personalized-Image-Generation"},{"id":"http://arxiv.org/abs/2502.13078v1","updated":"2025-02-18T17:31:26Z","published":"2025-02-18T17:31:26Z","title":"L4P: Low-Level 4D Vision Perception Unified","summary":"  The spatio-temporal relationship between the pixels of a video carries\ncritical information for low-level 4D perception. A single model that reasons\nabout it should be able to solve several such tasks well. Yet, most\nstate-of-the-art methods rely on architectures specialized for the task at\nhand. We present L4P (pronounced \"LAP\"), a feedforward, general-purpose\narchitecture that solves low-level 4D perception tasks in a unified framework.\nL4P combines a ViT-based backbone with per-task heads that are lightweight and\ntherefore do not require extensive training. Despite its general and\nfeedforward formulation, our method matches or surpasses the performance of\nexisting specialized methods on both dense tasks, such as depth or optical flow\nestimation, and sparse tasks, such as 2D/3D tracking. Moreover, it solves all\nthose tasks at once in a time comparable to that of individual single-task\nmethods.\n","authors":["Abhishek Badki","Hang Su","Bowen Wen","Orazio Gallo"],"pdf_url":"https://arxiv.org/pdf/2502.13078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.11718v2","updated":"2025-02-18T17:28:45Z","published":"2022-07-24T11:14:46Z","title":"TIPS: Text-Induced Pose Synthesis","summary":"  In computer vision, human pose synthesis and transfer deal with probabilistic\nimage generation of a person in a previously unseen pose from an already\navailable observation of that person. Though researchers have recently proposed\nseveral methods to achieve this task, most of these techniques derive the\ntarget pose directly from the desired target image on a specific dataset,\nmaking the underlying process challenging to apply in real-world scenarios as\nthe generation of the target image is the actual aim. In this paper, we first\npresent the shortcomings of current pose transfer algorithms and then propose a\nnovel text-based pose transfer technique to address those issues. We divide the\nproblem into three independent stages: (a) text to pose representation, (b)\npose refinement, and (c) pose rendering. To the best of our knowledge, this is\none of the first attempts to develop a text-based pose transfer framework where\nwe also introduce a new dataset DF-PASS, by adding descriptive pose annotations\nfor the images of the DeepFashion dataset. The proposed method generates\npromising results with significant qualitative and quantitative scores in our\nexperiments.\n","authors":["Prasun Roy","Subhankar Ghosh","Saumik Bhattacharya","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2207.11718v2.pdf","comment":"Accepted in The European Conference on Computer Vision (ECCV) 2022"},{"id":"http://arxiv.org/abs/2405.05241v3","updated":"2025-02-18T17:20:48Z","published":"2024-05-08T17:37:57Z","title":"BenthicNet: A global compilation of seafloor images for deep learning\n  applications","summary":"  Advances in underwater imaging enable collection of extensive seafloor image\ndatasets necessary for monitoring important benthic ecosystems. The ability to\ncollect seafloor imagery has outpaced our capacity to analyze it, hindering\nmobilization of this crucial environmental information. Machine learning\napproaches provide opportunities to increase the efficiency with which seafloor\nimagery is analyzed, yet large and consistent datasets to support development\nof such approaches are scarce. Here we present BenthicNet: a global compilation\nof seafloor imagery designed to support the training and evaluation of\nlarge-scale image recognition models. An initial set of over 11.4 million\nimages was collected and curated to represent a diversity of seafloor\nenvironments using a representative subset of 1.3 million images. These are\naccompanied by 3.1 million annotations translated to the CATAMI scheme, which\nspan 190,000 of the images. A large deep learning model was trained on this\ncompilation and preliminary results suggest it has utility for automating large\nand small-scale image analysis tasks. The compilation and model are made openly\navailable for reuse at https://doi.org/10.20383/103.0614.\n","authors":["Scott C. Lowe","Benjamin Misiuk","Isaac Xu","Shakhboz Abdulazizov","Amit R. Baroi","Alex C. Bastos","Merlin Best","Vicki Ferrini","Ariell Friedman","Deborah Hart","Ove Hoegh-Guldberg","Daniel Ierodiaconou","Julia Mackin-McLaughlin","Kathryn Markey","Pedro S. Menandro","Jacquomo Monk","Shreya Nemani","John O'Brien","Elizabeth Oh","Luba Y. Reshitnyk","Katleen Robert","Chris M. Roelfsema","Jessica A. Sameoto","Alexandre C. G. Schimel","Jordan A. Thomson","Brittany R. Wilson","Melisa C. Wong","Craig J. Brown","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2405.05241v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06777v2","updated":"2025-02-18T17:18:45Z","published":"2022-02-14T14:58:05Z","title":"Multi-scale Attention Guided Pose Transfer","summary":"  Pose transfer refers to the probabilistic image generation of a person with a\npreviously unseen novel pose from another image of that person having a\ndifferent pose. Due to potential academic and commercial applications, this\nproblem is extensively studied in recent years. Among the various approaches to\nthe problem, attention guided progressive generation is shown to produce\nstate-of-the-art results in most cases. In this paper, we present an improved\nnetwork architecture for pose transfer by introducing attention links at every\nresolution level of the encoder and decoder. By utilizing such dense\nmulti-scale attention guided approach, we are able to achieve significant\nimprovement over the existing methods both visually and analytically. We\nconclude our findings with extensive qualitative and quantitative comparisons\nagainst several existing methods on the DeepFashion dataset.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2202.06777v2.pdf","comment":"Accepted in Pattern Recognition (PR) 2023"},{"id":"http://arxiv.org/abs/2502.13071v1","updated":"2025-02-18T17:17:38Z","published":"2025-02-18T17:17:38Z","title":"RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye\n  View for 3D Object Detection","summary":"  While recent low-cost radar-camera approaches have shown promising results in\nmulti-modal 3D object detection, both sensors face challenges from\nenvironmental and intrinsic disturbances. Poor lighting or adverse weather\nconditions degrade camera performance, while radar suffers from noise and\npositional ambiguity. Achieving robust radar-camera 3D object detection\nrequires consistent performance across varying conditions, a topic that has not\nyet been fully explored. In this work, we first conduct a systematic analysis\nof robustness in radar-camera detection on five kinds of noises and propose\nRobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D\nGaussian Expansion (3DGE) module to mitigate inaccuracies in radar points,\nincluding position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS\nand velocity priors to generate a deformable kernel map and variance for kernel\nsize adjustment and value distribution. Additionally, we introduce a\nweather-adaptive fusion module, which adaptively fuses radar and camera\nfeatures based on camera signal confidence. Extensive experiments on the\npopular benchmark, nuScenes, show that our model achieves competitive results\nin regular and noisy conditions.\n","authors":["Jingtong Yue","Zhiwei Lin","Xin Lin","Xiaoyu Zhou","Xiangtai Li","Lu Qi","Yongtao Wang","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2502.13071v1.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2301.05191v2","updated":"2025-02-18T17:08:41Z","published":"2023-01-12T18:19:00Z","title":"A Unified Framework for Event-based Frame Interpolation with Ad-hoc\n  Deblurring in the Wild","summary":"  Effective video frame interpolation hinges on the adept handling of motion in\nthe input scene. Prior work acknowledges asynchronous event information for\nthis, but often overlooks whether motion induces blur in the video, limiting\nits scope to sharp frame interpolation. We instead propose a unified framework\nfor event-based frame interpolation that performs deblurring ad-hoc and thus\nworks both on sharp and blurry input videos. Our model consists in a\nbidirectional recurrent network that incorporates the temporal dimension of\ninterpolation and fuses information from the input frames and the events\nadaptively based on their temporal proximity. To enhance the generalization\nfrom synthetic data to real event cameras, we integrate self-supervised\nframework with the proposed model to enhance the generalization on real-world\ndatasets in the wild. At the dataset level, we introduce a novel real-world\nhigh-resolution dataset with events and color videos named HighREV, which\nprovides a challenging evaluation setting for the examined task. Extensive\nexperiments show that our network consistently outperforms previous\nstate-of-the-art methods on frame interpolation, single image deblurring, and\nthe joint task of both. Experiments on domain transfer reveal that\nself-supervised training effectively mitigates the performance degradation\nobserved when transitioning from synthetic data to real-world data. Code and\ndatasets are available at https://github.com/AHupuJR/REFID.\n","authors":["Lei Sun","Daniel Gehrig","Christos Sakaridis","Mathias Gehrig","Jingyun Liang","Peng Sun","Zhijie Xu","Kaiwei Wang","Luc Van Gool","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2301.05191v2.pdf","comment":"Accepted to T-PAMI"},{"id":"http://arxiv.org/abs/2502.13061v1","updated":"2025-02-18T17:07:29Z","published":"2025-02-18T17:07:29Z","title":"Improved Fine-Tuning of Large Multimodal Models for Hateful Meme\n  Detection","summary":"  Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While large multimodal models\nhave shown strong generalization across various tasks, they exhibit poor\ngeneralization to hateful meme detection due to the dynamic nature of memes\ntied to emerging social trends and breaking news. Recent work further\nhighlights the limitations of conventional supervised fine-tuning for large\nmultimodal models in this context. To address these challenges, we propose\nLarge Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a\nnovel two-stage fine-tuning framework designed to improve both in-domain\naccuracy and cross-domain generalization. Experimental results on six widely\nused meme classification datasets demonstrate that LMM-RGCL achieves\nstate-of-the-art performance, outperforming agent-based systems such as\nVPD-PALI-X-55B. Furthermore, our method effectively generalizes to\nout-of-domain memes under low-resource settings, surpassing models like GPT-4o.\n","authors":["Jingbiao Mei","Jinghong Chen","Guangyu Yang","Weizhe Lin","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2502.13061v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2501.18623v2","updated":"2025-02-18T16:53:58Z","published":"2025-01-27T00:20:48Z","title":"VLMaterial: Procedural Material Generation with Large Vision-Language\n  Models","summary":"  Procedural materials, represented as functional node graphs, are ubiquitous\nin computer graphics for photorealistic material appearance design. They allow\nusers to perform intuitive and precise editing to achieve desired visual\nappearances. However, creating a procedural material given an input image\nrequires professional knowledge and significant effort. In this work, we\nleverage the ability to convert procedural materials into standard Python\nprograms and fine-tune a large pre-trained vision-language model (VLM) to\ngenerate such programs from input images. To enable effective fine-tuning, we\nalso contribute an open-source procedural material dataset and propose to\nperform program-level augmentation by prompting another pre-trained large\nlanguage model (LLM). Through extensive evaluation, we show that our method\noutperforms previous methods on both synthetic and real-world examples.\n","authors":["Beichen Li","Rundi Wu","Armando Solar-Lezama","Changxi Zheng","Liang Shi","Bernd Bickel","Wojciech Matusik"],"pdf_url":"https://arxiv.org/pdf/2501.18623v2.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2406.10322v3","updated":"2025-02-18T16:52:29Z","published":"2024-06-14T17:41:55Z","title":"LieRE: Generalizing Rotary Position Encodings","summary":"  Transformer architectures rely on position encodings to capture token\ndependencies. Rotary Position Encoding (RoPE) has emerged as a popular choice\nin language models due to its efficient encoding of relative position\ninformation through key-query rotations. However, RoPE faces significant\nlimitations beyond language processing: it is constrained to one-dimensional\nsequence data and, even with learnable phases, offers limited representational\ncapacity. We address these challenges with Lie Relative Encodings (LieRE),\nwhich replaces RoPE's block-2D rotation matrix with a learned, dense,\nhigh-dimensional rotation matrix of variable sparsity. Through extensive\nevaluation on three image datasets across 2D and 3D classification tasks, LieRE\nachieves 2\\% relative improvement over state-of-the-art baselines on 2D tasks\nand 1.5\\% on 3D tasks, while demonstrating superior generalization to higher\nresolutions. Our implementation is computationally efficient, with results\nreproducible on 4 A100 GPUs in 30 minutes on CIFAR100, and we release our code\nto facilitate further research.\n","authors":["Sophie Ostmeier","Brian Axelrod","Michael E. Moseley","Akshay Chaudhari","Curtis Langlotz"],"pdf_url":"https://arxiv.org/pdf/2406.10322v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13037v1","updated":"2025-02-18T16:49:47Z","published":"2025-02-18T16:49:47Z","title":"Enhancing Power Grid Inspections with Machine Learning","summary":"  Ensuring the safety and reliability of power grids is critical as global\nenergy demands continue to rise. Traditional inspection methods, such as manual\nobservations or helicopter surveys, are resource-intensive and lack\nscalability. This paper explores the use of 3D computer vision to automate\npower grid inspections, utilizing the TS40K dataset -- a high-density,\nannotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic\nsegmentation, our approach addresses challenges like class imbalance and noisy\ndata to enhance the detection of critical grid components such as power lines\nand towers. The benchmark results indicate significant performance\nimprovements, with IoU scores reaching 95.53% for the detection of power lines\nusing transformer-based models. Our findings illustrate the potential for\nintegrating ML into grid maintenance workflows, increasing efficiency and\nenabling proactive risk management strategies.\n","authors":["Diogo Lavado","Ricardo Santos","Andre Coelho","Joao Santos","Alessandra Micheletti","Claudia Soares"],"pdf_url":"https://arxiv.org/pdf/2502.13037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13034v1","updated":"2025-02-18T16:48:18Z","published":"2025-02-18T16:48:18Z","title":"Natural Language Generation from Visual Sequences: Challenges and Future\n  Directions","summary":"  The ability to use natural language to talk about visual content is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. Various studies have focused on generating text for single images. In\ncontrast, comparatively little attention has been paid to exhaustively\nanalyzing and advancing work on multiple-image vision-to-text settings. In this\nposition paper, we claim that any task dealing with temporally ordered\nsequences of multiple images or frames is an instance of a broader, more\ngeneral problem involving the understanding of intricate relationships between\nthe visual content and the corresponding text. We comprehensively analyze five\ntasks that are instances of this problem and argue that they pose a common set\nof challenges and share similarities in terms of modeling and evaluation\napproaches. Based on the insights from these various aspects and stages of\nmulti-image-to-text generation, we highlight several open questions and suggest\nfuture research directions. We believe that these directions can advance the\nunderstanding of complex phenomena in this domain and the development of better\nmodels.\n","authors":["Aditya K Surikuchi","Raquel Fernández","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2502.13034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08643v2","updated":"2025-02-18T16:45:59Z","published":"2025-02-12T18:57:22Z","title":"A Real-to-Sim-to-Real Approach to Robotic Manipulation with\n  VLM-Generated Iterative Keypoint Rewards","summary":"  Task specification for robotic manipulation in open-world environments is\nchallenging, requiring flexible and adaptive objectives that align with human\nintentions and can evolve through iterative feedback. We introduce Iterative\nKeypoint Reward (IKER), a visually grounded, Python-based reward function that\nserves as a dynamic task specification. Our framework leverages VLMs to\ngenerate and refine these reward functions for multi-step manipulation tasks.\nGiven RGB-D observations and free-form language instructions, we sample\nkeypoints in the scene and generate a reward function conditioned on these\nkeypoints. IKER operates on the spatial relationships between keypoints,\nleveraging commonsense priors about the desired behaviors, and enabling precise\nSE(3) control. We reconstruct real-world scenes in simulation and use the\ngenerated rewards to train reinforcement learning (RL) policies, which are then\ndeployed into the real world-forming a real-to-sim-to-real loop. Our approach\ndemonstrates notable capabilities across diverse scenarios, including both\nprehensile and non-prehensile tasks, showcasing multi-step task execution,\nspontaneous error recovery, and on-the-fly strategy adjustments. The results\nhighlight IKER's effectiveness in enabling robots to perform multi-step tasks\nin dynamic environments through iterative reward shaping.\n","authors":["Shivansh Patel","Xinchen Yin","Wenlong Huang","Shubham Garg","Hooshang Nayyeri","Li Fei-Fei","Svetlana Lazebnik","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2502.08643v2.pdf","comment":"ICRA 2025, Project Page: https://iker-robot.github.io/"},{"id":"http://arxiv.org/abs/2502.13027v1","updated":"2025-02-18T16:45:01Z","published":"2025-02-18T16:45:01Z","title":"A deep learning framework for efficient pathology image analysis","summary":"  Artificial intelligence (AI) has transformed digital pathology by enabling\nbiomarker prediction from high-resolution whole slide images (WSIs). However,\ncurrent methods are computationally inefficient, processing thousands of\nredundant tiles per WSI and requiring complex aggregator models. We introduce\nEAGLE (Efficient Approach for Guided Local Examination), a deep learning\nframework that emulates pathologists by selectively analyzing informative\nregions. EAGLE incorporates two foundation models: CHIEF for efficient tile\nselection and Virchow2 for extracting high-quality features. Benchmarking was\nconducted against leading slide- and tile-level foundation models across 31\ntasks from four cancer types, spanning morphology, biomarker prediction and\nprognosis. EAGLE outperformed state-of-the-art foundation models by up to 23%\nand achieved the highest AUROC overall. It processed a slide in 2.27 seconds,\nreducing computational time by more than 99% compared to existing models. This\nefficiency enables real-time workflows, allows pathologists to validate all\ntiles which are used by the model during analysis, and eliminates dependence on\nhigh-performance computing, making AI-powered pathology more accessible. By\nreliably identifying meaningful regions and minimizing artifacts, EAGLE\nprovides robust and interpretable outputs, supporting rapid slide searches,\nintegration into multi-omics pipelines and emerging clinical foundation models.\n","authors":["Peter Neidlinger","Tim Lenz","Sebastian Foersch","Chiara M. L. Loeffler","Jan Clusmann","Marco Gustav","Lawrence A. Shaktah","Rupert Langer","Bastian Dislich","Lisa A. Boardman","Amy J. French","Ellen L. Goode","Andrea Gsur","Stefanie Brezina","Marc J. Gunter","Robert Steinfelder","Hans-Michael Behrens","Christoph Röcken","Tabitha Harrison","Ulrike Peters","Amanda I. Phipps","Giuseppe Curigliano","Nicola Fusco","Antonio Marra","Michael Hoffmeister","Hermann Brenner","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2502.13027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13023v1","updated":"2025-02-18T16:43:11Z","published":"2025-02-18T16:43:11Z","title":"Detection and Geographic Localization of Natural Objects in the Wild: A\n  Case Study on Palms","summary":"  Palms are ecologically and economically indicators of tropical forest health,\nbiodiversity, and human impact that support local economies and global forest\nproduct supply chains. While palm detection in plantations is well-studied,\nefforts to map naturally occurring palms in dense forests remain limited by\noverlapping crowns, uneven shading, and heterogeneous landscapes. We develop\nPRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline\nfor detecting and localizing palms in dense tropical forests using large\northomosaic images. Orthomosaics are created from thousands of aerial images\nand spanning several to hundreds of gigabytes. Our contributions are threefold.\nFirst, we construct a large UAV-derived orthomosaic dataset collected across 21\necologically diverse sites in western Ecuador, annotated with 8,830 bounding\nboxes and 5,026 palm center points. Second, we evaluate multiple\nstate-of-the-art object detectors based on efficiency and performance,\nintegrating zero-shot SAM 2 as the segmentation backbone, and refining the\nresults for precise geographic mapping. Third, we apply calibration methods to\nalign confidence scores with IoU and explore saliency maps for feature\nexplainability. Though optimized for palms, PRISM is adaptable for identifying\nother natural objects, such as eastern white pines. Future work will explore\ntransfer learning for lower-resolution datasets (0.5 to 1m).\n","authors":["Kangning Cui","Rongkun Zhu","Manqi Wang","Wei Tang","Gregory D. Larsen","Victor P. Pauca","Sarra Alqahtani","Fan Yang","David Segurado","David Lutz","Jean-Michel Morel","Miles R. Silman"],"pdf_url":"https://arxiv.org/pdf/2502.13023v1.pdf","comment":"15 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.13017v1","updated":"2025-02-18T16:36:52Z","published":"2025-02-18T16:36:52Z","title":"Mean of Means: Human Localization with Calibration-free and\n  Unconstrained Camera Settings (extended version)","summary":"  Accurate human localization is crucial for various applications, especially\nin the Metaverse era. Existing high precision solutions rely on expensive,\ntag-dependent hardware, while vision-based methods offer a cheaper, tag-free\nalternative. However, current vision solutions based on stereo vision face\nlimitations due to rigid perspective transformation principles and error\npropagation in multi-stage SVD solvers. These solutions also require multiple\nhigh-resolution cameras with strict setup constraints.To address these\nlimitations, we propose a probabilistic approach that considers all points on\nthe human body as observations generated by a distribution centered around the\nbody's geometric center. This enables us to improve sampling significantly,\nincreasing the number of samples for each point of interest from hundreds to\nbillions. By modeling the relation between the means of the distributions of\nworld coordinates and pixel coordinates, leveraging the Central Limit Theorem,\nwe ensure normality and facilitate the learning process. Experimental results\ndemonstrate human localization accuracy of 96\\% within a 0.3$m$ range and\nnearly 100\\% accuracy within a 0.5$m$ range, achieved at a low cost of only 10\nUSD using two web cameras with a resolution of 640$\\times$480 pixels.\n","authors":["Tianyi Zhang","Wengyu Zhang","Xulu Zhang","Jiaxin Wu","Xiao-Yong Wei","Jiannong Cao","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2502.13017v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.20870"},{"id":"http://arxiv.org/abs/2303.07989v2","updated":"2025-02-18T16:24:45Z","published":"2023-03-14T15:44:45Z","title":"A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing","summary":"  Air-writing refers to virtually writing linguistic characters through hand\ngestures in three-dimensional space with six degrees of freedom. This paper\nproposes a generic video camera-aided convolutional neural network (CNN) based\nair-writing framework. Gestures are performed using a marker of fixed color in\nfront of a generic video camera, followed by color-based segmentation to\nidentify the marker and track the trajectory of the marker tip. A pre-trained\nCNN is then used to classify the gesture. The recognition accuracy is further\nimproved using transfer learning with the newly acquired data. The performance\nof the system varies significantly on the illumination condition due to\ncolor-based segmentation. In a less fluctuating illumination condition, the\nsystem is able to recognize isolated unistroke numerals of multiple languages.\nThe proposed framework has achieved 97.7%, 95.4% and 93.7% recognition rates in\nperson independent evaluations on English, Bengali and Devanagari numerals,\nrespectively.\n","authors":["Prasun Roy","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2303.07989v2.pdf","comment":"Accepted in The International Conference on Frontiers of Handwriting\n  Recognition (ICFHR) 2018"},{"id":"http://arxiv.org/abs/2409.07967v2","updated":"2025-02-18T16:22:14Z","published":"2024-09-12T11:54:25Z","title":"Locality-aware Cross-modal Correspondence Learning for Dense\n  Audio-Visual Events Localization","summary":"  Dense-localization Audio-Visual Events (DAVE) aims to identify time\nboundaries and corresponding categories for events that can be heard and seen\nconcurrently in an untrimmed video. Existing DAVE solutions extract audio and\nvisual features through modality-specific encoders and fuse them via dense\ncross-attention. The independent processing of each modality neglects their\ncomplementarity, resulting in modality-specific noise, while dense attention\nfails to account for local temporal continuity of events, causing irrelevant\nsignal distractions. In this paper, we present LoCo, a Locality-aware\ncross-modal Correspondence learning framework for DAVE. The core idea is to\nexplore local temporal continuity nature of audio-visual events, which serves\nas informative yet free supervision signals to guide the filtering of\nirrelevant information and inspire the extraction of complementary multimodal\ninformation during both unimodal and cross-modal learning stages. i)\nSpecifically, LoCo applies Locality-aware Correspondence Correction (LCC) to\nunimodal features via leveraging cross-modal local-correlated properties\nwithout any extra annotations. This enforces unimodal encoders to highlight\nsimilar semantics shared by audio and visual features. ii) To better aggregate\nsuch audio and visual features, we further customize Cross-modal Dynamic\nPerception layer (CDP) in cross-modal feature pyramid to understand local\ntemporal patterns of audio-visual events by imposing local consistency within\nmultimodal features in a data-driven manner. By incorporating LCC and CDP, LoCo\nprovides solid performance gains and outperforms existing DAVE methods.\n","authors":["Ling Xing","Hongyu Qu","Rui Yan","Xiangbo Shu","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2409.07967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12994v1","updated":"2025-02-18T16:15:32Z","published":"2025-02-18T16:15:32Z","title":"SHADeS: Self-supervised Monocular Depth Estimation Through\n  Non-Lambertian Image Decomposition","summary":"  Purpose: Visual 3D scene reconstruction can support colonoscopy navigation.\nIt can help in recognising which portions of the colon have been visualised and\ncharacterising the size and shape of polyps. This is still a very challenging\nproblem due to complex illumination variations, including abundant specular\nreflections. We investigate how to effectively decouple light and depth in this\nproblem.\n  Methods: We introduce a self-supervised model that simultaneously\ncharacterises the shape and lighting of the visualised colonoscopy scene. Our\nmodel estimates shading, albedo, depth, and specularities (SHADeS) from single\nimages. Unlike previous approaches (IID), we use a non-Lambertian model that\ntreats specular reflections as a separate light component. The implementation\nof our method is available at https://github.com/RemaDaher/SHADeS.\n  Results: We demonstrate on real colonoscopy images (Hyper Kvasir) that\nprevious models for light decomposition (IID) and depth estimation (MonoVIT,\nModoDepth2) are negatively affected by specularities. In contrast, SHADeS can\nsimultaneously produce light decomposition and depth maps that are robust to\nspecular regions. We also perform a quantitative comparison on phantom data\n(C3VD) where we further demonstrate the robustness of our model.\n  Conclusion: Modelling specular reflections improves depth estimation in\ncolonoscopy. We propose an effective self-supervised approach that uses this\ninsight to jointly estimate light decomposition and depth. Light decomposition\nhas the potential to help with other problems, such as place recognition within\nthe colon.\n","authors":["Rema Daher","Francisco Vasconcelos","Danail Stoyanov"],"pdf_url":"https://arxiv.org/pdf/2502.12994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07832v9","updated":"2025-02-18T16:14:02Z","published":"2024-07-31T14:49:35Z","title":"LADDER: Language Driven Slice Discovery and Error Rectification","summary":"  Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.\n","authors":["Shantanu Ghosh","Rayan Syed","Chenyu Wang","Clare B. Poynton","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2408.07832v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03688v2","updated":"2025-02-18T16:09:59Z","published":"2024-11-06T06:14:24Z","title":"Where Do We Stand with Implicit Neural Representations? A Technical and\n  Performance Survey","summary":"  Implicit Neural Representations (INRs) have emerged as a paradigm in\nknowledge representation, offering exceptional flexibility and performance\nacross a diverse range of applications. INRs leverage multilayer perceptrons\n(MLPs) to model data as continuous implicit functions, providing critical\nadvantages such as resolution independence, memory efficiency, and\ngeneralisation beyond discretised data structures. Their ability to solve\ncomplex inverse problems makes them particularly effective for tasks including\naudio reconstruction, image representation, 3D object reconstruction, and\nhigh-dimensional data synthesis. This survey provides a comprehensive review of\nstate-of-the-art INR methods, introducing a clear taxonomy that categorises\nthem into four key areas: activation functions, position encoding, combined\nstrategies, and network structure optimisation. We rigorously analyse their\ncritical properties, such as full differentiability, smoothness, compactness,\nand adaptability to varying resolutions while also examining their strengths\nand limitations in addressing locality biases and capturing fine details. Our\nexperimental comparison offers new insights into the trade-offs between\ndifferent approaches, showcasing the capabilities and challenges of the latest\nINR techniques across various tasks. In addition to identifying areas where\ncurrent methods excel, we highlight key limitations and potential avenues for\nimprovement, such as developing more expressive activation functions, enhancing\npositional encoding mechanisms, and improving scalability for complex,\nhigh-dimensional data. This survey serves as a roadmap for researchers,\noffering practical guidance for future exploration in the field of INRs. We aim\nto foster new methodologies by outlining promising research directions for INRs\nand applications.\n","authors":["Amer Essakine","Yanqi Cheng","Chun-Wun Cheng","Lipei Zhang","Zhongying Deng","Lei Zhu","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2411.03688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12985v1","updated":"2025-02-18T16:08:47Z","published":"2025-02-18T16:08:47Z","title":"PartSDF: Part-Based Implicit Neural Representation for Composite 3D\n  Shape Parametrization and Optimization","summary":"  Accurate 3D shape representation is essential in engineering applications\nsuch as design, optimization, and simulation. In practice, engineering\nworkflows require structured, part-aware representations, as objects are\ninherently designed as assemblies of distinct components. However, most\nexisting methods either model shapes holistically or decompose them without\npredefined part structures, limiting their applicability in real-world design\ntasks. We propose PartSDF, a supervised implicit representation framework that\nexplicitly models composite shapes with independent, controllable parts while\nmaintaining shape consistency. Despite its simple single-decoder architecture,\nPartSDF outperforms both supervised and unsupervised baselines in\nreconstruction and generation tasks. We further demonstrate its effectiveness\nas a structured shape prior for engineering applications, enabling precise\ncontrol over individual components while preserving overall coherence. Code\navailable at https://github.com/cvlab-epfl/PartSDF.\n","authors":["Nicolas Talabot","Olivier Clerc","Arda Cinar Demirtas","Doruk Oner","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2502.12985v1.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2010.12669v4","updated":"2025-02-18T16:00:45Z","published":"2020-10-23T21:07:40Z","title":"Position and Rotation Invariant Sign Language Recognition from 3D Kinect\n  Data with Recurrent Neural Networks","summary":"  Sign language is a gesture-based symbolic communication medium among speech\nand hearing impaired people. It also serves as a communication bridge between\nnon-impaired and impaired populations. Unfortunately, in most situations, a\nnon-impaired person is not well conversant in such symbolic languages\nrestricting the natural information flow between these two categories.\nTherefore, an automated translation mechanism that seamlessly translates sign\nlanguage into natural language can be highly advantageous. In this paper, we\nattempt to perform recognition of 30 basic Indian sign gestures. Gestures are\nrepresented as temporal sequences of 3D maps (RGB + depth), each consisting of\n3D coordinates of 20 body joints captured by the Kinect sensor. A recurrent\nneural network (RNN) is employed as the classifier. To improve the classifier's\nperformance, we use geometric transformation for the alignment correction of\ndepth frames. In our experiments, the model achieves 84.81% accuracy.\n","authors":["Prasun Roy","Saumik Bhattacharya","Partha Pratim Roy","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2010.12669v4.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.12975v1","updated":"2025-02-18T15:56:46Z","published":"2025-02-18T15:56:46Z","title":"Instance-Level Moving Object Segmentation from a Single Image with\n  Events","summary":"  Moving object segmentation plays a crucial role in understanding dynamic\nscenes involving multiple moving objects, while the difficulties lie in taking\ninto account both spatial texture structures and temporal motion cues. Existing\nmethods based on video frames encounter difficulties in distinguishing whether\npixel displacements of an object are caused by camera motion or object motion\ndue to the complexities of accurate image-based motion modeling. Recent\nadvances exploit the motion sensitivity of novel event cameras to counter\nconventional images' inadequate motion modeling capabilities, but instead lead\nto challenges in segmenting pixel-level object masks due to the lack of dense\ntexture structures in events. To address these two limitations imposed by\nunimodal settings, we propose the first instance-level moving object\nsegmentation framework that integrates complementary texture and motion cues.\nOur model incorporates implicit cross-modal masked attention augmentation,\nexplicit contrastive feature learning, and flow-guided motion enhancement to\nexploit dense texture information from a single image and rich motion\ninformation from events, respectively. By leveraging the augmented texture and\nmotion features, we separate mask segmentation from motion classification to\nhandle varying numbers of independently moving objects. Through extensive\nevaluations on multiple datasets, as well as ablation experiments with\ndifferent input settings and real-time efficiency analysis of the proposed\nframework, we believe that our first attempt to incorporate image and event\ndata for practical deployment can provide new insights for future work in\nevent-based motion related works. The source code with model training and\npre-trained weights is released at https://npucvr.github.io/EvInsMOS\n","authors":["Zhexiong Wan","Bin Fan","Le Hui","Yuchao Dai","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2502.12975v1.pdf","comment":"accepted by IJCV"},{"id":"http://arxiv.org/abs/2502.12948v1","updated":"2025-02-18T15:30:48Z","published":"2025-02-18T15:30:48Z","title":"Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for\n  Improved Text-Based Learning for LGE Detection","summary":"  Detection of hyperenhancement from cardiac LGE MRI images is a complex task\nrequiring significant clinical expertise. Although deep learning-based models\nhave shown promising results for the task, they require large amounts of data\nwith fine-grained annotations. Clinical reports generated for cardiac MR\nstudies contain rich, clinically relevant information, including the location,\nextent and etiology of any scars present. Although recently developed\nCLIP-based training enables pretraining models with image-text pairs, it\nrequires large amounts of data and further finetuning strategies on downstream\ntasks. In this study, we use various strategies rooted in domain knowledge to\ntrain a model for LGE detection solely using text from clinical reports, on a\nrelatively small clinical cohort of 965 patients. We improve performance\nthrough the use of synthetic data augmentation, by systematically creating scar\nimages and associated text. In addition, we standardize the orientation of the\nimages in an anatomy-informed way to enable better alignment of spatial and\ntext features. We also use a captioning loss to enable fine-grained supervision\nand explore the effect of pretraining of the vision encoder on performance.\nFinally, ablation studies are carried out to elucidate the contributions of\neach design component to the overall performance of the model.\n","authors":["Athira J Jacob","Puneet Sharma","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2502.12948v1.pdf","comment":"Poster at Workshop on Large Language Models and Generative AI for\n  Health at AAAI 2025"},{"id":"http://arxiv.org/abs/2404.12917v3","updated":"2025-02-18T15:17:38Z","published":"2024-04-19T14:42:42Z","title":"R3L: Relative Representations for Reinforcement Learning","summary":"  Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. It is known that variations\nin input domains (e.g., different panorama colors due to seasonal changes) or\ntask domains (e.g., altering the target speed of a car) can disrupt agent\nperformance, necessitating new training for each variation. Recent advancements\nin the field of representation learning have demonstrated the possibility of\ncombining components from different neural networks to create new models in a\nzero-shot fashion. In this paper, we build upon relative representations, a\nframework that maps encoder embeddings to a universal space. We adapt this\nframework to the Visual Reinforcement Learning setting, allowing to combine\nagents components to create new agents capable of effectively handling novel\nvisual-task pairs not encountered during training. Our findings highlight the\npotential for model reuse, significantly reducing the need for retraining and,\nconsequently, the time and computational resources required.\n","authors":["Antonio Pio Ricciardi","Valentino Maiorca","Luca Moschella","Riccardo Marin","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2404.12917v3.pdf","comment":"12 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2502.12917v1","updated":"2025-02-18T14:59:18Z","published":"2025-02-18T14:59:18Z","title":"Contrast-Unity for Partially-Supervised Temporal Sentence Grounding","summary":"  Temporal sentence grounding aims to detect event timestamps described by the\nnatural language query from given untrimmed videos. The existing\nfully-supervised setting achieves great results but requires expensive\nannotation costs; while the weakly-supervised setting adopts cheap labels but\nperforms poorly. To pursue high performance with less annotation costs, this\npaper introduces an intermediate partially-supervised setting, i.e., only\nshort-clip is available during training. To make full use of partial labels, we\nspecially design one contrast-unity framework, with the two-stage goal of\nimplicit-explicit progressive grounding. In the implicit stage, we align\nevent-query representations at fine granularity using comprehensive quadruple\ncontrastive learning: event-query gather, event-background separation,\nintra-cluster compactness and inter-cluster separability. Then, high-quality\nrepresentations bring acceptable grounding pseudo-labels. In the explicit\nstage, to explicitly optimize grounding objectives, we train one\nfully-supervised model using obtained pseudo-labels for grounding refinement\nand denoising. Extensive experiments and thoroughly ablations on Charades-STA\nand ActivityNet Captions demonstrate the significance of partial supervision,\nas well as our superior performance.\n","authors":["Haicheng Wang","Chen Ju","Weixiong Lin","Chaofan Ma","Shuai Xiao","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12917v1.pdf","comment":"Accepted by ICASSP 2025.The first two authors share the same\n  contribution. arXiv admin note: text overlap with arXiv:2302.09850"},{"id":"http://arxiv.org/abs/2409.17020v2","updated":"2025-02-18T14:54:00Z","published":"2024-09-25T15:23:46Z","title":"PTQ4RIS: Post-Training Quantization for Referring Image Segmentation","summary":"  Referring Image Segmentation (RIS), aims to segment the object referred by a\ngiven sentence in an image by understanding both visual and linguistic\ninformation. However, existing RIS methods tend to explore top-performance\nmodels, disregarding considerations for practical applications on\nresources-limited edge devices. This oversight poses a significant challenge\nfor on-device RIS inference. To this end, we propose an effective and efficient\npost-training quantization framework termed PTQ4RIS. Specifically, we first\nconduct an in-depth analysis of the root causes of performance degradation in\nRIS model quantization and propose dual-region quantization (DRQ) and\nreorder-based outlier-retained quantization (RORQ) to address the quantization\ndifficulties in visual and text encoders. Extensive experiments on three\nbenchmarks with different bits settings (from 8 to 4 bits) demonstrates its\nsuperior performance. Importantly, we are the first PTQ method specifically\ndesigned for the RIS task, highlighting the feasibility of PTQ in RIS\napplications. Code and video are available at\n{https://github.com/gugu511yy/PTQ4RIS}.\n","authors":["Xiaoyan Jiang","Hang Yang","Kaiying Zhu","Xihe Qiu","Shibo Zhao","Sifan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.17020v2.pdf","comment":"Accepted by ICRA 2025.(Update the code link.)"},{"id":"http://arxiv.org/abs/2405.20324v2","updated":"2025-02-18T14:43:42Z","published":"2024-05-30T17:57:26Z","title":"Don't drop your samples! Coherence-aware training benefits Conditional\n  diffusion","summary":"  Conditional diffusion models are powerful generative models that can leverage\nvarious types of conditional information, such as class labels, segmentation\nmasks, or text captions. However, in many real-world scenarios, conditional\ninformation may be noisy or unreliable due to human annotation errors or weak\nalignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a\nnovel method that integrates coherence in conditional information into\ndiffusion models, allowing them to learn from noisy annotations without\ndiscarding data. We assume that each data point has an associated coherence\nscore that reflects the quality of the conditional information. We then\ncondition the diffusion model on both the conditional information and the\ncoherence score. In this way, the model learns to ignore or discount the\nconditioning when the coherence is low. We show that CAD is theoretically sound\nand empirically effective on various conditional generation tasks. Moreover, we\nshow that leveraging coherence generates realistic and diverse samples that\nrespect conditional information better than models trained on cleaned datasets\nwhere samples with low coherence have been discarded.\n","authors":["Nicolas Dufour","Victor Besnier","Vicky Kalogeiton","David Picard"],"pdf_url":"https://arxiv.org/pdf/2405.20324v2.pdf","comment":"Accepted at CVPR 2024 as a Highlight. Project page:\n  https://nicolas-dufour.github.io/cad.html"},{"id":"http://arxiv.org/abs/2207.14624v3","updated":"2025-02-18T14:37:22Z","published":"2022-07-29T11:50:35Z","title":"Post-processing of coronary and myocardial spatial data","summary":"  Numerical simulations of real-world phenomena require a computational scheme\nand a computational domain. In the context of haemodynamics, the computational\ndomain is the blood vessel network through which blood flows. Such networks\ncontain millions of vessels that are joined in series and in parallel. It is\ncomputationally unfeasible to explicitly simulate blood flow throughout the\nnetwork. From a single porcine left coronary arterial tree, we develop a data\npipeline to obtain computational domains for haemodynamic simulations in the\nmyocardium from a graph representing a partial coronary arterial tree. In\naddition, we develop a method to ascertain which subregions of the\nleft-ventricular wall are more likely to be perfused via a given artery, using\na comparison with the American Heart Association division of the left ventricle\nfor validation.\n","authors":["Jay Aodh Mackenzie","Megan Jeanne Miller","Nicholas Hill","Mette Olufsen"],"pdf_url":"https://arxiv.org/pdf/2207.14624v3.pdf","comment":"25 pages, 25 figures"},{"id":"http://arxiv.org/abs/2502.12894v1","updated":"2025-02-18T14:29:52Z","published":"2025-02-18T14:29:52Z","title":"CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image","summary":"  Recovering high-quality 3D scenes from a single RGB image is a challenging\ntask in computer graphics. Current methods often struggle with domain-specific\nlimitations or low-quality object generation. To address these, we propose CAST\n(Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel\nmethod for 3D scene reconstruction and recovery. CAST starts by extracting\nobject-level 2D segmentation and relative depth information from the input\nimage, followed by using a GPT-based model to analyze inter-object spatial\nrelationships. This enables the understanding of how objects relate to each\nother within the scene, ensuring more coherent reconstruction. CAST then\nemploys an occlusion-aware large-scale 3D generation model to independently\ngenerate each object's full geometry, using MAE and point cloud conditioning to\nmitigate the effects of occlusions and partial object information, ensuring\naccurate alignment with the source image's geometry and texture. To align each\nobject with the scene, the alignment generation model computes the necessary\ntransformations, allowing the generated meshes to be accurately placed and\nintegrated into the scene's point cloud. Finally, CAST incorporates a\nphysics-aware correction step that leverages a fine-grained relation graph to\ngenerate a constraint graph. This graph guides the optimization of object\nposes, ensuring physical consistency and spatial coherence. By utilizing Signed\nDistance Fields (SDF), the model effectively addresses issues such as\nocclusions, object penetration, and floating objects, ensuring that the\ngenerated scene accurately reflects real-world physical interactions. CAST can\nbe leveraged in robotics, enabling efficient real-to-simulation workflows and\nproviding realistic, scalable simulation environments for robotic systems.\n","authors":["Kaixin Yao","Longwen Zhang","Xinhao Yan","Yan Zeng","Qixuan Zhang","Lan Xu","Wei Yang","Jiayuan Gu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2502.12894v1.pdf","comment":"Project Page: https://sites.google.com/view/cast4"},{"id":"http://arxiv.org/abs/2502.12892v1","updated":"2025-02-18T14:29:11Z","published":"2025-02-18T14:29:11Z","title":"Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept\n  Extraction in Large Vision Models","summary":"  Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine\nlearning interpretability, enabling the unsupervised decomposition of model\nrepresentations into a dictionary of abstract, human-interpretable concepts.\nHowever, we reveal a fundamental limitation: existing SAEs exhibit severe\ninstability, as identical models trained on similar datasets can produce\nsharply different dictionaries, undermining their reliability as an\ninterpretability tool. To address this issue, we draw inspiration from the\nArchetypal Analysis framework introduced by Cutler & Breiman (1994) and present\nArchetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the convex\nhull of data. This geometric anchoring significantly enhances the stability of\ninferred dictionaries, and their mildly relaxed variants RA-SAEs further match\nstate-of-the-art reconstruction abilities. To rigorously assess dictionary\nquality learned by SAEs, we introduce two new benchmarks that test (i)\nplausibility, if dictionaries recover \"true\" classification directions and (ii)\nidentifiability, if dictionaries disentangle synthetic concept mixtures. Across\nall evaluations, RA-SAEs consistently yield more structured representations\nwhile uncovering novel, semantically meaningful concepts in large-scale vision\nmodels.\n","authors":["Thomas Fel","Ekdeep Singh Lubana","Jacob S. Prince","Matthew Kowal","Victor Boutin","Isabel Papadimitriou","Binxu Wang","Martin Wattenberg","Demba Ba","Talia Konkle"],"pdf_url":"https://arxiv.org/pdf/2502.12892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12860v1","updated":"2025-02-18T13:48:22Z","published":"2025-02-18T13:48:22Z","title":"An Experimental Study of SOTA LiDAR Segmentation Models","summary":"  Point cloud segmentation (PCS) is to classify each point in point clouds. The\ntask enables robots to parse their 3D surroundings and run autonomously.\nAccording to different point cloud representations, existing PCS models can be\nroughly divided into point-, voxel-, and range image-based models. However, no\nwork has been found to report comprehensive comparisons among the\nstate-of-the-art point-, voxel-, and range image-based models from an\napplication perspective, bringing difficulty in utilizing these models for\nreal-world scenarios. In this paper, we provide thorough comparisons among the\nmodels by considering the LiDAR data motion compensation and the metrics of\nmodel parameters, max GPU memory allocated during testing, inference latency,\nframes per second, intersection-over-union (IoU) and mean IoU (mIoU) scores.\nThe experimental results benefit engineers when choosing a reasonable PCS model\nfor an application and inspire researchers in the PCS field to design more\npractical models for a real-world scenario.\n","authors":["Bike Chen","Antti Tikanmäki","Juha Röning"],"pdf_url":"https://arxiv.org/pdf/2502.12860v1.pdf","comment":"No comments"},{"id":"http://arxiv.org/abs/2502.12849v1","updated":"2025-02-18T13:38:19Z","published":"2025-02-18T13:38:19Z","title":"Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection","summary":"  In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.\n","authors":["Gianluca Guglielmo","Marc Masana"],"pdf_url":"https://arxiv.org/pdf/2502.12849v1.pdf","comment":"Code is available at https://github.com/gigug/LIR"},{"id":"http://arxiv.org/abs/2502.03228v2","updated":"2025-02-18T13:00:47Z","published":"2025-02-05T14:44:17Z","title":"GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM","summary":"  The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread\nattention due to its excellent performance in real-time high-fidelity\nrendering. However, in real-world environments with dynamic objects, existing\n3DGS-based SLAM systems often face mapping errors and tracking drift issues. To\naddress these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM\nsystem tailored for dynamic scenes. In terms of tracking, unlike traditional\nmethods, we directly perform dynamic segmentation on Gaussians and map them\nback to the front-end to obtain dynamic point labels through a Gaussian pyramid\nnetwork, achieving precise dynamic removal and robust tracking. For mapping, we\nimpose rendering penalties on dynamically labeled Gaussians, which are updated\nthrough the network, to avoid irreversible erroneous removal caused by simple\npruning. Our results on real-world datasets demonstrate that our method is\ncompetitive in tracking compared to baseline methods, generating fewer\nartifacts and higher-quality reconstructions in rendering.\n","authors":["Mingrui Li","Weijian Chen","Na Cheng","Jingyuan Xu","Dong Li","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.03228v2.pdf","comment":"The paper was accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2410.04780v2","updated":"2025-02-18T12:47:58Z","published":"2024-10-07T06:45:22Z","title":"Mitigating Modality Prior-Induced Hallucinations in Multimodal Large\n  Language Models via Deciphering Attention Causality","summary":"  Multimodal Large Language Models (MLLMs) have emerged as a central focus in\nboth industry and academia, but often suffer from biases introduced by visual\nand language priors, which can lead to multimodal hallucination. These biases\narise from the visual encoder and the Large Language Model (LLM) backbone,\naffecting the attention mechanism responsible for aligning multimodal inputs.\nExisting decoding-based mitigation methods focus on statistical correlations\nand overlook the causal relationships between attention mechanisms and model\noutput, limiting their effectiveness in addressing these biases. To tackle this\nissue, we propose a causal inference framework termed CausalMM that applies\nstructural causal modeling to MLLMs, treating modality priors as a confounder\nbetween attention mechanisms and output. Specifically, by employing backdoor\nadjustment and counterfactual reasoning at both the visual and language\nattention levels, our method mitigates the negative effects of modality priors\nand enhances the alignment of MLLM's inputs and outputs, with a maximum score\nimprovement of 65.3% on 6 VLind-Bench indicators and 164 points on MME\nBenchmark compared to conventional methods. Extensive experiments validate the\neffectiveness of our approach while being a plug-and-play solution. Our code is\navailable at: https://github.com/The-Martyr/CausalMM\n","authors":["Guanyu Zhou","Yibo Yan","Xin Zou","Kun Wang","Aiwei Liu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04780v2.pdf","comment":"Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2407.20891v5","updated":"2025-02-18T12:44:35Z","published":"2024-07-30T15:07:13Z","title":"Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian\n  Neural Networks","summary":"  Computational complexity of Bayesian learning is impeding its adoption in\npractical, large-scale tasks. Despite demonstrations of significant merits such\nas improved robustness and resilience to unseen or out-of-distribution inputs\nover their non- Bayesian counterparts, their practical use has faded to near\ninsignificance. In this study, we introduce an innovative framework to mitigate\nthe computational burden of Bayesian neural networks (BNNs). Our approach\nfollows the principle of Bayesian techniques based on deep ensembles, but\nsignificantly reduces their cost via multiple low-rank perturbations of\nparameters arising from a pre-trained neural network. Both vanilla version of\nensembles as well as more sophisticated schemes such as Bayesian learning with\nStein Variational Gradient Descent (SVGD), previously deemed impractical for\nlarge models, can be seamlessly implemented within the proposed framework,\ncalled Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a\ndramatic reduction in the number of trainable parameters required to\napproximate a Bayesian posterior; and ii) it not only maintains, but in some\ninstances, surpasses the performance of conventional Bayesian learning methods\nand non-Bayesian baselines. Our results with large-scale tasks such as\nImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the\neffectiveness and versatility of Bella in building highly scalable and\npractical Bayesian deep models for real-world applications.\n","authors":["Bao Gia Doan","Afshar Shamsi","Xiao-Yu Guo","Arash Mohammadi","Hamid Alinejad-Rokny","Dino Sejdinovic","Damien Teney","Damith C. Ranasinghe","Ehsan Abbasnejad"],"pdf_url":"https://arxiv.org/pdf/2407.20891v5.pdf","comment":"This paper is accepted in AAAI'25\", and the code is available at\n  https://bnn-bella.github.io/BNN-Bella/"},{"id":"http://arxiv.org/abs/2502.12819v1","updated":"2025-02-18T12:27:06Z","published":"2025-02-18T12:27:06Z","title":"Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh\n  Representations","summary":"  Purpose: Enabling a comprehensive and robust assessment of carotid artery\nplaques in 3D through extraction and visualization of quantitative plaque\nparameters. These parameters have potential applications in stroke risk\nanalysis, evaluation of therapy effectiveness, and plaque progression\nprediction. Methods: We propose a novel method for extracting a plaque mesh\nfrom 3D vessel wall segmentation using distance encoding on the inner and outer\nwall mesh for precise plaque structure analysis. A case-specific threshold,\nderived from the normal vessel wall thickness, was applied to extract plaques\nfrom a dataset of 202 T1-weighted black-blood MRI scans of subjects with up to\n50% stenosis. Applied to baseline and one-year follow-up data, the method\nsupports detailed plaque morphology analysis over time, including plaque volume\nquantification, aided by improved visualization via mesh unfolding. Results: We\nsuccessfully extracted plaque meshes from 341 carotid arteries, capturing a\nwide range of plaque shapes with volumes ranging from 2.69{\\mu}l to\n847.7{\\mu}l. The use of a case-specific threshold effectively eliminated false\npositives in young, healthy subjects. Conclusion: The proposed method enables\nprecise extraction of plaque meshes from 3D vessel wall segmentation masks\nenabling a correspondence between baseline and one-year follow-up examinations.\nUnfolding the plaque meshes enhances visualization, while the mesh-based\nanalysis allows quantification of plaque parameters independent of voxel\nresolution.\n","authors":["Hinrich Rahlfs","Markus Hüllebrand","Sebastian Schmitter","Christoph Strecker","Andreas Harloff","Anja Hennemuth"],"pdf_url":"https://arxiv.org/pdf/2502.12819v1.pdf","comment":"13 pages, 5 Figures, Submitted to the International Journal of\n  Computer Assisted Radiology and Surgery"},{"id":"http://arxiv.org/abs/2404.13425v2","updated":"2025-02-18T12:16:57Z","published":"2024-04-20T17:19:54Z","title":"AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models","summary":"  Vision-Language Models (VLMs) play a crucial role in the advancement of\nArtificial General Intelligence (AGI). As AGI rapidly evolves, addressing\nsecurity concerns has emerged as one of the most significant challenges for\nVLMs. In this paper, we present extensive experiments that expose the\nvulnerabilities of conventional adaptation methods for VLMs, highlighting\nsignificant security risks. Moreover, as VLMs grow in size, the application of\ntraditional adversarial adaptation techniques incurs substantial computational\ncosts. To address these issues, we propose a parameter-efficient adversarial\nadaptation method called \\textbf{\\textit{AdvLoRA}} based on Low-Rank\nAdaptation. We investigate and reveal the inherent low-rank properties involved\nin adversarial adaptation for VLMs. Different from LoRA, we enhance the\nefficiency and robustness of adversarial adaptation by introducing a novel\nreparameterization method that leverages parameter clustering and alignment.\nAdditionally, we propose an adaptive parameter update strategy to further\nbolster robustness. These innovations enable our AdvLoRA to mitigate issues\nrelated to model security and resource wastage. Extensive experiments confirm\nthe effectiveness and efficiency of AdvLoRA.\n","authors":["Yuheng Ji","Yue Liu","Zhicheng Zhang","Zhao Zhang","Yuting Zhao","Xiaoshuai Hao","Gang Zhou","Xingwei Zhang","Xiaolong Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.13425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12801v1","updated":"2025-02-18T12:05:27Z","published":"2025-02-18T12:05:27Z","title":"Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations","summary":"  We propose a novel approach that uses sparse annotations from clinical\nstudies to train a 3D segmentation of the carotid artery wall. We use a\ncenterline annotation to sample perpendicular cross-sections of the carotid\nartery and use an adversarial 2D network to segment them. These annotations are\nthen transformed into 3D pseudo-labels for training of a 3D convolutional\nneural network, circumventing the creation of manual 3D masks. For pseudo-label\ncreation in the bifurcation area we propose the use of cross-sections\nperpendicular to the bifurcation axis and show that this enhances segmentation\nperformance. Different sampling distances had a lesser impact. The proposed\nmethod allows for efficient training of 3D segmentation, offering potential\nimprovements in the assessment of carotid artery stenosis and allowing the\nextraction of 3D biomarkers such as plaque volume.\n","authors":["Hinrich Rahlfs","Markus Hüllebrand","Sebastian Schmitter","Christoph Strecker","Andreas Harloff","Anja Hennemuth"],"pdf_url":"https://arxiv.org/pdf/2502.12801v1.pdf","comment":"Presented at MICAD 2024 Conference"},{"id":"http://arxiv.org/abs/2502.12799v1","updated":"2025-02-18T12:00:47Z","published":"2025-02-18T12:00:47Z","title":"Towards Text-Image Interleaved Retrieval","summary":"  Current multimodal information retrieval studies mainly focus on single-image\ninputs, which limits real-world applications involving multiple images and\ntext-image interleaved content. In this work, we introduce the text-image\ninterleaved retrieval (TIIR) task, where the query and document are interleaved\ntext-image sequences, and the model is required to understand the semantics\nfrom the interleaved context for effective retrieval. We construct a TIIR\nbenchmark based on naturally interleaved wikiHow tutorials, where a specific\npipeline is designed to generate interleaved queries. To explore the task, we\nadapt several off-the-shelf retrievers and build a dense baseline by\ninterleaved multimodal large language model (MLLM). We then propose a novel\nMatryoshka Multimodal Embedder (MME), which compresses the number of visual\ntokens at different granularity, to address the challenge of excessive visual\ntokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption\nof existing models does not consistently yield effective results. Our MME\nachieves significant improvements over the baseline by substantially fewer\nvisual tokens. We provide extensive analysis and will release the dataset and\ncode to facilitate future research.\n","authors":["Xin Zhang","Ziqi Dai","Yongqi Li","Yanzhao Zhang","Dingkun Long","Pengjun Xie","Meishan Zhang","Jun Yu","Wenjie Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12799v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.01946v2","updated":"2025-02-18T11:59:46Z","published":"2025-02-04T02:41:00Z","title":"HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for\n  Multi-session Radar SLAM","summary":"  Recently, radars have been widely featured in robotics for their robustness\nin challenging weather conditions. Two commonly used radar types are spinning\nradars and phased-array radars, each offering distinct sensor characteristics.\nExisting datasets typically feature only a single type of radar, leading to the\ndevelopment of algorithms limited to that specific kind. In this work, we\nhighlight that combining different radar types offers complementary advantages,\nwhich can be leveraged through a heterogeneous radar dataset. Moreover, this\nnew dataset fosters research in multi-session and multi-robot scenarios where\nrobots are equipped with different types of radars. In this context, we\nintroduce the HeRCULES dataset, a comprehensive, multi-modal dataset with\nheterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first\ndataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering\nunparalleled localization, mapping, and place recognition capabilities. The\ndataset covers diverse weather and lighting conditions and a range of urban\ntraffic scenarios, enabling a comprehensive analysis across various\nenvironments. The sequence paths with multiple revisits and ground truth pose\nfor each sensor enhance its suitability for place recognition research. We\nexpect the HeRCULES dataset to facilitate odometry, mapping, place recognition,\nand sensor fusion research. The dataset and development tools are available at\nhttps://sites.google.com/view/herculesdataset.\n","authors":["Hanjun Kim","Minwoo Jung","Chiyun Noh","Sangwoo Jung","Hyunho Song","Wooseong Yang","Hyesu Jang","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2502.01946v2.pdf","comment":"2025 IEEE International Conference on Robotics and Automation (ICRA\n  2025)"},{"id":"http://arxiv.org/abs/2502.12794v1","updated":"2025-02-18T11:56:51Z","published":"2025-02-18T11:56:51Z","title":"RAPID: Retrieval Augmented Training of Differentially Private Diffusion\n  Models","summary":"  Differentially private diffusion models (DPDMs) harness the remarkable\ngenerative capabilities of diffusion models while enforcing differential\nprivacy (DP) for sensitive data. However, existing DPDM training approaches\noften suffer from significant utility loss, large memory footprint, and\nexpensive inference cost, impeding their practical uses. To overcome such\nlimitations, we present RAPID: Retrieval Augmented PrIvate Diffusion model, a\nnovel approach that integrates retrieval augmented generation (RAG) into DPDM\ntraining. Specifically, RAPID leverages available public data to build a\nknowledge base of sample trajectories; when training the diffusion model on\nprivate data, RAPID computes the early sampling steps as queries, retrieves\nsimilar trajectories from the knowledge base as surrogates, and focuses on\ntraining the later sampling steps in a differentially private manner. Extensive\nevaluation using benchmark datasets and models demonstrates that, with the same\nprivacy guarantee, RAPID significantly outperforms state-of-the-art approaches\nby large margins in generative quality, memory footprint, and inference cost,\nsuggesting that retrieval-augmented DP training represents a promising\ndirection for developing future privacy-preserving generative models. The code\nis available at: https://github.com/TanqiuJiang/RAPID\n","authors":["Tanqiu Jiang","Changjiang Li","Fenglong Ma","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12794v1.pdf","comment":"Published in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12791v1","updated":"2025-02-18T11:52:25Z","published":"2025-02-18T11:52:25Z","title":"Beyond Timesteps: A Novel Activation-wise Membrane Potential Propagation\n  Mechanism for Spiking Neural Networks in 3D cloud","summary":"  Due to the similar characteristics between event-based visual data and point\nclouds, recent studies have emerged that treat event data as event clouds to\nlearn based on point cloud analysis. Additionally, some works approach point\nclouds from the perspective of event vision, employing Spiking Neural Network\n(SNN) due to their asynchronous nature. However, these contributions are often\ndomain-specific, making it difficult to extend their applicability to other\nintersecting fields. Moreover, while SNN-based visual tasks have seen\nsignificant growth, the conventional timestep-wise iterative activation\nstrategy largely limits their real-world applications by large timesteps,\nresulting in significant delays and increased computational costs. Although\nsome innovative methods achieve good performance with short timesteps (<10),\nfew have fundamentally restructured the update strategy of spiking neurons to\ncompletely overcome the limitations of timesteps. In response to these\nconcerns, we propose a novel and general activation strategy for spiking\nneurons called Activation-wise Membrane Potential Propagation (AMP2). This\napproach extends the concept of timesteps from a manually crafted parameter\nwithin the activation function to any existing network structure. In\nexperiments on common point cloud tasks (classification, object, and scene\nsegmentation) and event cloud tasks (action recognition), we found that AMP2\nstabilizes SNN training, maintains competitive performance, and reduces latency\ncompared to the traditional timestep-wise activation paradigm.\n","authors":["Jian Song","Boxuan Zheng","Xiangfei Yang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12752v1","updated":"2025-02-18T11:13:06Z","published":"2025-02-18T11:13:06Z","title":"High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion","summary":"  Despite recent advances in Novel View Synthesis (NVS), generating\nhigh-fidelity views from single or sparse observations remains a significant\nchallenge. Existing splatting-based approaches often produce distorted geometry\ndue to splatting errors. While diffusion-based methods leverage rich 3D priors\nto achieve improved geometry, they often suffer from texture hallucination. In\nthis paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion\nmodel designed to synthesize high-fidelity novel views from a single image.\nSpecifically, we propose an aligned synthesis strategy for precise control of\ntarget viewpoints and geometry-consistent view synthesis. To mitigate texture\nhallucination, we design a texture bridge module that enables high-fidelity\ntexture generation through adaptive feature fusion. In this manner, SplatDiff\nleverages the strengths of splatting and diffusion to generate novel views with\nconsistent geometry and high-fidelity details. Extensive experiments verify the\nstate-of-the-art performance of SplatDiff in single-view NVS. Additionally,\nwithout extra training, SplatDiff shows remarkable zero-shot performance across\ndiverse tasks, including sparse-view NVS and stereo video conversion.\n","authors":["Xiang Zhang","Yang Zhang","Lukas Mehl","Markus Gross","Christopher Schroers"],"pdf_url":"https://arxiv.org/pdf/2502.12752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13989v2","updated":"2025-02-18T11:03:28Z","published":"2024-05-22T20:53:23Z","title":"TS40K: a 3D Point Cloud Dataset of Rural Terrain and Electrical\n  Transmission System","summary":"  Research on supervised learning algorithms in 3D scene understanding has\nrisen in prominence and witness great increases in performance across several\ndatasets. The leading force of this research is the problem of autonomous\ndriving followed by indoor scene segmentation. However, openly available 3D\ndata on these tasks mainly focuses on urban scenarios. In this paper, we\npropose TS40K, a 3D point cloud dataset that encompasses more than 40,000 Km on\nelectrical transmission systems situated in European rural terrain. This is not\nonly a novel problem for the research community that can aid in the high-risk\nmission of power-grid inspection, but it also offers 3D point clouds with\ndistinct characteristics from those in self-driving and indoor 3D data, such as\nhigh point-density and no occlusion. In our dataset, each 3D point is labeled\nwith 1 out of 22 annotated classes. We evaluate the performance of\nstate-of-the-art methods on our dataset concerning 3D semantic segmentation and\n3D object detection. Finally, we provide a comprehensive analysis of the\nresults along with key challenges such as using labels that were not originally\nintended for learning tasks.\n","authors":["Diogo Lavado","Cláudia Soares","Alessandra Micheletti","Ricardo Santos","André Coelho","João Santos"],"pdf_url":"https://arxiv.org/pdf/2405.13989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12742v1","updated":"2025-02-18T10:59:04Z","published":"2025-02-18T10:59:04Z","title":"3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from\n  Cortical Surfaces","summary":"  Despite recent advances in medical image generation, existing methods\nstruggle to produce anatomically plausible 3D structures. In synthetic brain\nmagnetic resonance images (MRIs), characteristic fissures are often missing,\nand reconstructed cortical surfaces appear scattered rather than densely\nconvoluted. To address this issue, we introduce Cor2Vox, the first diffusion\nmodel-based method that translates continuous cortical shape priors to\nsynthetic brain MRIs. To achieve this, we leverage a Brownian bridge process\nwhich allows for direct structured mapping between shape contours and medical\nimages. Specifically, we adapt the concept of the Brownian bridge diffusion\nmodel to 3D and extend it to embrace various complementary shape\nrepresentations. Our experiments demonstrate significant improvements in the\ngeometric accuracy of reconstructed structures compared to previous voxel-based\napproaches. Moreover, Cor2Vox excels in image quality and diversity, yielding\nhigh variation in non-target structures like the skull. Finally, we highlight\nthe capability of our approach to simulate cortical atrophy at the sub-voxel\nlevel. Our code is available at https://github.com/ai-med/Cor2Vox.\n","authors":["Fabian Bongratz","Yitong Li","Sama Elbaroudy","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2502.12742v1.pdf","comment":"Accepted by Information Processing in Medical Imaging (IPMI) 2025"},{"id":"http://arxiv.org/abs/2502.12723v1","updated":"2025-02-18T10:39:00Z","published":"2025-02-18T10:39:00Z","title":"myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking\n  Dataset","summary":"  This paper presents the myEye2Wheeler dataset, a unique resource of\nreal-world gaze behaviour of two-wheeler drivers navigating complex Indian\ntraffic. Most datasets are from four-wheeler drivers on well-planned roads and\nhomogeneous traffic. Our dataset offers a critical lens into the unique visual\nattention patterns and insights into the decision-making of Indian two-wheeler\ndrivers. The analysis demonstrates that existing saliency models, like\nTASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to\nwhen applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),\nhighlighting the need for models specifically tailored to the traffic\nconditions. By introducing the dataset, we not only fill a significant gap in\ntwo-wheeler driver behaviour research in India but also emphasise the critical\nneed for developing context-specific saliency models. The larger aim is to\nimprove road safety for two-wheeler users and lane-planning to support a\ncost-effective mode of transport.\n","authors":["Bhaiya Vaibhaw Kumar","Deepti Rawat","Tanvi Kandalla","Aarnav Nagariya","Kavita Vemuri"],"pdf_url":"https://arxiv.org/pdf/2502.12723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12713v1","updated":"2025-02-18T10:26:16Z","published":"2025-02-18T10:26:16Z","title":"Uncertainty Propagation for Echocardiography Clinical Metric Estimation\n  via Contour Sampling","summary":"  Echocardiography plays a fundamental role in the extraction of important\nclinical parameters (e.g. left ventricular volume and ejection fraction)\nrequired to determine the presence and severity of heart-related conditions.\nWhen deploying automated techniques for computing these parameters, uncertainty\nestimation is crucial for assessing their utility. Since clinical parameters\nare usually derived from segmentation maps, there is no clear path for\nconverting pixel-wise uncertainty values into uncertainty estimates in the\ndownstream clinical metric calculation. In this work, we propose a novel\nuncertainty estimation method based on contouring rather than segmentation. Our\nmethod explicitly predicts contour location uncertainty from which contour\nsamples can be drawn. Finally, the sampled contours can be used to propagate\nuncertainty to clinical metrics. Our proposed method not only provides accurate\nuncertainty estimations for the task of contouring but also for the downstream\nclinical metrics on two cardiac ultrasound datasets. Code is available at:\nhttps://github.com/ThierryJudge/contouring-uncertainty.\n","authors":["Thierry Judge","Olivier Bernard","Woo-Jin Cho Kim","Alberto Gomez","Arian Beqiri","Agisilaos Chartsias","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2502.12713v1.pdf","comment":"10 pages, submitted to IEEE TMI"},{"id":"http://arxiv.org/abs/2501.19036v2","updated":"2025-02-18T09:57:37Z","published":"2025-01-31T11:09:16Z","title":"RedundancyLens: Revealing and Exploiting Visual Token Processing\n  Redundancy for Efficient Decoder-Only MLLMs","summary":"  Current Multimodal Large Language Model (MLLM) architectures face a critical\ntradeoff between performance and efficiency: decoder-only architectures achieve\nhigher performance but lower efficiency, while cross-attention-based\narchitectures offer greater efficiency but lower performance. The key\ndistinction lies in how visual tokens are processed. Decoder-only architectures\napply self-attention and FFN operations on visual tokens, while cross-attention\narchitectures skip these computations. To investigate whether redundancy exists\nin this computationally expensive process, we propose a training-free framework\nfor analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and\nHollow Attention, which enable adjustable reductions in computations for visual\ntokens, as well as a Layer Ranking Algorithm that prioritizes layers for these\nreductions. Extensive experiments demonstrate substantial, structured, and\nclustered redundancy unique to decoder-only MLLMs, offering valuable insights\nfor future MLLM architecture design. Furthermore, by leveraging our reduction\nframework as a training-free inference acceleration approach, we achieve\nperformance comparable to or better than state-of-the-art methods while\nremaining compatible with them. Code will be publicly available at\nhttps://github.com/L-Hugh/RedundancyLens.\n","authors":["Hongliang Li","Jiaxin Zhang","Wenhui Liao","Dezhi Peng","Kai Ding","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2501.19036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12690v1","updated":"2025-02-18T09:51:03Z","published":"2025-02-18T09:51:03Z","title":"Fast Data Aware Neural Architecture Search via Supernet Accelerated\n  Evaluation","summary":"  Tiny machine learning (TinyML) promises to revolutionize fields such as\nhealthcare, environmental monitoring, and industrial maintenance by running\nmachine learning models on low-power embedded systems. However, the complex\noptimizations required for successful TinyML deployment continue to impede its\nwidespread adoption. A promising route to simplifying TinyML is through\nautomatic machine learning (AutoML), which can distill elaborate optimization\nworkflows into accessible key decisions. Notably, Hardware Aware Neural\nArchitecture Searches - where a computer searches for an optimal TinyML model\nbased on predictive performance and hardware metrics - have gained significant\ntraction, producing some of today's most widely used TinyML models.\nNevertheless, limiting optimization solely to neural network architectures can\nprove insufficient. Because TinyML systems must operate under extremely tight\nresource constraints, the choice of input data configuration, such as\nresolution or sampling rate, also profoundly impacts overall system efficiency.\nAchieving truly optimal TinyML systems thus requires jointly tuning both input\ndata and model architecture. Despite its importance, this \"Data Aware Neural\nArchitecture Search\" remains underexplored. To address this gap, we propose a\nnew state-of-the-art Data Aware Neural Architecture Search technique and\ndemonstrate its effectiveness on the novel TinyML ``Wake Vision'' dataset. Our\nexperiments show that across varying time and hardware constraints, Data Aware\nNeural Architecture Search consistently discovers superior TinyML systems\ncompared to purely architecture-focused methods, underscoring the critical role\nof data-aware optimization in advancing TinyML.\n","authors":["Emil Njor","Colby Banbury","Xenofon Fafoutis"],"pdf_url":"https://arxiv.org/pdf/2502.12690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10674v2","updated":"2025-02-18T09:51:00Z","published":"2025-01-18T06:41:48Z","title":"Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The\n  answer is No!","summary":"  Multimodal Large Language Models (MLLMs) have achieved significant\nadvancements in tasks like Visual Question Answering (VQA) by leveraging\nfoundational Large Language Models (LLMs). However, their abilities in specific\nareas such as visual temporal understanding, which is crucial for comprehending\nreal-world dynamics, remain underexplored. To address this, we propose a\nchallenging evaluation benchmark named TemporalVQA, consisting of two parts: 1)\nTemporal Order Understanding and 2) Time-lapse Estimation. The first part\nrequires MLLMs to determine the sequence of events by analyzing temporally\nconsecutive video frames. The second part presents image pairs with varying\ntime differences, framed as multiple-choice questions, asking MLLMs to estimate\nthe time-lapse between images with options ranging from seconds to years. Our\nevaluations of advanced MLLMs, including models like GPT-4o and Gemini-1.5-Pro,\nreveal significant challenges: GPT-4o achieved only 49.1% average consistent\naccuracy in temporal order task and 70% in time-lapse estimation, with\nopen-source models performing even poorly. These findings underscore the\nlimitations of current MLLMs in visual temporal understanding and reasoning,\nhighlighting the need for further improvements for their temporal capability.\nOur dataset can be found at\nhttps://huggingface.co/datasets/fazliimam/temporal-vqa.\n","authors":["Mohamed Fazli Imam","Chenyang Lyu","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2501.10674v2.pdf","comment":"Our dataset can be found at\n  \\url{https://huggingface.co/datasets/fazliimam/temporal-vqa}"},{"id":"http://arxiv.org/abs/2502.12677v1","updated":"2025-02-18T09:32:29Z","published":"2025-02-18T09:32:29Z","title":"Spiking Vision Transformer with Saccadic Attention","summary":"  The combination of Spiking Neural Networks (SNNs) and Vision Transformers\n(ViTs) holds potential for achieving both energy efficiency and high\nperformance, particularly suitable for edge vision applications. However, a\nsignificant performance gap still exists between SNN-based ViTs and their ANN\ncounterparts. Here, we first analyze why SNN-based ViTs suffer from limited\nperformance and identify a mismatch between the vanilla self-attention\nmechanism and spatio-temporal spike trains. This mismatch results in degraded\nspatial relevance and limited temporal interactions. To address these issues,\nwe draw inspiration from biological saccadic attention mechanisms and introduce\nan innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the\nspatial domain, SSSA employs a novel spike distribution-based method to\neffectively assess the relevance between Query and Key pairs in SNN-based ViTs.\nTemporally, SSSA employs a saccadic interaction module that dynamically focuses\non selected visual areas at each timestep and significantly enhances whole\nscene understanding through temporal interactions. Building on the SSSA\nmechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive\nexperiments across various visual tasks demonstrate that SNN-ViT achieves\nstate-of-the-art performance with linear computational complexity. The\neffectiveness and efficiency of the SNN-ViT highlight its potential for\npower-critical edge vision applications.\n","authors":["Shuai Wang","Malu Zhang","Dehao Zhang","Ammar Belatreche","Yichen Xiao","Yu Liang","Yimeng Shan","Qian Sun","Enqi Zhang","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12677v1.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12673v1","updated":"2025-02-18T09:24:15Z","published":"2025-02-18T09:24:15Z","title":"ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by\n  NeRFs Composition","summary":"  Efficient and accurate 3D reconstruction is essential for applications in\ncultural heritage. This study addresses the challenge of visualizing objects\nwithin large-scale scenes at a high level of detail (LOD) using Neural Radiance\nFields (NeRFs). The aim is to improve the visual fidelity of chosen objects\nwhile maintaining the efficiency of the computations by focusing on details\nonly for relevant content. The proposed ROI-NeRFs framework divides the scene\ninto a Scene NeRF, which represents the overall scene at moderate detail, and\nmultiple ROI NeRFs that focus on user-defined objects of interest. An\nobject-focused camera selection module automatically groups relevant cameras\nfor each NeRF training during the decomposition phase. In the composition\nphase, a Ray-level Compositional Rendering technique combines information from\nthe Scene NeRF and ROI NeRFs, allowing simultaneous multi-object rendering\ncomposition. Quantitative and qualitative experiments conducted on two\nreal-world datasets, including one on a complex eighteen's century cultural\nheritage room, demonstrate superior performance compared to baseline methods,\nimproving LOD for object regions, minimizing artifacts, and without\nsignificantly increasing inference time.\n","authors":["Quoc-Anh Bui","Gilles Rougeron","Géraldine Morin","Simone Gasparini"],"pdf_url":"https://arxiv.org/pdf/2502.12673v1.pdf","comment":"17 pages including appendix, 16 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.05988v2","updated":"2025-02-18T09:11:16Z","published":"2025-02-09T18:39:35Z","title":"SNAT-YOLO: Efficient Cross-Layer Aggregation Network for Edge-Oriented\n  Gangue Detection","summary":"  To address the issues of slow detection speed,low accuracy,difficulty in\ndeployment on industrial edge devices,and large parameter and computational\nrequirements in deep learning-based coal gangue target detection methods,we\npropose a lightweight coal gangue target detection algorithm based on an\nimproved YOLOv11.First,we use the lightweight network ShuffleNetV2 as the\nbackbone to enhance detection speed.Second,we introduce a lightweight\ndownsampling operation,ADown,which reduces model complexity while improving\naverage detection accuracy.Third,we improve the C2PSA module in YOLOv11 by\nincorporating the Triplet Attention mechanism,resulting in the proposed\nC2PSA-TriAtt module,which enhances the model's ability to focus on different\ndimensions of images.Fourth,we propose the Inner-FocalerIoU loss function to\nreplace the existing CIoU loss function.Experimental results show that our\nmodel achieves a detection accuracy of 99.10% in coal gangue detection\ntasks,reduces the model size by 38%,the number of parameters by 41%,and the\ncomputational cost by 40%,while decreasing the average detection time per image\nby 1 ms.The improved model demonstrates enhanced detection speed and\naccuracy,making it suitable for deployment on industrial edge mobile\ndevices,thus contributing positively to coal processing and efficient\nutilization of coal resources.\n","authors":["Shang Li"],"pdf_url":"https://arxiv.org/pdf/2502.05988v2.pdf","comment":"In Figure 1, due to our mistake, some parts of the picture are\n  incorrect. We are making changes for resubmission"},{"id":"http://arxiv.org/abs/2502.05503v2","updated":"2025-02-18T09:07:09Z","published":"2025-02-08T09:31:26Z","title":"A Physical Coherence Benchmark for Evaluating Video Generation Models\n  via Optical Flow-guided Frame Prediction","summary":"  Recent advances in video generation models demonstrate their potential as\nworld simulators, but they often struggle with videos deviating from physical\nlaws, a key concern overlooked by most text-to-video benchmarks. We introduce a\nbenchmark designed specifically to assess the Physical Coherence of generated\nvideos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of\nphysical principles, capturing key physical laws observable in video content.\nWe evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and\nconducted manual assessments. Additionally, we propose an automated evaluation\nmodel: PhyCoPredictor, a diffusion model that generates optical flow and video\nframes in a cascade manner. Through a consistency evaluation comparing\nautomated and manual sorting, the experimental results show that PhyCoPredictor\ncurrently aligns most closely with human evaluation. Therefore, it can\neffectively evaluate the physical coherence of videos, providing insights for\nfuture model optimization. Our benchmark, including physical coherence prompts,\nthe automatic evaluation tool PhyCoPredictor, and the generated video dataset,\nhas been released on GitHub at https://github.com/Jeckinchen/PhyCoBench.\n","authors":["Yongfan Chen","Xiuwen Zhu","Tianyu Li","Hao Chen","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2502.05503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17663v3","updated":"2025-02-18T09:01:21Z","published":"2024-09-26T09:21:48Z","title":"Explanation Bottleneck Models","summary":"  Recent concept-based interpretable models have succeeded in providing\nmeaningful explanations by pre-defined concept sets. However, the dependency on\nthe pre-defined concepts restricts the application because of the limited\nnumber of concepts for explanations. This paper proposes a novel interpretable\ndeep neural network called explanation bottleneck models (XBMs). XBMs generate\na text explanation from the input without pre-defined concepts and then predict\na final task prediction based on the generated explanation by leveraging\npre-trained vision-language encoder-decoder models. To achieve both the target\ntask performance and the explanation quality, we train XBMs through the target\ntask loss with the regularization penalizing the explanation decoder via the\ndistillation from the frozen pre-trained decoder. Our experiments, including a\ncomparison to state-of-the-art concept bottleneck models, confirm that XBMs\nprovide accurate and fluent natural language explanations without pre-defined\nconcept sets. Code is available at https://github.com/yshinya6/xbm/.\n","authors":["Shin'ya Yamaguchi","Kosuke Nishida"],"pdf_url":"https://arxiv.org/pdf/2409.17663v3.pdf","comment":"Accepted to AAAI 2025 (Oral)"},{"id":"http://arxiv.org/abs/2410.12694v2","updated":"2025-02-18T08:49:57Z","published":"2024-10-16T15:54:11Z","title":"VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine","summary":"  Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.\n","authors":["Lingxiao Luo","Bingda Tang","Xuanzhong Chen","Rong Han","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12640v1","updated":"2025-02-18T08:42:55Z","published":"2025-02-18T08:42:55Z","title":"RecDreamer: Consistent Text-to-3D Generation via Uniform Score\n  Distillation","summary":"  Current text-to-3D generation methods based on score distillation often\nsuffer from geometric inconsistencies, leading to repeated patterns across\ndifferent poses of 3D assets. This issue, known as the Multi-Face Janus\nproblem, arises because existing methods struggle to maintain consistency\nacross varying poses and are biased toward a canonical pose. While recent work\nhas improved pose control and approximation, these efforts are still limited by\nthis inherent bias, which skews the guidance during generation. To address\nthis, we propose a solution called RecDreamer, which reshapes the underlying\ndata distribution to achieve a more consistent pose representation. The core\nidea behind our method is to rectify the prior distribution, ensuring that pose\nvariation is uniformly distributed rather than biased toward a canonical form.\nBy modifying the prescribed distribution through an auxiliary function, we can\nreconstruct the density of the distribution to ensure compliance with specific\nmarginal constraints. In particular, we ensure that the marginal distribution\nof poses follows a uniform distribution, thereby eliminating the biases\nintroduced by the prior knowledge. We incorporate this rectified data\ndistribution into existing score distillation algorithms, a process we refer to\nas uniform score distillation. To efficiently compute the posterior\ndistribution required for the auxiliary function, RecDreamer introduces a\ntraining-free classifier that estimates pose categories in a plug-and-play\nmanner. Additionally, we utilize various approximation techniques for noisy\nstates, significantly improving system performance. Our experimental results\ndemonstrate that RecDreamer effectively mitigates the Multi-Face Janus problem,\nleading to more consistent 3D asset generation across different poses.\n","authors":["Chenxi Zheng","Yihong Lin","Bangzhen Liu","Xuemiao Xu","Yongwei Nie","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2502.12640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08557v4","updated":"2025-02-18T08:34:28Z","published":"2023-03-15T12:18:16Z","title":"Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey","summary":"  While deep learning excels in computer vision tasks with abundant labeled\ndata, its performance diminishes significantly in scenarios with limited\nlabeled samples. To address this, Few-shot learning (FSL) enables models to\nperform the target tasks with very few labeled examples by leveraging prior\nknowledge from related tasks. However, traditional FSL assumes that both the\nrelated and target tasks come from the same domain, which is a restrictive\nassumption in many real-world scenarios where domain differences are common. To\novercome this limitation, Cross-domain few-shot learning (CDFSL) has gained\nattention, as it allows source and target data to come from different domains\nand label spaces. This paper presents the first comprehensive review of\nCross-domain Few-shot Learning (CDFSL), a field that has received less\nattention compared to traditional FSL due to its unique challenges. We aim to\nprovide both a position paper and a tutorial for researchers, covering key\nproblems, existing methods, and future research directions. The review begins\nwith a formal definition of CDFSL, outlining its core challenges, followed by a\nsystematic analysis of current approaches, organized under a clear taxonomy.\nFinally, we discuss promising future directions in terms of problem setups,\napplications, and theoretical advancements.\n","authors":["Huali Xu","Shuaifeng Zhi","Shuzhou Sun","Vishal M. Patel","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08557v4.pdf","comment":"Accepted at ACM Computing Surveys; 35 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.12635v1","updated":"2025-02-18T08:28:29Z","published":"2025-02-18T08:28:29Z","title":"Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in\n  Visual Instruction Tuning","summary":"  Visual Instruction Tuning (VIT) enhances Multimodal Large Language Models\n(MLLMs) but it is hindered by corrupted datasets containing hallucinated\ncontent, incorrect responses, and poor OCR quality. While prior works focus on\ndataset refinement through high-quality data collection or rule-based\nfiltering, they are costly or limited to specific types of corruption. To\ndeeply understand how corrupted data affects MLLMs, in this paper, we\nsystematically investigate this issue and find that while corrupted data\ndegrades the performance of MLLMs, its effects are largely superficial in that\nthe performance of MLLMs can be largely restored by either disabling a small\nsubset of parameters or post-training with a small amount of clean data.\nAdditionally, corrupted MLLMs exhibit improved ability to distinguish clean\nsamples from corrupted ones, enabling the dataset cleaning without external\nhelp. Based on those insights, we propose a corruption-robust training paradigm\ncombining self-validation and post-training, which significantly outperforms\nexisting corruption mitigation strategies.\n","authors":["Yunhao Gou","Hansi Yang","Zhili Liu","Kai Chen","Yihan Zeng","Lanqing Hong","Zhenguo Li","Qun Liu","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12632v1","updated":"2025-02-18T08:22:50Z","published":"2025-02-18T08:22:50Z","title":"MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length\n  Video Generation","summary":"  Diffusion models are successful for synthesizing high-quality videos but are\nlimited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained\nfootage (e.g. over minutes) still remains an open research question. In this\npaper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers),\na new diffusion model specialized for long video generation. MALT Diffusion (or\njust MALT) handles long videos by subdividing them into short segments and\ndoing segment-level autoregressive generation. To achieve this, we first\npropose recurrent attention layers that encode multiple segments into a compact\nmemory latent vector; by maintaining this memory vector over time, MALT is able\nto condition on it and continuously generate new footage based on a long\ntemporal context. We also present several training techniques that enable the\nmodel to generate frames over a long horizon with consistent quality and\nminimal degradation. We validate the effectiveness of MALT through experiments\non long video benchmarks. We first perform extensive analysis of MALT in\nlong-contextual understanding capability and stability using popular long video\nbenchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video\ngeneration on UCF-101, outperforming the previous state-of-the-art of 648.4.\nFinally, we explore MALT's capabilities in a text-to-video generation setting\nand show that it can produce long videos compared with recent techniques for\nlong text-to-video generation.\n","authors":["Sihyun Yu","Meera Hahn","Dan Kondratyuk","Jinwoo Shin","Agrim Gupta","José Lezama","Irfan Essa","David Ross","Jonathan Huang"],"pdf_url":"https://arxiv.org/pdf/2502.12632v1.pdf","comment":"preprint. 26 pages"},{"id":"http://arxiv.org/abs/2501.14174v2","updated":"2025-02-18T08:16:03Z","published":"2025-01-24T01:50:19Z","title":"Dreamweaver: Learning Compositional World Representations from Pixels","summary":"  Humans have an innate ability to decompose their perceptions of the world\ninto objects and their attributes, such as colors, shapes, and movement\npatterns. This cognitive process enables us to imagine novel futures by\nrecombining familiar concepts. However, replicating this ability in artificial\nintelligence systems has proven challenging, particularly when it comes to\nmodeling videos into compositional concepts and generating unseen, recomposed\nfutures without relying on auxiliary data, such as text, masks, or bounding\nboxes. In this paper, we propose Dreamweaver, a neural architecture designed to\ndiscover hierarchical and compositional representations from raw videos and\ngenerate compositional future simulations. Our approach leverages a novel\nRecurrent Block-Slot Unit (RBSU) to decompose videos into their constituent\nobjects and attributes. In addition, Dreamweaver uses a multi-future-frame\nprediction objective to capture disentangled representations for dynamic\nconcepts more effectively as well as static concepts. In experiments, we\ndemonstrate our model outperforms current state-of-the-art baselines for world\nmodeling when evaluated under the DCI framework across multiple datasets.\nFurthermore, we show how the modularized concept representations of our model\nenable compositional imagination, allowing the generation of novel videos by\nrecombining attributes from different objects.\n","authors":["Junyeob Baek","Yi-Fu Wu","Gautam Singh","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2501.14174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12627v1","updated":"2025-02-18T08:12:47Z","published":"2025-02-18T08:12:47Z","title":"DAMamba: Vision State Space Model with Dynamic Adaptive Scan","summary":"  State space models (SSMs) have recently garnered significant attention in\ncomputer vision. However, due to the unique characteristics of image data,\nadapting SSMs from natural language processing to computer vision has not\noutperformed the state-of-the-art convolutional neural networks (CNNs) and\nVision Transformers (ViTs). Existing vision SSMs primarily leverage manually\ndesigned scans to flatten image patches into sequences locally or globally.\nThis approach disrupts the original semantic spatial adjacency of the image and\nlacks flexibility, making it difficult to capture complex image structures. To\naddress this limitation, we propose Dynamic Adaptive Scan (DAS), a data-driven\nmethod that adaptively allocates scanning orders and regions. This enables more\nflexible modeling capabilities while maintaining linear computational\ncomplexity and global modeling capacity. Based on DAS, we further propose the\nvision backbone DAMamba, which significantly outperforms current\nstate-of-the-art vision Mamba models in vision tasks such as image\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Notably, it surpasses some of the latest state-of-the-art CNNs\nand ViTs. Code will be available at https://github.com/ltzovo/DAMamba.\n","authors":["Tanzhe Li","Caoshuo Li","Jiayi Lyu","Hongjuan Pei","Baochang Zhang","Taisong Jin","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2502.12627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10729v2","updated":"2025-02-18T08:07:39Z","published":"2025-02-15T08:46:01Z","title":"VarGes: Improving Variation in Co-Speech 3D Gesture Generation via\n  StyleCLIPS","summary":"  Generating expressive and diverse human gestures from audio is crucial in\nfields like human-computer interaction, virtual reality, and animation. Though\nexisting methods have achieved remarkable performance, they often exhibit\nlimitations due to constrained dataset diversity and the restricted amount of\ninformation derived from audio inputs. To address these challenges, we present\nVarGes, a novel variation-driven framework designed to enhance co-speech\ngesture generation by integrating visual stylistic cues while maintaining\nnaturalness. Our approach begins with the Variation-Enhanced Feature Extraction\n(VEFE) module, which seamlessly incorporates \\textcolor{blue}{style-reference}\nvideo data into a 3D human pose estimation network to extract StyleCLIPS,\nthereby enriching the input with stylistic information. Subsequently, we employ\nthe Variation-Compensation Style Encoder (VCSE), a transformer-style encoder\nequipped with an additive attention mechanism pooling layer, to robustly encode\ndiverse StyleCLIPS representations and effectively manage stylistic variations.\nFinally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio\nfeatures with StyleCLIPS encodings via cross-attention, injecting this fused\ndata into a cross-conditional autoregressive model to modulate 3D human gesture\ngeneration based on audio input and stylistic clues. The efficacy of our\napproach is validated on benchmark datasets, where it outperforms existing\nmethods in terms of gesture diversity and naturalness. The code and video\nresults will be made publicly available upon\nacceptance:https://github.com/mookerr/VarGES/ .\n","authors":["Ming Meng","Ke Mu","Yonggui Zhu","Zhe Zhu","Haoyu Sun","Heyang Yan","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2502.10729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12604v1","updated":"2025-02-18T07:34:54Z","published":"2025-02-18T07:34:54Z","title":"S2C: Learning Noise-Resistant Differences for Unsupervised Change\n  Detection in Multimodal Remote Sensing Images","summary":"  Unsupervised Change Detection (UCD) in multimodal Remote Sensing (RS) images\nremains a difficult challenge due to the inherent spatio-temporal complexity\nwithin data, and the heterogeneity arising from different imaging sensors.\nInspired by recent advancements in Visual Foundation Models (VFMs) and\nContrastive Learning (CL) methodologies, this research aims to develop CL\nmethodologies to translate implicit knowledge in VFM into change\nrepresentations, thus eliminating the need for explicit supervision. To this\nend, we introduce a Semantic-to-Change (S2C) learning framework for UCD in both\nhomogeneous and multimodal RS images. Differently from existing CL\nmethodologies that typically focus on learning multi-temporal similarities, we\nintroduce a novel triplet learning strategy that explicitly models temporal\ndifferences, which are crucial to the CD task. Furthermore, random spatial and\nspectral perturbations are introduced during the training to enhance robustness\nto temporal noise. In addition, a grid sparsity regularization is defined to\nsuppress insignificant changes, and an IoU-matching algorithm is developed to\nrefine the CD results. Experiments on four benchmark CD datasets demonstrate\nthat the proposed S2C learning framework achieves significant improvements in\naccuracy, surpassing current state-of-the-art by over 31\\%, 9\\%, 23\\%, and\n15\\%, respectively. It also demonstrates robustness and sample efficiency,\nsuitable for training and adaptation of various Visual Foundation Models (VFMs)\nor backbone neural networks. The relevant code will be available at:\ngithub.com/DingLei14/S2C.\n","authors":["Lei Ding","Xibing Zuo","Danfeng Hong","Haitao Guo","Jun Lu","Zhihui Gong","Lorenzo Bruzzone"],"pdf_url":"https://arxiv.org/pdf/2502.12604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11158v2","updated":"2025-02-18T07:25:32Z","published":"2025-02-16T15:12:40Z","title":"AnyRefill: A Unified, Data-Efficient Framework for Left-Prompt-Guided\n  Vision Tasks","summary":"  In this paper, we present a novel Left-Prompt-Guided (LPG) paradigm to\naddress a diverse range of reference-based vision tasks. Inspired by the human\ncreative process, we reformulate these tasks using a left-right stitching\nformulation to construct contextual input. Building upon this foundation, we\npropose AnyRefill, an extension of LeftRefill, that effectively adapts\nText-to-Image (T2I) models to various vision tasks. AnyRefill leverages the\ninpainting priors of advanced T2I model based on the Diffusion Transformer\n(DiT) architecture, and incorporates flexible components to enhance its\ncapabilities. By combining task-specific LoRAs with the stitching input,\nAnyRefill unlocks its potential across diverse tasks, including conditional\ngeneration, visual perception, and image editing, without requiring additional\nvisual encoders. Meanwhile, AnyRefill exhibits remarkable data efficiency,\nrequiring minimal task-specific fine-tuning while maintaining high generative\nperformance. Through extensive ablation studies, we demonstrate that AnyRefill\noutperforms other image condition injection methods and achieves competitive\nresults compared to state-of-the-art open-source methods. Notably, AnyRefill\ndelivers results comparable to advanced commercial tools, such as IC-Light and\nSeedEdit, even in challenging scenarios. Comprehensive experiments and ablation\nstudies across versatile tasks validate the strong generation of the proposed\nsimple yet effective LPG formulation, establishing AnyRefill as a unified,\nhighly data-efficient solution for reference-based vision tasks.\n","authors":["Ming Xie","Chenjie Cao","Yunuo Cai","Xiangyang Xue","Yu-Gang Jiang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.11158v2.pdf","comment":"19 pages, submitted to TPAMI"},{"id":"http://arxiv.org/abs/2502.12600v1","updated":"2025-02-18T07:23:22Z","published":"2025-02-18T07:23:22Z","title":"Revisiting the Generalization Problem of Low-level Vision Models Through\n  the Lens of Image Deraining","summary":"  Generalization remains a significant challenge for low-level vision models,\nwhich often struggle with unseen degradations in real-world scenarios despite\ntheir success in controlled benchmarks. In this paper, we revisit the\ngeneralization problem in low-level vision models. Image deraining is selected\nas a case study due to its well-defined and easily decoupled structure,\nallowing for more effective observation and analysis. Through comprehensive\nexperiments, we reveal that the generalization issue is not primarily due to\nlimited network capacity but rather the failure of existing training\nstrategies, which leads networks to overfit specific degradation patterns. Our\nfindings show that guiding networks to focus on learning the underlying image\ncontent, rather than the degradation patterns, is key to improving\ngeneralization. We demonstrate that balancing the complexity of background\nimages and degradations in the training data helps networks better fit the\nimage distribution. Furthermore, incorporating content priors from pre-trained\ngenerative models significantly enhances generalization. Experiments on both\nimage deraining and image denoising validate the proposed strategies. We\nbelieve the insights and solutions will inspire further research and improve\nthe generalization of low-level vision models.\n","authors":["Jinfan Hu","Zhiyuan You","Jinjin Gu","Kaiwen Zhu","Tianfan Xue","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2502.12600v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2305.15134"},{"id":"http://arxiv.org/abs/2502.12591v1","updated":"2025-02-18T07:06:36Z","published":"2025-02-18T07:06:36Z","title":"CutPaste&Find: Efficient Multimodal Hallucination Detector with\n  Visual-aid Knowledge Base","summary":"  Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nreasoning capabilities, but they remain susceptible to hallucination,\nparticularly object hallucination where non-existent objects or incorrect\nattributes are fabricated in generated descriptions. Existing detection methods\nachieve strong performance but rely heavily on expensive API calls and\niterative LVLM-based validation, making them impractical for large-scale or\noffline use. To address these limitations, we propose CutPaste\\&Find, a\nlightweight and training-free framework for detecting hallucinations in\nLVLM-generated outputs. Our approach leverages off-the-shelf visual and\nlinguistic modules to perform multi-step verification efficiently without\nrequiring LVLM inference. At the core of our framework is a Visual-aid\nKnowledge Base that encodes rich entity-attribute relationships and associated\nimage representations. We introduce a scaling factor to refine similarity\nscores, mitigating the issue of suboptimal alignment values even for\nground-truth image-text pairs. Comprehensive evaluations on benchmark datasets,\nincluding POPE and R-Bench, demonstrate that CutPaste\\&Find achieves\ncompetitive hallucination detection performance while being significantly more\nefficient and cost-effective than previous methods.\n","authors":["Cong-Duy Nguyen","Xiaobao Wu","Duc Anh Vu","Shuai Zhao","Thong Nguyen","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2502.12591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12582v1","updated":"2025-02-18T06:39:28Z","published":"2025-02-18T06:39:28Z","title":"Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action\n  Recognition","summary":"  In real-world action recognition systems, incorporating more attributes helps\nachieve a more comprehensive understanding of human behavior. However, using a\nsingle model to simultaneously recognize multiple attributes can lead to a\ndecrease in accuracy. In this work, we propose a novel method i.e. Adaptive\nAttribute Prototype Model (AAPM) for human action recognition, which captures\nrich action-relevant attribute information and strikes a balance between\naccuracy and robustness. Firstly, we introduce the Text-Constrain Module (TCM)\nto incorporate textual information from potential labels, and constrain the\nconstruction of different attributes prototype representations. In addition, we\nexplore the Attribute Assignment Method (AAM) to address the issue of training\nbias and increase robustness during the training process.Furthermore, we\nconstruct a new video dataset with attribute-based multi-label called\nMulti-Kinetics for evaluation, which contains various attribute labels (e.g.\naction, scene, object, etc.) related to human behavior. Extensive experiments\ndemonstrate that our AAPM achieves the state-of-the-art performance in both\nattribute-based multi-label few-shot action recognition and single-label\nfew-shot action recognition. The project and dataset are available at an\nanonymous account https://github.com/theAAPM/AAPM\n","authors":["Juefeng Xiao","Tianqi Xiang","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2502.12582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12579v1","updated":"2025-02-18T06:31:08Z","published":"2025-02-18T06:31:08Z","title":"CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for\n  Text-to-Image Generation","summary":"  Diffusion models have emerged as a dominant approach for text-to-image\ngeneration. Key components such as the human preference alignment and\nclassifier-free guidance play a crucial role in ensuring generation quality.\nHowever, their independent application in current text-to-image models\ncontinues to face significant challenges in achieving strong text-image\nalignment, high generation quality, and consistency with human aesthetic\nstandards. In this work, we for the first time, explore facilitating the\ncollaboration of human performance alignment and test-time sampling to unlock\nthe potential of text-to-image models. Consequently, we introduce CHATS\n(Combining Human-Aligned optimization and Test-time Sampling), a novel\ngenerative framework that separately models the preferred and dispreferred\ndistributions and employs a proxy-prompt-based sampling strategy to utilize the\nuseful information contained in both distributions. We observe that CHATS\nexhibits exceptional data efficiency, achieving strong performance with only a\nsmall, high-quality funetuning dataset. Extensive experiments demonstrate that\nCHATS surpasses traditional preference alignment methods, setting new\nstate-of-the-art across various standard benchmarks.\n","authors":["Minghao Fu","Guo-Hua Wang","Liangfu Cao","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04906v3","updated":"2025-02-18T06:29:36Z","published":"2024-06-07T12:58:14Z","title":"RU-AI: A Large Multimodal Dataset for Machine-Generated Content\n  Detection","summary":"  The recent generative AI models' capability of creating realistic and\nhuman-like content is significantly transforming the ways in which people\ncommunicate, create and work. The machine-generated content is a double-edged\nsword. On one hand, it can benefit the society when used appropriately. On the\nother hand, it may mislead people, posing threats to the society, especially\nwhen mixed together with natural content created by humans. Hence, there is an\nurgent need to develop effective methods to detect machine-generated content.\nHowever, the lack of aligned multimodal datasets inhibited the development of\nsuch methods, particularly in triple-modality settings (e.g., text, image, and\nvoice). In this paper, we introduce RU-AI, a new large-scale multimodal dataset\nfor robust and effective detection of machine-generated content in text, image\nand voice. Our dataset is constructed on the basis of three large publicly\navailable datasets: Flickr8K, COCO and Places205, by adding their corresponding\nAI duplicates, resulting in a total of 1,475,370 instances. In addition, we\ncreated an additional noise variant of the dataset for testing the robustness\nof detection models. We conducted extensive experiments with the current SOTA\ndetection methods on our dataset. The results reveal that existing models still\nstruggle to achieve accurate and robust detection on our dataset. We hope that\nthis new data set can promote research in the field of machine-generated\ncontent detection, fostering the responsible use of generative AI. The source\ncode and datasets are available at https://github.com/ZhihaoZhang97/RU-AI.\n","authors":["Liting Huang","Zhihao Zhang","Yiran Zhang","Xiyue Zhou","Shoujin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.04906v3.pdf","comment":"Accepted by WWW'25 Resource Track"},{"id":"http://arxiv.org/abs/2304.08733v2","updated":"2025-02-18T06:20:21Z","published":"2023-04-18T05:09:07Z","title":"Human and AI Perceptual Differences in Image Classification Errors","summary":"  Artificial intelligence (AI) models for computer vision trained with\nsupervised machine learning are assumed to solve classification tasks by\nimitating human behavior learned from training labels. Most efforts in recent\nvision research focus on measuring the model task performance using\nstandardized benchmarks such as accuracy. However, limited work has sought to\nunderstand the perceptual difference between humans and machines. To fill this\ngap, this study first analyzes the statistical distributions of mistakes from\nthe two sources and then explores how task difficulty level affects these\ndistributions. We find that even when AI learns an excellent model from the\ntraining data, one that outperforms humans in overall accuracy, these AI models\nhave significant and consistent differences from human perception. We\ndemonstrate the importance of studying these differences with a simple human-AI\nteaming algorithm that outperforms humans alone, AI alone, or AI-AI teaming.\n","authors":["Minghao Liu","Jiaheng Wei","Yang Liu","James Davis"],"pdf_url":"https://arxiv.org/pdf/2304.08733v2.pdf","comment":"AAAI 25 Oral"},{"id":"http://arxiv.org/abs/2502.12570v1","updated":"2025-02-18T06:15:02Z","published":"2025-02-18T06:15:02Z","title":"GVTNet: Graph Vision Transformer For Face Super-Resolution","summary":"  Recent advances in face super-resolution research have utilized the\nTransformer architecture. This method processes the input image into a series\nof small patches. However, because of the strong correlation between different\nfacial components in facial images. When it comes to super-resolution of\nlow-resolution images, existing algorithms cannot handle the relationships\nbetween patches well, resulting in distorted facial components in the\nsuper-resolution results. To solve the problem, we propose a transformer\narchitecture based on graph neural networks called graph vision transformer\nnetwork. We treat each patch as a graph node and establish an adjacency matrix\nbased on the information between patches. In this way, the patch only interacts\nbetween neighboring patches, further processing the relationship of facial\ncomponents. Quantitative and visualization experiments have underscored the\nsuperiority of our algorithm over state-of-the-art techniques. Through detailed\ncomparisons, we have demonstrated that our algorithm possesses more advanced\nsuper-resolution capabilities, particularly in enhancing facial components. The\nPyTorch code is available at https://github.com/continueyang/GVTNet\n","authors":["Chao Yang","Yong Fan","Cheng Lu","Minghao Yuan","Zhijing Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12567v1","updated":"2025-02-18T06:07:14Z","published":"2025-02-18T06:07:14Z","title":"DeltaDiff: A Residual-Guided Diffusion Model for Enhanced Image\n  Super-Resolution","summary":"  Recently, the application of diffusion models in super-resolution tasks has\nbecome a popular research direction. Existing work is focused on fully\nmigrating diffusion models to SR tasks. The diffusion model is proposed in the\nfield of image generation, so in order to make the generated results diverse,\nthe diffusion model combines random Gaussian noise and distributed sampling to\nincrease the randomness of the model.\n  However, the essence of super-resolution tasks requires the model to generate\nhigh-resolution images with fidelity. Excessive addition of random factors can\nresult in the model generating detailed information that does not belong to the\nHR image. To address this issue, we propose a new diffusion model called\nDeltadiff, which uses only residuals between images for diffusion, making the\nentire diffusion process more stable. The experimental results show that our\nmethod surpasses state-of-the-art models and generates results with better\nfidelity. Our code and model are publicly available at\nhttps://github.com/continueyang/DeltaDiff\n","authors":["Chao Yang","Yong Fan","Cheng Lu","Zhijing Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12381v3","updated":"2025-02-18T06:00:26Z","published":"2024-10-16T09:04:57Z","title":"HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex\n  Diagrams in Coding Tasks","summary":"  Understanding and reasoning over diagrams is a fundamental aspect of human\nintelligence. While Large Multimodal Models (LMMs) have demonstrated impressive\ncapabilities across various tasks, existing benchmarks lack comprehensive\nevaluation of their diagram interpretation and reasoning abilities,\nparticularly in coding contexts. We present HumanEval-V, a rigorous benchmark\nof human-annotated coding tasks that spans six task types and evaluates diverse\nvisual reasoning capabilities. Each task features carefully crafted diagrams\npaired with function signatures and test cases, employing novel code generation\ntasks to thoroughly assess models' diagram comprehension. Through extensive\nexperiments with 22 LMMs, we find that even top-performing models achieve\nmodest success rates, with Claude 3.5 Sonnet reaching only 36.8% pass@1,\nhighlighting substantial room for improvement. Our analysis reveals that\ncurrent LMMs struggle with spatial transformations, topological relationships,\nand dynamic patterns that humans find intuitive. These findings provide\nvaluable insights for advancing LMMs' visual reasoning abilities. We have\nopen-sourced our code and benchmark at\nhttps://github.com/HumanEval-V/HumanEval-V-Benchmark.\n","authors":["Fengji Zhang","Linquan Wu","Huiyu Bai","Guancheng Lin","Xiao Li","Xiao Yu","Yue Wang","Bei Chen","Jacky Keung"],"pdf_url":"https://arxiv.org/pdf/2410.12381v3.pdf","comment":"homepage https://humaneval-v.github.io/"},{"id":"http://arxiv.org/abs/2502.12558v1","updated":"2025-02-18T05:50:23Z","published":"2025-02-18T05:50:23Z","title":"MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment\n  Retrieval Within Long Videos","summary":"  Retrieval augmented generation (RAG) holds great promise in addressing\nchallenges associated with long video understanding. These methods retrieve\nuseful moments from long videos for their presented tasks, thereby enabling\nmultimodal large language models (MLLMs) to generate high-quality answers in a\ncost-effective way. In this work, we present MomentSeeker, a comprehensive\nbenchmark to evaluate retrieval models' performance in handling general\nlong-video moment retrieval (LVMR) tasks. MomentSeeker offers three key\nadvantages. First, it incorporates long videos of over 500 seconds on average,\nmaking it the first benchmark specialized for long-video moment retrieval.\nSecond, it covers a wide range of task categories (including Moment Search,\nCaption Alignment, Image-conditioned Moment Search, and Video-conditioned\nMoment Search) and diverse application scenarios (e.g., sports, movies,\ncartoons, and ego), making it a comprehensive tool for assessing retrieval\nmodels' general LVMR performance. Additionally, the evaluation tasks are\ncarefully curated through human annotation, ensuring the reliability of\nassessment. We further fine-tune an MLLM-based LVMR retriever on synthetic\ndata, which demonstrates strong performance on our benchmark. We perform\nextensive experiments with various popular multimodal retrievers based on our\nbenchmark, whose results highlight the challenges of LVMR and limitations for\nexisting methods. Our created resources will be shared with community to\nadvance future research in this field.\n","authors":["Huaying Yuan","Jian Ni","Yueze Wang","Junjie Zhou","Zhengyang Liang","Zheng Liu","Zhao Cao","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.12558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20756v4","updated":"2025-02-18T05:45:04Z","published":"2024-07-30T11:57:40Z","title":"SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision\n  Language Models","summary":"  Vision-Language Models (VLMs) have recently emerged, demonstrating remarkable\nvision-understanding capabilities. However, training these models requires\nlarge-scale datasets, which brings challenges related to efficiency,\neffectiveness, quality, and privacy of web data. In this paper, we introduce\nSynthVLM, a novel data synthesis and curation method for generating\nimage-caption pairs. Unlike traditional methods, where captions are generated\nfrom images, SynthVLM utilizes advanced diffusion models and high-quality\ncaptions to automatically synthesize and select high-resolution images from\ntext descriptions, thereby creating precisely aligned image-text pairs. To\ndemonstrate the power of SynthVLM, we introduce SynthVLM-100K, a high-quality\ndataset consisting of 100,000 curated and synthesized image-caption pairs. In\nboth model and human evaluations, SynthVLM-100K outperforms traditional\nreal-world datasets. Leveraging this dataset, we develop a new family of\nmultimodal large language models (MLLMs), SynthVLM-7B and SynthVLM-13B, which\nachieve state-of-the-art (SOTA) performance on various vision\nquestion-answering (VQA) tasks. Notably, our models outperform LLaVA across\nmost metrics with only 18\\% pretrain data. Furthermore, SynthVLM-7B and\nSynthVLM-13B attain SOTA performance on the MMLU benchmark, demonstrating that\nthe high-quality SynthVLM-100K dataset preserves language abilities. To\nfacilitate future research, our dataset and the complete data generating and\ncurating methods are open-sourced at\nhttps://github.com/starriver030515/SynthVLM.\n","authors":["Zheng Liu","Hao Liang","Bozhou Li","Tianyi Bai","Wentao Xiong","Chong Chen","Conghui He","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2407.20756v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12546v1","updated":"2025-02-18T05:15:52Z","published":"2025-02-18T05:15:52Z","title":"Spatiotemporal Multi-Camera Calibration using Freely Moving People","summary":"  We propose a novel method for spatiotemporal multi-camera calibration using\nfreely moving people in multiview videos. Since calibrating multiple cameras\nand finding matches across their views are inherently interdependent,\nperforming both in a unified framework poses a significant challenge. We\naddress these issues as a single registration problem of matching two sets of\n3D points, leveraging human motion in dynamic multi-person scenes. To this end,\nwe utilize 3D human poses obtained from an off-the-shelf monocular 3D human\npose estimator and transform them into 3D points on a unit sphere, to solve the\nrotation, time offset, and the association alternatingly. We employ a\nprobabilistic approach that can jointly solve both problems of aligning\nspatiotemporal data and establishing correspondences through soft assignment\nbetween two views. The translation is determined by applying coplanarity\nconstraints. The pairwise registration results are integrated into a multiview\nsetup, and then a nonlinear optimization method is used to improve the accuracy\nof the camera poses, temporal offsets, and multi-person associations. Extensive\nexperiments on synthetic and real data demonstrate the effectiveness and\nflexibility of the proposed method as a practical marker-free calibration tool.\n","authors":["Sang-Eun Lee","Ko Nishino","Shohei Nobuhara"],"pdf_url":"https://arxiv.org/pdf/2502.12546v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.12545v1","updated":"2025-02-18T05:15:19Z","published":"2025-02-18T05:15:19Z","title":"IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with\n  360$^\\circ$ Cameras","summary":"  We present a novel 3D reconstruction pipeline for 360$^\\circ$ cameras for 3D\nmapping and rendering of indoor environments. Traditional Structure-from-Motion\n(SfM) methods may not work well in large-scale indoor scenes due to the\nprevalence of textureless and repetitive regions. To overcome these challenges,\nour approach (IM360) leverages the wide field of view of omnidirectional images\nand integrates the spherical camera model into every core component of the SfM\npipeline. In order to develop a comprehensive 3D reconstruction solution, we\nintegrate a neural implicit surface reconstruction technique to generate\nhigh-quality surfaces from sparse input data. Additionally, we utilize a\nmesh-based neural rendering approach to refine texture maps and accurately\ncapture view-dependent properties by combining diffuse and specular components.\nWe evaluate our pipeline on large-scale indoor scenes from the Matterport3D and\nStanford2D3D datasets. In practice, IM360 demonstrate superior performance in\nterms of textured mesh reconstruction over SOTA. We observe accuracy\nimprovements in terms of camera localization and registration as well as\nrendering high frequency details.\n","authors":["Dongki Jung","Jaehoon Choi","Yonghan Lee","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2502.12545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09874v2","updated":"2025-02-18T05:10:50Z","published":"2025-02-14T02:51:25Z","title":"FrGNet: A fourier-guided weakly-supervised framework for nuclear\n  instance segmentation","summary":"  Nuclear instance segmentation has played a critical role in pathology image\nanalysis. The main challenges arise from the difficulty in accurately\nsegmenting instances and the high cost of precise mask-level annotations for\nfully-supervised training.In this work, we propose a fourier guidance framework\nfor solving the weakly-supervised nuclear instance segmentation problem. In\nthis framework, we construct a fourier guidance module to fuse the priori\ninformation into the training process of the model, which facilitates the model\nto capture the relevant features of the nuclear. Meanwhile, in order to further\nimprove the model's ability to represent the features of nuclear, we propose\nthe guide-based instance level contrastive module. This module makes full use\nof the framework's own properties and guide information to effectively enhance\nthe representation features of nuclear. We show on two public datasets that our\nmodel can outperform current SOTA methods under fully-supervised design, and in\nweakly-supervised experiments, with only a small amount of labeling our model\nstill maintains close to the performance under full supervision.In addition, we\nalso perform generalization experiments on a private dataset, and without any\nlabeling, our model is able to segment nuclear images that have not been seen\nduring training quite effectively. As open science, all codes and pre-trained\nmodels are available at https://github.com/LQY404/FrGNet.\n","authors":["Peng Ling","Wenxiao Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.09874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12541v1","updated":"2025-02-18T05:04:29Z","published":"2025-02-18T05:04:29Z","title":"When Segmentation Meets Hyperspectral Image: New Paradigm for\n  Hyperspectral Image Classification","summary":"  Hyperspectral image (HSI) classification is a cornerstone of remote sensing,\nenabling precise material and land-cover identification through rich spectral\ninformation. While deep learning has driven significant progress in this task,\nsmall patch-based classifiers, which account for over 90% of the progress, face\nlimitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach\nconsiders a limited receptive field, resulting in insufficient spatial\nstructural information critical for object-level identification and noise-like\nmisclassifications even within uniform regions; (2) undefined optimal patch\nsizes lead to coarse label predictions, which degrade performance; and (3) a\nlack of multi-shape awareness around objects. To address these challenges, we\ndraw inspiration from large-scale image segmentation techniques, which excel at\nhandling object boundaries-a capability essential for semantic labeling in HSI\nclassification. However, their application remains under-explored in this task\ndue to (1) the prevailing notion that larger patch sizes degrade performance,\n(2) the extensive unlabeled regions in HSI groundtruth, and (3) the\nmisalignment of input shapes between HSI data and segmentation models. Thus, in\nthis study, we propose a novel paradigm and baseline, HSIseg, for HSI\nclassification that leverages segmentation techniques combined with a novel\nDynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We\nalso introduce an intuitive progressive learning framework with adaptive\npseudo-labeling to iteratively incorporate unlabeled regions into the training\nprocess, thereby advancing the application of segmentation techniques.\nAdditionally, we incorporate auxiliary data through multi-source data\ncollaboration, promoting better feature interaction. Validated on five public\nHSI datasets, our proposal outperforms state-of-the-art methods.\n","authors":["Weilian Zhou","Weixuan Xie","Sei-ichiro Kamata","Man Sing Wong"," Huiying"," Hou","Haipeng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12535v1","updated":"2025-02-18T04:38:45Z","published":"2025-02-18T04:38:45Z","title":"Learning Transformation-Isomorphic Latent Space for Accurate Hand Pose\n  Estimation","summary":"  Vision-based regression tasks, such as hand pose estimation, have achieved\nhigher accuracy and faster convergence through representation learning.\nHowever, existing representation learning methods often encounter the following\nissues: the high semantic level of features extracted from images is inadequate\nfor regressing low-level information, and the extracted features include\ntask-irrelevant information, reducing their compactness and interfering with\nregression tasks. To address these challenges, we propose TI-Net, a highly\nversatile visual Network backbone designed to construct a Transformation\nIsomorphic latent space. Specifically, we employ linear transformations to\nmodel geometric transformations in the latent space and ensure that {\\rm\nTI-Net} aligns them with those in the image space. This ensures that the latent\nfeatures capture compact, low-level information beneficial for pose estimation\ntasks. We evaluated TI-Net on the hand pose estimation task to demonstrate the\nnetwork's superiority. On the DexYCB dataset, TI-Net achieved a 10% improvement\nin the PA-MPJPE metric compared to specialized state-of-the-art (SOTA) hand\npose estimation methods. Our code will be released in the future.\n","authors":["Kaiwen Ren","Lei Hu","Zhiheng Zhang","Yongjing Ye","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2502.12535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12527v1","updated":"2025-02-18T04:25:42Z","published":"2025-02-18T04:25:42Z","title":"Comprehensive Assessment and Analysis for NSFW Content Erasure in\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have gained widespread application\nacross various domains, demonstrating remarkable creative potential. However,\nthe strong generalization capabilities of these models can inadvertently led\nthey to generate NSFW content even with efforts on filtering NSFW content from\nthe training dataset, posing risks to their safe deployment. While several\nconcept erasure methods have been proposed to mitigate this issue, a\ncomprehensive evaluation of their effectiveness remains absent. To bridge this\ngap, we present the first systematic investigation of concept erasure methods\nfor NSFW content and its sub-themes in text-to-image diffusion models. At the\ntask level, we provide a holistic evaluation of 11 state-of-the-art baseline\nmethods with 14 variants. Specifically, we analyze these methods from six\ndistinct assessment perspectives, including three conventional perspectives,\ni.e., erasure proportion, image quality, and semantic alignment, and three new\nperspectives, i.e., excessive erasure, the impact of explicit and implicit\nunsafe prompts, and robustness. At the tool level, we perform a detailed\ntoxicity analysis of NSFW datasets and compare the performance of different\nNSFW classifiers, offering deeper insights into their performance alongside a\ncompilation of comprehensive evaluation metrics. Our benchmark not only\nsystematically evaluates concept erasure methods, but also delves into the\nunderlying factors influencing their performance at the insight level. By\nsynthesizing insights from various evaluation perspectives, we provide a deeper\nunderstanding of the challenges and opportunities in the field, offering\nactionable guidance and inspiration for advancing research and practical\napplications in concept erasure.\n","authors":["Die Chen","Zhiwen Li","Cen Chen","Xiaodan Li","Jinyan Ye"],"pdf_url":"https://arxiv.org/pdf/2502.12527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12524v1","updated":"2025-02-18T04:20:14Z","published":"2025-02-18T04:20:14Z","title":"YOLOv12: Attention-Centric Real-Time Object Detectors","summary":"  Enhancing the network architecture of the YOLO framework has been crucial for\na long time, but has focused on CNN-based improvements despite the proven\nsuperiority of attention mechanisms in modeling capabilities. This is because\nattention-based models cannot match the speed of CNN-based models. This paper\nproposes an attention-centric YOLO framework, namely YOLOv12, that matches the\nspeed of previous CNN-based ones while harnessing the performance benefits of\nattention mechanisms. YOLOv12 surpasses all popular real-time object detectors\nin accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP\nwith an inference latency of 1.64 ms on a T4 GPU, outperforming advanced\nYOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage\nextends to other model scales. YOLOv12 also surpasses end-to-end real-time\ndetectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats\nRT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the\ncomputation and 45% of the parameters. More comparisons are shown in Figure 1.\n","authors":["Yunjie Tian","Qixiang Ye","David Doermann"],"pdf_url":"https://arxiv.org/pdf/2502.12524v1.pdf","comment":"https://github.com/sunsmarterjie/yolov12"},{"id":"http://arxiv.org/abs/2502.12520v1","updated":"2025-02-18T04:09:46Z","published":"2025-02-18T04:09:46Z","title":"SAFEERASER: Enhancing Safety in Multimodal Large Language Models through\n  Multimodal Machine Unlearning","summary":"  As Multimodal Large Language Models (MLLMs) develop, their potential security\nissues have become increasingly prominent. Machine Unlearning (MU), as an\neffective strategy for forgetting specific knowledge in training data, has been\nwidely used in privacy protection. However, MU for safety in MLLM has yet to be\nfully explored. To address this issue, we propose SAFEERASER, a safety\nunlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs.\nWe comprehensively evaluate unlearning methods from two perspectives: forget\nquality and model utility. Our findings show that existing MU methods struggle\nto maintain model performance while implementing the forget operation and often\nsuffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to\nalleviate over-forgetting through decouple prompt during unlearning process. To\nquantitatively measure over-forgetting mitigated by PD Loss, we propose a new\nmetric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate\nthat combining PD Loss with existing unlearning methods can effectively prevent\nover-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B\nand LLaVA-13B, while maintaining forget quality and model utility. Our code and\ndataset will be released upon acceptance. Warning: This paper contains examples\nof harmful language and images, and reader discretion is recommended.\n","authors":["Junkai Chen","Zhijie Deng","Kening Zheng","Yibo Yan","Shuliang Liu","PeiJun Wu","Peijie Jiang","Jia Liu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.12520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13540v2","updated":"2025-02-18T04:00:30Z","published":"2024-12-18T06:35:18Z","title":"Benchmarking and Improving Large Vision-Language Models for Fundamental\n  Visual Graph Understanding and Reasoning","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross diverse tasks. Despite great success, recent studies show that LVLMs\nencounter substantial limitations when engaging with visual graphs. To study\nthe reason behind these limitations, we propose VGCure, a comprehensive\nbenchmark covering 22 tasks for examining the fundamental graph understanding\nand reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs\nreveal that LVLMs are weak in basic graph understanding and reasoning tasks,\nparticularly those concerning relational or structurally complex information.\nBased on this observation, we propose a structure-aware fine-tuning framework\nto enhance LVLMs with structure learning abilities through three\nself-supervised learning tasks. Experiments validate the effectiveness of our\nmethod in improving LVLMs' performance on fundamental and downstream graph\nlearning tasks, as well as enhancing their robustness against complex visual\ngraphs.\n","authors":["Yingjie Zhu","Xuefeng Bai","Kehai Chen","Yang Xiang","Jun Yu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12513v1","updated":"2025-02-18T03:58:38Z","published":"2025-02-18T03:58:38Z","title":"RealSyn: An Effective and Scalable Multimodal Interleaved Document\n  Transformation Paradigm","summary":"  After pre-training on extensive image-text pairs, Contrastive Language-Image\nPre-training (CLIP) demonstrates promising performance on a wide variety of\nbenchmarks. However, a substantial volume of non-paired data, such as\nmultimodal interleaved documents, remains underutilized for vision-language\nrepresentation learning. To fully leverage these unpaired documents, we\ninitially establish a Real-World Data Extraction pipeline to extract\nhigh-quality images and texts. Then we design a hierarchical retrieval method\nto efficiently associate each image with multiple semantically relevant\nrealistic texts. To further enhance fine-grained visual information, we propose\nan image semantic augmented generation module for synthetic text production.\nFurthermore, we employ a semantic balance sampling strategy to improve dataset\ndiversity, enabling better learning of long-tail concepts. Based on these\ninnovations, we construct RealSyn, a dataset combining realistic and synthetic\ntexts, available in three scales: 15M, 30M, and 100M. Extensive experiments\ndemonstrate that RealSyn effectively advances vision-language representation\nlearning and exhibits strong scalability. Models pre-trained on RealSyn achieve\nstate-of-the-art performance on multiple downstream tasks. To facilitate future\nresearch, the RealSyn dataset and pre-trained model weights are released at\nhttps://github.com/deepglint/RealSyn.\n","authors":["Tiancheng Gu","Kaicheng Yang","Chaoyi Zhang","Yin Xie","Xiang An","Ziyong Feng","Dongnan Liu","Weidong Cai","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2502.12513v1.pdf","comment":"16 pages, 12 figures, Webpage: https://garygutc.github.io/RealSyn"},{"id":"http://arxiv.org/abs/2410.07610v3","updated":"2025-02-18T03:55:28Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $50,000\\times$ fewer multimodal data pairs to\nbridge the modalities given pre-trained unimodal encoders on ImageNet\nclassification and misinformative news caption detection. CSA surpasses the\nstate-of-the-art method to map unimodal features to multimodal features. We\nalso demonstrate the ability of CSA with modalities beyond image and text,\npaving the way for future modality pairs with limited paired multimodal data\nbut abundant unpaired unimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10982v2","updated":"2025-02-18T03:43:41Z","published":"2025-02-16T04:00:06Z","title":"TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction","summary":"  3D facial reconstruction from a single in-the-wild image is a crucial task in\nhuman-centered computer vision tasks. While existing methods can recover\naccurate facial shapes, there remains significant space for improvement in\nfine-grained expression capture. Current approaches struggle with irregular\nmouth shapes, exaggerated expressions, and asymmetrical facial movements. We\npresent TEASER (Token EnhAnced Spatial modeling for Expressions\nReconstruction), which addresses these challenges and enhances 3D facial\ngeometry performance. TEASER tackles two main limitations of existing methods:\ninsufficient photometric loss for self-reconstruction and inaccurate\nlocalization of subtle expressions. We introduce a multi-scale tokenizer to\nextract facial appearance information. Combined with a neural renderer, these\ntokens provide precise geometric guidance for expression reconstruction.\nFurthermore, TEASER incorporates a pose-dependent landmark loss to further\nimprove geometric performances. Our approach not only significantly enhances\nexpression reconstruction quality but also offers interpretable tokens suitable\nfor various downstream applications, such as photorealistic facial video\ndriving, expression transfer, and identity swapping. Quantitative and\nqualitative experimental results across multiple datasets demonstrate that\nTEASER achieves state-of-the-art performance in precise expression\nreconstruction.\n","authors":["Yunfei Liu","Lei Zhu","Lijian Lin","Ye Zhu","Ailing Zhang","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2502.10982v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10954v2","updated":"2025-02-18T03:41:03Z","published":"2025-02-16T02:17:05Z","title":"Learning to Stop Overthinking at Test Time","summary":"  Test time scaling is currently one of the most active research areas that\nshows promise after training time scaling has reached its limits. Deep-thinking\n(DT) models are a class of recurrent models that can perform easy-to-hard\ngeneralization by assigning more compute to harder test samples. However, due\nto their inability to determine the complexity of a test sample, DT models have\nto use a large amount of computation for both easy and hard test samples.\nExcessive test time computation is wasteful and can cause the ``overthinking''\nproblem where more test time computation leads to worse results. In this paper,\nwe introduce a test time training method for determining the optimal amount of\ncomputation needed for each sample during test time. We also propose\nConv-LiGRU, a novel recurrent architecture for efficient and robust visual\nreasoning. Extensive experiments demonstrate that Conv-LiGRU is more stable\nthan DT, effectively mitigates the ``overthinking'' phenomenon, and achieves\nsuperior accuracy.\n","authors":["Hieu Tran Bao","Nguyen Cong Dat","Nguyen Duc Anh","Hoang Thanh-Tung"],"pdf_url":"https://arxiv.org/pdf/2502.10954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19660v3","updated":"2025-02-18T03:39:37Z","published":"2024-07-29T02:49:55Z","title":"A Causally Informed Pretraining Approach for Multimodal Foundation\n  Models: Applications in Remote Sensing","summary":"  Self-supervised learning has emerged as a powerful paradigm for pretraining\nfoundation models using large-scale data. Existing pretraining approaches\npredominantly rely on masked reconstruction or next-token prediction\nstrategies, demonstrating strong performance across various downstream tasks,\nincluding geoscience applications. However, these approaches do not fully\ncapture the causal interplay between different geospatial and environmental\nvariables. To address this limitation, we propose Causally Informed\nVariable-Step Forecasting (CI-VSF), a novel pretraining task that models\nforecasting as a conditional generation task, where driver variables (e.g.,\nweather) inform the prediction of response variables (e.g., satellite imagery).\nWe demonstrate that pretraining in such a fashion leads to enhanced performance\nwhen finetuned on both prediction (e.g., crop mapping, missing image\nprediction, soil moisture estimation) and forecasting (e.g., future image\nforecasting, soil moisture forecasting) downstream tasks when compared to other\npretraining approaches. While we use remote sensing as our main application to\ndemonstrate the efficacy of our proposed pretraining strategy over existing\nparadigms, it is applicable to any domain that involves known causal\nrelationships amongst a set of variables.\n","authors":["Praveen Ravirathinam","Ankush Khandelwal","Rahul Ghosh","Vipin Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.19660v3.pdf","comment":"13 pages with appendix"},{"id":"http://arxiv.org/abs/2502.12488v1","updated":"2025-02-18T03:18:29Z","published":"2025-02-18T03:18:29Z","title":"Enhancing Audio-Visual Spiking Neural Networks through\n  Semantic-Alignment and Cross-Modal Residual Learning","summary":"  Humans interpret and perceive the world by integrating sensory information\nfrom multiple modalities, such as vision and hearing. Spiking Neural Networks\n(SNNs), as brain-inspired computational models, exhibit unique advantages in\nemulating the brain's information processing mechanisms. However, existing SNN\nmodels primarily focus on unimodal processing and lack efficient cross-modal\ninformation fusion, thereby limiting their effectiveness in real-world\nmultimodal scenarios. To address this challenge, we propose a\nsemantic-alignment cross-modal residual learning (S-CMRL) framework, a\nTransformer-based multimodal SNN architecture designed for effective\naudio-visual integration. S-CMRL leverages a spatiotemporal spiking attention\nmechanism to extract complementary features across modalities, and incorporates\na cross-modal residual learning strategy to enhance feature integration.\nAdditionally, a semantic alignment optimization mechanism is introduced to\nalign cross-modal features within a shared semantic space, improving their\nconsistency and complementarity. Extensive experiments on three benchmark\ndatasets CREMA-D, UrbanSound8K-AV, and MNISTDVS-NTIDIGITS demonstrate that\nS-CMRL significantly outperforms existing multimodal SNN methods, achieving the\nstate-of-the-art performance. The code is publicly available at\nhttps://github.com/Brain-Cog-Lab/S-CMRL.\n","authors":["Xiang He","Dongcheng Zhao","Yiting Dong","Guobin Shen","Xin Yang","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.12488v1.pdf","comment":"The manuscript is under review and the code is available\n  https://github.com/Brain-Cog-Lab/S-CMRL"},{"id":"http://arxiv.org/abs/2502.12481v1","updated":"2025-02-18T03:08:37Z","published":"2025-02-18T03:08:37Z","title":"Predicate Hierarchies Improve Few-Shot State Classification","summary":"  State classification of objects and their relations is core to many\nlong-horizon tasks, particularly in robot planning and manipulation. However,\nthe combinatorial explosion of possible object-predicate combinations, coupled\nwith the need to adapt to novel real-world environments, makes it a desideratum\nfor state classification models to generalize to novel queries with few\nexamples. To this end, we propose PHIER, which leverages predicate hierarchies\nto generalize effectively in few-shot scenarios. PHIER uses an object-centric\nscene encoder, self-supervised losses that infer semantic relations between\npredicates, and a hyperbolic distance metric that captures hierarchical\nstructure; it learns a structured latent space of image-predicate pairs that\nguides reasoning over state classification queries. We evaluate PHIER in the\nCALVIN and BEHAVIOR robotic environments and show that PHIER significantly\noutperforms existing methods in few-shot, out-of-distribution state\nclassification, and demonstrates strong zero- and few-shot generalization from\nsimulated to real-world tasks. Our results demonstrate that leveraging\npredicate hierarchies improves performance on state classification tasks with\nlimited data.\n","authors":["Emily Jin","Joy Hsu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2502.12481v1.pdf","comment":"ICLR 2025. First two authors contributed equally. Project page:\n  https://emilyzjin.github.io/projects/phier.html"},{"id":"http://arxiv.org/abs/2501.05767v3","updated":"2025-02-18T02:40:14Z","published":"2025-01-10T07:56:23Z","title":"Migician: Revealing the Magic of Free-Form Multi-Image Grounding in\n  Multimodal Large Language Models","summary":"  The recent advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly improved their fine-grained perception of single images and\ngeneral comprehension across multiple images. However, existing MLLMs still\nface challenges in achieving precise grounding in complex multi-image\nscenarios. To address this, we first explore a Chain-of-Thought (CoT) framework\nthat integrates single-image grounding with multi-image comprehension. While\npartially effective, it remains unstable and struggles to capture abstract\nvisual information due to its non-end-to-end nature. Therefore, we introduce\nMigician, the first multi-image grounding model capable of performing free-form\nand accurate grounding across multiple images. To support this, we present the\nMGrounding-630k dataset, which comprises data for several multi-image grounding\ntasks derived from existing datasets, along with newly generated free-form\ngrounding instruction-following data. Furthermore, we propose MIG-Bench, a\ncomprehensive benchmark specifically designed for evaluating multi-image\ngrounding capabilities. Experimental results demonstrate that our model\nachieves significantly superior multi-image grounding capabilities,\noutperforming the best existing MLLMs by 24.94% and even surpassing much larger\n70B models. Our code, model, dataset, and benchmark are fully open-sourced at\nhttps://migician-vg.github.io/.\n","authors":["You Li","Heyu Huang","Chi Chen","Kaiyu Huang","Chao Huang","Zonghao Guo","Zhiyuan Liu","Jinan Xu","Yuhua Li","Ruixuan Li","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2501.05767v3.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.12456v1","updated":"2025-02-18T02:37:34Z","published":"2025-02-18T02:37:34Z","title":"Not-So-Optimal Transport Flows for 3D Point Cloud Generation","summary":"  Learning generative models of 3D point clouds is one of the fundamental\nproblems in 3D generative learning. One of the key properties of point clouds\nis their permutation invariance, i.e., changing the order of points in a point\ncloud does not change the shape they represent. In this paper, we analyze the\nrecently proposed equivariant OT flows that learn permutation invariant\ngenerative models for point-based molecular data and we show that these models\nscale poorly on large point clouds. Also, we observe learning (equivariant) OT\nflows is generally challenging since straightening flow trajectories makes the\nlearned flow model complex at the beginning of the trajectory. To remedy these,\nwe propose not-so-optimal transport flow models that obtain an approximate OT\nby an offline OT precomputation, enabling an efficient construction of OT pairs\nfor training. During training, we can additionally construct a hybrid coupling\nby combining our approximate OT and independent coupling to make the target\nflow models easier to learn. In an extensive empirical study, we show that our\nproposed model outperforms prior diffusion- and flow-based approaches on a wide\nrange of unconditional generation and shape completion on the ShapeNet\nbenchmark.\n","authors":["Ka-Hei Hui","Chao Liu","Xiaohui Zeng","Chi-Wing Fu","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2502.12456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00569v3","updated":"2025-02-18T02:37:23Z","published":"2024-12-31T17:54:29Z","title":"Probing Visual Language Priors in VLMs","summary":"  Despite recent advances in Vision-Language Models (VLMs), they may over-rely\non visual language priors existing in their training data rather than true\nvisual reasoning. To investigate this, we introduce ViLP, a benchmark featuring\ndeliberately out-of-distribution images synthesized via image generation models\nand out-of-distribution Q&A pairs. Each question in ViLP is coupled with three\npotential answers and three corresponding images: one that can be resolved by\ntext priors alone and two that demand visual reasoning. Although, humans\nachieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves\nonly 66.17% on ViLP. To alleviate this, we propose a self-improving framework\nin which models generate new VQA data, then apply pixel-level and semantic\ncorruptions to form \"good-bad\" image pairs for self-training. Our training\nobjectives compel VLMs to focus more on the actual visual inputs, and we\ndemonstrate their effectiveness in boosting the performance of open-source\nVLMs, including LLaVA-v1.5 and Cambrian.\n","authors":["Tiange Luo","Ang Cao","Gunhee Lee","Justin Johnson","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2501.00569v3.pdf","comment":"https://huggingface.co/ViLP"},{"id":"http://arxiv.org/abs/2502.12454v1","updated":"2025-02-18T02:36:16Z","published":"2025-02-18T02:36:16Z","title":"Benchmarking Zero-Shot Facial Emotion Annotation with Large Language\n  Models: A Multi-Class and Multi-Frame Approach in DailyLife","summary":"  This study investigates the feasibility and performance of using large\nlanguage models (LLMs) to automatically annotate human emotions in everyday\nscenarios. We conducted experiments on the DailyLife subset of the publicly\navailable FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot\nlabeling of key frames extracted from video segments. Under a seven-class\nemotion taxonomy (\"Angry,\" \"Disgust,\" \"Fear,\" \"Happy,\" \"Neutral,\" \"Sad,\"\n\"Surprise\"), the LLM achieved an average precision of approximately 50%. In\ncontrast, when limited to ternary emotion classification\n(negative/neutral/positive), the average precision increased to approximately\n64%. Additionally, we explored a strategy that integrates multiple frames\nwithin 1-2 second video clips to enhance labeling performance and reduce costs.\nThe results indicate that this approach can slightly improve annotation\naccuracy. Overall, our preliminary findings highlight the potential application\nof zero-shot LLMs in human facial emotion annotation tasks, offering new\navenues for reducing labeling costs and broadening the applicability of LLMs in\ncomplex multimodal environments.\n","authors":["He Zhang","Xinyi Fu"],"pdf_url":"https://arxiv.org/pdf/2502.12454v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.12449v1","updated":"2025-02-18T02:30:14Z","published":"2025-02-18T02:30:14Z","title":"YUNet: Improved YOLOv11 Network for Skyline Detection","summary":"  Skyline detection plays an important role in geolocalizaion, flight control,\nvisual navigation, port security, etc. The appearance of the sky and non-sky\nareas are variable, because of different weather or illumination environment,\nwhich brings challenges to skyline detection. In this research, we proposed the\nYUNet algorithm, which improved the YOLOv11 architecture to segment the sky\nregion and extract the skyline in complicated and variable circumstances. To\nimprove the ability of multi-scale and large range contextual feature fusion,\nthe YOLOv11 architecture is extended as an UNet-like architecture, consisting\nof an encoder, neck and decoder submodule. The encoder extracts the multi-scale\nfeatures from the given images. The neck makes fusion of these multi-scale\nfeatures. The decoder applies the fused features to complete the prediction\nrebuilding. To validate the proposed approach, the YUNet was tested on\nSkyfinder and CH1 datasets for segmentation and skyline detection respectively.\nOur test shows that the IoU of YUnet segmentation can reach 0.9858, and the\naverage error of YUnet skyline detection is just 1.36 pixels. The\nimplementation is published at\nhttps://github.com/kuazhangxiaoai/SkylineDet-YOLOv11Seg.git.\n","authors":["Gang Yang","Miao Wang","Quan Zhou","Jiangchuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.12449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12427v1","updated":"2025-02-18T01:52:41Z","published":"2025-02-18T01:52:41Z","title":"Multi Image Super Resolution Modeling for Earth System Models","summary":"  Super-resolution (SR) techniques are essential for improving Earth System\nModel (ESM) data's spatial resolution, which helps better understand complex\nenvironmental processes. This paper presents a new algorithm, ViFOR, which\ncombines Vision Transformers (ViT) and Implicit Neural Representation Networks\n(INRs) to generate High-Resolution (HR) images from Low-Resolution (LR) inputs.\nViFOR introduces a novel integration of Fourier-based activation functions\nwithin the Vision Transformer architecture, enabling it to effectively capture\nglobal context and high-frequency details critical for accurate SR\nreconstruction. The results show that ViFOR outperforms state-of-the-art\nmethods such as ViT, Sinusoidal Representation Networks (SIREN), and SR\nGenerative Adversarial Networks (SRGANs) based on metrics like Peak\nSignal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) both for global as\nwell as the local imagery. ViFOR improves PSNR of up to 4.18 dB, 1.56 dB, and\n1.73 dB over ViT for full images in the Source Temperature, Shortwave, and\nLongwave Flux.\n","authors":["Ehsan Zeraatkar","Salah A Faroughi","Jelena Tešić"],"pdf_url":"https://arxiv.org/pdf/2502.12427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12425v1","updated":"2025-02-18T01:49:45Z","published":"2025-02-18T01:49:45Z","title":"Robust Disentangled Counterfactual Learning for Physical Audiovisual\n  Commonsense Reasoning","summary":"  In this paper, we propose a new Robust Disentangled Counterfactual Learning\n(RDCL) approach for physical audiovisual commonsense reasoning. The task aims\nto infer objects' physics commonsense based on both video and audio input, with\nthe main challenge being how to imitate the reasoning ability of humans, even\nunder the scenario of missing modalities. Most of the current methods fail to\ntake full advantage of different characteristics in multi-modal data, and\nlacking causal reasoning ability in models impedes the progress of implicit\nphysical knowledge inferring. To address these issues, our proposed RDCL method\ndecouples videos into static (time-invariant) and dynamic (time-varying)\nfactors in the latent space by the disentangled sequential encoder, which\nadopts a variational autoencoder (VAE) to maximize the mutual information with\na contrastive loss function. Furthermore, we introduce a counterfactual\nlearning module to augment the model's reasoning ability by modeling physical\nknowledge relationships among different objects under counterfactual\nintervention. To alleviate the incomplete modality data issue, we introduce a\nrobust multimodal learning method to recover the missing data by decomposing\nthe shared features and model-specific features. Our proposed method is a\nplug-and-play module that can be incorporated into any baseline including VLMs.\nIn experiments, we show that our proposed method improves the reasoning\naccuracy and robustness of baseline methods and achieves the state-of-the-art\nperformance.\n","authors":["Mengshi Qi","Changsheng Lv","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2502.12425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12418v1","updated":"2025-02-18T01:36:10Z","published":"2025-02-18T01:36:10Z","title":"Boosting Illuminant Estimation in Deep Color Constancy through Enhancing\n  Brightness Robustness","summary":"  Color constancy estimates illuminant chromaticity to correct color-biased\nimages. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models\nhave made substantial advancements. Nevertheless, the potential risks in DNNCC\ndue to the vulnerability of deep neural networks have not yet been explored. In\nthis paper, we conduct the first investigation into the impact of a key factor\nin color constancy-brightness-on DNNCC from a robustness perspective. Our\nevaluation reveals that several mainstream DNNCC models exhibit high\nsensitivity to brightness despite their focus on chromaticity estimation. This\nsheds light on a potential limitation of existing DNNCC models: their\nsensitivity to brightness may hinder performance given the widespread\nbrightness variations in real-world datasets. From the insights of our\nanalysis, we propose a simple yet effective brightness robustness enhancement\nstrategy for DNNCC models, termed BRE. The core of BRE is built upon the\nadaptive step-size adversarial brightness augmentation technique, which\nidentifies high-risk brightness variation and generates augmented images via\nexplicit brightness adjustment. Subsequently, BRE develops a\nbrightness-robustness-aware model optimization strategy that integrates\nadversarial brightness training and brightness contrastive loss, significantly\nbolstering the brightness robustness of DNNCC models. BRE is\nhyperparameter-free and can be integrated into existing DNNCC models, without\nincurring additional overhead during the testing phase. Experiments on two\npublic color constancy datasets-ColorChecker and Cube+-demonstrate that the\nproposed BRE consistently enhances the illuminant estimation performance of\nexisting DNNCC models, reducing the estimation error by an average of 5.04%\nacross six mainstream DNNCC models, underscoring the critical role of enhancing\nbrightness robustness in these models.\n","authors":["Mengda Xie","Chengzhi Zhong","Yiling He","Zhan Qin","Meie Fang"],"pdf_url":"https://arxiv.org/pdf/2502.12418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12415v1","updated":"2025-02-18T01:26:07Z","published":"2025-02-18T01:26:07Z","title":"Gaseous Object Detection","summary":"  Object detection, a fundamental and challenging problem in computer vision,\nhas experienced rapid development due to the effectiveness of deep learning.\nThe current objects to be detected are mostly rigid solid substances with\napparent and distinct visual characteristics. In this paper, we endeavor on a\nscarcely explored task named Gaseous Object Detection (GOD), which is\nundertaken to explore whether the object detection techniques can be extended\nfrom solid substances to gaseous substances. Nevertheless, the gas exhibits\nsignificantly different visual characteristics: 1) saliency deficiency, 2)\narbitrary and ever-changing shapes, 3) lack of distinct boundaries. To\nfacilitate the study on this challenging task, we construct a GOD-Video dataset\ncomprising 600 videos (141,017 frames) that cover various attributes with\nmultiple types of gases. A comprehensive benchmark is established based on this\ndataset, allowing for a rigorous evaluation of frame-level and video-level\ndetectors. Deduced from the Gaussian dispersion model, the physics-inspired\nVoxel Shift Field (VSF) is designed to model geometric irregularities and\never-changing shapes in potential 3D space. By integrating VSF into Faster\nRCNN, the VSF RCNN serves as a simple but strong baseline for gaseous object\ndetection. Our work aims to attract further research into this valuable albeit\nchallenging area.\n","authors":["Kailai Zhou","Yibo Wang","Tao Lv","Qiu Shen","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2502.12415v1.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)"},{"id":"http://arxiv.org/abs/2502.12406v1","updated":"2025-02-18T00:40:51Z","published":"2025-02-18T00:40:51Z","title":"Multi-vision-based Picking Point Localisation of Target Fruit for\n  Harvesting Robots","summary":"  This paper presents multi-vision-based localisation strategies for harvesting\nrobots. Identifying picking points accurately is essential for robotic\nharvesting because insecure grasping can lead to economic loss through fruit\ndamage and dropping. In this study, two multi-vision-based localisation\nmethods, namely the analytical approach and model-based algorithms, were\nemployed. The actual geometric centre points of fruits were collected using a\nmotion capture system (mocap), and two different surface points Cfix and Ceih\nwere extracted using two Red-Green-Blue-Depth (RGB-D) cameras. First, the\npicking points of the target fruit were detected using analytical methods.\nSecond, various primary and ensemble learning methods were employed to predict\nthe geometric centre of target fruits by taking surface points as input.\nAdaboost regression, the most successful model-based localisation algorithm,\nachieved 88.8% harvesting accuracy with a Mean Euclidean Distance (MED) of 4.40\nmm, while the analytical approach reached 81.4% picking success with a MED of\n14.25 mm, both demonstrating better performance than the single-camera, which\nhad a picking success rate of 77.7% with a MED of 24.02 mm. To evaluate the\neffect of picking point accuracy in collecting fruits, a series of robotic\nharvesting experiments were performed utilising a collaborative robot (cobot).\nIt is shown that multi-vision systems can improve picking point localisation,\nresulting in higher success rates of picking in robotic harvesting.\n","authors":["C. Beldek","A. Dunn","J. Cunningham","E. Sariyildiz","S. L. Phung","G. Alici"],"pdf_url":"https://arxiv.org/pdf/2502.12406v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2407.17328v2","updated":"2025-02-18T00:26:21Z","published":"2024-07-24T14:52:18Z","title":"DarSwin-Unet: Distortion Aware Encoder-Decoder Architecture","summary":"  Wide-angle fisheye images are becoming increasingly common for perception\ntasks in applications such as robotics, security, and mobility (e.g. drones,\navionics). However, current models often either ignore the distortions in\nwide-angle images or are not suitable to perform pixel-level tasks. In this\npaper, we present an encoder-decoder model based on a radial transformer\narchitecture that adapts to distortions in wide-angle lenses by leveraging the\nphysical characteristics defined by the radial distortion profile. In contrast\nto the original model, which only performs classification tasks, we introduce a\nU-Net architecture, DarSwin-Unet, designed for pixel level tasks. Furthermore,\nwe propose a novel strategy that minimizes sparsity when sampling the image for\ncreating its input tokens. Our approach enhances the model capability to handle\npixel-level tasks in wide-angle fisheye images, making it more effective for\nreal-world applications. Compared to other baselines, DarSwin-Unet achieves the\nbest results across different datasets, with significant gains when trained on\nbounded levels of distortions (very low, low, medium, and high) and tested on\nall, including out-of-distribution distortions. We demonstrate its performance\non depth estimation and show through extensive experiments that DarSwin-Unet\ncan perform zero-shot adaptation to unseen distortions of different wide-angle\nlenses.\n","authors":["Akshaya Athwale","Ichrak Shili","Émile Bergeron","Ola Ahmad","Jean-François Lalonde"],"pdf_url":"https://arxiv.org/pdf/2407.17328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06462v4","updated":"2025-02-18T00:12:02Z","published":"2024-01-12T09:17:32Z","title":"AttributionScanner: A Visual Analytics System for Model Validation with\n  Metadata-Free Slice Finding","summary":"  Data slice finding is an emerging technique for validating machine learning\n(ML) models by identifying and analyzing subgroups in a dataset that exhibit\npoor performance, often characterized by distinct feature sets or descriptive\nmetadata. However, in the context of validating vision models involving\nunstructured image data, this approach faces significant challenges, including\nthe laborious and costly requirement for additional metadata and the complex\ntask of interpreting the root causes of underperformance. To address these\nchallenges, we introduce AttributionScanner, an innovative human-in-the-loop\nVisual Analytics (VA) system, designed for metadata-free data slice finding.\nOur system identifies interpretable data slices that involve common model\nbehaviors and visualizes these patterns through an Attribution Mosaic design.\nOur interactive interface provides straightforward guidance for users to\ndetect, interpret, and annotate predominant model issues, such as spurious\ncorrelations (model biases) and mislabeled data, with minimal effort.\nAdditionally, it employs a cutting-edge model regularization technique to\nmitigate the detected issues and enhance the model's performance. The efficacy\nof AttributionScanner is demonstrated through use cases involving two benchmark\ndatasets, with qualitative and quantitative evaluations showcasing its\nsubstantial effectiveness in vision model validation, ultimately leading to\nmore reliable and accurate models.\n","authors":["Xiwei Xuan","Jorge Piazentin Ono","Liang Gou","Kwan-Liu Ma","Liu Ren"],"pdf_url":"https://arxiv.org/pdf/2401.06462v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13335v1","updated":"2025-02-18T23:30:10Z","published":"2025-02-18T23:30:10Z","title":"Geometry-Aware Diffusion Models for Multiview Scene Inpainting","summary":"  In this paper, we focus on 3D scene inpainting, where parts of an input image\nset, captured from different viewpoints, are masked out. The main challenge\nlies in generating plausible image completions that are geometrically\nconsistent across views. Most recent work addresses this challenge by combining\ngenerative models with a 3D radiance field to fuse information across\nviewpoints. However, a major drawback of these methods is that they often\nproduce blurry images due to the fusion of inconsistent cross-view images. To\navoid blurry inpaintings, we eschew the use of an explicit or implicit radiance\nfield altogether and instead fuse cross-view information in a learned space. In\nparticular, we introduce a geometry-aware conditional generative model, capable\nof inpainting multi-view consistent images based on both geometric and\nappearance cues from reference images. A key advantage of our approach over\nexisting methods is its unique ability to inpaint masked scenes with a limited\nnumber of views (i.e., few-view inpainting), whereas previous methods require\nrelatively large image sets for their 3D model fitting step. Empirically, we\nevaluate and compare our scene-centric inpainting method on two datasets,\nSPIn-NeRF and NeRFiller, which contain images captured at narrow and wide\nbaselines, respectively, and achieve state-of-the-art 3D inpainting performance\non both. Additionally, we demonstrate the efficacy of our approach in the\nfew-view setting compared to prior methods.\n","authors":["Ahmad Salimi","Tristan Aumentado-Armstrong","Marcus A. Brubaker","Konstantinos G. Derpanis"],"pdf_url":"https://arxiv.org/pdf/2502.13335v1.pdf","comment":"Our project page is available at https://geomvi.github.io"},{"id":"http://arxiv.org/abs/2412.00702v2","updated":"2025-02-18T22:21:19Z","published":"2024-12-01T06:53:12Z","title":"Enhancing Skin Lesion Classification Generalization with Active Domain\n  Adaptation","summary":"  We propose a method to improve the generalization of skin lesion\nclassification models by combining self-supervised learning (SSL) and active\ndomain adaptation (ADA). The main steps of the approach include selection of an\nSSL pre-trained model on natural image datasets, subsequent SSL retraining on\nall available skin-lesion datasets, fine-tuning of the model on source domain\ndata with labels, and application of ADA methods on target domain data. The\nefficacy of the proposed approach is assessed in ten skin lesion datasets with\nfive different ADA methods, demonstrating its potential to improve\ngeneralization in settings with different amounts of domain shifts.\n","authors":["Jun Ye"],"pdf_url":"https://arxiv.org/pdf/2412.00702v2.pdf","comment":"7 pages, 5 figures, 2 table"},{"id":"http://arxiv.org/abs/2411.01390v3","updated":"2025-02-18T22:16:41Z","published":"2024-11-03T00:52:14Z","title":"A New Logic For Pediatric Brain Tumor Segmentation","summary":"  In this paper, we present a novel approach for segmenting pediatric brain\ntumors using a deep learning architecture, inspired by expert radiologists'\nsegmentation strategies. Our model delineates four distinct tumor labels and is\nbenchmarked on a held-out PED BraTS 2024 test set (i.e., pediatric brain tumor\ndatasets introduced by BraTS). Furthermore, we evaluate our model's performance\nagainst the state-of-the-art (SOTA) model using a new external dataset of 30\npatients from CBTN (Children's Brain Tumor Network), labeled in accordance with\nthe PED BraTS 2024 guidelines and 2023 BraTS Adult Glioma dataset. We compare\nsegmentation outcomes with the winning algorithm from the PED BraTS 2023\nchallenge as the SOTA model. Our proposed algorithm achieved an average Dice\nscore of 0.642 and an HD95 of 73.0 mm on the CBTN test data, outperforming the\nSOTA model, which achieved a Dice score of 0.626 and an HD95 of 84.0 mm.\nMoreover, our model exhibits strong generalizability, attaining a 0.877 Dice\nscore in whole tumor segmentation on the BraTS 2023 Adult Glioma dataset,\nsurpassing existing SOTA. Our results indicate that the proposed model is a\nstep towards providing more accurate segmentation for pediatric brain tumors,\nwhich is essential for evaluating therapy response and monitoring patient\nprogress. Our source code is available at\nhttps://github.com/NUBagciLab/Pediatric-Brain-Tumor-Segmentation-Model.\n","authors":["Max Bengtsson","Elif Keles","Gorkem Durak","Syed Anwar","Yuri S. Velichko","Marius G. Linguraru","Angela J. Waanders","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2411.01390v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16820v2","updated":"2025-02-18T21:18:48Z","published":"2024-04-25T17:58:43Z","title":"Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and\n  Human Ratings","summary":"  While text-to-image (T2I) generative models have become ubiquitous, they do\nnot necessarily generate images that align with a given prompt. While previous\nwork has evaluated T2I alignment by proposing metrics, benchmarks, and\ntemplates for collecting human judgements, the quality of these components is\nnot systematically measured. Human-rated prompt sets are generally small and\nthe reliability of the ratings -- and thereby the prompt set used to compare\nmodels -- is not evaluated. We address this gap by performing an extensive\nstudy evaluating auto-eval metrics and human templates. We provide three main\ncontributions: (1) We introduce a comprehensive skills-based benchmark that can\ndiscriminate models across different human templates. This skills-based\nbenchmark categorises prompts into sub-skills, allowing a practitioner to\npinpoint not only which skills are challenging, but at what level of complexity\na skill becomes challenging. (2) We gather human ratings across four templates\nand four T2I models for a total of >100K annotations. This allows us to\nunderstand where differences arise due to inherent ambiguity in the prompt and\nwhere they arise due to differences in metric and model quality. (3) Finally,\nwe introduce a new QA-based auto-eval metric that is better correlated with\nhuman ratings than existing metrics for our new dataset, across different human\ntemplates, and on TIFA160.\n","authors":["Olivia Wiles","Chuhan Zhang","Isabela Albuquerque","Ivana Kajić","Su Wang","Emanuele Bugliarello","Yasumasa Onoe","Chris Knutsen","Cyrus Rashtchian","Jordi Pont-Tuset","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2404.16820v2.pdf","comment":"Accepted to ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2403.14559v3","updated":"2025-02-18T21:16:27Z","published":"2024-03-21T16:59:45Z","title":"VAPO: Visibility-Aware Keypoint Localization for Efficient 6DoF Object\n  Pose Estimation","summary":"  Localizing predefined 3D keypoints in a 2D image is an effective way to\nestablish 3D-2D correspondences for 6DoF object pose estimation. However,\nunreliable localization results of invisible keypoints degrade the quality of\ncorrespondences. In this paper, we address this issue by localizing the\nimportant keypoints in terms of visibility. Since keypoint visibility\ninformation is currently missing in the dataset collection process, we propose\nan efficient way to generate binary visibility labels from available\nobject-level annotations, for keypoints of both asymmetric objects and\nsymmetric objects. We further derive real-valued visibility-aware importance\nfrom binary labels based on the PageRank algorithm. Taking advantage of the\nflexibility of our visibility-aware importance, we construct VAPO\n(Visibility-Aware POse estimator) by integrating the visibility-aware\nimportance with a state-of-the-art pose estimation algorithm, along with\nadditional positional encoding. VAPO can work in both CAD-based and CAD-free\nsettings. Extensive experiments are conducted on popular pose estimation\nbenchmarks including Linemod, Linemod-Occlusion, and YCB-V, demonstrating that\nVAPO clearly achieves state-of-the-art performances. Our code is available at\nhttps://github.com/RuyiLian/VAPO.\n","authors":["Ruyi Lian","Yuewei Lin","Longin Jan Latecki","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2403.14559v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04940v5","updated":"2025-02-18T21:12:22Z","published":"2023-06-08T05:13:34Z","title":"LayerAct: Advanced Activation Mechanism for Robust Inference of CNNs","summary":"  In this work, we propose a novel activation mechanism called LayerAct for\nCNNs. This approach is motivated by our theoretical and experimental analyses,\nwhich demonstrate that Layer Normalization (LN) can mitigate a limitation of\nexisting activation functions regarding noise robustness. However, LN is known\nto be disadvantageous in CNNs due to its tendency to make activation outputs\nhomogeneous. The proposed method is designed to be more robust than existing\nactivation functions by reducing the upper bound of influence caused by input\nshifts without inheriting LN's limitation. We provide analyses and experiments\nshowing that LayerAct functions exhibit superior robustness compared to\nElementAct functions. Experimental results on three clean and noisy benchmark\ndatasets for image classification tasks indicate that LayerAct functions\noutperform other activation functions in handling noisy datasets while\nachieving superior performance on clean datasets in most cases.\n","authors":["Kihyuk Yoon","Chiehyeon Lim"],"pdf_url":"https://arxiv.org/pdf/2306.04940v5.pdf","comment":"7 pages, 5 figures, 4 tables except acknowledge, reference, and\n  appendix. Accepted for the main track of AAAI 25"},{"id":"http://arxiv.org/abs/2502.13234v1","updated":"2025-02-18T19:12:51Z","published":"2025-02-18T19:12:51Z","title":"MotionMatcher: Motion Customization of Text-to-Video Diffusion Models\n  via Motion Feature Matching","summary":"  Text-to-video (T2V) diffusion models have shown promising capabilities in\nsynthesizing realistic videos from input text prompts. However, the input text\ndescription alone provides limited control over the precise objects movements\nand camera framing. In this work, we tackle the motion customization problem,\nwhere a reference video is provided as motion guidance. While most existing\nmethods choose to fine-tune pre-trained diffusion models to reconstruct the\nframe differences of the reference video, we observe that such strategy suffer\nfrom content leakage from the reference video, and they cannot capture complex\nmotion accurately. To address this issue, we propose MotionMatcher, a motion\ncustomization framework that fine-tunes the pre-trained T2V diffusion model at\nthe feature level. Instead of using pixel-level objectives, MotionMatcher\ncompares high-level, spatio-temporal motion features to fine-tune diffusion\nmodels, ensuring precise motion learning. For the sake of memory efficiency and\naccessibility, we utilize a pre-trained T2V diffusion model, which contains\nconsiderable prior knowledge about video motion, to compute these motion\nfeatures. In our experiments, we demonstrate state-of-the-art motion\ncustomization performances, validating the design of our framework.\n","authors":["Yen-Siang Wu","Chi-Pin Huang","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2502.13234v1.pdf","comment":"Project page: https://www.csie.ntu.edu.tw/~b09902097/motionmatcher/"},{"id":"http://arxiv.org/abs/2502.13196v1","updated":"2025-02-18T17:46:57Z","published":"2025-02-18T17:46:57Z","title":"GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting\n  View Synthesis","summary":"  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance\nFields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to\nrepresent complex geometry and appearance, GS achieves faster rendering times\nand reduced memory consumption compared to the neural network approach used in\nNeRF. However, quality assessment of GS-generated static content is not yet\nexplored in-depth. This paper describes a subjective quality assessment study\nthat aims to evaluate synthesized videos obtained with several static GS\nstate-of-the-art methods. The methods were applied to diverse visual scenes,\ncovering both 360-degree and forward-facing (FF) camera trajectories. Moreover,\nthe performance of 18 objective quality metrics was analyzed using the scores\nresulting from the subjective study, providing insights into their strengths,\nlimitations, and alignment with human perception. All videos and scores are\nmade available providing a comprehensive database that can be used as benchmark\non GS view synthesis and objective quality metrics.\n","authors":["Pedro Martin","António Rodrigues","João Ascenso","Maria Paula Queluz"],"pdf_url":"https://arxiv.org/pdf/2502.13196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1903.01192v4","updated":"2025-02-18T16:58:45Z","published":"2019-03-04T11:56:53Z","title":"STEFANN: Scene Text Editor using Font Adaptive Neural Network","summary":"  Textual information in a captured scene plays an important role in scene\ninterpretation and decision making. Though there exist methods that can\nsuccessfully detect and interpret complex text regions present in a scene, to\nthe best of our knowledge, there is no significant prior work that aims to\nmodify the textual information in an image. The ability to edit text directly\non images has several advantages including error correction, text restoration\nand image reusability. In this paper, we propose a method to modify text in an\nimage at character-level. We approach the problem in two stages. At first, the\nunobserved character (target) is generated from an observed character (source)\nbeing modified. We propose two different neural network architectures - (a)\nFANnet to achieve structural consistency with source font and (b) Colornet to\npreserve source color. Next, we replace the source character with the generated\ncharacter maintaining both geometric and visual consistency with neighboring\ncharacters. Our method works as a unified platform for modifying text in\nimages. We present the effectiveness of our method on COCO-Text and ICDAR\ndatasets both qualitatively and quantitatively.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/1903.01192v4.pdf","comment":"Accepted in The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.09077v3","updated":"2025-02-18T06:52:49Z","published":"2024-04-13T20:43:46Z","title":"CuriousLLM: Elevating Multi-Document Question Answering with\n  LLM-Enhanced Knowledge Graph Reasoning","summary":"  Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.\n","authors":["Zukang Yang","Zixuan Zhu","Xuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09077v3.pdf","comment":"Accepted for publication in NAACL 2025. The official version will be\n  available in the ACL Anthology"},{"id":"http://arxiv.org/abs/2502.11414v2","updated":"2025-02-18T07:04:29Z","published":"2025-02-17T03:55:51Z","title":"Unbiased Learning to Rank with Query-Level Click Propensity Estimation:\n  Beyond Pointwise Observation and Relevance","summary":"  Most existing unbiased learning-to-rank (ULTR) approaches are based on the\nuser examination hypothesis, which assumes that users will click a result only\nif it is both relevant and observed (typically modeled by position). However,\nin real-world scenarios, users often click only one or two results after\nexamining multiple relevant options, due to limited patience or because their\ninformation needs have already been satisfied. Motivated by this, we propose a\nquery-level click propensity model to capture the probability that users will\nclick on different result lists, allowing for non-zero probabilities that users\nmay not click on an observed relevant result. We hypothesize that this\npropensity increases when more potentially relevant results are present, and\nrefer to this user behavior as relevance saturation bias. Our method introduces\na Dual Inverse Propensity Weighting (DualIPW) mechanism -- combining\nquery-level and position-level IPW -- to address both relevance saturation and\nposition bias. Through theoretical derivation, we prove that DualIPW can learn\nan unbiased ranking model. Experiments on the real-world Baidu-ULTR dataset\ndemonstrate that our approach significantly outperforms state-of-the-art ULTR\nbaselines. The code and dataset information can be found at\nhttps://github.com/Trustworthy-Information-Access/DualIPW.\n","authors":["Lulu Yu","Keping Bi","Jiafeng Guo","Shihao Liu","Dawei Yin","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.11414v2.pdf","comment":"5 pages, 3 figures, accepted by The ACM Web Conference (WWW) 2025\n  Short Paper Track"},{"id":"http://arxiv.org/abs/2502.10157v2","updated":"2025-02-18T02:41:53Z","published":"2025-02-14T13:36:20Z","title":"SessionRec: Next Session Prediction Paradigm For Generative Sequential\n  Recommendation","summary":"  We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency.\n","authors":["Lei Huang","Hao Guo","Linzhi Peng","Long Zhang","Xiaoteng Wang","Daoyuan Wang","Shichao Wang","Jinpeng Wang","Lei Wang","Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02231v4","updated":"2025-02-18T11:00:19Z","published":"2024-03-04T17:21:19Z","title":"CODE-ACCORD: A Corpus of building regulatory data for rule generation\n  towards automatic compliance checking","summary":"  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and\nConstruction (AEC) sector necessitates automating the interpretation of\nbuilding regulations to achieve its full potential. Converting textual rules\ninto machine-readable formats is challenging due to the complexities of natural\nlanguage and the scarcity of resources for advanced Machine Learning (ML).\nAddressing these challenges, we introduce CODE-ACCORD, a dataset of 862\nsentences from the building regulations of England and Finland. Only the\nself-contained sentences, which express complete rules without needing\nadditional context, were considered as they are essential for ACC. Each\nsentence was manually annotated with entities and relations by a team of 12\nannotators to facilitate machine-readable rule generation, followed by careful\ncuration to ensure accuracy. The final dataset comprises 4,297 entities and\n4,329 relations across various categories, serving as a robust ground truth.\nCODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,\nincluding text classification, entity recognition, and relation extraction. It\nenables applying recent trends, such as deep neural networks and large language\nmodels, to ACC.\n","authors":["Hansi Hettiarachchi","Amna Dridi","Mohamed Medhat Gaber","Pouyan Parsafard","Nicoleta Bocaneala","Katja Breitenfelder","Gonçal Costa","Maria Hedblom","Mihaela Juganaru-Mathieu","Thamer Mecharnia","Sumee Park","He Tan","Abdel-Rahman H. Tawil","Edlira Vakaj"],"pdf_url":"https://arxiv.org/pdf/2403.02231v4.pdf","comment":"This is a preprint of an article published in the Scientific Data\n  Journal"},{"id":"http://arxiv.org/abs/2502.11747v2","updated":"2025-02-18T16:24:11Z","published":"2025-02-17T12:40:35Z","title":"Open-Ended and Knowledge-Intensive Video Question Answering","summary":"  Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.\n","authors":["Md Zarif Ul Alam","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2502.11747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12974v1","updated":"2025-02-18T15:56:34Z","published":"2025-02-18T15:56:34Z","title":"Learning More Effective Representations for Dense Retrieval through\n  Deliberate Thinking Before Search","summary":"  Recent dense retrievers usually thrive on the emergency capabilities of Large\nLanguage Models (LLMs), using them to encode queries and documents into an\nembedding space for retrieval. These LLM-based dense retrievers have shown\npromising performance across various retrieval scenarios. However, relying on a\nsingle embedding to represent documents proves less effective in capturing\ndifferent perspectives of documents for matching. In this paper, we propose\nDeliberate Thinking based Dense Retriever (DEBATER), which enhances these\nLLM-based retrievers by enabling them to learn more effective document\nrepresentations through a step-by-step thinking process. DEBATER introduces the\nChain-of-Deliberation mechanism to iteratively optimize document\nrepresentations using a continuous chain of thought. To consolidate information\nfrom various thinking steps, DEBATER also incorporates the Self Distillation\nmechanism, which identifies the most informative thinking steps and integrates\nthem into a unified text embedding. Experimental results show that DEBATER\nsignificantly outperforms existing methods across several retrieval benchmarks,\ndemonstrating superior accuracy and robustness. All codes are available at\nhttps://github.com/OpenBMB/DEBATER.\n","authors":["Yifan Ji","Zhipeng Xu","Zhenghao Liu","Yukun Yan","Shi Yu","Yishan Li","Zhiyuan Liu","Yu Gu","Ge Yu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.12974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12799v1","updated":"2025-02-18T12:00:47Z","published":"2025-02-18T12:00:47Z","title":"Towards Text-Image Interleaved Retrieval","summary":"  Current multimodal information retrieval studies mainly focus on single-image\ninputs, which limits real-world applications involving multiple images and\ntext-image interleaved content. In this work, we introduce the text-image\ninterleaved retrieval (TIIR) task, where the query and document are interleaved\ntext-image sequences, and the model is required to understand the semantics\nfrom the interleaved context for effective retrieval. We construct a TIIR\nbenchmark based on naturally interleaved wikiHow tutorials, where a specific\npipeline is designed to generate interleaved queries. To explore the task, we\nadapt several off-the-shelf retrievers and build a dense baseline by\ninterleaved multimodal large language model (MLLM). We then propose a novel\nMatryoshka Multimodal Embedder (MME), which compresses the number of visual\ntokens at different granularity, to address the challenge of excessive visual\ntokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption\nof existing models does not consistently yield effective results. Our MME\nachieves significant improvements over the baseline by substantially fewer\nvisual tokens. We provide extensive analysis and will release the dataset and\ncode to facilitate future research.\n","authors":["Xin Zhang","Ziqi Dai","Yongqi Li","Yanzhao Zhang","Dingkun Long","Pengjun Xie","Meishan Zhang","Jun Yu","Wenjie Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12799v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2501.09493v2","updated":"2025-02-18T10:46:28Z","published":"2025-01-16T12:06:56Z","title":"Large Language Models as Evaluators for Conversational Recommender\n  Systems: Benchmarking System Performance from a User-Centric Perspective","summary":"  Conversational recommender systems (CRS) involve both recommendation and\ndialogue tasks, which makes their evaluation a unique challenge. Although past\nresearch has analyzed various factors that may affect user satisfaction with\nCRS interactions from the perspective of user studies, few evaluation metrics\nfor CRS have been proposed. Recent studies have shown that LLMs can align with\nhuman preferences, and several LLM-based text quality evaluation measures have\nbeen introduced. However, the application of LLMs in CRS evaluation remains\nrelatively limited. To address this research gap and advance the development of\nuser-centric conversational recommender systems, this study proposes an\nautomated LLM-based CRS evaluation framework, building upon existing research\nin human-computer interaction and psychology. The framework evaluates CRS from\nfour dimensions: dialogue behavior, language expression, recommendation items,\nand response content. We use this framework to evaluate four different\nconversational recommender systems.\n","authors":["Nuo Chen","Quanyu Dai","Xiaoyu Dong","Xiao-Ming Wu","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2501.09493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02850v2","updated":"2025-02-18T10:43:41Z","published":"2024-11-05T06:44:15Z","title":"WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African\n  clean water access, sanitation and hygiene","summary":"  This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate\nrural African communities on clean water access, sanitation, and hygiene (WASH)\nprinciples. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach\nto address the limitations of previous approaches with limited reach or missing\ncontextualization. The paper details the development process, employing Design\nScience Research Methodology. The evaluation consisted of two phases: content\nvalidation by four WASH experts and community validation by potential users.\nContent validation confirmed WASHtsApp's ability to provide accurate and\nrelevant WASH-related information. Community validation indicated high user\nacceptance and perceived usefulness of the chatbot. The paper concludes by\ndiscussing the potential for further development, including incorporating local\nlanguages and user data analysis for targeted interventions. It also proposes\nfuture research cycles focused on wider deployment and leveraging user data for\neducational purposes.\n","authors":["Simon Kloker","Alex Cedric Luyima","Matthew Bazanya"],"pdf_url":"https://arxiv.org/pdf/2411.02850v2.pdf","comment":"Working Paper. Accepted at IST-Africa Conference 2025, Nairobi"},{"id":"http://arxiv.org/abs/2408.02854v4","updated":"2025-02-18T10:22:15Z","published":"2024-08-05T22:34:28Z","title":"Creating a Taxonomy for Retrieval Augmented Generation Applications","summary":"  In this research, we develop a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define retrieval augmented\ngeneration (RAG) applications, facilitating the adoption of this technology for\ndifferent application domains. To the best of our knowledge, no holistic RAG\napplication taxonomies have been developed so far. We employ the method foreign\nto ACL and thus contribute to the set of methods in the taxonomy creation. It\ncomprises four iterative phases designed to refine and enhance our\nunderstanding and presentation of RAG's core dimensions. We have developed a\ntotal of five meta-dimensions and sixteen dimensions to comprehensively capture\nthe concept of RAG applications. Thus, the taxonomy can be used to better\nunderstand RAG applications and to derive design knowledge for future solutions\nin specific application domains.\n","authors":["Irina Nikishina","Özge Sevgili","Mahei Manhai Li","Chris Biemann","Martin Semmann"],"pdf_url":"https://arxiv.org/pdf/2408.02854v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08672v2","updated":"2025-02-18T10:06:06Z","published":"2024-04-05T05:14:46Z","title":"Taxonomy and Analysis of Sensitive User Queries in Generative AI Search","summary":"  Although there has been a growing interest among industries in integrating\ngenerative LLMs into their services, limited experience and scarcity of\nresources act as a barrier in launching and servicing large-scale LLM-based\nservices. In this paper, we share our experiences in developing and operating\ngenerative AI models within a national-scale search engine, with a specific\nfocus on the sensitiveness of user queries. We propose a taxonomy for sensitive\nsearch queries, outline our approaches, and present a comprehensive analysis\nreport on sensitive queries from actual users. We believe that our experiences\nin launching generative AI search systems can contribute to reducing the\nbarrier in building generative LLM-based services.\n","authors":["Hwiyeol Jo","Taiwoo Park","Hyunwoo Lee","Nayoung Choi","Changbong Kim","Ohjoon Kwon","Donghyeon Jeon","Eui-Hyeon Lee","Kyoungho Shin","Sun Suk Lim","Kyungmi Kim","Jihye Lee","Sun Kim"],"pdf_url":"https://arxiv.org/pdf/2404.08672v2.pdf","comment":"NAACL2025(Findings)"},{"id":"http://arxiv.org/abs/2402.08241v2","updated":"2025-02-18T09:36:34Z","published":"2024-02-13T06:12:17Z","title":"Causal Learning for Trustworthy Recommender Systems: A Survey","summary":"  Recommender Systems (RS) have significantly advanced online content filtering\nand personalized decision-making. However, emerging vulnerabilities in RS have\ncatalyzed a paradigm shift towards Trustworthy RS (TRS). Despite substantial\nprogress on TRS, most efforts focus on data correlations while overlooking the\nfundamental causal nature of recommendations. This drawback hinders TRS from\nidentifying the root cause of trustworthiness issues, leading to limited\nfairness, robustness, and explainability. To bridge this gap, causal learning\nemerges as a class of promising methods to augment TRS. These methods, grounded\nin reliable causality, excel in mitigating various biases and noise while\noffering insightful explanations for TRS. However, there is a lack of timely\nand dedicated surveys in this vibrant area. This paper creates an overview of\nTRS from the perspective of causal learning. We begin by presenting the\nadvantages and common procedures of Causality-oriented TRS (CTRS). Then, we\nidentify potential trustworthiness challenges at each stage and link them to\nviable causal solutions, followed by a classification of CTRS methods. Finally,\nwe discuss several future directions for advancing this field.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Longbing Cao","Fang Chen","Xiuzhen Zhang","Dietmar Jannach","Charu C. Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2402.08241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11916v3","updated":"2025-02-18T09:29:49Z","published":"2025-01-21T06:43:16Z","title":"Generating with Fairness: A Modality-Diffused Counterfactual Framework\n  for Incomplete Multimodal Recommendations","summary":"  Incomplete scenario is a prevalent, practical, yet challenging setting in\nMultimodal Recommendations (MMRec), where some item modalities are missing due\nto various factors. Recently, a few efforts have sought to improve the\nrecommendation accuracy by exploring generic structures from incomplete data.\nHowever, two significant gaps persist: 1) the difficulty in accurately\ngenerating missing data due to the limited ability to capture modality\ndistributions; and 2) the critical but overlooked visibility bias, where items\nwith missing modalities are more likely to be disregarded due to the\nprioritization of items' multimodal data over user preference alignment. This\nbias raises serious concerns about the fair treatment of items. To bridge these\ntwo gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)\nframework for incomplete multimodal recommendations. MoDiCF features two key\nmodules: a novel modality-diffused data completion module and a new\ncounterfactual multimodal recommendation module. The former, equipped with a\nparticularly designed multimodal generative framework, accurately generates and\niteratively refines missing data from learned modality-specific distribution\nspaces. The latter, grounded in the causal perspective, effectively mitigates\nthe negative causal effects of visibility bias and thus assures fairness in\nrecommendations. Both modules work collaboratively to address the two\naforementioned significant gaps for generating more accurate and fair results.\nExtensive experiments on three real-world datasets demonstrate the superior\nperformance of MoDiCF in terms of both recommendation accuracy and fairness.\nThe code and processed datasets are released at\nhttps://github.com/JinLi-i/MoDiCF.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Shui Yu","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.11916v3.pdf","comment":"Accepted by WWW 2025"},{"id":"http://arxiv.org/abs/2412.01269v5","updated":"2025-02-18T09:05:29Z","published":"2024-12-02T08:35:54Z","title":"CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search","summary":"  Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines.\n","authors":["Kaixin Wu","Yixin Ji","Zeyuan Chen","Qiang Wang","Cunxiang Wang","Hong Liu","Baijun Ji","Jia Xu","Zhongyi Liu","Jinjie Gu","Yuan Zhou","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2412.01269v5.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.12634v1","updated":"2025-02-18T08:24:53Z","published":"2025-02-18T08:24:53Z","title":"Introducing Context Information in Lifelong Sequential Modeling using\n  Temporal Convolutional Networks","summary":"  The importance of lifelong sequential modeling (LSM) is growing in the realm\nof social media recommendation systems. A key component in this process is the\nattention module, which derives interest representations with respect to\ncandidate items from the sequence. Typically, attention modules function in a\npoint-wise fashion, concentrating only on the relevance of individual items in\nthe sequence to the candidate item. However, the context information in the\nneighboring items that is useful for more accurately evaluating the\nsignificance of each item has not been taken into account. In this study, we\nintroduce a novel network which employs the Temporal Convolutional Network\n(TCN) to generate context-aware representations for each item throughout the\nlifelong sequence. These improved representations are then utilized in the\nattention module to produce context-aware interest representations. Expanding\non this TCN framework, we present a enhancement module which includes multiple\nTCN layers and their respective attention modules to capture interest\nrepresentations across different context scopes. Additionally, we also\nincorporate a lightweight sub-network to create convolution filters based on\nusers' basic profile features. These personalized filters are then applied in\nthe TCN layers instead of the original global filters to produce more\nuser-specific representations. We performed experiments on both a public\ndataset and a proprietary dataset. The findings indicate that the proposed\nnetwork surpasses existing methods in terms of prediction accuracy and online\nperformance metrics.\n","authors":["Ting Guo","Zhaoyang Yang","Qinsong Zeng","Ming Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12634v1.pdf","comment":"10 pages, including 1 page of reference, 7 figures"},{"id":"http://arxiv.org/abs/2502.12586v1","updated":"2025-02-18T06:42:38Z","published":"2025-02-18T06:42:38Z","title":"G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable\n  Recommendation","summary":"  Explainable recommendation has demonstrated significant advantages in\ninforming users about the logic behind recommendations, thereby increasing\nsystem transparency, effectiveness, and trustworthiness. To provide\npersonalized and interpretable explanations, existing works often combine the\ngeneration capabilities of large language models (LLMs) with collaborative\nfiltering (CF) information. CF information extracted from the user-item\ninteraction graph captures the user behaviors and preferences, which is crucial\nfor providing informative explanations. However, due to the complexity of graph\nstructure, effectively extracting the CF information from graphs still remains\na challenge. Moreover, existing methods often struggle with the integration of\nextracted CF information with LLMs due to its implicit representation and the\nmodality gap between graph structures and natural language explanations. To\naddress these challenges, we propose G-Refer, a framework using graph\nretrieval-augmented large language models (LLMs) for explainable\nrecommendation. Specifically, we first employ a hybrid graph retrieval\nmechanism to retrieve explicit CF signals from both structural and semantic\nperspectives. The retrieved CF information is explicitly formulated as\nhuman-understandable text by the proposed graph translation and accounts for\nthe explanations generated by LLMs. To bridge the modality gap, we introduce\nknowledge pruning and retrieval-augmented fine-tuning to enhance the ability of\nLLMs to process and utilize the retrieved CF information to generate\nexplanations. Extensive experiments show that G-Refer achieves superior\nperformance compared with existing methods in both explainability and\nstability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.\n","authors":["Yuhan Li","Xinni Zhang","Linhao Luo","Heng Chang","Yuxiang Ren","Irwin King","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2502.12586v1.pdf","comment":"Accepted by WWW 2025, research track"},{"id":"http://arxiv.org/abs/2501.17191v2","updated":"2025-02-18T06:30:09Z","published":"2025-01-27T09:29:55Z","title":"Aspect-Aware Decomposition for Opinion Summarization","summary":"  Opinion summarization plays a key role in deriving meaningful insights from\nlarge-scale online reviews. To make this process more explainable and grounded,\nwe propose a modular approach guided by review aspects which separates the\ntasks of aspect identification, opinion consolidation, and meta-review\nsynthesis, enabling greater transparency and ease of inspection. We conduct\nextensive experiments across datasets representing scientific research,\nbusiness, and product domains. Results show that our method generates more\ngrounded summaries compared to strong baseline models, as verified through\nautomated and human evaluations. Additionally, our modular approach, which\nincorporates reasoning based on review aspects, produces more informative\nintermediate outputs than knowledge-agnostic decomposed prompting. These\nintermediate outputs can also effectively support humans in summarizing\nopinions from large volumes of reviews.\n","authors":["Miao Li","Jey Han Lau","Eduard Hovy","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2501.17191v2.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2312.16262v2","updated":"2025-02-18T06:06:11Z","published":"2023-12-26T08:24:24Z","title":"Adaptive In-Context Learning with Large Language Models for Bundle\n  Generation","summary":"  Most existing bundle generation approaches fall short in generating\nfixed-size bundles. Furthermore, they often neglect the underlying user intents\nreflected by the bundles in the generation process, resulting in less\nintelligible bundles. This paper addresses these limitations through the\nexploration of two interrelated tasks, i.e., personalized bundle generation and\nthe underlying intent inference, based on different user sessions. Inspired by\nthe reasoning capabilities of large language models (LLMs), we propose an\nadaptive in-context learning paradigm, which allows LLMs to draw tailored\nlessons from related sessions as demonstrations, enhancing the performance on\ntarget sessions. Specifically, we first employ retrieval augmented generation\nto identify nearest neighbor sessions, and then carefully design prompts to\nguide LLMs in executing both tasks on these neighbor sessions. To tackle\nreliability and hallucination challenges, we further introduce (1) a\nself-correction strategy promoting mutual improvements of the two tasks without\nsupervision signals and (2) an auto-feedback mechanism for adaptive supervision\nbased on the distinct mistakes made by LLMs on different neighbor sessions.\nThereby, the target session can gain customized lessons for improved\nperformance by observing the demonstrations of its neighbor sessions.\nExperiments on three real-world datasets demonstrate the effectiveness of our\nproposed method.\n","authors":["Zhu Sun","Kaidong Feng","Jie Yang","Xinghua Qu","Hui Fang","Yew-Soon Ong","Wenyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2312.16262v2.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.00077v5","updated":"2025-02-18T05:19:16Z","published":"2024-06-22T15:32:53Z","title":"Differentially Private Graph Diffusion with Applications in Personalized\n  PageRanks","summary":"  Graph diffusion, which iteratively propagates real-valued substances among\nthe graph, is used in numerous graph/network-involved applications. However,\nreleasing diffusion vectors may reveal sensitive linking information in the\ndata such as transaction information in financial network data. However,\nprotecting the privacy of graph data is challenging due to its interconnected\nnature. This work proposes a novel graph diffusion framework with edge-level\ndifferential privacy guarantees by using noisy diffusion iterates. The\nalgorithm injects Laplace noise per diffusion iteration and adopts a\ndegree-based thresholding function to mitigate the high sensitivity induced by\nlow-degree nodes. Our privacy loss analysis is based on Privacy Amplification\nby Iteration (PABI), which to our best knowledge, is the first effort that\nanalyzes PABI with Laplace noise and provides relevant applications. We also\nintroduce a novel Infinity-Wasserstein distance tracking method, which tightens\nthe analysis of privacy leakage and makes PABI more applicable in practice. We\nevaluate this framework by applying it to Personalized Pagerank computation for\nranking tasks. Experiments on real-world network data demonstrate the\nsuperiority of our method under stringent privacy conditions.\n","authors":["Rongzhe Wei","Eli Chien","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2407.00077v5.pdf","comment":"Github Code Available"},{"id":"http://arxiv.org/abs/2410.07610v3","updated":"2025-02-18T03:55:28Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $50,000\\times$ fewer multimodal data pairs to\nbridge the modalities given pre-trained unimodal encoders on ImageNet\nclassification and misinformative news caption detection. CSA surpasses the\nstate-of-the-art method to map unimodal features to multimodal features. We\nalso demonstrate the ability of CSA with modalities beyond image and text,\npaving the way for future modality pairs with limited paired multimodal data\nbut abundant unpaired unimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09628v2","updated":"2025-02-18T03:53:05Z","published":"2024-11-27T00:40:51Z","title":"Bridging AI and Science: Implications from a Large-Scale Literature\n  Analysis of AI4Science","summary":"  Artificial Intelligence has proven to be a transformative tool for advancing\nscientific research across a wide range of disciplines. However, a significant\ngap still exists between AI and scientific communities, limiting the full\npotential of AI methods in driving broad scientific discovery. Existing efforts\nin identifying and bridging this gap have often relied on qualitative\nexamination of small samples of literature, offering a limited perspective on\nthe broader AI4Science landscape. In this work, we present a large-scale\nanalysis of the AI4Science literature, starting by using large language models\nto identify scientific problems and AI methods in publications from top science\nand AI venues. Leveraging this new dataset, we quantitatively highlight key\ndisparities between AI methods and scientific problems, revealing substantial\nopportunities for deeper AI integration across scientific disciplines.\nFurthermore, we explore the potential and challenges of facilitating\ncollaboration between AI and scientific communities through the lens of link\nprediction. Our findings and tools aim to promote more impactful\ninterdisciplinary collaborations and accelerate scientific discovery through\ndeeper and broader AI integration. Our code and dataset are available at:\nhttps://github.com/charles-pyj/Bridging-AI-and-Science.\n","authors":["Yutong Xie","Yijun Pan","Hua Xu","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2412.09628v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2409.18511v4","updated":"2025-02-18T02:33:04Z","published":"2024-09-27T07:46:06Z","title":"Do We Need Domain-Specific Embedding Models? An Empirical Investigation","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns. Moreover, we find that the\nperformance of general-purpose embedding models on MTEB is not correlated with\ntheir performance on FinMTEB, indicating the need for domain-specific embedding\nbenchmarks for domain-specific embedding models. This study sheds light on\ndeveloping domain-specific embedding models in the LLM era. FinMTEB comes with\nopen-source code at https://github.com/yixuantt/FinMTEB\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.18511v4.pdf","comment":"https://github.com/yixuantt/FinMTEB, The newer version:\n  arXiv:2502.10990"},{"id":"http://arxiv.org/abs/2502.12448v1","updated":"2025-02-18T02:29:51Z","published":"2025-02-18T02:29:51Z","title":"From Principles to Applications: A Comprehensive Survey of Discrete\n  Tokenizers in Generation, Comprehension, Recommendation, and Information\n  Retrieval","summary":"  Discrete tokenizers have emerged as indispensable components in modern\nmachine learning systems, particularly within the context of autoregressive\nmodeling and large language models (LLMs). These tokenizers serve as the\ncritical interface that transforms raw, unstructured data from diverse\nmodalities into discrete tokens, enabling LLMs to operate effectively across a\nwide range of tasks. Despite their central role in generation, comprehension,\nand recommendation systems, a comprehensive survey dedicated to discrete\ntokenizers remains conspicuously absent in the literature. This paper addresses\nthis gap by providing a systematic review of the design principles,\napplications, and challenges of discrete tokenizers. We begin by dissecting the\nsub-modules of tokenizers and systematically demonstrate their internal\nmechanisms to provide a comprehensive understanding of their functionality and\ndesign. Building on this foundation, we synthesize state-of-the-art methods,\ncategorizing them into multimodal generation and comprehension tasks, and\nsemantic tokens for personalized recommendations. Furthermore, we critically\nanalyze the limitations of existing tokenizers and outline promising directions\nfor future research. By presenting a unified framework for understanding\ndiscrete tokenizers, this survey aims to guide researchers and practitioners in\naddressing open challenges and advancing the field, ultimately contributing to\nthe development of more robust and versatile AI systems.\n","authors":["Jian Jia","Jingtong Gao","Ben Xue","Junhao Wang","Qingpeng Cai","Quan Chen","Xiangyu Zhao","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2502.12448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12442v1","updated":"2025-02-18T02:24:42Z","published":"2025-02-18T02:24:42Z","title":"HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) systems often struggle with imperfect\nretrieval, as traditional retrievers focus on lexical or semantic similarity\nrather than logical relevance. To address this, we propose HopRAG, a novel RAG\nframework that augments retrieval with logical reasoning through\ngraph-structured knowledge exploration. During indexing, HopRAG constructs a\npassage graph, with text chunks as vertices and logical connections established\nvia LLM-generated pseudo-queries as edges. During retrieval, it employs a\nretrieve-reason-prune mechanism: starting with lexically or semantically\nsimilar passages, the system explores multi-hop neighbors guided by\npseudo-queries and LLM reasoning to identify truly relevant ones. Extensive\nexperiments demonstrate HopRAG's superiority, achieving 76.78\\% higher answer\naccuracy and 65.07\\% improved retrieval F1 score compared to conventional\nmethods. The repository is available at https://github.com/LIU-Hao-2002/HopRAG.\n","authors":["Hao Liu","Zhengren Wang","Xi Chen","Zhiyu Li","Feiyu Xiong","Qinhan Yu","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01845v2","updated":"2025-02-18T01:29:55Z","published":"2022-05-04T01:49:36Z","title":"Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds","summary":"  Discovering latent topics from text corpora has been studied for decades.\nMany existing topic models adopt a fully unsupervised setting, and their\ndiscovered topics may not cater to users' particular interests due to their\ninability of leveraging user guidance. Although there exist seed-guided topic\ndiscovery approaches that leverage user-provided seeds to discover\ntopic-representative terms, they are less concerned with two factors: (1) the\nexistence of out-of-vocabulary seeds and (2) the power of pre-trained language\nmodels (PLMs). In this paper, we generalize the task of seed-guided topic\ndiscovery to allow out-of-vocabulary seeds. We propose a novel framework, named\nSeeTopic, wherein the general knowledge of PLMs and the local semantics learned\nfrom the input corpus can mutually benefit each other. Experiments on three\nreal datasets from different domains demonstrate the effectiveness of SeeTopic\nin terms of topic coherence, accuracy, and diversity.\n","authors":["Yu Zhang","Yu Meng","Xuan Wang","Sheng Wang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2205.01845v2.pdf","comment":"12 pages; Accepted to NAACL 2022"},{"id":"http://arxiv.org/abs/2502.12398v1","updated":"2025-02-18T00:12:52Z","published":"2025-02-18T00:12:52Z","title":"Solving the Cold Start Problem on One's Own as an End User via\n  Preference Transfer","summary":"  We propose a new approach that enables end users to directly solve the cold\nstart problem by themselves. The cold start problem is a common issue in\nrecommender systems, and many methods have been proposed to address the problem\non the service provider's side. However, when the service provider does not\ntake action, users are left with poor recommendations and no means to improve\ntheir experience. We propose an algorithm, Pretender, that allows end users to\nproactively solve the cold start problem on their own. Pretender does not\nrequire any special support from the service provider and can be deployed\nindependently by users. We formulate the problem as minimizing the distance\nbetween the source and target distributions and optimize item selection from\nthe target service accordingly. Furthermore, we establish theoretical\nguarantees for Pretender based on a discrete quadrature problem. We conduct\nexperiments on real-world datasets to demonstrate the effectiveness of\nPretender.\n","authors":["Ryoma Sato"],"pdf_url":"https://arxiv.org/pdf/2502.12398v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.13245v1","updated":"2025-02-18T19:18:01Z","published":"2025-02-18T19:18:01Z","title":"Range Retrieval with Graph-Based Indices","summary":"  Retrieving points based on proximity in a high-dimensional vector space is a\ncrucial step in information retrieval applications. The approximate nearest\nneighbor search (ANNS) problem, which identifies the $k$ nearest neighbors for\na query (approximately, since exactly is hard), has been extensively studied in\nrecent years. However, comparatively little attention has been paid to the\nrelated problem of finding all points within a given distance of a query, the\nrange retrieval problem, despite its applications in areas such as duplicate\ndetection, plagiarism checking, and facial recognition. In this paper, we\npresent a set of algorithms for range retrieval on graph-based vector indices,\nwhich are known to achieve excellent performance on ANNS queries. Since a range\nquery may have anywhere from no matching results to thousands of matching\nresults in the database, we introduce a set of range retrieval algorithms based\non modifications of the standard graph search that adapt to terminate quickly\non queries in the former group, and to put more resources into finding results\nfor the latter group. Due to the lack of existing benchmarks for range\nretrieval, we also undertake a comprehensive study of range characteristics of\nexisting embedding datasets, and select a suitable range retrieval radius for\neight existing datasets with up to 100 million points in addition to the one\nexisting benchmark. We test our algorithms on these datasets, and find up to\n100x improvement in query throughput over a naive baseline approach, with 5-10x\nimprovement on average, and strong performance up to 100 million data points.\n","authors":["Magdalen Dobson Manohar","Taekseung Kim","Guy E. Belloch"],"pdf_url":"https://arxiv.org/pdf/2502.13245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13233v1","updated":"2025-02-18T19:12:15Z","published":"2025-02-18T19:12:15Z","title":"SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question\n  Answering?","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in general\ndomains but often struggle with tasks requiring specialized knowledge.\nConventional Retrieval-Augmented Generation (RAG) techniques typically retrieve\nexternal information from static knowledge bases, which can be outdated or\nincomplete, missing fine-grained clinical details essential for accurate\nmedical question answering. In this work, we propose SearchRAG, a novel\nframework that overcomes these limitations by leveraging real-time search\nengines. Our method employs synthetic query generation to convert complex\nmedical questions into search-engine-friendly queries and utilizes\nuncertainty-based knowledge selection to filter and incorporate the most\nrelevant and informative medical knowledge into the LLM's input. Experimental\nresults demonstrate that our method significantly improves response accuracy in\nmedical question answering tasks, particularly for complex questions requiring\ndetailed and up-to-date knowledge.\n","authors":["Yucheng Shi","Tianze Yang","Canyu Chen","Quanzheng Li","Tianming Liu","Xiang Li","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.13233v1.pdf","comment":"8 pages, three figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2302.14728v2","updated":"2025-02-18T17:48:45Z","published":"2023-02-28T16:34:55Z","title":"Semantically Consistent Person Image Generation","summary":"  We propose a data-driven approach for context-aware person image generation.\nSpecifically, we attempt to generate a person image such that the synthesized\ninstance can blend into a complex scene. In our method, the position, scale,\nand appearance of the generated person are semantically conditioned on the\nexisting persons in the scene. The proposed technique is divided into three\nsequential steps. At first, we employ a Pix2PixHD model to infer a coarse\nsemantic mask that represents the new person's spatial location, scale, and\npotential pose. Next, we use a data-centric approach to select the closest\nrepresentation from a precomputed cluster of fine semantic masks. Finally, we\nadopt a multi-scale, attention-guided architecture to transfer the appearance\nattributes from an exemplar image. The proposed strategy enables us to\nsynthesize semantically coherent realistic persons that can blend into an\nexisting scene without altering the global context. We conclude our findings\nwith relevant qualitative and quantitative evaluations.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2302.14728v2.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2024"},{"id":"http://arxiv.org/abs/2206.02717v2","updated":"2025-02-18T17:40:45Z","published":"2022-06-06T16:18:15Z","title":"Scene Aware Person Image Generation through Global Contextual\n  Conditioning","summary":"  Person image generation is an intriguing yet challenging problem. However,\nthis task becomes even more difficult under constrained situations. In this\nwork, we propose a novel pipeline to generate and insert contextually relevant\nperson images into an existing scene while preserving the global semantics.\nMore specifically, we aim to insert a person such that the location, pose, and\nscale of the person being inserted blends in with the existing persons in the\nscene. Our method uses three individual networks in a sequential pipeline. At\nfirst, we predict the potential location and the skeletal structure of the new\nperson by conditioning a Wasserstein Generative Adversarial Network (WGAN) on\nthe existing human skeletons present in the scene. Next, the predicted skeleton\nis refined through a shallow linear network to achieve higher structural\naccuracy in the generated image. Finally, the target image is generated from\nthe refined skeleton using another generative network conditioned on a given\nimage of the target person. In our experiments, we achieve high-resolution\nphoto-realistic generation results while preserving the general context of the\nscene. We conclude our paper with multiple qualitative and quantitative\nbenchmarks on the results.\n","authors":["Prasun Roy","Subhankar Ghosh","Saumik Bhattacharya","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2206.02717v2.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2022"},{"id":"http://arxiv.org/abs/2207.11718v2","updated":"2025-02-18T17:28:45Z","published":"2022-07-24T11:14:46Z","title":"TIPS: Text-Induced Pose Synthesis","summary":"  In computer vision, human pose synthesis and transfer deal with probabilistic\nimage generation of a person in a previously unseen pose from an already\navailable observation of that person. Though researchers have recently proposed\nseveral methods to achieve this task, most of these techniques derive the\ntarget pose directly from the desired target image on a specific dataset,\nmaking the underlying process challenging to apply in real-world scenarios as\nthe generation of the target image is the actual aim. In this paper, we first\npresent the shortcomings of current pose transfer algorithms and then propose a\nnovel text-based pose transfer technique to address those issues. We divide the\nproblem into three independent stages: (a) text to pose representation, (b)\npose refinement, and (c) pose rendering. To the best of our knowledge, this is\none of the first attempts to develop a text-based pose transfer framework where\nwe also introduce a new dataset DF-PASS, by adding descriptive pose annotations\nfor the images of the DeepFashion dataset. The proposed method generates\npromising results with significant qualitative and quantitative scores in our\nexperiments.\n","authors":["Prasun Roy","Subhankar Ghosh","Saumik Bhattacharya","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2207.11718v2.pdf","comment":"Accepted in The European Conference on Computer Vision (ECCV) 2022"},{"id":"http://arxiv.org/abs/2202.06777v2","updated":"2025-02-18T17:18:45Z","published":"2022-02-14T14:58:05Z","title":"Multi-scale Attention Guided Pose Transfer","summary":"  Pose transfer refers to the probabilistic image generation of a person with a\npreviously unseen novel pose from another image of that person having a\ndifferent pose. Due to potential academic and commercial applications, this\nproblem is extensively studied in recent years. Among the various approaches to\nthe problem, attention guided progressive generation is shown to produce\nstate-of-the-art results in most cases. In this paper, we present an improved\nnetwork architecture for pose transfer by introducing attention links at every\nresolution level of the encoder and decoder. By utilizing such dense\nmulti-scale attention guided approach, we are able to achieve significant\nimprovement over the existing methods both visually and analytically. We\nconclude our findings with extensive qualitative and quantitative comparisons\nagainst several existing methods on the DeepFashion dataset.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2202.06777v2.pdf","comment":"Accepted in Pattern Recognition (PR) 2023"},{"id":"http://arxiv.org/abs/2502.12623v1","updated":"2025-02-18T08:09:42Z","published":"2025-02-18T08:09:42Z","title":"DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning","summary":"  Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a\npre-alignment Transformer to enhance modality fusion prior to input into text\nLLMs, tailoring DeepResonance for multi-way instruction tuning. Our model\nachieves state-of-the-art performances across six music understanding tasks,\nhighlighting the benefits of the auxiliary modalities and the structural\nsuperiority of DeepResonance. We plan to open-source the models and the newly\nconstructed datasets.\n","authors":["Zhuoyuan Mao","Mengjie Zhao","Qiyu Wu","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2502.12623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12562v1","updated":"2025-02-18T05:57:35Z","published":"2025-02-18T05:57:35Z","title":"SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings","summary":"  Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA.\n","authors":["Weikai Lu","Hao Peng","Huiping Zhuang","Cen Chen","Ziqian Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.12562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12489v1","updated":"2025-02-18T03:18:54Z","published":"2025-02-18T03:18:54Z","title":"A Comprehensive Survey on Generative AI for Video-to-Music Generation","summary":"  The burgeoning growth of video-to-music generation can be attributed to the\nascendancy of multimodal generative models. However, there is a lack of\nliterature that comprehensively combs through the work in this field. To fill\nthis gap, this paper presents a comprehensive review of video-to-music\ngeneration using deep generative AI techniques, focusing on three key\ncomponents: visual feature extraction, music generation frameworks, and\nconditioning mechanisms. We categorize existing approaches based on their\ndesigns for each component, clarifying the roles of different strategies.\nPreceding this, we provide a fine-grained classification of video and music\nmodalities, illustrating how different categories influence the design of\ncomponents within the generation pipelines. Furthermore, we summarize available\nmultimodal datasets and evaluation metrics while highlighting ongoing\nchallenges in the field.\n","authors":["Shulei Ji","Songruoyao Wu","Zihao Wang","Shuyu Li","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18398v2","updated":"2025-02-18T21:39:25Z","published":"2024-04-29T03:19:39Z","title":"UMETTS: A Unified Framework for Emotional Text-to-Speech Synthesis with\n  Multimodal Prompts","summary":"  Emotional Text-to-Speech (E-TTS) synthesis has garnered significant attention\nin recent years due to its potential to revolutionize human-computer\ninteraction. However, current E-TTS approaches often struggle to capture the\nintricacies of human emotions, primarily relying on oversimplified emotional\nlabels or single-modality input. In this paper, we introduce the Unified\nMultimodal Prompt-Induced Emotional Text-to-Speech System (UMETTS), a novel\nframework that leverages emotional cues from multiple modalities to generate\nhighly expressive and emotionally resonant speech. The core of UMETTS consists\nof two key components: the Emotion Prompt Alignment Module (EP-Align) and the\nEmotion Embedding-Induced TTS Module (EMI-TTS). (1) EP-Align employs\ncontrastive learning to align emotional features across text, audio, and visual\nmodalities, ensuring a coherent fusion of multimodal information. (2)\nSubsequently, EMI-TTS integrates the aligned emotional embeddings with\nstate-of-the-art TTS models to synthesize speech that accurately reflects the\nintended emotions. Extensive evaluations show that UMETTS achieves significant\nimprovements in emotion accuracy and speech naturalness, outperforming\ntraditional E-TTS methods on both objective and subjective metrics.\n","authors":["Zhi-Qi Cheng","Xiang Li","Jun-Yan He","Junyao Chen","Xiaomao Fan","Xiaojiang Peng","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2404.18398v2.pdf","comment":"Accepted to ICASSP 2025, Code available at\n  https://github.com/KTTRCDL/UMETTS"},{"id":"http://arxiv.org/abs/2406.19859v3","updated":"2025-02-18T20:28:02Z","published":"2024-06-28T11:58:26Z","title":"MetaDesigner: Advancing Artistic Typography Through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis","summary":"  MetaDesigner introduces a transformative framework for artistic typography\nsynthesis, powered by Large Language Models (LLMs) and grounded in a\nuser-centric design paradigm. Its foundation is a multi-agent system comprising\nthe Pipeline, Glyph, and Texture agents, which collectively orchestrate the\ncreation of customizable WordArt, ranging from semantic enhancements to\nintricate textural elements. A central feedback mechanism leverages insights\nfrom both multimodal models and user evaluations, enabling iterative refinement\nof design parameters. Through this iterative process, MetaDesigner dynamically\nadjusts hyperparameters to align with user-defined stylistic and thematic\npreferences, consistently delivering WordArt that excels in visual quality and\ncontextual resonance. Empirical evaluations underscore the system's versatility\nand effectiveness across diverse WordArt applications, yielding outputs that\nare both aesthetically compelling and context-sensitive.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Qi He","Wangmeng Xiang","Hanyuan Chen","Jin-Peng Lan","Xianhui Lin","Kang Zhu","Bin Luo","Yifeng Geng","Xuansong Xie","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19859v3.pdf","comment":"Accepted by ICLR 2025, Project:\n  https://modelscope.cn/studios/WordArt/WordArt"},{"id":"http://arxiv.org/abs/2502.13196v1","updated":"2025-02-18T17:46:57Z","published":"2025-02-18T17:46:57Z","title":"GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting\n  View Synthesis","summary":"  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance\nFields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to\nrepresent complex geometry and appearance, GS achieves faster rendering times\nand reduced memory consumption compared to the neural network approach used in\nNeRF. However, quality assessment of GS-generated static content is not yet\nexplored in-depth. This paper describes a subjective quality assessment study\nthat aims to evaluate synthesized videos obtained with several static GS\nstate-of-the-art methods. The methods were applied to diverse visual scenes,\ncovering both 360-degree and forward-facing (FF) camera trajectories. Moreover,\nthe performance of 18 objective quality metrics was analyzed using the scores\nresulting from the subjective study, providing insights into their strengths,\nlimitations, and alignment with human perception. All videos and scores are\nmade available providing a comprehensive database that can be used as benchmark\non GS view synthesis and objective quality metrics.\n","authors":["Pedro Martin","António Rodrigues","João Ascenso","Maria Paula Queluz"],"pdf_url":"https://arxiv.org/pdf/2502.13196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1903.01192v4","updated":"2025-02-18T16:58:45Z","published":"2019-03-04T11:56:53Z","title":"STEFANN: Scene Text Editor using Font Adaptive Neural Network","summary":"  Textual information in a captured scene plays an important role in scene\ninterpretation and decision making. Though there exist methods that can\nsuccessfully detect and interpret complex text regions present in a scene, to\nthe best of our knowledge, there is no significant prior work that aims to\nmodify the textual information in an image. The ability to edit text directly\non images has several advantages including error correction, text restoration\nand image reusability. In this paper, we propose a method to modify text in an\nimage at character-level. We approach the problem in two stages. At first, the\nunobserved character (target) is generated from an observed character (source)\nbeing modified. We propose two different neural network architectures - (a)\nFANnet to achieve structural consistency with source font and (b) Colornet to\npreserve source color. Next, we replace the source character with the generated\ncharacter maintaining both geometric and visual consistency with neighboring\ncharacters. Our method works as a unified platform for modifying text in\nimages. We present the effectiveness of our method on COCO-Text and ICDAR\ndatasets both qualitatively and quantitatively.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/1903.01192v4.pdf","comment":"Accepted in The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020"}]},"2025-02-19T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.10797v4","updated":"2025-02-19T06:00:55Z","published":"2024-06-16T03:45:45Z","title":"STAR: Scale-wise Text-conditioned AutoRegressive image generation","summary":"  We introduce STAR, a text-to-image model that employs a scale-wise\nauto-regressive paradigm. Unlike VAR, which is constrained to class-conditioned\nsynthesis for images up to 256$\\times$256, STAR enables text-driven image\ngeneration up to 1024$\\times$1024 through three key designs. First, we\nintroduce a pre-trained text encoder to extract and adopt representations for\ntextual constraints, enhancing details and generalizability. Second, given the\ninherent structural correlation across different scales, we leverage 2D Rotary\nPositional Encoding (RoPE) and tweak it into a normalized version, ensuring\nconsistent interpretation of relative positions across token maps and\nstabilizing the training process. Third, we observe that simultaneously\nsampling all tokens within a single scale can disrupt inter-token\nrelationships, leading to structural instability, particularly in\nhigh-resolution generation. To address this, we propose a novel stable sampling\nmethod that incorporates causal relationships into the sampling process,\nensuring both rich details and stable structures. Compared to previous\ndiffusion models and auto-regressive models, STAR surpasses existing benchmarks\nin fidelity, text-image consistency, and aesthetic quality, requiring just\n2.21s for 1024$\\times$1024 images on A100. This highlights the potential of\nauto-regressive methods in high-quality image synthesis, offering new\ndirections for the text-to-image generation.\n","authors":["Xiaoxiao Ma","Mohan Zhou","Tao Liang","Yalong Bai","Tiejun Zhao","Biye Li","Huaian Chen","Yi Jin"],"pdf_url":"https://arxiv.org/pdf/2406.10797v4.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.12945v2","updated":"2025-02-19T02:28:34Z","published":"2025-02-18T15:29:05Z","title":"LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation","summary":"  Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies.\n","authors":["Junchen Fu","Xuri Ge","Kaiwen Zheng","Ioannis Arapakis","Xin Xin","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2502.12945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12691v2","updated":"2025-02-19T13:00:18Z","published":"2025-02-18T09:51:11Z","title":"Spherical Dense Text-to-Image Synthesis","summary":"  Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF.\n","authors":["Timon Winter","Stanislav Frolov","Brian Bernhard Moser","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2502.12691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12534v2","updated":"2025-02-19T03:34:42Z","published":"2025-02-18T04:38:38Z","title":"NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud\n  Serialization","summary":"  We present a novel approach to large-scale point cloud surface reconstruction\nby developing an efficient framework that converts an irregular point cloud\ninto a signed distance field (SDF). Our backbone builds upon recent\ntransformer-based architectures (i.e., PointTransformerV3), that serializes the\npoint cloud into a locality-preserving sequence of tokens. We efficiently\npredict the SDF value at a point by aggregating nearby tokens, where fast\napproximate neighbors can be retrieved thanks to the serialization. We\nserialize the point cloud at different levels/scales, and non-linearly\naggregate a feature to predict the SDF value. We show that aggregating across\nmultiple scales is critical to overcome the approximations introduced by the\nserialization (i.e. false negatives in the neighborhood). Our frameworks sets\nthe new state-of-the-art in terms of accuracy and efficiency (better or similar\nperformance with half the latency of the best prior method, coupled with a\nsimpler implementation), particularly on outdoor datasets where sparse-grid\nmethods have shown limited performance.\n","authors":["Zhen Li","Weiwei Sun","Shrisudhan Govindarajan","Shaobo Xia","Daniel Rebain","Kwang Moo Yi","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2502.12534v2.pdf","comment":"Project page: see https://theialab.github.io/noksr/"},{"id":"http://arxiv.org/abs/2502.12320v2","updated":"2025-02-19T09:04:01Z","published":"2025-02-17T20:46:54Z","title":"Towards Fusing Point Cloud and Visual Representations for Imitation\n  Learning","summary":"  Learning for manipulation requires using policies that have access to rich\nsensory information such as point clouds or RGB images. Point clouds\nefficiently capture geometric structures, making them essential for\nmanipulation tasks in imitation learning. In contrast, RGB images provide rich\ntexture and semantic information that can be crucial for certain tasks.\nExisting approaches for fusing both modalities assign 2D image features to\npoint clouds. However, such approaches often lose global contextual information\nfrom the original images. In this work, we propose FPV-Net, a novel imitation\nlearning method that effectively combines the strengths of both point cloud and\nRGB modalities. Our method conditions the point-cloud encoder on global and\nlocal image tokens using adaptive layer norm conditioning, leveraging the\nbeneficial properties of both modalities. Through extensive experiments on the\nchallenging RoboCasa benchmark, we demonstrate the limitations of relying on\neither modality alone and show that our method achieves state-of-the-art\nperformance across all tasks.\n","authors":["Atalay Donat","Xiaogang Jia","Xi Huang","Aleksandar Taranovic","Denis Blessing","Ge Li","Hongyi Zhou","Hanyi Zhang","Rudolf Lioutikov","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2502.12320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12293v2","updated":"2025-02-19T06:54:24Z","published":"2025-02-17T20:01:08Z","title":"Data-Efficient Limited-Angle CT Using Deep Priors and Regularization","summary":"  Reconstructing an image from its Radon transform is a fundamental computed\ntomography (CT) task arising in applications such as X-ray scans. In many\npractical scenarios, a full 180-degree scan is not feasible, or there is a\ndesire to reduce radiation exposure. In these limited-angle settings, the\nproblem becomes ill-posed, and methods designed for full-view data often leave\nsignificant artifacts. We propose a very low-data approach to reconstruct the\noriginal image from its Radon transform under severe angle limitations. Because\nthe inverse problem is ill-posed, we combine multiple regularization methods,\nincluding Total Variation, a sinogram filter, Deep Image Prior, and a\npatch-level autoencoder. We use a differentiable implementation of the Radon\ntransform, which allows us to use gradient-based techniques to solve the\ninverse problem. Our method is evaluated on a dataset from the Helsinki\nTomography Challenge 2022, where the goal is to reconstruct a binary disk from\nits limited-angle sinogram. We only use a total of 12 data points--eight for\nlearning a prior and four for hyperparameter selection--and achieve results\ncomparable to the best synthetic data-driven approaches.\n","authors":["Ilmari Vahteristo","Zhi-Song Liu","Andreas Rupp"],"pdf_url":"https://arxiv.org/pdf/2502.12293v2.pdf","comment":"12 pages, 2 reference pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.13968v1","updated":"2025-02-19T18:59:56Z","published":"2025-02-19T18:59:56Z","title":"Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting\n  Objects","summary":"  Separable 3D reconstruction of multiple objects from multi-view RGB images --\nresulting in two different 3D shapes for the two objects with a clear\nseparation between them -- remains a sparsely researched problem. It is\nchallenging due to severe mutual occlusions and ambiguities along the objects'\ninteraction boundaries. This paper investigates the setting and introduces a\nnew neuro-implicit method that can reconstruct the geometry and appearance of\ntwo objects undergoing close interactions while disjoining both in 3D, avoiding\nsurface inter-penetrations and enabling novel-view synthesis of the observed\nscene. The framework is end-to-end trainable and supervised using a novel\nalpha-blending regularisation that ensures that the two geometries are well\nseparated even under extreme occlusions. Our reconstruction method is\nmarkerless and can be applied to rigid as well as articulated objects. We\nintroduce a new dataset consisting of close interactions between a human and an\nobject and also evaluate on two scenes of humans performing martial arts. The\nexperiments confirm the effectiveness of our framework and substantial\nimprovements using 3D and novel view synthesis metrics compared to several\nexisting approaches applicable in our setting.\n","authors":["Suhas Gopal","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2502.13968v1.pdf","comment":"17 pages, 20 figures and 6 tables; International Conference on 3D\n  Vision (3DV) 2025; Project page:\n  https://vcai.mpi-inf.mpg.de/projects/separable-recon/"},{"id":"http://arxiv.org/abs/2502.13967v1","updated":"2025-02-19T18:59:44Z","published":"2025-02-19T18:59:44Z","title":"FlexTok: Resampling Images into 1D Token Sequences of Flexible Length","summary":"  Image tokenization has enabled major advances in autoregressive image\ngeneration by providing compressed, discrete representations that are more\nefficient to process than raw pixels. While traditional approaches use 2D grid\ntokenization, recent methods like TiTok have shown that 1D tokenization can\nachieve high generation quality by eliminating grid redundancies. However,\nthese methods typically use a fixed number of tokens and thus cannot adapt to\nan image's inherent complexity. We introduce FlexTok, a tokenizer that projects\n2D images into variable-length, ordered 1D token sequences. For example, a\n256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,\nhierarchically and semantically compressing its information. By training a\nrectified flow model as the decoder and using nested dropout, FlexTok produces\nplausible reconstructions regardless of the chosen token sequence length. We\nevaluate our approach in an autoregressive generation setting using a simple\nGPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to\n128 tokens, outperforming TiTok and matching state-of-the-art methods with far\nfewer tokens. We further extend the model to support to text-conditioned image\ngeneration and examine how FlexTok relates to traditional 2D tokenization. A\nkey finding is that FlexTok enables next-token prediction to describe images in\na coarse-to-fine \"visual vocabulary\", and that the number of tokens to generate\ndepends on the complexity of the generation task.\n","authors":["Roman Bachmann","Jesse Allardice","David Mizrahi","Enrico Fini","Oğuzhan Fatih Kar","Elmira Amirloo","Alaaeldin El-Nouby","Amir Zamir","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2502.13967v1.pdf","comment":"Project page at https://flextok.epfl.ch/"},{"id":"http://arxiv.org/abs/2502.13964v1","updated":"2025-02-19T18:59:17Z","published":"2025-02-19T18:59:17Z","title":"A Training-Free Framework for Precise Mobile Manipulation of Small\n  Everyday Objects","summary":"  Many everyday mobile manipulation tasks require precise interaction with\nsmall objects, such as grasping a knob to open a cabinet or pressing a light\nswitch. In this paper, we develop Servoing with Vision Models (SVM), a\nclosed-loop training-free framework that enables a mobile manipulator to tackle\nsuch precise tasks involving the manipulation of small objects. SVM employs an\nRGB-D wrist camera and uses visual servoing for control. Our novelty lies in\nthe use of state-of-the-art vision models to reliably compute 3D targets from\nthe wrist image for diverse tasks and under occlusion due to the end-effector.\nTo mitigate occlusion artifacts, we employ vision models to out-paint the\nend-effector thereby significantly enhancing target localization. We\ndemonstrate that aided by out-painting methods, open-vocabulary object\ndetectors can serve as a drop-in module to identify semantic targets (e.g.\nknobs) and point tracking methods can reliably track interaction sites\nindicated by user clicks. This training-free method obtains an 85% zero-shot\nsuccess rate on manipulating unseen objects in novel environments in the real\nworld, outperforming an open-loop control method and an imitation learning\nbaseline trained on 1000+ demonstrations by an absolute success rate of 50%.\n","authors":["Arjun Gupta","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.13964v1.pdf","comment":"Project webpage: https://arjung128.github.io/svm"},{"id":"http://arxiv.org/abs/2502.12545v2","updated":"2025-02-19T18:52:15Z","published":"2025-02-18T05:15:19Z","title":"IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with\n  360$^\\circ$ Cameras","summary":"  We present a novel 3D reconstruction pipeline for 360$^\\circ$ cameras for 3D\nmapping and rendering of indoor environments. Traditional Structure-from-Motion\n(SfM) methods may not work well in large-scale indoor scenes due to the\nprevalence of textureless and repetitive regions. To overcome these challenges,\nour approach (IM360) leverages the wide field of view of omnidirectional images\nand integrates the spherical camera model into every core component of the SfM\npipeline. In order to develop a comprehensive 3D reconstruction solution, we\nintegrate a neural implicit surface reconstruction technique to generate\nhigh-quality surfaces from sparse input data. Additionally, we utilize a\nmesh-based neural rendering approach to refine texture maps and accurately\ncapture view-dependent properties by combining diffuse and specular components.\nWe evaluate our pipeline on large-scale indoor scenes from the Matterport3D and\nStanford2D3D datasets. In practice, IM360 demonstrate superior performance in\nterms of textured mesh reconstruction over SOTA. We observe accuracy\nimprovements in terms of camera localization and registration as well as\nrendering high frequency details.\n","authors":["Dongki Jung","Jaehoon Choi","Yonghan Lee","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2502.12545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13951v1","updated":"2025-02-19T18:49:31Z","published":"2025-02-19T18:49:31Z","title":"IP-Composer: Semantic Composition of Visual Concepts","summary":"  Content creators often draw inspiration from multiple visual sources,\ncombining distinct elements to craft new compositions. Modern computational\napproaches now aim to emulate this fundamental creative process. Although\nrecent diffusion models excel at text-guided compositional synthesis, text as a\nmedium often lacks precise control over visual details. Image-based composition\napproaches can capture more nuanced features, but existing methods are\ntypically limited in the range of concepts they can capture, and require\nexpensive training procedures or specialized data. We present IP-Composer, a\nnovel training-free approach for compositional image generation that leverages\nmultiple image references simultaneously, while using natural language to\ndescribe the concept to be extracted from each image. Our method builds on\nIP-Adapter, which synthesizes novel images conditioned on an input image's CLIP\nembedding. We extend this approach to multiple visual inputs by crafting\ncomposite embeddings, stitched from the projections of multiple input images\nonto concept-specific CLIP-subspaces identified through text. Through\ncomprehensive evaluation, we show that our approach enables more precise\ncontrol over a larger range of visual concept compositions.\n","authors":["Sara Dorfman","Dana Cohen-Bar","Rinon Gal","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2502.13951v1.pdf","comment":"Project Page: https://ip-composer.github.io/IP-Composer/"},{"id":"http://arxiv.org/abs/2312.11535v3","updated":"2025-02-19T18:45:10Z","published":"2023-12-15T19:07:51Z","title":"High-Quality 3D Creation from A Single Image Using Subject-Specific\n  Knowledge Prior","summary":"  In this paper, we address the critical bottleneck in robotics caused by the\nscarcity of diverse 3D data by presenting a novel two-stage approach for\ngenerating high-quality 3D models from a single image. This method is motivated\nby the need to efficiently expand 3D asset creation, particularly for robotics\ndatasets, where the variety of object types is currently limited compared to\ngeneral image datasets. Unlike previous methods that primarily rely on general\ndiffusion priors, which often struggle to align with the reference image, our\napproach leverages subject-specific prior knowledge. By incorporating\nsubject-specific priors in both geometry and texture, we ensure precise\nalignment between the generated 3D content and the reference object.\nSpecifically, we introduce a shading mode-aware prior into the NeRF\noptimization process, enhancing the geometry and refining texture in the coarse\noutputs to achieve superior quality. Extensive experiments demonstrate that our\nmethod significantly outperforms prior approaches.\n","authors":["Nan Huang","Ting Zhang","Yuhui Yuan","Dong Chen","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.11535v3.pdf","comment":"ICRA2025, Project Page:\n  https://nnanhuang.github.io/projects/customize-it-3d/"},{"id":"http://arxiv.org/abs/2502.13945v1","updated":"2025-02-19T18:40:47Z","published":"2025-02-19T18:40:47Z","title":"GPU-Friendly Laplacian Texture Blending","summary":"  Texture and material blending is one of the leading methods for adding\nvariety to rendered virtual worlds, creating composite materials, and\ngenerating procedural content. When done naively, it can introduce either\nvisible seams or contrast loss, leading to an unnatural look not representative\nof blended textures. Earlier work proposed addressing this problem through\ncareful manual parameter tuning, lengthy per-texture statistics precomputation,\nlook-up tables, or training deep neural networks. In this work, we propose an\nalternative approach based on insights from image processing and Laplacian\npyramid blending. Our approach does not require any precomputation or increased\nmemory usage (other than the presence of a regular, non-Laplacian, texture\nmipmap chain), does not produce ghosting, preserves sharp local features, and\ncan run in real time on the GPU at the cost of a few additional lower mipmap\ntexture taps.\n","authors":["Bartlomiej Wronski"],"pdf_url":"https://arxiv.org/pdf/2502.13945v1.pdf","comment":"19 pages, 13 figures, Journal of Computer Graphics Techniques (JCGT)"},{"id":"http://arxiv.org/abs/2306.06081v5","updated":"2025-02-19T18:39:54Z","published":"2023-05-25T09:04:31Z","title":"Carefully Blending Adversarial Training, Purification, and Aggregation\n  Improves Adversarial Robustness","summary":"  In this work, we propose a novel adversarial defence mechanism for image\nclassification - CARSO - blending the paradigms of adversarial training and\nadversarial purification in a synergistic robustness-enhancing way. The method\nbuilds upon an adversarially-trained classifier, and learns to map its internal\nrepresentation associated with a potentially perturbed input onto a\ndistribution of tentative clean reconstructions. Multiple samples from such\ndistribution are classified by the same adversarially-trained model, and a\ncarefully chosen aggregation of its outputs finally constitutes the robust\nprediction of interest. Experimental evaluation by a well-established benchmark\nof strong adaptive attacks, across different image datasets, shows that CARSO\nis able to defend itself against adaptive end-to-end white-box attacks devised\nfor stochastic defences. Paying a modest clean accuracy toll, our method\nimproves by a significant margin the state-of-the-art for Cifar-10, Cifar-100,\nand TinyImageNet-200 $\\ell_\\infty$ robust classification accuracy against\nAutoAttack. Code, and instructions to obtain pre-trained models are available\nat: https://github.com/emaballarin/CARSO .\n","authors":["Emanuele Ballarin","Alessio Ansuini","Luca Bortolussi"],"pdf_url":"https://arxiv.org/pdf/2306.06081v5.pdf","comment":"25 pages, 1 figure, 16 tables"},{"id":"http://arxiv.org/abs/2502.13942v1","updated":"2025-02-19T18:35:43Z","published":"2025-02-19T18:35:43Z","title":"A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning\n  with Large Vision and Language Models","summary":"  A large-scale vision and language model that has been pretrained on massive\ndata encodes visual and linguistic prior, which makes it easier to generate\nimages and language that are more natural and realistic. Despite this, there is\nstill a significant domain gap between the modalities of vision and language,\nespecially when training data is scarce in few-shot settings, where only very\nlimited data are available for training. In order to mitigate this issue, a\nmulti-modal meta-learning framework has been proposed to bridge the gap between\ntwo frozen pretrained large vision and language models by introducing a tunable\nprompt connecting these two large models. For few-shot image captioning, the\nexisting multi-model meta-learning framework utilizes a one-step prompting\nscheme to accumulate the visual features of input images to guide the language\nmodel, which struggles to generate accurate image descriptions with only a few\ntraining samples. Instead, we propose a chain-of-thought (CoT) meta-learning\nscheme as a multi-step image captioning procedure to better imitate how humans\ndescribe images. In addition, we further propose to learn different\nmeta-parameters of the model corresponding to each CoT step in distinct\nsubspaces to avoid interference. We evaluated our method on three commonly used\nimage captioning datasets, i.e., MSCOCO, Flickr8k, and Flickr30k, under\nfew-shot settings. The results of our experiments indicate that our\nchain-of-thought subspace meta-learning strategy is superior to the baselines\nin terms of performance across different datasets measured by different\nmetrics.\n","authors":["Hao Huang","Shuaihang Yuan","Yu Hao","Congcong Wen","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2502.13942v1.pdf","comment":"11 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.13936v1","updated":"2025-02-19T18:24:02Z","published":"2025-02-19T18:24:02Z","title":"Image compositing is all you need for data augmentation","summary":"  This paper investigates the impact of various data augmentation techniques on\nthe performance of object detection models. Specifically, we explore classical\naugmentation methods, image compositing, and advanced generative models such as\nStable Diffusion XL and ControlNet. The objective of this work is to enhance\nmodel robustness and improve detection accuracy, particularly when working with\nlimited annotated data. Using YOLOv8, we fine-tune the model on a custom\ndataset consisting of commercial and military aircraft, applying different\naugmentation strategies. Our experiments show that image compositing offers the\nhighest improvement in detection performance, as measured by precision, recall,\nand mean Average Precision (mAP@0.50). Other methods, including Stable\nDiffusion XL and ControlNet, also demonstrate significant gains, highlighting\nthe potential of advanced data augmentation techniques for object detection\ntasks. The results underline the importance of dataset diversity and\naugmentation in achieving better generalization and performance in real-world\napplications. Future work will explore the integration of semi-supervised\nlearning methods and further optimizations to enhance model performance across\nlarger and more complex datasets.\n","authors":["Ang Jia Ning Shermaine","Michalis Lazarou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2502.13936v1.pdf","comment":"Accepted in VISAPP 2025"},{"id":"http://arxiv.org/abs/2411.19700v2","updated":"2025-02-19T18:21:07Z","published":"2024-11-29T13:42:10Z","title":"Explaining the Impact of Training on Vision Models via Activation\n  Clustering","summary":"  Recent developments in the field of explainable artificial intelligence (XAI)\nfor vision models investigate the information extracted by their feature\nencoder. We contribute to this effort and propose Neuro-Activated Vision\nExplanations (NAVE), which extracts the information captured by the encoder by\nclustering the feature activations of the frozen network to be explained. The\nmethod does not aim to explain the model's prediction but to answer questions\nsuch as which parts of the image are processed similarly or which information\nis kept in deeper layers. Experimentally, we leverage NAVE to show that the\ntraining dataset and the level of supervision affect which concepts are\ncaptured. In addition, our method reveals the impact of registers on vision\ntransformers (ViT) and the information saturation caused by the watermark\nClever Hans effect in the training set.\n","authors":["Ahcène Boubekki","Samuel G. Fadel","Sebastian Mair"],"pdf_url":"https://arxiv.org/pdf/2411.19700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13935v1","updated":"2025-02-19T18:18:27Z","published":"2025-02-19T18:18:27Z","title":"Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation","summary":"  Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.\n","authors":["Zeki Doruk Erden","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2502.13935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13928v1","updated":"2025-02-19T18:05:42Z","published":"2025-02-19T18:05:42Z","title":"Symmetrical Visual Contrastive Optimization: Aligning Vision-Language\n  Models with Minimal Contrastive Images","summary":"  Recent studies have shown that Large Vision-Language Models (VLMs) tend to\nneglect image content and over-rely on language-model priors, resulting in\nerrors in visually grounded tasks and hallucinations. We hypothesize that this\nissue arises because existing VLMs are not explicitly trained to generate texts\nthat are accurately grounded in fine-grained image details. To enhance visual\nfeedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive\nOptimization), a novel finetuning objective that steers the model toward\ncapturing important visual details and aligning them with corresponding text\ntokens. To further facilitate this detailed alignment, we introduce MVC, a\npaired image-text dataset built by automatically filtering and augmenting\nvisual counterfactual data to challenge the model with hard contrastive cases\ninvolving Minimal Visual Contrasts. Experiments show that our method\nconsistently improves VLM performance across diverse benchmarks covering\nvarious abilities and domains, achieving up to a 22% reduction in\nhallucinations, and significant gains in vision-centric and general tasks.\nNotably, these improvements become increasingly pronounced in benchmarks with\nhigher visual dependency. In short, S-VCO offers a significant enhancement of\nVLM's visually-dependent task performance while retaining or even improving the\nmodel's general abilities. We opensource our code at https://s-vco.github.io/\n","authors":["Shengguang Wu","Fan-Yun Sun","Kaiyue Wen","Nick Haber"],"pdf_url":"https://arxiv.org/pdf/2502.13928v1.pdf","comment":"Project Website: https://s-vco.github.io/"},{"id":"http://arxiv.org/abs/2502.13923v1","updated":"2025-02-19T18:00:14Z","published":"2025-02-19T18:00:14Z","title":"Qwen2.5-VL Technical Report","summary":"  We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.\n","authors":["Shuai Bai","Keqin Chen","Xuejing Liu","Jialin Wang","Wenbin Ge","Sibo Song","Kai Dang","Peng Wang","Shijie Wang","Jun Tang","Humen Zhong","Yuanzhi Zhu","Mingkun Yang","Zhaohai Li","Jianqiang Wan","Pengfei Wang","Wei Ding","Zheren Fu","Yiheng Xu","Jiabo Ye","Xi Zhang","Tianbao Xie","Zesen Cheng","Hang Zhang","Zhibo Yang","Haiyang Xu","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.13923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13898v1","updated":"2025-02-19T17:31:59Z","published":"2025-02-19T17:31:59Z","title":"GroundCap: A Visually Grounded Image Captioning Dataset","summary":"  Current image captioning systems lack the ability to link descriptive text to\nspecific visual elements, making their outputs difficult to verify. While\nrecent approaches offer some grounding capabilities, they cannot track object\nidentities across multiple references or ground both actions and objects\nsimultaneously. We propose a novel ID-based grounding system that enables\nconsistent object reference tracking and action-object linking, and present\nGroundCap, a dataset containing 52,016 images from 77 movies, with 344\nhuman-annotated and 52,016 automatically generated captions. Each caption is\ngrounded on detected objects (132 classes) and actions (51 classes) using a tag\nsystem that maintains object identity while linking actions to the\ncorresponding objects. Our approach features persistent object IDs for\nreference tracking, explicit action-object linking, and segmentation of\nbackground elements through K-means clustering. We propose gMETEOR, a metric\ncombining caption quality with grounding accuracy, and establish baseline\nperformance by fine-tuning Pixtral-12B. Human evaluation demonstrates our\napproach's effectiveness in producing verifiable descriptions with coherent\nobject references.\n","authors":["Daniel A. P. Oliveira","Lourenço Teodoro","David Martins de Matos"],"pdf_url":"https://arxiv.org/pdf/2502.13898v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2410.18195v2","updated":"2025-02-19T17:31:08Z","published":"2024-10-23T18:01:09Z","title":"Personalized Instance-based Navigation Toward User-Specific Objects in\n  Realistic Environments","summary":"  In the last years, the research interest in visual navigation towards objects\nin indoor environments has grown significantly. This growth can be attributed\nto the recent availability of large navigation datasets in photo-realistic\nsimulated environments, like Gibson and Matterport3D. However, the navigation\ntasks supported by these datasets are often restricted to the objects present\nin the environment at acquisition time. Also, they fail to account for the\nrealistic scenario in which the target object is a user-specific instance that\ncan be easily confused with similar objects and may be found in multiple\nlocations within the environment. To address these limitations, we propose a\nnew task denominated Personalized Instance-based Navigation (PIN), in which an\nembodied agent is tasked with locating and reaching a specific personal object\nby distinguishing it among multiple instances of the same category. The task is\naccompanied by PInNED, a dedicated new dataset composed of photo-realistic\nscenes augmented with additional 3D objects. In each episode, the target object\nis presented to the agent using two modalities: a set of visual reference\nimages on a neutral background and manually annotated textual descriptions.\nThrough comprehensive evaluations and analyses, we showcase the challenges of\nthe PIN task as well as the performance and shortcomings of currently available\nmethods designed for object-driven navigation, considering modular and\nend-to-end agents.\n","authors":["Luca Barsellotti","Roberto Bigazzi","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2410.18195v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track. Project page:\n  https://aimagelab.github.io/pin/"},{"id":"http://arxiv.org/abs/2502.13894v1","updated":"2025-02-19T17:27:47Z","published":"2025-02-19T17:27:47Z","title":"NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants","summary":"  Navigating unfamiliar environments presents significant challenges for\nhousehold robots, requiring the ability to recognize and reason about novel\ndecoration and layout. Existing reinforcement learning methods cannot be\ndirectly transferred to new environments, as they typically rely on extensive\nmapping and exploration, leading to time-consuming and inefficient. To address\nthese challenges, we try to transfer the logical knowledge and the\ngeneralization ability of pre-trained foundation models to zero-shot\nnavigation. By integrating a large vision-language model with a diffusion\nnetwork, our approach named \\mname ~constructs a visual predictor that\ncontinuously predicts the agent's potential observations in the next step which\ncan assist robots generate robust actions. Furthermore, to adapt the temporal\nproperty of navigation, we introduce temporal historical information to ensure\nthat the predicted image is aligned with the navigation scene. We then\ncarefully designed an information fusion framework that embeds the predicted\nfuture frames as guidance into goal-reaching policy to solve downstream image\nnavigation tasks. This approach enhances navigation control and generalization\nacross both simulated and real-world environments. Through extensive\nexperimentation, we demonstrate the robustness and versatility of our method,\nshowcasing its potential to improve the efficiency and effectiveness of robotic\nnavigation in diverse settings.\n","authors":["Yiran Qin","Ao Sun","Yuze Hong","Benyou Wang","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13894v1.pdf","comment":"Accepted to ICRA2025"},{"id":"http://arxiv.org/abs/2410.02098v4","updated":"2025-02-19T17:22:17Z","published":"2024-10-02T23:39:10Z","title":"EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice\n  Routing","summary":"  Diffusion transformers have been widely adopted for text-to-image synthesis.\nWhile scaling these models up to billions of parameters shows promise, the\neffectiveness of scaling beyond current sizes remains underexplored and\nchallenging. By explicitly exploiting the computational heterogeneity of image\ngenerations, we develop a new family of Mixture-of-Experts (MoE) models\n(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns\nto adaptively optimize the compute allocated to understand the input texts and\ngenerate the respective image patches, enabling heterogeneous computation\naligned with varying text-image complexities. This heterogeneity provides an\nefficient way of scaling EC-DIT up to 97 billion parameters and achieving\nsignificant improvements in training convergence, text-to-image alignment, and\noverall generation quality over dense models and conventional MoE models.\nThrough extensive ablations, we show that EC-DIT demonstrates superior\nscalability and adaptive compute allocation by recognizing varying textual\nimportance through end-to-end training. Notably, in text-to-image alignment\nevaluation, our largest models achieve a state-of-the-art GenEval score of\n71.68% and still maintain competitive inference speed with intuitive\ninterpretability.\n","authors":["Haotian Sun","Tao Lei","Bowen Zhang","Yanghao Li","Haoshuo Huang","Ruoming Pang","Bo Dai","Nan Du"],"pdf_url":"https://arxiv.org/pdf/2410.02098v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03672v2","updated":"2025-02-19T17:21:53Z","published":"2024-11-06T05:11:25Z","title":"MetaSSC: Enhancing 3D Semantic Scene Completion for Autonomous Driving\n  through Meta-Learning and Long-sequence Modeling","summary":"  Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs.\n","authors":["Yansong Qu","Zixuan Xu","Zilin Huang","Zihao Sheng","Tiantian Chen","Sikai Chen"],"pdf_url":"https://arxiv.org/pdf/2411.03672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13883v1","updated":"2025-02-19T17:08:04Z","published":"2025-02-19T17:08:04Z","title":"Multi-view Video-Pose Pretraining for Operating Room Surgical Activity\n  Recognition","summary":"  Understanding the workflow of surgical procedures in complex operating rooms\nrequires a deep understanding of the interactions between clinicians and their\nenvironment. Surgical activity recognition (SAR) is a key computer vision task\nthat detects activities or phases from multi-view camera recordings. Existing\nSAR models often fail to account for fine-grained clinician movements and\nmulti-view knowledge, or they require calibrated multi-view camera setups and\nadvanced point-cloud processing to obtain better results. In this work, we\npropose a novel calibration-free multi-view multi-modal pretraining framework\ncalled Multiview Pretraining for Video-Pose Surgical Activity Recognition\nPreViPS, which aligns 2D pose and vision embeddings across camera views. Our\nmodel follows CLIP-style dual-encoder architecture: one encoder processes\nvisual features, while the other encodes human pose embeddings. To handle the\ncontinuous 2D human pose coordinates, we introduce a tokenized discrete\nrepresentation to convert the continuous 2D pose coordinates into discrete pose\nembeddings, thereby enabling efficient integration within the dual-encoder\nframework. To bridge the gap between these two modalities, we propose several\npretraining objectives using cross- and in-modality geometric constraints\nwithin the embedding space and incorporating masked pose token prediction\nstrategy to enhance representation learning. Extensive experiments and ablation\nstudies demonstrate improvements over the strong baselines, while\ndata-efficiency experiments on two distinct operating room datasets further\nhighlight the effectiveness of our approach. We highlight the benefits of our\napproach for surgical activity recognition in both multi-view and single-view\nsettings, showcasing its practical applicability in complex surgical\nenvironments. Code will be made available at:\nhttps://github.com/CAMMA-public/PreViPS.\n","authors":["Idris Hamoud","Vinkle Srivastav","Muhammad Abdullah Jamal","Didier Mutter","Omid Mohareri","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2502.13883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13875v1","updated":"2025-02-19T16:58:42Z","published":"2025-02-19T16:58:42Z","title":"MEX: Memory-efficient Approach to Referring Multi-Object Tracking","summary":"  Referring Multi-Object Tracking (RMOT) is a relatively new concept that has\nrapidly gained traction as a promising research direction at the intersection\nof computer vision and natural language processing. Unlike traditional\nmulti-object tracking, RMOT identifies and tracks objects and incorporates\ntextual descriptions for object class names, making the approach more\nintuitive. Various techniques have been proposed to address this challenging\nproblem; however, most require the training of the entire network due to their\nend-to-end nature. Among these methods, iKUN has emerged as a particularly\npromising solution. Therefore, we further explore its pipeline and enhance its\nperformance. In this paper, we introduce a practical module dubbed\nMemory-Efficient Cross-modality -- MEX. This memory-efficient technique can be\ndirectly applied to off-the-shelf trackers like iKUN, resulting in significant\narchitectural improvements. Our method proves effective during inference on a\nsingle GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI\ndataset, which offers diverse autonomous driving scenes with relevant language\nexpressions, is particularly useful for studying this problem. Empirically, our\nmethod demonstrates effectiveness and efficiency regarding HOTA tracking\nscores, substantially improving memory allocation and processing speed.\n","authors":["Huu-Thien Tran","Phuoc-Sang Pham","Thai-Son Tran","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2502.13875v1.pdf","comment":"6 pages, 6 figures, 2024 International Conference on Advanced\n  Technologies for Communications (ATC), Signal Processing Track"},{"id":"http://arxiv.org/abs/2407.18552v3","updated":"2025-02-19T16:29:24Z","published":"2024-07-26T07:05:04Z","title":"Multimodal Emotion Recognition using Audio-Video Transformer Fusion with\n  Cross Attention","summary":"  Understanding emotions is a fundamental aspect of human communication.\nIntegrating audio and video signals offers a more comprehensive understanding\nof emotional states compared to traditional methods that rely on a single data\nsource, such as speech or facial expressions. Despite its potential, multimodal\nemotion recognition faces significant challenges, particularly in\nsynchronization, feature extraction, and fusion of diverse data sources. To\naddress these issues, this paper introduces a novel transformer-based model\nnamed Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA\nmodel employs a transformer fusion approach to effectively capture and\nsynchronize interlinked features from both audio and video inputs, thereby\nresolving synchronization problems. Additionally, the Cross Attention mechanism\nwithin AVT-CA selectively extracts and emphasizes critical features while\ndiscarding irrelevant ones from both modalities, addressing feature extraction\nand fusion challenges. Extensive experimental analysis conducted on the\nCMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the\nproposed model. The results underscore the importance of AVT-CA in developing\nprecise and reliable multimodal emotion recognition systems for practical\napplications.\n","authors":["Joe Dhanith P R","Shravan Venkatraman","Vigya Sharma","Santhosh Malarvannan","Modigari Narendra"],"pdf_url":"https://arxiv.org/pdf/2407.18552v3.pdf","comment":"38 Pages, 9 Tables, 12 Figures"},{"id":"http://arxiv.org/abs/2502.13859v1","updated":"2025-02-19T16:27:23Z","published":"2025-02-19T16:27:23Z","title":"MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object\n  Detection","summary":"  Video Camouflaged Object Detection (VCOD) is a challenging task which aims to\nidentify objects that seamlessly concealed within the background in videos. The\ndynamic properties of video enable detection of camouflaged objects through\nmotion cues or varied perspectives. Previous VCOD datasets primarily contain\nanimal objects, limiting the scope of research to wildlife scenarios. However,\nthe applications of VCOD extend beyond wildlife and have significant\nimplications in security, art, and medical fields. Addressing this problem, we\nconstruct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve\nhigh-quality annotations, we design a semi-automatic iterative annotation\npipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD\nis the largest VCOD dataset to date, introducing multiple object categories\nincluding human, animal, medical, and vehicle objects for the first time, while\nalso expanding background diversity across various environments. This expanded\nscope increases the practical applicability of the VCOD task in camouflaged\nobject detection. Alongside this dataset, we introduce a one-steam video\ncamouflage object detection model that performs both feature extraction and\ninformation fusion without additional motion feature fusion modules. Our\nframework achieves state-of-the-art results on the existing VCOD animal dataset\nand the proposed MSVCOD. The dataset and code will be made publicly available.\n","authors":["Shuyong Gao","Yu'ang Feng","Qishan Wang","Lingyi Hong","Xinyu Zhou","Liu Fei","Yan Wang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13859v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2308.10968v3","updated":"2025-02-19T16:24:49Z","published":"2023-08-21T18:26:35Z","title":"Regularization by Neural Style Transfer for MRI Field-Transfer\n  Reconstruction with Limited Data","summary":"  Recent advances in MRI reconstruction have demonstrated remarkable success\nthrough deep learning-based models. However, most existing methods rely heavily\non large-scale, task-specific datasets, making reconstruction in data-limited\nsettings a critical yet underexplored challenge. While regularization by\ndenoising (RED) leverages denoisers as priors for reconstruction, we propose\nRegularization by Neural Style Transfer (RNST), a novel framework that\nintegrates a neural style transfer (NST) engine with a denoiser to enable\nmagnetic field-transfer reconstruction. RNST generates high-field-quality\nimages from low-field inputs without requiring paired training data, leveraging\nstyle priors to address limited-data settings. Our experiment results\ndemonstrate RNST's ability to reconstruct high-quality images across diverse\nanatomical planes (axial, coronal, sagittal) and noise levels, achieving\nsuperior clarity, contrast, and structural fidelity compared to lower-field\nreferences. Crucially, RNST maintains robustness even when style and content\nimages lack exact alignment, broadening its applicability in clinical\nenvironments where precise reference matches are unavailable. By combining the\nstrengths of NST and denoising, RNST offers a scalable, data-efficient solution\nfor MRI field-transfer reconstruction, demonstrating significant potential for\nresource-limited settings.\n","authors":["Guoyao Shen","Yancheng Zhu","Mengyu Li","Ryan McNaughton","Hernan Jara","Sean B. Andersson","Chad W. Farris","Stephan Anderson","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.10968v3.pdf","comment":"27 pages, 9 figures, 3 tables, 1 algorithm chart"},{"id":"http://arxiv.org/abs/2502.13855v1","updated":"2025-02-19T16:20:14Z","published":"2025-02-19T16:20:14Z","title":"MagicGeo: Training-Free Text-Guided Geometric Diagram Generation","summary":"  Geometric diagrams are critical in conveying mathematical and scientific\nconcepts, yet traditional diagram generation methods are often manual and\nresource-intensive. While text-to-image generation has made strides in\nphotorealistic imagery, creating accurate geometric diagrams remains a\nchallenge due to the need for precise spatial relationships and the scarcity of\ngeometry-specific datasets. This paper presents MagicGeo, a training-free\nframework for generating geometric diagrams from textual descriptions. MagicGeo\nformulates the diagram generation process as a coordinate optimization problem,\nensuring geometric correctness through a formal language solver, and then\nemploys coordinate-aware generation. The framework leverages the strong\nlanguage translation capability of large language models, while formal\nmathematical solving ensures geometric correctness. We further introduce\nMagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and\ndemonstrate that MagicGeo outperforms current methods in both qualitative and\nquantitative evaluations. This work provides a scalable, accurate solution for\nautomated diagram generation, with significant implications for educational and\nacademic applications.\n","authors":["Junxiao Wang","Ting Zhang","Heng Yu","Jingdong Wang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2502.13855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08106v2","updated":"2025-02-19T16:18:04Z","published":"2025-02-12T04:07:14Z","title":"PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced\n  Text-to-Image Generation","summary":"  Diffusion models have made significant advancements in recent years. However,\ntheir performance often deteriorates when trained or fine-tuned on imbalanced\ndatasets. This degradation is largely due to the disproportionate\nrepresentation of majority and minority data in image-text pairs. In this\npaper, we propose a general fine-tuning approach, dubbed PoGDiff, to address\nthis challenge. Rather than directly minimizing the KL divergence between the\npredicted and ground-truth distributions, PoGDiff replaces the ground-truth\ndistribution with a Product of Gaussians (PoG), which is constructed by\ncombining the original ground-truth targets with the predicted distribution\nconditioned on a neighboring text embedding. Experiments on real-world datasets\ndemonstrate that our method effectively addresses the imbalance problem in\ndiffusion models, improving both generation accuracy and quality.\n","authors":["Ziyan Wang","Sizhe Wei","Xiaoming Huo","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09959v2","updated":"2025-02-19T16:11:13Z","published":"2024-12-13T08:34:46Z","title":"Efficient Dataset Distillation via Diffusion-Driven Patch Selection for\n  Improved Generalization","summary":"  Dataset distillation offers an efficient way to reduce memory and\ncomputational costs by optimizing a smaller dataset with performance comparable\nto the full-scale original. However, for large datasets and complex deep\nnetworks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space\nlimits performance, reducing its practicality. Recent approaches employ\npre-trained diffusion models to generate informative images directly, avoiding\npixel-level optimization and achieving notable results. However, these methods\noften face challenges due to distribution shifts between pre-trained models and\ntarget datasets, along with the need for multiple distillation steps across\nvarying settings. To address these issues, we propose a novel framework\northogonal to existing diffusion-based distillation methods, leveraging\ndiffusion models for selection rather than generation. Our method starts by\npredicting noise generated by the diffusion model based on input images and\ntext prompts (with or without label text), then calculates the corresponding\nloss for each pair. With the loss differences, we identify distinctive regions\nof the original images. Additionally, we perform intra-class clustering and\nranking on selected patches to maintain diversity constraints. This streamlined\nframework enables a single-step distillation process, and extensive experiments\ndemonstrate that our approach outperforms state-of-the-art methods across\nvarious metrics.\n","authors":["Xinhao Zhong","Shuoyang Sun","Xulin Gu","Zhaoyang Xu","Yaowei Wang","Jianlong Wu","Bin Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09959v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.13838v1","updated":"2025-02-19T15:59:07Z","published":"2025-02-19T15:59:07Z","title":"Generative Video Semantic Communication via Multimodal Semantic Fusion\n  with Large Model","summary":"  Despite significant advancements in traditional syntactic communications\nbased on Shannon's theory, these methods struggle to meet the requirements of\n6G immersive communications, especially under challenging transmission\nconditions. With the development of generative artificial intelligence (GenAI),\nprogress has been made in reconstructing videos using high-level semantic\ninformation. In this paper, we propose a scalable generative video semantic\ncommunication framework that extracts and transmits semantic information to\nachieve high-quality video reconstruction. Specifically, at the transmitter,\ndescription and other condition signals (e.g., first frame, sketches, etc.) are\nextracted from the source video, functioning as text and structural semantics,\nrespectively. At the receiver, the diffusion-based GenAI large models are\nutilized to fuse the semantics of the multiple modalities for reconstructing\nthe video. Simulation results demonstrate that, at an ultra-low channel\nbandwidth ratio (CBR), our scheme effectively captures semantic information to\nreconstruct videos aligned with human perception under different\nsignal-to-noise ratios. Notably, the proposed ``First Frame+Desc.\" scheme\nconsistently achieves CLIP score exceeding 0.92 at CBR = 0.0057 for SNR > 0 dB.\nThis demonstrates its robust performance even under low SNR conditions.\n","authors":["Hang Yin","Li Qiao","Yu Ma","Shuo Sun","Kan Li","Zhen Gao","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.13838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11752v2","updated":"2025-02-19T15:53:04Z","published":"2025-01-20T21:24:15Z","title":"Are generative models fair? A study of racial bias in dermatological\n  image generation","summary":"  Racial bias in medicine, such as in dermatology, presents significant ethical\nand clinical challenges. This is likely to happen because there is a\nsignificant underrepresentation of darker skin tones in training datasets for\nmachine learning models. While efforts to address bias in dermatology have\nfocused on improving dataset diversity and mitigating disparities in\ndiscriminative models, the impact of racial bias on generative models remains\nunderexplored. Generative models, such as Variational Autoencoders (VAEs), are\nincreasingly used in healthcare applications, yet their fairness across diverse\nskin tones is currently not well understood. In this study, we evaluate the\nfairness of generative models in clinical dermatology with respect to racial\nbias. For this purpose, we first train a VAE with a perceptual loss to generate\nand reconstruct high-quality skin images across different skin tones. We\nutilize the Fitzpatrick17k dataset to examine how racial bias influences the\nrepresentation and performance of these models. Our findings indicate that VAE\nperformance is, as expected, influenced by representation, i.e. increased skin\ntone representation comes with increased performance on the given skin tone.\nHowever, we also observe, even independently of representation, that the VAE\nperforms better for lighter skin tones. Additionally, the uncertainty estimates\nproduced by the VAE are ineffective in assessing the model's fairness. These\nresults highlight the need for more representative dermatological datasets, but\nalso a need for better understanding the sources of bias in such model, as well\nas improved uncertainty quantification mechanisms to detect and address racial\nbias in generative models for trustworthy healthcare technologies.\n","authors":["Miguel López-Pérez","Søren Hauberg","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2501.11752v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.00064v2","updated":"2025-02-19T15:51:43Z","published":"2024-11-25T21:47:02Z","title":"DiffGuard: Text-Based Safety Checker for Diffusion Models","summary":"  Recent advances in Diffusion Models have enabled the generation of images\nfrom text, with powerful closed-source models like DALL-E and Midjourney\nleading the way. However, open-source alternatives, such as StabilityAI's\nStable Diffusion, offer comparable capabilities. These open-source models,\nhosted on Hugging Face, come equipped with ethical filter protections designed\nto prevent the generation of explicit images. This paper reveals first their\nlimitations and then presents a novel text-based safety filter that outperforms\nexisting solutions. Our research is driven by the critical need to address the\nmisuse of AI-generated content, especially in the context of information\nwarfare. DiffGuard enhances filtering efficacy, achieving a performance that\nsurpasses the best existing filters by over 14%.\n","authors":["Massine El Khader","Elias Al Bouzidi","Abdellah Oumida","Mohammed Sbaihi","Eliott Binard","Jean-Philippe Poli","Wassila Ouerdane","Boussad Addad","Katarzyna Kapusta"],"pdf_url":"https://arxiv.org/pdf/2412.00064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13818v1","updated":"2025-02-19T15:31:13Z","published":"2025-02-19T15:31:13Z","title":"Building Age Estimation: A New Multi-Modal Benchmark Dataset and\n  Community Challenge","summary":"  Estimating the construction year of buildings is of great importance for\nsustainability. Sustainable buildings minimize energy consumption and are a key\npart of responsible and sustainable urban planning and development to\neffectively combat climate change. By using Artificial Intelligence (AI) and\nrecently proposed Transformer models, we are able to estimate the construction\nepoch of buildings from a multi-modal dataset. In this paper, we introduce a\nnew benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD),\ncontaining top-view Very High Resolution (VHR) images, Earth Observation (EO)\nmulti-spectral data from the Copernicus Sentinel-2 satellite constellation, and\nstreet-view images in many different cities in Europe, co-localized with\nrespect to the building under study and labelled with the construction epoch.\nWe assess EO generalization performance on new/ previously unseen cities that\nhave been held-out from training and appear only during inference. In this\nwork, we present the community-based data challenge we organized based on MyCD.\nThe ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we\npresent the Top-4 performing models, and the main evaluation results. During\ninference, the performance of the models using both all three input modalities\nand only the two top-view modalities, i.e. without the street-view images, is\nexamined. The evaluation results show that the models are effective and can\nachieve good performance on this difficult real-world task of estimating the\nage of buildings, even on previously unseen cities, as well as even using only\nthe two top-view modalities (i.e. VHR and Sentinel-2) during inference.\n","authors":["Nikolaos Dionelis","Nicolas Longépé","Alessandra Feliciotti","Mattia Marconcini","Devis Peressutti","Nika Oman Kadunc","JaeWan Park","Hagai Raja Sinulingga","Steve Andreas Immanuel","Ba Tran","Caroline Arnold"],"pdf_url":"https://arxiv.org/pdf/2502.13818v1.pdf","comment":"6 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.13808v1","updated":"2025-02-19T15:24:34Z","published":"2025-02-19T15:24:34Z","title":"MGFI-Net: A Multi-Grained Feature Integration Network for Enhanced\n  Medical Image Segmentation","summary":"  Medical image segmentation plays a crucial role in various clinical\napplications. A major challenge in medical image segmentation is achieving\naccurate delineation of regions of interest in the presence of noise, low\ncontrast, or complex anatomical structures. Existing segmentation models often\nneglect the integration of multi-grained information and fail to preserve edge\ndetails, which are critical for precise segmentation. To address these\nchallenges, we propose a novel image semantic segmentation model called the\nMulti-Grained Feature Integration Network (MGFI-Net). Our MGFI-Net is designed\nwith two dedicated modules to tackle these issues. First, to enhance\nsegmentation accuracy, we introduce a Multi-Grained Feature Extraction Module,\nwhich leverages hierarchical relationships between different feature scales to\nselectively focus on the most relevant information. Second, to preserve edge\ndetails, we incorporate an Edge Enhancement Module that effectively retains and\nintegrates boundary information to refine segmentation results. Extensive\nexperiments demonstrate that MGFI-Net not only outperforms state-of-the-art\nmethods in terms of segmentation accuracy but also achieves superior time\nefficiency, establishing it as a leading solution for real-time medical image\nsegmentation.\n","authors":["Yucheng Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.13808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11718v2","updated":"2025-02-19T15:19:17Z","published":"2025-02-17T12:02:23Z","title":"ChineseSimpleVQA -- \"See the World, Discover Knowledge\": A Chinese\n  Factuality Evaluation for Large Vision Language Models","summary":"  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n","authors":["Jihao Gu","Yingyao Wang","Pi Bu","Chen Wang","Ziming Wang","Tengtao Song","Donglai Wei","Jiale Yuan","Yingxiu Zhao","Yancheng He","Shilong Li","Jiaheng Liu","Meng Cao","Jun Song","Yingshui Tan","Xiang Li","Wenbo Su","Zhicheng Zheng","Xiaoyong Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11718v2.pdf","comment":"24 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.13803v1","updated":"2025-02-19T15:12:43Z","published":"2025-02-19T15:12:43Z","title":"3D Gaussian Splatting aided Localization for Large and Complex\n  Indoor-Environments","summary":"  The field of visual localization has been researched for several decades and\nhas meanwhile found many practical applications. Despite the strong progress in\nthis field, there are still challenging situations in which established methods\nfail. We present an approach to significantly improve the accuracy and\nreliability of established visual localization methods by adding rendered\nimages. In detail, we first use a modern visual SLAM approach that provides a\n3D Gaussian Splatting (3DGS) based map to create reference data. We demonstrate\nthat enriching reference data with images rendered from 3DGS at randomly\nsampled poses significantly improves the performance of both geometry-based\nvisual localization and Scene Coordinate Regression (SCR) methods. Through\ncomprehensive evaluation in a large industrial environment, we analyze the\nperformance impact of incorporating these additional rendered views.\n","authors":["Vincent Ress","Jonas Meyer","Wei Zhang","David Skuddis","Uwe Soergel","Norbert Haala"],"pdf_url":"https://arxiv.org/pdf/2502.13803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11858v2","updated":"2025-02-19T15:04:12Z","published":"2025-02-17T14:50:34Z","title":"Rethinking Audio-Visual Adversarial Vulnerability from Temporal and\n  Modality Perspectives","summary":"  While audio-visual learning equips models with a richer understanding of the\nreal world by leveraging multiple sensory modalities, this integration also\nintroduces new vulnerabilities to adversarial attacks.\n  In this paper, we present a comprehensive study of the adversarial robustness\nof audio-visual models, considering both temporal and modality-specific\nvulnerabilities. We propose two powerful adversarial attacks: 1) a temporal\ninvariance attack that exploits the inherent temporal redundancy across\nconsecutive time segments and 2) a modality misalignment attack that introduces\nincongruence between the audio and visual modalities. These attacks are\ndesigned to thoroughly assess the robustness of audio-visual models against\ndiverse threats. Furthermore, to defend against such attacks, we introduce a\nnovel audio-visual adversarial training framework. This framework addresses key\nchallenges in vanilla adversarial training by incorporating efficient\nadversarial perturbation crafting tailored to multi-modal data and an\nadversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds\ndataset demonstrate that our proposed temporal and modality-based attacks in\ndegrading model performance can achieve state-of-the-art performance, while our\nadversarial training defense largely improves the adversarial robustness as\nwell as the adversarial training efficiency.\n","authors":["Zeliang Zhang","Susan Liang","Daiki Shimada","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11858v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.13789v1","updated":"2025-02-19T14:57:51Z","published":"2025-02-19T14:57:51Z","title":"From Correctness to Comprehension: AI Agents for Personalized Error\n  Diagnosis in Education","summary":"  Large Language Models (LLMs), such as GPT-4, have demonstrated impressive\nmathematical reasoning capabilities, achieving near-perfect performance on\nbenchmarks like GSM8K. However, their application in personalized education\nremains limited due to an overemphasis on correctness over error diagnosis and\nfeedback generation. Current models fail to provide meaningful insights into\nthe causes of student mistakes, limiting their utility in educational contexts.\nTo address these challenges, we present three key contributions. First, we\nintroduce \\textbf{MathCCS} (Mathematical Classification and Constructive\nSuggestions), a multi-modal benchmark designed for systematic error analysis\nand tailored feedback. MathCCS includes real-world problems, expert-annotated\nerror categories, and longitudinal student data. Evaluations of\nstate-of-the-art models, including \\textit{Qwen2-VL}, \\textit{LLaVA-OV},\n\\textit{Claude-3.5-Sonnet} and \\textit{GPT-4o}, reveal that none achieved\nclassification accuracy above 30\\% or generated high-quality suggestions\n(average scores below 4/10), highlighting a significant gap from human-level\nperformance. Second, we develop a sequential error analysis framework that\nleverages historical data to track trends and improve diagnostic precision.\nFinally, we propose a multi-agent collaborative framework that combines a Time\nSeries Agent for historical analysis and an MLLM Agent for real-time\nrefinement, enhancing error classification and feedback generation. Together,\nthese contributions provide a robust platform for advancing personalized\neducation, bridging the gap between current AI capabilities and the demands of\nreal-world teaching.\n","authors":["Yi-Fan Zhang","Hang Li","Dingjie Song","Lichao Sun","Tianlong Xu","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.13789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13764v1","updated":"2025-02-19T14:24:25Z","published":"2025-02-19T14:24:25Z","title":"An Overall Real-Time Mechanism for Classification and Quality Evaluation\n  of Rice","summary":"  Rice is one of the most widely cultivated crops globally and has been\ndeveloped into numerous varieties. The quality of rice during cultivation is\nprimarily determined by its cultivar and characteristics. Traditionally, rice\nclassification and quality assessment rely on manual visual inspection, a\nprocess that is both time-consuming and prone to errors. However, with\nadvancements in machine vision technology, automating rice classification and\nquality evaluation based on its cultivar and characteristics has become\nincreasingly feasible, enhancing both accuracy and efficiency. This study\nproposes a real-time evaluation mechanism for comprehensive rice grain\nassessment, integrating a one-stage object detection approach, a deep\nconvolutional neural network, and traditional machine learning techniques. The\nproposed framework enables rice variety identification, grain completeness\ngrading, and grain chalkiness evaluation. The rice grain dataset used in this\nstudy comprises approximately 20,000 images from six widely cultivated rice\nvarieties in China. Experimental results demonstrate that the proposed\nmechanism achieves a mean average precision (mAP) of 99.14% in the object\ndetection task and an accuracy of 97.89% in the classification task.\nFurthermore, the framework attains an average accuracy of 97.56% in grain\ncompleteness grading within the same rice variety, contributing to an effective\nquality evaluation system.\n","authors":["Wanke Xia","Ruxin Peng","Haoqi Chu","Xinlei Zhu","Zhiyu Yang","Yaojun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.13764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13759v1","updated":"2025-02-19T14:21:25Z","published":"2025-02-19T14:21:25Z","title":"Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and\n  Human-Like Reasoning Framework","summary":"  Geolocation, the task of identifying an image's location, requires complex\nreasoning and is crucial for navigation, monitoring, and cultural preservation.\nHowever, current methods often produce coarse, imprecise, and non-interpretable\nlocalization. A major challenge lies in the quality and scale of existing\ngeolocation datasets. These datasets are typically small-scale and\nautomatically constructed, leading to noisy data and inconsistent task\ndifficulty, with images that either reveal answers too easily or lack\nsufficient clues for reliable inference. To address these challenges, we\nintroduce a comprehensive geolocation framework with three key components:\nGeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval,\nan evaluation metric, collectively designed to address critical challenges and\ndrive advancements in geolocation research. At the core of this framework is\nGeoComp (Geolocation Competition Dataset), a large-scale dataset collected from\na geolocation game platform involving 740K users over two years. It comprises\n25 million entries of metadata and 3 million geo-tagged locations spanning much\nof the globe, with each location annotated thousands to tens of thousands of\ntimes by human users. The dataset offers diverse difficulty levels for detailed\nanalysis and highlights key gaps in current models. Building on this dataset,\nwe propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning\nframework designed to enhance the reasoning capabilities of Large Vision Models\n(LVMs) in geolocation tasks. GeoCoT improves performance by integrating\ncontextual and spatial cues through a multi-step process that mimics human\ngeolocation reasoning. Finally, using the GeoEval metric, we demonstrate that\nGeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing\ninterpretability.\n","authors":["Zirui Song","Jingpu Yang","Yuan Huang","Jonathan Tonglet","Zeyu Zhang","Tao Cheng","Meng Fang","Iryna Gurevych","Xiuying Chen"],"pdf_url":"https://arxiv.org/pdf/2502.13759v1.pdf","comment":"Access dataset: https://huggingface.co/datasets/ShirohAO/tuxun"},{"id":"http://arxiv.org/abs/2502.13754v1","updated":"2025-02-19T14:16:47Z","published":"2025-02-19T14:16:47Z","title":"Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware\n  Graph Transformer for Video Captioning","summary":"  Existing video captioning methods merely provide shallow or simplistic\nrepresentations of object behaviors, resulting in superficial and ambiguous\ndescriptions. However, object behavior is dynamic and complex. To\ncomprehensively capture the essence of object behavior, we propose a dynamic\naction semantic-aware graph transformer. Firstly, a multi-scale temporal\nmodeling module is designed to flexibly learn long and short-term latent action\nfeatures. It not only acquires latent action features across time scales, but\nalso considers local latent action details, enhancing the coherence and\nsensitiveness of latent action representations. Secondly, a visual-action\nsemantic aware module is proposed to adaptively capture semantic\nrepresentations related to object behavior, enhancing the richness and\naccurateness of action representations. By harnessing the collaborative efforts\nof these two modules,we can acquire rich behavior representations to generate\nhuman-like natural descriptions. Finally, this rich behavior representations\nand object representations are used to construct a temporal objects-action\ngraph, which is fed into the graph transformer to model the complex temporal\ndependencies between objects and actions. To avoid adding complexity in the\ninference phase, the behavioral knowledge of the objects will be distilled into\na simple network through knowledge distillation. The experimental results on\nMSVD and MSR-VTT datasets demonstrate that the proposed method achieves\nsignificant performance improvements across multiple metrics.\n","authors":["Caihua Liu","Xu Li","Wenjing Xue","Wei Tang","Xia Feng"],"pdf_url":"https://arxiv.org/pdf/2502.13754v1.pdf","comment":"5 pages, 3 figures, published ICASSP"},{"id":"http://arxiv.org/abs/2410.23073v5","updated":"2025-02-19T14:13:25Z","published":"2024-10-30T14:46:35Z","title":"RSNet: A Light Framework for The Detection of Multi-scale Remote Sensing\n  Targets","summary":"  Recent advancements in synthetic aperture radar (SAR) ship detection using\ndeep learning have significantly improved accuracy and speed, yet effectively\ndetecting small objects in complex backgrounds with fewer parameters remains a\nchallenge. This letter introduces RSNet, a lightweight framework constructed to\nenhance ship detection in SAR imagery. To ensure accuracy with fewer\nparameters, we proposed Waveletpool-ContextGuided (WCG) as its backbone,\nguiding global context understanding through multi-scale wavelet features for\neffective detection in complex scenes. Additionally, Waveletpool-StarFusion\n(WSF) is introduced as the neck, employing a residual wavelet element-wise\nmultiplication structure to achieve higher dimensional nonlinear features\nwithout increasing network width. The Lightweight-Shared (LS) module is\ndesigned as detect components to achieve efficient detection through\nlightweight shared convolutional structure and multi-format compatibility.\nExperiments on the SAR Ship Detection Dataset (SSDD) and High-Resolution SAR\nImage Dataset (HRSID) demonstrate that RSNet achieves a strong balance between\nlightweight design and detection performance, surpassing many state-of-the-art\ndetectors, reaching 72.5\\% and 67.6\\% in \\textbf{\\(\\mathbf{mAP_{.50:.95}}\\)\n}respectively with 1.49M parameters. Our code will be released soon.\n","authors":["Hongyu Chen","Chengcheng Chen","Fei Wang","Yuhu Shi","Weiming Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.23073v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13740v1","updated":"2025-02-19T14:05:50Z","published":"2025-02-19T14:05:50Z","title":"Benchmarking of Different YOLO Models for CAPTCHAs Detection and\n  Classification","summary":"  This paper provides an analysis and comparison of the YOLOv5, YOLOv8 and\nYOLOv10 models for webpage CAPTCHAs detection using the datasets collected from\nthe web and darknet as well as synthetized data of webpages. The study examines\nthe nano (n), small (s), and medium (m) variants of YOLO architectures and use\nmetrics such as Precision, Recall, F1 score, mAP@50 and inference speed to\ndetermine the real-life utility. Additionally, the possibility of tuning the\ntrained model to detect new CAPTCHA patterns efficiently was examined as it is\na crucial part of real-life applications. The image slicing method was proposed\nas a way to improve the metrics of detection on oversized input images which\ncan be a common scenario in webpages analysis. Models in version nano achieved\nthe best results in terms of speed, while more complexed architectures scored\nbetter in terms of other metrics.\n","authors":["Mikołaj Wysocki","Henryk Gierszal","Piotr Tyczka","Sophia Karagiorgou","George Pantelis"],"pdf_url":"https://arxiv.org/pdf/2502.13740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08514v2","updated":"2025-02-19T14:02:50Z","published":"2025-01-15T01:52:54Z","title":"Multimodal Fake News Video Explanation Generation: Dataset, Model, and\n  Evaluation","summary":"  Although existing methods have addressed fake news video detection as a\nclassification problem, it is not clear why certain news content is identified\nas fake. Without proper explanation, end users may not be able to understand\nthe potential meaning of fake news. Therefore, we propose a novel task, Fake\nNews Video Explanation (FNVE), to generate natural language explanations that\nreveal the falseness of news videos. To this end, we first developed ONVE and\nVTSE, two new datasets to explain fake news video posts. Then, we propose a\nMultimodal Relation Graph Transformer (MRGT) model to benchmark ONVE and VTSE.\nMRGT introduces a multimodal relation graph to comprehensively represent\nmultimodal relations and then introduces a BART-based decoder to explain\ngenerations. The experimental results show that the proposed MRGT outperforms\nthe strong baselines. In addition, the human evaluation on the annotated ONVE\nand VTSE also achieves high scores in terms of adequacy rating.\n","authors":["Lizhi Chen","Zhong Qian","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13734v1","updated":"2025-02-19T14:02:00Z","published":"2025-02-19T14:02:00Z","title":"CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models","summary":"  Performing accurate confidence quantification and assessment is important for\ndeep neural networks to predict their failures, improve their performance and\nenhance their capabilities in real-world applications, for their practical\ndeployment in real life. For pixel-wise regression tasks, confidence\nquantification and assessment has not been well addressed in the literature, in\ncontrast to classification tasks like semantic segmentation. The softmax output\nlayer is not used in deep neural networks that solve pixel-wise regression\nproblems. In this paper, to address these problems, we develop, train and\nevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our\nmodel CARE computes and assigns confidence to regression output results. We\nfocus on solving regression problems as downstream tasks of an AI Foundation\nModel for Earth Observation (EO). We evaluate the proposed model CARE and\nexperimental results on data from the Copernicus Sentinel-2 satellite\nconstellation for estimating the density of buildings show that the proposed\nmethod can be successfully applied to regression problems. We also show that\nour approach outperforms other methods.\n","authors":["Nikolaos Dionelis","Jente Bosmans","Nicolas Longépé"],"pdf_url":"https://arxiv.org/pdf/2502.13734v1.pdf","comment":"5 pages, 3 figures, Submitted"},{"id":"http://arxiv.org/abs/2502.13716v1","updated":"2025-02-19T13:40:43Z","published":"2025-02-19T13:40:43Z","title":"Event-Based Video Frame Interpolation With Cross-Modal Asymmetric\n  Bidirectional Motion Fields","summary":"  Video Frame Interpolation (VFI) aims to generate intermediate video frames\nbetween consecutive input frames. Since the event cameras are bio-inspired\nsensors that only encode brightness changes with a micro-second temporal\nresolution, several works utilized the event camera to enhance the performance\nof VFI. However, existing methods estimate bidirectional inter-frame motion\nfields with only events or approximations, which can not consider the complex\nmotion in real-world scenarios. In this paper, we propose a novel event-based\nVFI framework with cross-modal asymmetric bidirectional motion field\nestimation. In detail, our EIF-BiOFNet utilizes each valuable characteristic of\nthe events and images for direct estimation of inter-frame motion fields\nwithout any approximation methods. Moreover, we develop an interactive\nattention-based frame synthesis network to efficiently leverage the\ncomplementary warping-based and synthesis-based features. Finally, we build a\nlarge-scale event-based VFI dataset, ERF-X170FPS, with a high frame rate,\nextreme motion, and dynamic textures to overcome the limitations of previous\nevent-based VFI datasets. Extensive experimental results validate that our\nmethod shows significant performance improvement over the state-of-the-art VFI\nmethods on various datasets. Our project pages are available at:\nhttps://github.com/intelpro/CBMNet\n","authors":["Taewoo Kim","Yujeong Chae","Hyun-Kurl Jang","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2502.13716v1.pdf","comment":"Accepted in CVPR2023(Highlight)"},{"id":"http://arxiv.org/abs/2502.13693v1","updated":"2025-02-19T13:05:50Z","published":"2025-02-19T13:05:50Z","title":"Medical Image Classification with KAN-Integrated Transformers and\n  Dilated Neighborhood Attention","summary":"  Convolutional networks, transformers, hybrid models, and Mamba-based\narchitectures have demonstrated strong performance across various medical image\nclassification tasks. However, these methods were primarily designed to\nclassify clean images using labeled data. In contrast, real-world clinical data\noften involve image corruptions that are unique to multi-center studies and\nstem from variations in imaging equipment across manufacturers. In this paper,\nwe introduce the Medical Vision Transformer (MedViTV2), a novel architecture\nincorporating Kolmogorov-Arnold Network (KAN) layers into the transformer\narchitecture for the first time, aiming for generalized medical image\nclassification. We have developed an efficient KAN block to reduce\ncomputational load while enhancing the accuracy of the original MedViT.\nAdditionally, to counteract the fragility of our MedViT when scaled up, we\npropose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the\nefficient fused dot-product attention kernel capable of capturing global\ncontext and expanding receptive fields to scale the model effectively and\naddressing feature collapse issues. Moreover, a hierarchical hybrid strategy is\nintroduced to stack our Local Feature Perception and Global Feature Perception\nblocks in an efficient manner, which balances local and global feature\nperceptions to boost performance. Extensive experiments on 17 medical image\nclassification datasets and 12 corrupted medical image datasets demonstrate\nthat MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments\nwith reduced computational complexity. MedViTV2 is 44\\% more computationally\nefficient than the previous version and significantly enhances accuracy,\nachieving improvements of 4.6\\% on MedMNIST, 5.8\\% on NonMNIST, and 13.4\\% on\nthe MedMNIST-C benchmark.\n","authors":["Omid Nejati Manzari","Hojat Asgariandehkordi","Taha Koleilat","Yiming Xiao","Hassan Rivaz"],"pdf_url":"https://arxiv.org/pdf/2502.13693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02643v2","updated":"2025-02-19T12:54:00Z","published":"2024-10-03T16:29:47Z","title":"Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based\n  Place Recognition","summary":"  Recent advances in robotics are driving real-world autonomy for long-term and\nlarge-scale missions, where loop closures via place recognition are vital for\nmitigating pose estimation drift. However, achieving real-time performance\nremains challenging for resource-constrained mobile robots and multi-robot\nsystems due to the computational burden of high-density sampling, which\nincreases the complexity of comparing and verifying query samples against a\ngrowing map database. Conventional methods often retain redundant information\nor miss critical data by relying on fixed sampling intervals or operating in\n3-D space instead of the descriptor feature space. To address these challenges,\nwe introduce the concept of sample space and propose a novel keyframe sampling\napproach for LiDAR-based place recognition. Our method minimizes redundancy\nwhile preserving essential information in the hyper-dimensional descriptor\nspace, supporting both learning-based and handcrafted descriptors. The proposed\napproach incorporates a sliding window optimization strategy to ensure\nefficient keyframe selection and real-time performance, enabling seamless\nintegration into robotic pipelines. In sum, our approach demonstrates robust\nperformance across diverse datasets, with the ability to adapt seamlessly from\nindoor to outdoor scenarios without parameter tuning, reducing loop closure\ndetection times and memory requirements.\n","authors":["Nikolaos Stathoulopoulos","Vidya Sumathy","Christoforos Kanellakis","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.02643v2.pdf","comment":"20 pages, 17 figures, 6 tables. Revised"},{"id":"http://arxiv.org/abs/2502.11955v2","updated":"2025-02-19T12:27:07Z","published":"2025-02-17T16:05:31Z","title":"pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM","summary":"  pySLAM is an open-source Python framework for Visual SLAM, supporting\nmonocular, stereo, and RGB-D cameras. It provides a flexible interface for\nintegrating both classical and modern local features, making it adaptable to\nvarious SLAM tasks. The framework includes different loop closure methods, a\nvolumetric reconstruction pipeline, and support for depth prediction models.\nAdditionally, it offers a suite of tools for visual odometry and SLAM\napplications. Designed for both beginners and experienced researchers, pySLAM\nencourages community contributions, fostering collaborative development in the\nfield of Visual SLAM.\n","authors":["Luigi Freda"],"pdf_url":"https://arxiv.org/pdf/2502.11955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17438v2","updated":"2025-02-19T12:16:37Z","published":"2024-02-27T11:50:44Z","title":"V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal\n  Correspondence","summary":"  Reconstructing the cortex from longitudinal magnetic resonance imaging (MRI)\nis indispensable for analyzing morphological alterations in the human brain.\nDespite the recent advancement of cortical surface reconstruction with deep\nlearning, challenges arising from longitudinal data are still persistent.\nEspecially the lack of strong spatiotemporal point correspondence between\nhighly convoluted brain surfaces hinders downstream analyses, as local\nmorphology is not directly comparable if the anatomical location is not matched\nprecisely. To address this issue, we present V2C-Long, the first dedicated deep\nlearning-based cortex reconstruction method for longitudinal MRI. V2C-Long\nexhibits strong inherent spatiotemporal correspondence across subjects and\nvisits, thereby reducing the need for surface-based post-processing. We\nestablish this correspondence directly during the reconstruction via the\ncomposition of two deep template-deformation networks and innovative\naggregation of within-subject templates in mesh space. We validate V2C-Long on\ntwo large neuroimaging studies, focusing on surface accuracy, consistency,\ngeneralization, test-retest reliability, and sensitivity. The results reveal a\nsubstantial improvement in longitudinal consistency and accuracy compared to\nexisting methods. In addition, we demonstrate stronger evidence for\nlongitudinal cortical atrophy in Alzheimer's disease than longitudinal\nFreeSurfer.\n","authors":["Fabian Bongratz","Jan Fecht","Anne-Marie Rickmann","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2402.17438v2.pdf","comment":"Imaging Neuroscience"},{"id":"http://arxiv.org/abs/2502.13637v1","updated":"2025-02-19T11:24:45Z","published":"2025-02-19T11:24:45Z","title":"Exploring Mutual Cross-Modal Attention for Context-Aware Human\n  Affordance Generation","summary":"  Human affordance learning investigates contextually relevant novel pose\nprediction such that the estimated pose represents a valid human action within\nthe scene. While the task is fundamental to machine perception and automated\ninteractive navigation agents, the exponentially large number of probable pose\nand action variations make the problem challenging and non-trivial. However,\nthe existing datasets and methods for human affordance prediction in 2D scenes\nare significantly limited in the literature. In this paper, we propose a novel\ncross-attention mechanism to encode the scene context for affordance prediction\nby mutually attending spatial feature maps from two different modalities. The\nproposed method is disentangled among individual subtasks to efficiently reduce\nthe problem complexity. First, we sample a probable location for a person\nwithin the scene using a variational autoencoder (VAE) conditioned on the\nglobal scene context encoding. Next, we predict a potential pose template from\na set of existing human pose candidates using a classifier on the local context\nencoding around the predicted location. In the subsequent steps, we use two\nVAEs to sample the scale and deformation parameters for the predicted pose\ntemplate by conditioning on the local context and template class. Our\nexperiments show significant improvements over the previous baseline of human\naffordance injection into complex 2D scenes.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2502.13637v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.13624v1","updated":"2025-02-19T11:00:34Z","published":"2025-02-19T11:00:34Z","title":"CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space\n  Models for Remote Physiological Measurement","summary":"  Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a\nnon-invasive solution for health monitoring. However, traditional\nsingle-modality approaches (RGB or Radio Frequency (RF)) face challenges in\nbalancing robustness and accuracy due to lighting variations, motion artifacts,\nand skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF\nfusion framework that leverages the complementary strengths of both modalities.\nIt introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic\nchanges in RF signals using timing differences between frames, enhancing the\nextraction of local and global features. Additionally, CardiacMamba employs a\nBidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier\nTransform (CFFT) to effectively capture and refine the frequency domain\ncharacteristics of RGB and RF signals, ultimately improving heart rate\nestimation accuracy and periodicity detection. Extensive experiments on the\nEquiPleth dataset demonstrate state-of-the-art performance, achieving marked\nimprovements in accuracy and robustness. CardiacMamba significantly mitigates\nskin tone bias, reducing performance disparities across demographic groups, and\nmaintains resilience under missing-modality scenarios. By addressing critical\nchallenges in fairness, adaptability, and precision, the framework advances\nrPPG technology toward reliable real-world deployment in healthcare. The codes\nare available at: https://github.com/WuZheng42/CardiacMamba.\n","authors":["Zheng Wu","Yiping Xie","Bo Zhao","Jiguang He","Fei Luo","Ning Deng","Zitong Yu"],"pdf_url":"https://arxiv.org/pdf/2502.13624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00692v2","updated":"2025-02-19T10:57:20Z","published":"2024-02-01T15:50:40Z","title":"A Framework for Building Point Cloud Cleaning, Plane Detection and\n  Semantic Segmentation","summary":"  This paper presents a framework to address the challenges involved in\nbuilding point cloud cleaning, plane detection, and semantic segmentation, with\nthe ultimate goal of enhancing building modeling. We focus in the cleaning\nstage on removing outliers from the acquired point cloud data by employing an\nadaptive threshold technique based on z-score measure. Following the cleaning\nprocess, we perform plane detection using the robust RANSAC paradigm. The goal\nis to carry out multiple plane segmentations, and to classify segments into\ndistinct categories, such as floors, ceilings, and walls. The resulting\nsegments can generate accurate and detailed point clouds representing the\nbuilding's architectural elements. Moreover, we address the problem of semantic\nsegmentation, which plays a vital role in the identification and classification\nof different components within the building, such as walls, windows, doors,\nroofs, and objects. Inspired by the PointNet architecture, we propose a deep\nlearning architecture for efficient semantic segmentation in buildings. The\nresults demonstrate the effectiveness of the proposed framework in handling\nbuilding modeling tasks, paving the way for improved accuracy and efficiency in\nthe field of building modelization.\n","authors":["Ilyass Abouelaziz","Youssef Mourchid"],"pdf_url":"https://arxiv.org/pdf/2402.00692v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08060v2","updated":"2025-02-19T10:49:11Z","published":"2024-11-09T20:20:36Z","title":"FuzzRisk: Online Collision Risk Estimation for Autonomous Vehicles based\n  on Depth-Aware Object Detection via Fuzzy Inference","summary":"  This paper presents a novel monitoring framework that infers the level of\ncollision risk for autonomous vehicles (AVs) based on their object detection\nperformance. The framework takes two sets of predictions from different\nalgorithms and associates their inconsistencies with the collision risk via\nfuzzy inference. The first set of predictions is obtained by retrieving\nsafety-critical 2.5D objects from a depth map, and the second set comes from\nthe ordinary AV's 3D object detector. We experimentally validate that, based on\nIntersection-over-Union (IoU) and a depth discrepancy measure, the\ninconsistencies between the two sets of predictions strongly correlate to the\nerror of the 3D object detector against ground truths. This correlation allows\nus to construct a fuzzy inference system and map the inconsistency measures to\nan AV collision risk indicator. In particular, we optimize the fuzzy inference\nsystem towards an existing offline metric that matches AV collision rates well.\nLastly, we validate our monitor's capability to produce relevant risk estimates\nwith the large-scale nuScenes dataset and demonstrate that it can safeguard an\nAV in closed-loop simulations.\n","authors":["Brian Hsuan-Cheng Liao","Yingjie Xu","Chih-Hong Cheng","Hasan Esen","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2411.08060v2.pdf","comment":"Accepted by ICRA 2025, 7 pages (IEEE double column format), 5\n  figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.05317v4","updated":"2025-02-19T10:39:58Z","published":"2024-10-05T03:47:06Z","title":"Accelerating Diffusion Transformers with Token-wise Feature Caching","summary":"  Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.\n","authors":["Chang Zou","Xuyang Liu","Ting Liu","Siteng Huang","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05317v4.pdf","comment":"ToCa is honored to be accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.13606v1","updated":"2025-02-19T10:37:04Z","published":"2025-02-19T10:37:04Z","title":"LaVCa: LLM-assisted Visual Cortex Captioning","summary":"  Understanding the property of neural populations (or voxels) in the human\nbrain can advance our comprehension of human perceptual and cognitive\nprocessing capabilities and contribute to developing brain-inspired computer\nmodels. Recent encoding models using deep neural networks (DNNs) have\nsuccessfully predicted voxel-wise activity. However, interpreting the\nproperties that explain voxel responses remains challenging because of the\nblack-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex\nCaptioning (LaVCa), a data-driven approach that uses large language models\n(LLMs) to generate natural-language captions for images to which voxels are\nselective. By applying LaVCa for image-evoked brain activity, we demonstrate\nthat LaVCa generates captions that describe voxel selectivity more accurately\nthan the previously proposed method. Furthermore, the captions generated by\nLaVCa quantitatively capture more detailed properties than the existing method\nat both the inter-voxel and intra-voxel levels. Furthermore, a more detailed\nanalysis of the voxel-specific properties generated by LaVCa reveals\nfine-grained functional differentiation within regions of interest (ROIs) in\nthe visual cortex and voxels that simultaneously represent multiple distinct\nconcepts. These findings offer profound insights into human visual\nrepresentations by assigning detailed captions throughout the visual cortex\nwhile highlighting the potential of LLM-based methods in understanding brain\nrepresentations. Please check out our webpage at\nhttps://sites.google.com/view/lavca-llm/\n","authors":["Takuya Matsuyama","Shinji Nishimoto","Yu Takagi"],"pdf_url":"https://arxiv.org/pdf/2502.13606v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2502.13593v1","updated":"2025-02-19T10:12:19Z","published":"2025-02-19T10:12:19Z","title":"Toward Robust Non-Transferable Learning: A Survey and Benchmark","summary":"  Over the past decades, researchers have primarily focused on improving the\ngeneralization abilities of models, with limited attention given to regulating\nsuch generalization. However, the ability of models to generalize to unintended\ndata (e.g., harmful or unauthorized data) can be exploited by malicious\nadversaries in unforeseen ways, potentially resulting in violations of model\nethics. Non-transferable learning (NTL), a task aimed at reshaping the\ngeneralization abilities of deep learning models, was proposed to address these\nchallenges. While numerous methods have been proposed in this field, a\ncomprehensive review of existing progress and a thorough analysis of current\nlimitations remain lacking. In this paper, we bridge this gap by presenting the\nfirst comprehensive survey on NTL and introducing NTLBench, the first benchmark\nto evaluate NTL performance and robustness within a unified framework.\nSpecifically, we first introduce the task settings, general framework, and\ncriteria of NTL, followed by a summary of NTL approaches. Furthermore, we\nemphasize the often-overlooked issue of robustness against various attacks that\ncan destroy the non-transferable mechanism established by NTL. Experiments\nconducted via NTLBench verify the limitations of existing NTL methods in\nrobustness. Finally, we discuss the practical applications of NTL, along with\nits future directions and associated challenges.\n","authors":["Ziming Hong","Yongli Xiang","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.13593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10156v2","updated":"2025-02-19T10:03:11Z","published":"2025-02-14T13:36:00Z","title":"MonoForce: Learnable Image-conditioned Physics Engine","summary":"  We propose a novel model for the prediction of robot trajectories on rough\noffroad terrain from the onboard camera images. This model enforces the laws of\nclassical mechanics through a physics-aware neural symbolic layer while\npreserving the ability to learn from large-scale data as it is end-to-end\ndifferentiable. The proposed hybrid model integrates a black-box component that\npredicts robot-terrain interaction forces with a neural-symbolic layer. This\nlayer includes a differentiable physics engine that computes the robot's\ntrajectory by querying these forces at the points of contact with the terrain.\nAs the proposed architecture comprises substantial geometrical and physics\npriors, the resulting model can also be seen as a learnable physics engine\nconditioned on real images that delivers $10^4$ trajectories per second. We\nargue and empirically demonstrate that this architecture reduces the\nsim-to-real gap and mitigates out-of-distribution sensitivity. The\ndifferentiability, in conjunction with the rapid simulation speed, makes the\nmodel well-suited for various applications including model predictive control,\ntrajectory shooting, supervised and reinforcement learning or SLAM. The codes\nand data are publicly available.\n","authors":["Ruslan Agishev","Karel Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2502.10156v2.pdf","comment":"Code: https://github.com/ctu-vras/monoforce"},{"id":"http://arxiv.org/abs/2405.15688v3","updated":"2025-02-19T10:02:25Z","published":"2024-05-24T16:27:05Z","title":"UNION: Unsupervised 3D Object Detection using Object Appearance-based\n  Pseudo-Classes","summary":"  Unsupervised 3D object detection methods have emerged to leverage vast\namounts of data without requiring manual labels for training. Recent approaches\nrely on dynamic objects for learning to detect mobile objects but penalize the\ndetections of static instances during training. Multiple rounds of\nself-training are used to add detected static instances to the set of training\ntargets; this procedure to improve performance is computationally expensive. To\naddress this, we propose the method UNION. We use spatial clustering and\nself-supervised scene flow to obtain a set of static and dynamic object\nproposals from LiDAR. Subsequently, object proposals' visual appearances are\nencoded to distinguish static objects in the foreground and background by\nselecting static instances that are visually similar to dynamic objects. As a\nresult, static and dynamic mobile objects are obtained together, and existing\ndetectors can be trained with a single training. In addition, we extend 3D\nobject discovery to detection by using object appearance-based cluster labels\nas pseudo-class labels for training object classification. We conduct extensive\nexperiments on the nuScenes dataset and increase the state-of-the-art\nperformance for unsupervised 3D object discovery, i.e. UNION more than doubles\nthe average precision to 39.5. The code is available at\ngithub.com/TedLentsch/UNION.\n","authors":["Ted Lentsch","Holger Caesar","Dariu M. Gavrila"],"pdf_url":"https://arxiv.org/pdf/2405.15688v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2312.00663v2","updated":"2025-02-19T09:52:00Z","published":"2023-12-01T15:47:04Z","title":"Generalized Robot 3D Vision-Language Model with Fast Rendering and\n  Pre-Training Vision-Language Alignment","summary":"  Deep neural network models have achieved remarkable progress in 3D scene\nunderstanding while trained in the closed-set setting and with full labels.\nHowever, the major bottleneck is that these models do not have the capacity to\nrecognize any unseen novel classes beyond the training categories in diverse\nreal-world applications. Therefore, we are in urgent need of a framework that\ncan simultaneously be applicable to both 3D point cloud segmentation and\ndetection, particularly in the circumstances where the labels are rather\nscarce. This work presents a generalized and straightforward framework for\ndealing with 3D scene understanding when the labeled scenes are quite limited.\nTo extract knowledge for novel categories from the pre-trained vision-language\nmodels, we propose a hierarchical feature-aligned pre-training and knowledge\ndistillation strategy to extract and distill meaningful information from\nlarge-scale vision-language models, which helps benefit the open-vocabulary\nscene understanding tasks. To encourage latent instance discrimination and to\nguarantee efficiency, we propose the unsupervised region-level semantic\ncontrastive learning scheme for point clouds, using confident predictions of\nthe neural network to discriminate the intermediate feature embeddings at\nmultiple stages. In the limited reconstruction case, our proposed approach,\ntermed WS3D++, ranks 1st on the large-scale ScanNet benchmark on both the task\nof semantic segmentation and instance segmentation. Extensive experiments with\nboth indoor and outdoor scenes demonstrated the effectiveness of our approach\nin both data-efficient learning and open-world few-shot learning. The code is\nmade publicly available at:\nhttps://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.\n","authors":["Kangcheng Liu","Yong-Jin Liu","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00663v2.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Manuscript Info: 17 Pages, 13 Figures, and 6 Tables"},{"id":"http://arxiv.org/abs/2312.04727v2","updated":"2025-02-19T08:52:07Z","published":"2023-12-07T22:13:37Z","title":"E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D\n  Medical Image Segmentation","summary":"  Deep neural networks have evolved as the leading approach in 3D medical image\nsegmentation due to their outstanding performance. However, the ever-increasing\nmodel size and computation cost of deep neural networks have become the primary\nbarrier to deploying them on real-world resource-limited hardware. In pursuit\nof improving performance and efficiency, we propose a 3D medical image\nsegmentation model, named Efficient to Efficient Network (E2ENet),\nincorporating two parametrically and computationally efficient designs. i.\nDynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse\ninformative multi-scale features while reducing redundancy. ii. Restricted\ndepth-shift in 3D convolution: it leverages the 3D spatial information while\nkeeping the model and computational complexity as 2D-based methods. We conduct\nextensive experiments on BTCV, AMOS-CT and Brain Tumor Segmentation Challenge,\ndemonstrating that E2ENet consistently achieves a superior trade-off between\naccuracy and efficiency than prior arts across various resource constraints.\nE2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while\nsaving over 68\\% parameter count and 29\\% FLOPs in the inference phase,\ncompared with the previous best-performing method. Our code has been made\navailable at: https://github.com/boqian333/E2ENet-Medical.\n","authors":["Boqian Wu","Qiao Xiao","Shiwei Liu","Lu Yin","Mykola Pechenizkiy","Decebal Constantin Mocanu","Maurice Van Keulen","Elena Mocanu"],"pdf_url":"https://arxiv.org/pdf/2312.04727v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2311.11821v2","updated":"2025-02-19T08:51:54Z","published":"2023-11-20T14:58:47Z","title":"Cross-View Graph Consistency Learning for Invariant Graph\n  Representations","summary":"  Graph representation learning is fundamental for analyzing graph-structured\ndata. Exploring invariant graph representations remains a challenge for most\nexisting graph representation learning methods. In this paper, we propose a\ncross-view graph consistency learning (CGCL) method that learns invariant graph\nrepresentations for link prediction. First, two complementary augmented views\nare derived from an incomplete graph structure through a coupled graph\nstructure augmentation scheme. This augmentation scheme mitigates the potential\ninformation loss that is commonly associated with various data augmentation\ntechniques involving raw graph data, such as edge perturbation, node removal,\nand attribute masking. Second, we propose a CGCL model that can learn invariant\ngraph representations. A cross-view training scheme is proposed to train the\nproposed CGCL model. This scheme attempts to maximize the consistency\ninformation between one augmented view and the graph structure reconstructed\nfrom the other augmented view. Furthermore, we offer a comprehensive\ntheoretical CGCL analysis. This paper empirically and experimentally\ndemonstrates the effectiveness of the proposed CGCL method, achieving\ncompetitive results on graph datasets in comparisons with several\nstate-of-the-art algorithms.\n","authors":["Jie Chen","Hua Mao","Wai Lok Woo","Chuanbin Liu","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2311.11821v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.13524v1","updated":"2025-02-19T08:21:59Z","published":"2025-02-19T08:21:59Z","title":"MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D\n  Medical Image Analysis","summary":"  Efficient evaluation of three-dimensional (3D) medical images is crucial for\ndiagnostic and therapeutic practices in healthcare. Recent years have seen a\nsubstantial uptake in applying deep learning and computer vision to analyse and\ninterpret medical images. Traditional approaches, such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), face significant computational\nchallenges, prompting the need for architectural advancements. Recent efforts\nhave led to the introduction of novel architectures like the ``Mamba'' model as\nalternative solutions to traditional CNNs or ViTs. The Mamba model excels in\nthe linear processing of one-dimensional data with low computational demands.\nHowever, Mamba's potential for 3D medical image analysis remains underexplored\nand could face significant computational challenges as the dimension increases.\nThis manuscript presents MobileViM, a streamlined architecture for efficient\nsegmentation of 3D medical images. In the MobileViM network, we invent a new\ndimension-independent mechanism and a dual-direction traversing approach to\nincorporate with a vision-Mamba-based framework. MobileViM also features a\ncross-scale bridging technique to improve efficiency and accuracy across\nvarious medical imaging modalities. With these enhancements, MobileViM achieves\nsegmentation speeds exceeding 90 frames per second (FPS) on a single graphics\nprocessing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster\nthan the state-of-the-art deep learning models for processing 3D images with\nthe same computational resources. In addition, experimental evaluations\ndemonstrate that MobileViM delivers superior performance, with Dice similarity\nscores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,\nATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses\nexisting models.\n","authors":["Wei Dai","Steven Wang","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.13524v1.pdf","comment":"The code is accessible through:\n  https://github.com/anthonyweidai/MobileViM_3D/"},{"id":"http://arxiv.org/abs/2403.13771v2","updated":"2025-02-19T07:56:14Z","published":"2024-03-20T17:33:02Z","title":"Interpreting Neurons in Deep Vision Networks with Language Models","summary":"  In this paper, we propose Describe-and-Dissect (DnD), a novel method to\ndescribe the roles of hidden neurons in vision networks. DnD utilizes recent\nadvancements in multimodal deep learning to produce complex natural language\ndescriptions, without the need for labeled training data or a predefined set of\nconcepts to choose from. Additionally, DnD is training-free, meaning we don't\ntrain any new models and can easily leverage more capable general purpose\nmodels in the future. We have conducted extensive qualitative and quantitative\nanalysis to show that DnD outperforms prior work by providing higher quality\nneuron descriptions. Specifically, our method on average provides the highest\nquality labels and is more than 2$\\times$ as likely to be selected as the best\nexplanation for a neuron than the best baseline. Finally, we present a use case\nproviding critical insights into land cover prediction models for\nsustainability applications. Our code and data are available at\nhttps://github.com/Trustworthy-ML-Lab/Describe-and-Dissect.\n","authors":["Nicholas Bai","Rahul A. Iyer","Tuomas Oikarinen","Akshay Kulkarni","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2403.13771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07117v2","updated":"2025-02-19T07:44:10Z","published":"2023-09-13T17:55:11Z","title":"PILOT: A Pre-Trained Model-Based Continual Learning Toolbox","summary":"  While traditional machine learning can effectively tackle a wide range of\nproblems, it primarily operates within a closed-world setting, which presents\nlimitations when dealing with streaming data. As a solution, incremental\nlearning emerges to address real-world scenarios involving new data's arrival.\nRecently, pre-training has made significant advancements and garnered the\nattention of numerous researchers. The strong performance of these pre-trained\nmodels (PTMs) presents a promising avenue for developing continual learning\nalgorithms that can effectively adapt to real-world scenarios. Consequently,\nexploring the utilization of PTMs in incremental learning has become essential.\nThis paper introduces a pre-trained model-based continual learning toolbox\nknown as PILOT. On the one hand, PILOT implements some state-of-the-art\nclass-incremental learning algorithms based on pre-trained models, such as L2P,\nDualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical\nclass-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within the\ncontext of pre-trained models to evaluate their effectiveness.\n","authors":["Hai-Long Sun","Da-Wei Zhou","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2309.07117v2.pdf","comment":"Accepted to SCIENCE CHINA Information Sciences. Code is available at\n  https://github.com/sun-hailong/LAMDA-PILOT"},{"id":"http://arxiv.org/abs/2502.13498v1","updated":"2025-02-19T07:33:10Z","published":"2025-02-19T07:33:10Z","title":"Improving Collision-Free Success Rate For Object Goal Visual Navigation\n  Via Two-Stage Training With Collision Prediction","summary":"  The object goal visual navigation is the task of navigating to a specific\ntarget object using egocentric visual observations. Recent end-to-end\nnavigation models based on deep reinforcement learning have achieved remarkable\nperformance in finding and reaching target objects. However, the collision\nproblem of these models during navigation remains unresolved, since the\ncollision is typically neglected when evaluating the success. Although\nincorporating a negative reward for collision during training appears\nstraightforward, it results in a more conservative policy, thereby limiting the\nagent's ability to reach targets. In addition, many of these models utilize\nonly RGB observations, further increasing the difficulty of collision avoidance\nwithout depth information. To address these limitations, a new concept --\ncollision-free success is introduced to evaluate the ability of navigation\nmodels to find a collision-free path towards the target object. A two-stage\ntraining method with collision prediction is proposed to improve the\ncollision-free success rate of the existing navigation models using RGB\nobservations. In the first training stage, the collision prediction module\nsupervises the agent's collision states during exploration to learn to predict\nthe possible collision. In the second stage, leveraging the trained collision\nprediction, the agent learns to navigate to the target without collision. The\nexperimental results in the AI2-THOR environment demonstrate that the proposed\nmethod greatly improves the collision-free success rate of different navigation\nmodels and outperforms other comparable collision-avoidance methods.\n","authors":["Shiwei Lian","Feitian Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01864v2","updated":"2025-02-19T07:30:17Z","published":"2025-01-03T15:29:46Z","title":"Towards Hard and Soft Shadow Removal via Dual-Branch Separation Network\n  and Vision Transformer","summary":"  Image shadow removal is a crucial task in computer vision. In real-world\nscenes, shadows alter image color and brightness, posing challenges for\nperception and texture recognition. Traditional and deep learning methods often\noverlook the distinct needs for handling hard and soft shadows, thereby lacking\ndetailed processing to specifically address each type of shadow in images.We\npropose a dual-path model that processes these shadows separately using\nspecially designed loss functions to accomplish the hard and soft shadow\nremoval. The model classifies shadow types and processes them through\nappropriate paths to produce shadow-free outputs, integrating a Vision\nTransformer with UNet++ for enhanced edge detail and feature fusion. Our model\noutperforms state-of-the-art methods and achieves 2.905 RMSE value on the ISTD\ndataset, which demonstrates greater effectiveness than typical single-path\napproaches.\n","authors":["Jiajia Liang"],"pdf_url":"https://arxiv.org/pdf/2501.01864v2.pdf","comment":"11 pages, 5 figures, IEEE International Conference on Machine\n  Learning and Cybernetics (ICMLC) 2024; Currently under review at IEEE"},{"id":"http://arxiv.org/abs/2406.18516v3","updated":"2025-02-19T07:20:13Z","published":"2024-06-26T17:40:30Z","title":"Denoising as Adaptation: Noise-Space Domain Adaptation for Image\n  Restoration","summary":"  Although learning-based image restoration methods have made significant\nprogress, they still struggle with limited generalization to real-world\nscenarios due to the substantial domain gap caused by training on synthetic\ndata. Existing methods address this issue by improving data synthesis\npipelines, estimating degradation kernels, employing deep internal learning,\nand performing domain adaptation and regularization. Previous domain adaptation\nmethods have sought to bridge the domain gap by learning domain-invariant\nknowledge in either feature or pixel space. However, these techniques often\nstruggle to extend to low-level vision tasks within a stable and compact\nframework. In this paper, we show that it is possible to perform domain\nadaptation via the noise space using diffusion models. In particular, by\nleveraging the unique property of how auxiliary conditional inputs influence\nthe multi-step denoising process, we derive a meaningful diffusion loss that\nguides the restoration model in progressively aligning both restored synthetic\nand real-world outputs with a target clean distribution. We refer to this\nmethod as denoising as adaptation. To prevent shortcuts during joint training,\nwe present crucial strategies such as channel-shuffling layer and\nresidual-swapping contrastive learning in the diffusion model. They implicitly\nblur the boundaries between conditioned synthetic and real data and prevent the\nreliance of the model on easily distinguishable features. Experimental results\non three classical image restoration tasks, namely denoising, deblurring, and\nderaining, demonstrate the effectiveness of the proposed method.\n","authors":["Kang Liao","Zongsheng Yue","Zhouxia Wang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2406.18516v3.pdf","comment":"Accepted by ICLR2025. Project Page:\n  https://kangliao929.github.io/projects/noise-da/"},{"id":"http://arxiv.org/abs/2502.13487v1","updated":"2025-02-19T07:20:07Z","published":"2025-02-19T07:20:07Z","title":"Transferring Textual Preferences to Vision-Language Understanding\n  through Model Merging","summary":"  Large vision-language models (LVLMs) perform outstandingly across various\nmultimodal tasks. However, their ability to evaluate generated content remains\nlimited, and training vision-language reward models (VLRMs) with preference\ndata is computationally expensive. This paper explores a training-free\nalternative by merging text-based reward models (RMs) with LVLMs to create\nVLRMs. Our approach shows that integrating these models leads to improved\nperformance over LVLMs' scoring and text-based RMs, offering an efficient\nmethod for incorporating textual preferences into LVLMs.\n","authors":["Chen-An Li","Tzu-Han Lin","Yun-Nung Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2502.13487v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2502.13484v1","updated":"2025-02-19T07:13:08Z","published":"2025-02-19T07:13:08Z","title":"2.5D U-Net with Depth Reduction for 3D CryoET Object Identification","summary":"  Cryo-electron tomography (cryoET) is a crucial technique for unveiling the\nstructure of protein complexes. Automatically analyzing tomograms captured by\ncryoET is an essential step toward understanding cellular structures. In this\npaper, we introduce the 4th place solution from the CZII - CryoET Object\nIdentification competition, which was organized to advance the development of\nautomated tomogram analysis techniques. Our solution adopted a heatmap-based\nkeypoint detection approach, utilizing an ensemble of two different types of\n2.5D U-Net models with depth reduction. Despite its highly unified and simple\narchitecture, our method achieved 4th place, demonstrating its effectiveness.\n","authors":["Yusuke Uchida","Takaaki Fukui"],"pdf_url":"https://arxiv.org/pdf/2502.13484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11161v2","updated":"2025-02-19T07:10:06Z","published":"2025-02-16T15:26:21Z","title":"BFA: Best-Feature-Aware Fusion for Multi-View Fine-grained Manipulation","summary":"  In real-world scenarios, multi-view cameras are typically employed for\nfine-grained manipulation tasks. Existing approaches (e.g., ACT) tend to treat\nmulti-view features equally and directly concatenate them for policy learning.\nHowever, it will introduce redundant visual information and bring higher\ncomputational costs, leading to ineffective manipulation. For a fine-grained\nmanipulation task, it tends to involve multiple stages while the most\ncontributed view for different stages is varied over time. In this paper, we\npropose a plug-and-play best-feature-aware (BFA) fusion strategy for multi-view\nmanipulation tasks, which is adaptable to various policies. Built upon the\nvisual backbone of the policy network, we design a lightweight network to\npredict the importance score of each view. Based on the predicted importance\nscores, the reweighted multi-view features are subsequently fused and input\ninto the end-to-end policy network, enabling seamless integration. Notably, our\nmethod demonstrates outstanding performance in fine-grained manipulations.\nExperimental results show that our approach outperforms multiple baselines by\n22-46% success rate on different tasks. Our work provides new insights and\ninspiration for tackling key challenges in fine-grained manipulations.\n","authors":["Zihan Lan","Weixin Mao","Haosheng Li","Le Wang","Tiancai Wang","Haoqiang Fan","Osamu Yoshie"],"pdf_url":"https://arxiv.org/pdf/2502.11161v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.08640v2","updated":"2025-02-19T06:48:30Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v2.pdf","comment":"Website: https://www.emergent-values.ai"},{"id":"http://arxiv.org/abs/2402.17237v2","updated":"2025-02-19T06:39:26Z","published":"2024-02-27T06:11:54Z","title":"MVAM: Multi-View Attention Method for Fine-grained Image-Text Matching","summary":"  Existing two-stream models, such as CLIP, encode images and text through\nindependent representations, showing good performance while ensuring retrieval\nspeed, have attracted attention from industry and academia. However, the single\nrepresentation often struggles to capture complex content fully. Such models\nmay ignore fine-grained information during matching, resulting in suboptimal\nretrieval results. To overcome this limitation and enhance the performance of\ntwo-stream models, we propose a Multi-view Attention Method (MVAM) for\nimage-text matching. This approach leverages diverse attention heads with\nunique view codes to learn multiple representations for images and text, which\nare then concatenated for matching. We also incorporate a diversity objective\nto explicitly encourage attention heads to focus on distinct aspects of the\ninput data, capturing complementary fine-grained details. This diversity\nenables the model to represent image-text pairs from multiple perspectives,\nensuring a more comprehensive understanding and alignment of critical content.\nOur method allows models to encode images and text from different perspectives\nand focus on more critical details, leading to better matching performance. Our\nexperiments on MSCOCO and Flickr30K demonstrate enhancements over existing\nmodels, and further case studies reveal that different attention heads can\nfocus on distinct content, achieving more comprehensive representations.\n","authors":["Wanqing Cui","Rui Cheng","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.17237v2.pdf","comment":"Published as a conference paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2410.09403v2","updated":"2025-02-19T06:07:47Z","published":"2024-10-12T07:16:22Z","title":"Many Heads Are Better Than One: Improved Scientific Idea Generation by A\n  LLM-Based Multi-Agent System","summary":"  The rapid advancement of scientific progress requires innovative tools that\ncan accelerate knowledge discovery. Although recent AI methods, particularly\nlarge language models (LLMs), have shown promise in tasks such as hypothesis\ngeneration and experimental design, they fall short of replicating the\ncollaborative nature of real-world scientific practices, where diverse experts\nwork together in teams to tackle complex problems. To address the limitations,\nwe propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),\ndesigned to mimic the teamwork inherent in scientific research. VirSci\norganizes a team of agents to collaboratively generate, evaluate, and refine\nresearch ideas. Through comprehensive experiments, we demonstrate that this\nmulti-agent approach outperforms the state-of-the-art method in producing novel\nscientific ideas. We further investigate the collaboration mechanisms that\ncontribute to its tendency to produce ideas with higher novelty, offering\nvaluable insights to guide future research and illuminating pathways toward\nbuilding a robust system for autonomous scientific discovery. The code is\navailable at https://github.com/open-sciencelab/Virtual-Scientists.\n","authors":["Haoyang Su","Renqi Chen","Shixiang Tang","Zhenfei Yin","Xinzhe Zheng","Jinzhe Li","Biqing Qi","Qi Wu","Hui Li","Wanli Ouyang","Philip Torr","Bowen Zhou","Nanqing Dong"],"pdf_url":"https://arxiv.org/pdf/2410.09403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02746v2","updated":"2025-02-19T05:59:59Z","published":"2024-10-03T17:56:09Z","title":"Contrastive Localized Language-Image Pre-Training","summary":"  Contrastive Language-Image Pre-training (CLIP) has been a celebrated method\nfor training vision encoders to generate image/text representations\nfacilitating various applications. Recently, CLIP has been widely adopted as\nthe vision backbone of multimodal large language models (MLLMs) to connect\nimage inputs for language interactions. The success of CLIP as a\nvision-language foundation model relies on aligning web-crawled noisy text\nannotations at image levels. Nevertheless, such criteria may become\ninsufficient for downstream tasks in need of fine-grained vision\nrepresentations, especially when region-level understanding is demanding for\nMLLMs. In this paper, we improve the localization capability of CLIP with\nseveral advances. We propose a pre-training method called Contrastive Localized\nLanguage-Image Pre-training (CLOC) by complementing CLIP with region-text\ncontrastive loss and modules. We formulate a new concept, promptable\nembeddings, of which the encoder produces image embeddings easy to transform\ninto region representations given spatial hints. To support large-scale\npre-training, we design a visually-enriched and spatially-localized captioning\nframework to effectively generate region-text pseudo-labels at scale. By\nscaling up to billions of annotated images, CLOC enables high-quality regional\nembeddings for image region recognition and retrieval tasks, and can be a\ndrop-in replacement of CLIP to enhance MLLMs, especially on referring and\ngrounding tasks.\n","authors":["Hong-You Chen","Zhengfeng Lai","Haotian Zhang","Xinze Wang","Marcin Eichner","Keen You","Meng Cao","Bowen Zhang","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2410.02746v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.13447v1","updated":"2025-02-19T05:45:56Z","published":"2025-02-19T05:45:56Z","title":"Enhancing Chest X-ray Classification through Knowledge Injection in\n  Cross-Modality Learning","summary":"  The integration of artificial intelligence in medical imaging has shown\ntremendous potential, yet the relationship between pre-trained knowledge and\nperformance in cross-modality learning remains unclear. This study investigates\nhow explicitly injecting medical knowledge into the learning process affects\nthe performance of cross-modality classification, focusing on Chest X-ray (CXR)\nimages. We introduce a novel Set Theory-based knowledge injection framework\nthat generates captions for CXR images with controllable knowledge granularity.\nUsing this framework, we fine-tune CLIP model on captions with varying levels\nof medical information. We evaluate the model's performance through zero-shot\nclassification on the CheXpert dataset, a benchmark for CXR classification. Our\nresults demonstrate that injecting fine-grained medical knowledge substantially\nimproves classification accuracy, achieving 72.5\\% compared to 49.9\\% when\nusing human-generated captions. This highlights the crucial role of\ndomain-specific knowledge in medical cross-modality learning. Furthermore, we\nexplore the influence of knowledge density and the use of domain-specific Large\nLanguage Models (LLMs) for caption generation, finding that denser knowledge\nand specialized LLMs contribute to enhanced performance. This research advances\nmedical image analysis by demonstrating the effectiveness of knowledge\ninjection for improving automated CXR classification, paving the way for more\naccurate and reliable diagnostic tools.\n","authors":["Yang Yan","Bingqing Yue","Qiaxuan Li","Man Huang","Jingyu Chen","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2502.13447v1.pdf","comment":"Accepted by ICASSP'25"},{"id":"http://arxiv.org/abs/2410.13321v3","updated":"2025-02-19T05:41:03Z","published":"2024-10-17T08:24:27Z","title":"Mitigating Hallucinations in Large Vision-Language Models via\n  Summary-Guided Decoding","summary":"  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in\ngenerating detailed and coherent responses from visual inputs. However, they\nare prone to generate hallucinations due to an over-reliance on language\npriors. To address this issue, we investigate the language priors in LVLMs and\nmake two key observations: (1) Even when predicting the tokens associated with\nimage-related part-of-speech (POS), models increasingly rely on linguistic\npriors as the token sequences grow, thereby amplifying hallucinations. (2)\nMethods that directly calibrate LVLM's output distribution to mitigate language\npriors can lead to a degradation in text quality or even exacerbate\nhallucinations. Based on these findings, we propose a novel method,\nSummary-Guided Decoding (SumGD). This method naturally encourages the model to\nfocus more on image information by reducing the text context through summaries,\nwhile controlling only the image-related POS tokens to maintain text quality.\nThrough experiments, we demonstrate that SumGD achieves state-of-the-art\nperformance on object hallucination benchmarks. Furthermore, in terms of the\ntrade-off between precision and recall, SumGD achieves Pareto optimality among\nthe existing methods. Lastly, we observe that although existing methods\nstruggle to balance the reduction of object hallucinations with maintaining\ntext quality, SumGD demonstrates robustness in handling this challenge.\n","authors":["Kyungmin Min","Minbeom Kim","Kang-il Lee","Dongryeol Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2410.13321v3.pdf","comment":"NAACL 2025 (Findings); Renamed SGD to SumGD in Summary-Guided\n  Decoding to prevent confusion with Stochastic Gradient Descent"},{"id":"http://arxiv.org/abs/2502.13440v1","updated":"2025-02-19T05:31:13Z","published":"2025-02-19T05:31:13Z","title":"Semi-supervised classification of bird vocalizations","summary":"  Changes in bird populations can indicate broader changes in ecosystems,\nmaking birds one of the most important animal groups to monitor. Combining\nmachine learning and passive acoustics enables continuous monitoring over\nextended periods without direct human involvement. However, most existing\ntechniques require extensive expert-labeled datasets for training and cannot\neasily detect time-overlapping calls in busy soundscapes. We propose a\nsemi-supervised acoustic bird detector designed to allow both the detection of\ntime-overlapping calls (when separated in frequency) and the use of few labeled\ntraining samples. The classifier is trained and evaluated on a combination of\ncommunity-recorded open-source data and long-duration soundscape recordings\nfrom Singapore. It achieves a mean F0.5 score of 0.701 across 315 classes from\n110 bird species on a hold-out test set, with an average of 11 labeled training\nsamples per class. It outperforms the state-of-the-art BirdNET classifier on a\ntest set of 103 bird species despite significantly fewer labeled training\nsamples. The detector is further tested on 144 microphone-hours of continuous\nsoundscape data. The rich soundscape in Singapore makes suppression of false\npositives a challenge on raw, continuous data streams. Nevertheless, we\ndemonstrate that achieving high precision in such environments with minimal\nlabeled training data is possible.\n","authors":["Simen Hexeberg","Mandar Chitre","Matthias Hoffmann-Kuhnt","Bing Wen Low"],"pdf_url":"https://arxiv.org/pdf/2502.13440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07856v3","updated":"2025-02-19T05:22:54Z","published":"2025-02-11T14:57:33Z","title":"MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE\n  Solvers","summary":"  In applications of diffusion models, controllable generation is of practical\nsignificance, but is also challenging. Current methods for controllable\ngeneration primarily focus on modifying the score function of diffusion models,\nwhile Mean Reverting (MR) Diffusion directly modifies the structure of the\nstochastic differential equation (SDE), making the incorporation of image\nconditions simpler and more natural. However, current training-free fast\nsamplers are not directly applicable to MR Diffusion. And thus MR Diffusion\nrequires hundreds of NFEs (number of function evaluations) to obtain\nhigh-quality samples. In this paper, we propose a new algorithm named MRS (MR\nSampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time\nSDE and the probability flow ordinary differential equation (PF-ODE) associated\nwith MR Diffusion, and derive semi-analytical solutions. The solutions consist\nof an analytical function and an integral parameterized by a neural network.\nBased on this solution, we can generate high-quality samples in fewer steps.\nOur approach does not require training and supports all mainstream\nparameterizations, including noise prediction, data prediction and velocity\nprediction. Extensive experiments demonstrate that MR Sampler maintains high\nsampling quality with a speedup of 10 to 20 times across ten different image\nrestoration tasks. Our algorithm accelerates the sampling procedure of MR\nDiffusion, making it more practical in controllable generation.\n","authors":["Ao Li","Wei Fang","Hongbo Zhao","Le Lu","Ge Yang","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07856v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2304.10701v8","updated":"2025-02-19T05:22:49Z","published":"2023-04-21T02:02:02Z","title":"GMValuator: Similarity-based Data Valuation for Generative Models","summary":"  Data valuation plays a crucial role in machine learning. Existing data\nvaluation methods have primarily focused on discriminative models, neglecting\ngenerative models that have recently gained considerable attention. A very few\nexisting attempts of data valuation method designed for deep generative models\neither concentrates on specific models or lacks robustness in their outcomes.\nMoreover, efficiency still reveals vulnerable shortcomings. To bridge the gaps,\nwe formulate the data valuation problem in generative models from a\nsimilarity-matching perspective. Specifically, we introduce Generative Model\nValuator (GMValuator), the first training-free and model-agnostic approach to\nprovide data valuation for generation tasks. It empowers efficient data\nvaluation through our innovatively similarity matching module, calibrates\nbiased contribution by incorporating image quality assessment, and attributes\ncredits to all training samples based on their contributions to the generated\nsamples. Additionally, we introduce four evaluation criteria for assessing data\nvaluation methods in generative models, aligning with principles of\nplausibility and truthfulness. GMValuator is extensively evaluated on various\ndatasets and generative architectures to demonstrate its effectiveness.\n","authors":["Jiaxi Yang","Wenglong Deng","Benlin Liu","Yangsibo Huang","James Zou","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2304.10701v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19458v2","updated":"2025-02-19T05:16:27Z","published":"2024-11-29T04:02:11Z","title":"Multiview Equivariance Improves 3D Correspondence Understanding with\n  Minimal Feature Finetuning","summary":"  Vision foundation models, particularly the ViT family, have revolutionized\nimage understanding by providing rich semantic features. However, despite their\nsuccess in 2D comprehension, their abilities on grasping 3D spatial\nrelationships are still unclear. In this work, we evaluate and enhance the 3D\nawareness of ViT-based models. We begin by systematically assessing their\nability to learn 3D equivariant features, specifically examining the\nconsistency of semantic embeddings across different viewpoints. Our findings\nindicate that improved 3D equivariance leads to better performance on various\ndownstream tasks, including pose estimation, tracking, and semantic transfer.\nBuilding on this insight, we propose a simple yet effective finetuning strategy\nbased on 3D correspondences, which significantly enhances the 3D correspondence\nunderstanding of existing vision models. Remarkably, finetuning on a single\nobject for one iteration results in substantial gains. Our code is available at\nhttps://github.com/qq456cvb/3DCorrEnhance.\n","authors":["Yang You","Yixin Li","Congyue Deng","Yue Wang","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2411.19458v2.pdf","comment":"10 pages; Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2501.14004v2","updated":"2025-02-19T05:03:35Z","published":"2025-01-23T13:07:41Z","title":"ME-CPT: Multi-Task Enhanced Cross-Temporal Point Transformer for Urban\n  3D Change Detection","summary":"  The point clouds collected by the Airborne Laser Scanning (ALS) system\nprovide accurate 3D information of urban land covers. By utilizing\nmulti-temporal ALS point clouds, semantic changes in urban area can be\ncaptured, demonstrating significant potential in urban planning, emergency\nmanagement, and infrastructure maintenance. Existing 3D change detection\nmethods struggle to efficiently extract multi-class semantic information and\nchange features, still facing the following challenges: (1) the difficulty of\naccurately modeling cross-temporal point clouds spatial relationships for\neffective change feature extraction; (2) class imbalance of change samples\nwhich hinders distinguishability of semantic features; (3) the lack of\nreal-world datasets for 3D semantic change detection. To resolve these\nchallenges, we propose the Multi-task Enhanced Cross-temporal Point Transformer\n(ME-CPT) network. ME-CPT establishes spatiotemporal correspondences between\npoint cloud across different epochs and employs attention mechanisms to jointly\nextract semantic change features, facilitating information exchange and change\ncomparison. Additionally, we incorporate a semantic segmentation task and\nthrough the multi-task training strategy, further enhance the\ndistinguishability of semantic features, reducing the impact of class imbalance\nin change types. Moreover, we release a 22.5 $km^2$ 3D semantic change\ndetection dataset, offering diverse scenes for comprehensive evaluation.\nExperiments on multiple datasets show that the proposed MT-CPT achieves\nsuperior performance compared to existing state-of-the-art methods. The source\ncode and dataset will be released upon acceptance at\nhttps://github.com/zhangluqi0209/ME-CPT.\n","authors":["Luqi Zhang","Haiping Wang","Chong Liu","Zhen Dong","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2501.14004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03280v2","updated":"2025-02-19T05:02:08Z","published":"2024-05-06T08:56:41Z","title":"Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural\n  Vision from Slow Brain Activity","summary":"  Reconstructing human dynamic vision from brain activity is a challenging task\nwith great scientific significance. Although prior video reconstruction methods\nhave made substantial progress, they still suffer from several limitations,\nincluding: (1) difficulty in simultaneously reconciling semantic (e.g.\ncategorical descriptions), structure (e.g. size and color), and consistent\nmotion information (e.g. order of frames); (2) low temporal resolution of fMRI,\nwhich poses a challenge in decoding multiple frames of video dynamics from a\nsingle fMRI frame; (3) reliance on video generation models, which introduces\nambiguity regarding whether the dynamics observed in the reconstructed videos\nare genuinely derived from fMRI data or are hallucinations from generative\nmodel. To overcome these limitations, we propose a two-stage model named\nMind-Animator. During the fMRI-to-feature stage, we decouple semantic,\nstructure, and motion features from fMRI. Specifically, we employ\nfMRI-vision-language tri-modal contrastive learning to decode semantic feature\nfrom fMRI and design a sparse causal attention mechanism for decoding\nmulti-frame video motion features through a next-frame-prediction task. In the\nfeature-to-video stage, these features are integrated into videos using an\ninflated Stable Diffusion, effectively eliminating external video data\ninterference. Extensive experiments on multiple video-fMRI datasets demonstrate\nthat our model achieves state-of-the-art performance. Comprehensive\nvisualization analyses further elucidate the interpretability of our model from\na neurobiological perspective. Project page:\nhttps://mind-animator-design.github.io/.\n","authors":["Yizhuo Lu","Changde Du","Chong Wang","Xuanliu Zhu","Liuyun Jiang","Xujin Li","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2405.03280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01814v2","updated":"2025-02-19T04:45:40Z","published":"2025-02-03T20:45:19Z","title":"PolyhedronNet: Representation Learning for Polyhedra with\n  Surface-attributed Graph","summary":"  Ubiquitous geometric objects can be precisely and efficiently represented as\npolyhedra. The transformation of a polyhedron into a vector, known as polyhedra\nrepresentation learning, is crucial for manipulating these shapes with\nmathematical and statistical tools for tasks like classification, clustering,\nand generation. Recent years have witnessed significant strides in this domain,\nyet most efforts focus on the vertex sequence of a polyhedron, neglecting the\ncomplex surface modeling crucial in real-world polyhedral objects. This study\nproposes \\textbf{PolyhedronNet}, a general framework tailored for learning\nrepresentations of 3D polyhedral objects. We propose the concept of the\nsurface-attributed graph to seamlessly model the vertices, edges, faces, and\ntheir geometric interrelationships within a polyhedron. To effectively learn\nthe representation of the entire surface-attributed graph, we first propose to\nbreak it down into local rigid representations to effectively learn each local\nregion's relative positions against the remaining regions without geometric\ninformation loss. Subsequently, we propose PolyhedronGNN to hierarchically\naggregate the local rigid representation via intra-face and inter-face\ngeometric message passing modules, to obtain a global representation that\nminimizes information loss while maintaining rotation and translation\ninvariance. Our experimental evaluations on four distinct datasets,\nencompassing both classification and retrieval tasks, substantiate\nPolyhedronNet's efficacy in capturing comprehensive and informative\nrepresentations of 3D polyhedral objects. Code and data are available at\n{https://github.com/dyu62/3D_polyhedron}.\n","authors":["Dazhou Yu","Genpei Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.01814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04518v4","updated":"2025-02-19T04:28:48Z","published":"2024-04-06T06:18:11Z","title":"MedIAnomaly: A comparative study of anomaly detection in medical images","summary":"  Anomaly detection (AD) aims at detecting abnormal samples that deviate from\nthe expected normal patterns. Generally, it can be trained merely on normal\ndata, without a requirement for abnormal samples, and thereby plays an\nimportant role in rare disease recognition and health screening in the medical\ndomain. Despite the emergence of numerous methods for medical AD, the lack of a\nfair and comprehensive evaluation causes ambiguous conclusions and hinders the\ndevelopment of this field. To address this problem, this paper builds a\nbenchmark with unified comparison. Seven medical datasets with five image\nmodalities, including chest X-rays, brain MRIs, retinal fundus images,\ndermatoscopic images, and histopathology images, are curated for extensive\nevaluation. Thirty typical AD methods, including reconstruction and\nself-supervised learning-based methods, are involved in comparison of\nimage-level anomaly classification and pixel-level anomaly segmentation.\nFurthermore, for the first time, we systematically investigate the effect of\nkey components in existing methods, revealing unresolved challenges and\npotential future directions. The datasets and code are available at\nhttps://github.com/caiyu6666/MedIAnomaly.\n","authors":["Yu Cai","Weiwen Zhang","Hao Chen","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.04518v4.pdf","comment":"Accepted to Medical Image Analysis, 2025"},{"id":"http://arxiv.org/abs/2502.07160v2","updated":"2025-02-19T03:43:57Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.13407v1","updated":"2025-02-19T03:33:54Z","published":"2025-02-19T03:33:54Z","title":"JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust\n  Multi-Teacher Knowledge Distillation Framework","summary":"  Deep learning has achieved significant success in the field of remote sensing\nimage change detection (CD), yet two major challenges remain: the scarcity of\nsub-meter, all-inclusive open-source CD datasets, and the difficulty of\nachieving consistent and satisfactory detection results across images with\nvarying change areas. To address these issues, we introduce the JL1-CD dataset,\nwhich contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5\nto 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation\n(MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD\ndatasets demonstrate that the MTKD framework significantly improves the\nperformance of CD models with various network architectures and parameter\nsizes, achieving new state-of-the-art results. The code is available at\nhttps://github.com/circleLZY/MTKD-CD.\n","authors":["Ziyuan Liu","Ruifei Zhu","Long Gao","Yuanxiu Zhou","Jingyu Ma","Yuantao Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13407v1.pdf","comment":"14 pages, 9 figures. Submitted to IEEE Transactions on Geoscience and\n  Remote Sensing (TGRS)"},{"id":"http://arxiv.org/abs/2403.11453v2","updated":"2025-02-19T03:20:12Z","published":"2024-03-18T04:01:26Z","title":"Hybrid Explicit Representation for Ultra-Realistic Head Avatars","summary":"  We introduce a novel approach to creating ultra-realistic head avatars and\nrendering them in real-time (>30fps at $2048 \\times 1334$ resolution). First,\nwe propose a hybrid explicit representation that combines the advantages of two\nprimitive-based efficient rendering techniques. UV-mapped 3D mesh is utilized\nto capture sharp and rich textures on smooth surfaces, while 3D Gaussian\nSplatting is employed to represent complex geometric structures. In the\npipeline of modeling an avatar, after tracking parametric models based on\ncaptured multi-view RGB videos, our goal is to simultaneously optimize the\ntexture and opacity map of mesh, as well as a set of 3D Gaussian splats\nlocalized and rigged onto the mesh facets. Specifically, we perform\n$\\alpha$-blending on the color and opacity values based on the merged and\nre-ordered z-buffer from the rasterization results of mesh and 3DGS. This\nprocess involves the mesh and 3DGS adaptively fitting the captured visual\ninformation to outline a high-fidelity digital avatar. To avoid artifacts\ncaused by Gaussian splats crossing the mesh facets, we design a stable hybrid\ndepth sorting strategy. Experiments illustrate that our modeled results exceed\nthose of state-of-the-art approaches.\n","authors":["Hongrui Cai","Yuting Xiao","Xuan Wang","Jiafei Li","Yudong Guo","Yanbo Fan","Shenghua Gao","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11453v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.13399v1","updated":"2025-02-19T03:18:29Z","published":"2025-02-19T03:18:29Z","title":"MaizeEar-SAM: Zero-Shot Maize Ear Phenotyping","summary":"  Quantifying the variation in yield component traits of maize (Zea mays L.),\nwhich together determine the overall productivity of this globally important\ncrop, plays a critical role in plant genetics research, plant breeding, and the\ndevelopment of improved farming practices. Grain yield per acre is calculated\nby multiplying the number of plants per acre, ears per plant, number of kernels\nper ear, and the average kernel weight. The number of kernels per ear is\ndetermined by the number of kernel rows per ear multiplied by the number of\nkernels per row. Traditional manual methods for measuring these two traits are\ntime-consuming, limiting large-scale data collection. Recent automation efforts\nusing image processing and deep learning encounter challenges such as high\nannotation costs and uncertain generalizability.\n  We tackle these issues by exploring Large Vision Models for zero-shot,\nannotation-free maize kernel segmentation. By using an open-source large vision\nmodel, the Segment Anything Model (SAM), we segment individual kernels in RGB\nimages of maize ears and apply a graph-based algorithm to calculate the number\nof kernels per row. Our approach successfully identifies the number of kernels\nper row across a wide range of maize ears, showing the potential of zero-shot\nlearning with foundation vision models combined with image processing\ntechniques to improve automation and reduce subjectivity in agronomic data\ncollection. All our code is open-sourced to make these affordable phenotyping\nmethods accessible to everyone.\n","authors":["Hossein Zaremehrjerdi","Lisa Coffey","Talukder Jubery","Huyu Liu","Jon Turkus","Kyle Linders","James C. Schnable","Patrick S. Schnable","Baskar Ganapathysubramanian"],"pdf_url":"https://arxiv.org/pdf/2502.13399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01689v3","updated":"2025-02-19T03:06:59Z","published":"2024-08-03T07:04:55Z","title":"Controllable Unlearning for Image-to-Image Generative Models via\n  $\\varepsilon$-Constrained Optimization","summary":"  While generative models have made significant advancements in recent years,\nthey also raise concerns such as privacy breaches and biases. Machine\nunlearning has emerged as a viable solution, aiming to remove specific training\ndata, e.g., containing private information and bias, from models. In this\npaper, we study the machine unlearning problem in Image-to-Image (I2I)\ngenerative models. Previous studies mainly treat it as a single objective\noptimization problem, offering a solitary solution, thereby neglecting the\nvaried user expectations towards the trade-off between complete unlearning and\nmodel utility. To address this issue, we propose a controllable unlearning\nframework that uses a control coefficient $\\varepsilon$ to control the\ntrade-off. We reformulate the I2I generative model unlearning problem into a\n$\\varepsilon$-constrained optimization problem and solve it with a\ngradient-based method to find optimal solutions for unlearning boundaries.\nThese boundaries define the valid range for the control coefficient. Within\nthis range, every yielded solution is theoretically guaranteed with Pareto\noptimality. We also analyze the convergence rate of our framework under various\ncontrol functions. Extensive experiments on two benchmark datasets across three\nmainstream I2I models demonstrate the effectiveness of our controllable\nunlearning framework.\n","authors":["Xiaohua Feng","Yuyuan Li","Chaochao Chen","Li Zhang","Longfei Li","Jun Zhou","Xiaolin Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.01689v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.13385v1","updated":"2025-02-19T02:50:51Z","published":"2025-02-19T02:50:51Z","title":"SNN-Driven Multimodal Human Action Recognition via Event Camera and\n  Skeleton Data Fusion","summary":"  Multimodal human action recognition based on RGB and skeleton data fusion,\nwhile effective, is constrained by significant limitations such as high\ncomputational complexity, excessive memory consumption, and substantial energy\ndemands, particularly when implemented with Artificial Neural Networks (ANN).\nThese limitations restrict its applicability in resource-constrained scenarios.\nTo address these challenges, we propose a novel Spiking Neural Network\n(SNN)-driven framework for multimodal human action recognition, utilizing event\ncamera and skeleton data. Our framework is centered on two key innovations: (1)\na novel multimodal SNN architecture that employs distinct backbone networks for\neach modality-an SNN-based Mamba for event camera data and a Spiking Graph\nConvolutional Network (SGN) for skeleton data-combined with a spiking semantic\nextraction module to capture deep semantic representations; and (2) a\npioneering SNN-based discretized information bottleneck mechanism for modality\nfusion, which effectively balances the preservation of modality-specific\nsemantics with efficient information compression. To validate our approach, we\npropose a novel method for constructing a multimodal dataset that integrates\nevent camera and skeleton data, enabling comprehensive evaluation. Extensive\nexperiments demonstrate that our method achieves superior performance in both\nrecognition accuracy and energy efficiency, offering a promising solution for\npractical applications.\n","authors":["Naichuan Zheng","Hailun Xia"],"pdf_url":"https://arxiv.org/pdf/2502.13385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13383v1","updated":"2025-02-19T02:46:52Z","published":"2025-02-19T02:46:52Z","title":"MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought\n  Verification","summary":"  According to the Test-Time Scaling, the integration of External Slow-Thinking\nwith the Verify mechanism has been demonstrated to enhance multi-round\nreasoning in large language models (LLMs). However, in the multimodal (MM)\ndomain, there is still a lack of a strong MM-Verifier. In this paper, we\nintroduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through\nlonger inference and more robust verification. First, we propose a two-step MM\nverification data synthesis method, which combines a simulation-based tree\nsearch with verification and uses rejection sampling to generate high-quality\nChain-of-Thought (COT) data. This data is then used to fine-tune the\nverification model, MM-Verifier. Additionally, we present a more efficient\nmethod for synthesizing MMCOT data, bridging the gap between text-based and\nmultimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner.\nOur MM-Verifier outperforms all larger models on the MathCheck, MathVista, and\nMathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness\nand scalability, with performance improving as data size increases. Finally,\nour approach achieves strong performance when combining MM-Reasoner and\nMM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o\n(63.8) with 12 rollouts.\n","authors":["Linzhuang Sun","Hao Liang","Jingxuan Wei","Bihui Yu","Tianpeng Li","Fan Yang","Zenan Zhou","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10803v2","updated":"2025-02-19T02:13:13Z","published":"2024-10-14T17:59:00Z","title":"Generalizable Humanoid Manipulation with 3D Diffusion Policies","summary":"  Humanoid robots capable of autonomous operation in diverse environments have\nlong been a goal for roboticists. However, autonomous manipulation by humanoid\nrobots has largely been restricted to one specific scene, primarily due to the\ndifficulty of acquiring generalizable skills and the expensiveness of\nin-the-wild humanoid robot data. In this work, we build a real-world robotic\nsystem to address this challenging problem. Our system is mainly an integration\nof 1) a whole-upper-body robotic teleoperation system to acquire human-like\nrobot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart\nand a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning\nalgorithm for humanoid robots to learn from noisy human data. We run more than\n2000 episodes of policy rollouts on the real robot for rigorous policy\nevaluation. Empowered by this system, we show that using only data collected in\none single scene and with only onboard computing, a full-sized humanoid robot\ncan autonomously perform skills in diverse real-world scenarios. Videos are\navailable at\n\\href{https://humanoid-manipulation.github.io}{humanoid-manipulation.github.io}.\n","authors":["Yanjie Ze","Zixuan Chen","Wenhao Wang","Tianyi Chen","Xialin He","Ying Yuan","Xue Bin Peng","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10803v2.pdf","comment":"Project website: https://humanoid-manipulation.github.io"},{"id":"http://arxiv.org/abs/2502.13372v1","updated":"2025-02-19T02:11:41Z","published":"2025-02-19T02:11:41Z","title":"MoVer: Motion Verification for Motion Graphics Animations","summary":"  While large vision-language models can generate motion graphics animations\nfrom text prompts, they regularly fail to include all of spatio-temporal\nproperties described in the prompt. We introduce MoVer, a motion verification\nDSL based on first-order logic that can check spatio-temporal properties of a\nmotion graphics animation. We identify a general set of such properties that\npeople commonly use to describe animations (e.g., the direction and timing of\nmotions, the relative positioning of objects, etc.). We implement these\nproperties as predicates in MoVer and provide an execution engine that can\napply a MoVer program to any input SVG-based motion graphics animation. We then\ndemonstrate how MoVer can be used in an LLM-based synthesis and verification\npipeline for iteratively refining motion graphics animations. Given a text\nprompt, our pipeline synthesizes a motion graphics animation and a\ncorresponding MoVer program. Executing the verification program on the\nanimation yields a report of the predicates that failed and the report can be\nautomatically fed back to LLM to iteratively correct the animation. To evaluate\nour pipeline, we build a synthetic dataset of 5600 text prompts paired with\nground truth MoVer verification programs. We find that while our LLM-based\npipeline is able to automatically generate a correct motion graphics animation\nfor 58.8% of the test prompts without any iteration, this number raises to\n93.6% with up to 50 correction iterations. Project website:\nhttps://mover-dsl.github.io/\n","authors":["Jiaju Ma","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2502.13372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18538v2","updated":"2025-02-19T01:58:44Z","published":"2024-10-24T08:38:20Z","title":"SMITE: Segment Me In TimE","summary":"  Segmenting an object in a video presents significant challenges. Each pixel\nmust be accurately labelled, and these labels must remain consistent across\nframes. The difficulty increases when the segmentation is with arbitrary\ngranularity, meaning the number of segments can vary arbitrarily, and masks are\ndefined based on only one or a few sample images. In this paper, we address\nthis issue by employing a pre-trained text to image diffusion model\nsupplemented with an additional tracking mechanism. We demonstrate that our\napproach can effectively manage various segmentation scenarios and outperforms\nstate-of-the-art alternatives.\n","authors":["Amirhossein Alimohammadi","Sauradip Nag","Saeid Asgari Taghanaki","Andrea Tagliasacchi","Ghassan Hamarneh","Ali Mahdavi Amiri"],"pdf_url":"https://arxiv.org/pdf/2410.18538v2.pdf","comment":"ICLR 2025; Project page is at https://segment-me-in-time.github.io/"},{"id":"http://arxiv.org/abs/2502.13363v1","updated":"2025-02-19T01:53:03Z","published":"2025-02-19T01:53:03Z","title":"Pretrained Image-Text Models are Secretly Video Captioners","summary":"  Developing video captioning models is computationally expensive. The dynamic\nnature of video also complicates the design of multimodal models that can\neffectively caption these sequences. However, we find that by using minimal\ncomputational resources and without complex modifications to address video\ndynamics, an image-based model can be repurposed to outperform several\nspecialised video captioning systems. Our adapted model demonstrates top tier\nperformance on major benchmarks, ranking 2nd on MSRVTT and MSVD, and 3rd on\nVATEX. We transform it into a competitive video captioner by post training a\ntypical image captioning model BLIP2 with only 6,000 video text pairs and\nsimply concatenating frames (significantly fewer data than other methods),\nwhich use 2.5 to 144 million pairs. From a resource optimization perspective,\nthis video captioning study focuses on three fundamental factors: optimizing\nmodel scale, maximizing data efficiency, and incorporating reinforcement\nlearning. This extensive study demonstrates that a lightweight, image based\nadaptation strategy can rival state-of-the-art video captioning systems,\noffering a practical solution for low-resource scenarios.\n","authors":["Chunhui Zhang","Yiren Jian","Zhongyu Ouyang","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2502.13363v1.pdf","comment":"Accepted to the 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL 2025). The\n  first two authors contributed equally and were listed in random order"},{"id":"http://arxiv.org/abs/2412.08564v2","updated":"2025-02-19T01:01:54Z","published":"2024-12-11T17:32:21Z","title":"Template-Based Visual Program Distillation","summary":"  For users with limited computational resources, visual programming or\nprompting large language models (LLMs) to generate executable code for visual\ntasks, like visual question answering (VQA), remains largely inaccessible. Even\nwith techniques such as distillation, adapting visual programming to smaller\nmodels or specific datasets is still quite challenging due to high annotation\ncosts. We propose a low-cost visual program distillation method that can be\nused for models with fewer than 1 billion parameters and requires no\nhuman-generated program annotations. We achieve this through synthetic data\naugmentation based on decoupling programs into higher-level skills, called\ntemplates, and their corresponding arguments. Experimental results show that,\nwith a relatively small amount of question/answer data, small language models\ncan generate high-quality visual programs with the added benefit of much faster\ninference.\n","authors":["Michal Shlapentokh-Rothman","Yu-Xiong Wang","Derek Hoiem"],"pdf_url":"https://arxiv.org/pdf/2412.08564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00781v2","updated":"2025-02-19T00:28:22Z","published":"2024-07-17T10:09:48Z","title":"Hands-on STEM Learning Experiences using Digital Technologies","summary":"  The facilitation of STEM education can be enhanced by the provision of\nopportunities for learners to gain a better understanding of science through\nthe utilization of tangible and visual examples. The objective of this work is\nto present an account of our experiences and activities carried out in Italian\nschools with this novel approach. The selection of projects and experiences\ndiscussed --in which students develop a range of core competencies such as\ncollaboration, creativity, critical thinking, experimentation, prototyping,\ncommunication and problem-solving; include tangible complex 3D printed\nstructures, large micro-controller board replicas and the visualization of wind\ndynamics and tiny invisible elementary particles among others. These hands-on\nexperiences demonstrate the benefits on the use of digital fabrication\ntechnologies implemented within a FabLab for STEM learning.\n","authors":["Gaia Fior","Carlo Fonda","Enrique Canessa"],"pdf_url":"https://arxiv.org/pdf/2408.00781v2.pdf","comment":"to appear STEM Education Journal (2025) 9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.05177v2","updated":"2025-02-19T00:10:06Z","published":"2025-02-07T18:59:56Z","title":"Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuracy","summary":"  We introduce Long-VITA, a simple yet effective large multi-modal model for\nlong-context visual-language understanding tasks. It is adept at concurrently\nprocessing and analyzing modalities of image, video, and text over 4K frames or\n1M tokens while delivering advanced performances on short-context multi-modal\ntasks. We propose an effective multi-modal training schema that starts with\nlarge language models and proceeds through vision-language alignment, general\nknowledge learning, and two sequential stages of long-sequence fine-tuning. We\nfurther implement context-parallelism distributed inference and logits-masked\nlanguage modeling head to scale Long-VITA to infinitely long inputs of images\nand texts during model inference. Regarding training data, Long-VITA is built\non a mix of 17M samples from public datasets only and demonstrates the\nstate-of-the-art performance on various multi-modal benchmarks, compared\nagainst recent cutting-edge models with internal data. Long-VITA is fully\nreproducible and supports both NPU and GPU platforms for training and testing.\nBy leveraging our inference designs, Long-VITA models achieve a remarkable 2x\nprefill speedup and 4x context length extension in single node with 8 GPUs. We\nhope Long-VITA can serve as a competitive baseline and offer valuable insights\nfor the open-source community in advancing long-context multi-modal\nunderstanding.\n","authors":["Yunhang Shen","Chaoyou Fu","Shaoqi Dong","Xiong Wang","Yi-Fan Zhang","Peixian Chen","Mengdan Zhang","Haoyu Cao","Ke Li","Xiawu Zheng","Yan Zhang","Yiyi Zhou","Ran He","Caifeng Shan","Rongrong Ji","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2502.05177v2.pdf","comment":"https://github.com/VITA-MLLM/Long-VITA"},{"id":"http://arxiv.org/abs/2502.14156v1","updated":"2025-02-19T23:53:00Z","published":"2025-02-19T23:53:00Z","title":"Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X\n  Collaboration","summary":"  Vehicle-to-everything (V2X) collaborative perception has emerged as a\npromising solution to address the limitations of single-vehicle perception\nsystems. However, existing V2X datasets are limited in scope, diversity, and\nquality. To address these gaps, we present Mixed Signals, a comprehensive V2X\ndataset featuring 45.1k point clouds and 240.6k bounding boxes collected from\nthree connected autonomous vehicles (CAVs) equipped with two different types of\nLiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides\nprecisely aligned point clouds and bounding box annotations across 10 classes,\nensuring reliable data for perception training. We provide detailed statistical\nanalysis on the quality of our dataset and extensively benchmark existing V2X\nmethods on it. Mixed Signals V2X Dataset is one of the highest quality,\nlarge-scale datasets publicly available for V2X perception research. Details on\nthe website https://mixedsignalsdataset.cs.cornell.edu/.\n","authors":["Katie Z Luo","Minh-Quan Dao","Zhenzhen Liu","Mark Campbell","Wei-Lun Chao","Kilian Q. Weinberger","Ezio Malis","Vincent Fremont","Bharath Hariharan","Mao Shan","Stewart Worrall","Julie Stephany Berrio Perez"],"pdf_url":"https://arxiv.org/pdf/2502.14156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22454v2","updated":"2025-02-19T23:45:35Z","published":"2024-10-29T18:42:03Z","title":"Brain age identification from diffusion MRI synergistically predicts\n  neurodegenerative disease","summary":"  Estimated brain age from magnetic resonance image (MRI) and its deviation\nfrom chronological age can provide early insights into potential\nneurodegenerative diseases, supporting early detection and implementation of\nprevention strategies. Diffusion MRI (dMRI) presents an opportunity to build an\nearlier biomarker for neurodegenerative disease prediction because it captures\nsubtle microstructural changes that precede more perceptible macrostructural\nchanges. However, the coexistence of macro- and micro-structural information in\ndMRI raises the question of whether current dMRI-based brain age estimation\nmodels are leveraging the intended microstructural information or if they\ninadvertently rely on the macrostructural information. To develop a\nmicrostructure-specific brain age, we propose a method for brain age\nidentification from dMRI that mitigates the model's use of macrostructural\ninformation by non-rigidly registering all images to a standard template.\nImaging data from 13,398 participants across 12 datasets were used for the\ntraining and evaluation. We compare our brain age models, trained with and\nwithout macrostructural information mitigated, with an architecturally similar\nT1-weighted (T1w) MRI-based brain age model and two recent, popular, openly\navailable T1w MRI-based brain age models that primarily use macrostructural\ninformation. We observe difference between our dMRI-based brain age and T1w\nMRI-based brain age across stages of neurodegeneration, with dMRI-based brain\nage being older than T1w MRI-based brain age in participants transitioning from\ncognitively normal (CN) to mild cognitive impairment (MCI), but younger in\nparticipants already diagnosed with Alzheimer's disease (AD). Furthermore,\ndMRI-based brain age may offer advantages over T1w MRI-based brain age in\npredicting the transition from CN to MCI up to five years before diagnosis.\n","authors":["Chenyu Gao","Michael E. Kim","Karthik Ramadass","Praitayini Kanakaraj","Aravind R. Krishnan","Adam M. Saunders","Nancy R. Newlin","Ho Hin Lee","Qi Yang","Warren D. Taylor","Brian D. Boyd","Lori L. Beason-Held","Susan M. Resnick","Lisa L. Barnes","David A. Bennett","Katherine D. Van Schaik","Derek B. Archer","Timothy J. Hohman","Angela L. Jefferson","Ivana Išgum","Daniel Moyer","Yuankai Huo","Kurt G. Schilling","Lianrui Zuo","Shunxing Bao","Nazirah Mohd Khairi","Zhiyuan Li","Christos Davatzikos","Bennett A. Landman"],"pdf_url":"https://arxiv.org/pdf/2410.22454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14974v3","updated":"2025-02-19T23:28:40Z","published":"2024-05-23T18:21:59Z","title":"LOVA3: Learning to Visual Question Answering, Asking and Assessment","summary":"  Question answering, asking, and assessment are three innate human traits\ncrucial for understanding the world and acquiring knowledge. By enhancing these\ncapabilities, humans can more effectively utilize data, leading to better\ncomprehension and learning outcomes. Current Multimodal Large Language Models\n(MLLMs) primarily focus on question answering, often neglecting the full\npotential of questioning and assessment skills. Inspired by the human learning\nmechanism, we introduce LOVA3, an innovative framework named \"Learning tO\nVisual question Answering, Asking and Assessment,\" designed to equip MLLMs with\nthese additional capabilities. Our approach involves the creation of two\nsupplementary training tasks GenQA and EvalQA, aiming at fostering the skills\nof asking and assessing questions in the context of images. To develop the\nquestioning ability, we compile a comprehensive set of multimodal foundational\ntasks. For assessment, we introduce a new benchmark called EvalQABench,\ncomprising 64,000 training samples (split evenly between positive and negative\nsamples) and 5,000 validation and testing samples. We posit that enhancing\nMLLMs with the capabilities to answer, ask, and assess questions will enhance\ntheir multimodal comprehension, ultimately improving overall performance. To\nvalidate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate\nthem on a range of multimodal datasets and benchmarks. Our results demonstrate\nconsistent performance gains, underscoring the critical role of these\nadditional tasks in fostering comprehensive intelligence in MLLMs. The code is\navailable at https://github.com/showlab/LOVA3.\n","authors":["Henry Hengyuan Zhao","Pan Zhou","Difei Gao","Zechen Bai","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2405.14974v3.pdf","comment":"NeurIPS 2024. The code is available at\n  https://github.com/showlab/LOVA3"},{"id":"http://arxiv.org/abs/2502.14149v1","updated":"2025-02-19T23:28:39Z","published":"2025-02-19T23:28:39Z","title":"PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual\n  Question Answering in Pituitary Surgery","summary":"  Vision-Language Models (VLMs) in visual question answering (VQA) offer a\nunique opportunity to enhance intra-operative decision-making, promote\nintuitive interactions, and significantly advancing surgical education.\nHowever, the development of VLMs for surgical VQA is challenging due to limited\ndatasets and the risk of overfitting and catastrophic forgetting during full\nfine-tuning of pretrained weights. While parameter-efficient techniques like\nLow-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MoRA) address\nadaptation challenges, their uniform parameter distribution overlooks the\nfeature hierarchy in deep networks, where earlier layers, that learn general\nfeatures, require more parameters than later ones. This work introduces\nPitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank\nadaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting\nGPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames\nfrom 25 procedural videos with 745,972 question-answer sentence pairs, covering\nkey surgical elements such as phase and step recognition, context\nunderstanding, tool detection, localization, and interactions recognition.\nVector-MoLoRA incorporates the principles of LoRA and MoRA to develop a\nmatrix-low-rank adaptation strategy that employs vector ranking to allocate\nmore parameters to earlier layers, gradually reducing them in the later layers.\nOur approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets,\neffectively mitigates catastrophic forgetting while significantly enhancing\nperformance over recent baselines. Furthermore, our risk-coverage analysis\nhighlights its enhanced reliability and trustworthiness in handling uncertain\npredictions. Our source code and dataset is available\nat~\\url{https://github.com/HRL-Mike/PitVQA-Plus}.\n","authors":["Runlong He","Danyal Z. Khan","Evangelos B. Mazomenos","Hani J. Marcus","Danail Stoyanov","Matthew J. Clarkson","Mobarakol Islam"],"pdf_url":"https://arxiv.org/pdf/2502.14149v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.14142v1","updated":"2025-02-19T22:58:56Z","published":"2025-02-19T22:58:56Z","title":"Token Adaptation via Side Graph Convolution for Temporally and Spatially\n  Efficient Fine-tuning of 3D Point Cloud Transformers","summary":"  Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloud\nTransformers has emerged as a promising technique for 3D point cloud analysis.\nWhile existing PEFT methods attempt to minimize the number of tunable\nparameters, they still suffer from high temporal and spatial computational\ncosts during fine-tuning. This paper proposes a novel PEFT algorithm for 3D\npoint cloud Transformers, called Side Token Adaptation on a neighborhood Graph\n(STAG), to achieve superior temporal and spatial efficiency. STAG employs a\ngraph convolutional side network that operates in parallel with a frozen\nbackbone Transformer to adapt tokens to downstream tasks. STAG's side network\nrealizes high efficiency through three key components: connection with the\nbackbone that enables reduced gradient computation, parameter sharing\nframework, and efficient graph convolution. Furthermore, we present Point Cloud\nClassification 13 (PCC13), a new benchmark comprising diverse publicly\navailable 3D point cloud datasets, enabling comprehensive evaluation of PEFT\nmethods. Extensive experiments using multiple pre-trained models and PCC13\ndemonstrates the effectiveness of STAG. Specifically, STAG maintains\nclassification accuracy comparable to existing methods while reducing tunable\nparameters to only 0.43M and achieving significant reductions in both\ncomputational time and memory consumption for fine-tuning. Code and benchmark\nwill be available at: https://github.com/takahikof/STAG\n","authors":["Takahiko Furuya"],"pdf_url":"https://arxiv.org/pdf/2502.14142v1.pdf","comment":"Currently under review"},{"id":"http://arxiv.org/abs/2502.14140v1","updated":"2025-02-19T22:55:49Z","published":"2025-02-19T22:55:49Z","title":"ModSkill: Physical Character Skill Modularization","summary":"  Human motion is highly diverse and dynamic, posing challenges for imitation\nlearning algorithms that aim to generalize motor skills for controlling\nsimulated characters. Previous methods typically rely on a universal full-body\ncontroller for tracking reference motion (tracking-based model) or a unified\nfull-body skill embedding space (skill embedding). However, these approaches\noften struggle to generalize and scale to larger motion datasets. In this work,\nwe introduce a novel skill learning framework, ModSkill, that decouples complex\nfull-body skills into compositional, modular skills for independent body parts.\nOur framework features a skill modularization attention layer that processes\npolicy observations into modular skill embeddings that guide low-level\ncontrollers for each body part. We also propose an Active Skill Learning\napproach with Generative Adaptive Sampling, using large motion generation\nmodels to adaptively enhance policy learning in challenging tracking scenarios.\nOur results show that this modularized skill learning framework, enhanced by\ngenerative sampling, outperforms existing methods in precise full-body motion\ntracking and enables reusable skill embeddings for diverse goal-driven tasks.\n","authors":["Yiming Huang","Zhiyang Dou","Lingjie Liu"],"pdf_url":"https://arxiv.org/pdf/2502.14140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06330v2","updated":"2025-02-19T22:45:36Z","published":"2022-10-12T15:49:51Z","title":"CoRRECT: A Deep Unfolding Framework for Motion-Corrected Quantitative\n  R2* Mapping","summary":"  Quantitative MRI (qMRI) refers to a class of MRI methods for quantifying the\nspatial distribution of biological tissue parameters. Traditional qMRI methods\nusually deal separately with artifacts arising from accelerated data\nacquisition, involuntary physical motion, and magnetic-field inhomogeneities,\nleading to suboptimal end-to-end performance. This paper presents CoRRECT, a\nunified deep unfolding (DU) framework for qMRI consisting of a model-based\nend-to-end neural network, a method for motion-artifact reduction, and a\nself-supervised learning scheme. The network is trained to produce R2* maps\nwhose k-space data matches the real data by also accounting for motion and\nfield inhomogeneities. When deployed, CoRRECT only uses the k-space data\nwithout any pre-computed parameters for motion or inhomogeneity correction. Our\nresults on experimentally collected multi-Gradient-Recalled Echo (mGRE) MRI\ndata show that CoRRECT recovers motion and inhomogeneity artifact-free R2* maps\nin highly accelerated acquisition settings. This work opens the door to DU\nmethods that can integrate physical measurement models, biophysical signal\nmodels, and learned prior models for high-quality qMRI.\n","authors":["Xiaojian Xu","Weijie Gan","Satya V. V. N. Kothapalli","Dmitriy A. Yablonskiy","Ulugbek S. Kamilov"],"pdf_url":"https://arxiv.org/pdf/2210.06330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14129v1","updated":"2025-02-19T22:20:57Z","published":"2025-02-19T22:20:57Z","title":"GlossGau: Efficient Inverse Rendering for Glossy Surface with\n  Anisotropic Spherical Gaussian","summary":"  The reconstruction of 3D objects from calibrated photographs represents a\nfundamental yet intricate challenge in the domains of computer graphics and\nvision. Although neural reconstruction approaches based on Neural Radiance\nFields (NeRF) have shown remarkable capabilities, their processing costs remain\nsubstantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely\nimproves the training efficiency and facilitates to generate realistic\nrendering in real-time. However, due to the limited ability of Spherical\nHarmonics (SH) to represent high-frequency information, 3D-GS falls short in\nreconstructing glossy objects. Researchers have turned to enhance the specular\nexpressiveness of 3D-GS through inverse rendering. Yet these methods often\nstruggle to maintain the training and rendering efficiency, undermining the\nbenefits of Gaussian Splatting techniques. In this paper, we introduce\nGlossGau, an efficient inverse rendering framework that reconstructs scenes\nwith glossy surfaces while maintaining training and rendering speeds comparable\nto vanilla 3D-GS. Specifically, we explicitly model the surface normals,\nBidirectional Reflectance Distribution Function (BRDF) parameters, as well as\nincident lights and use Anisotropic Spherical Gaussian (ASG) to approximate the\nper-Gaussian Normal Distribution Function under the microfacet model. We\nutilize 2D Gaussian Splatting (2D-GS) as foundational primitives and apply\nregularization to significantly alleviate the normal estimation challenge\nencountered in related works. Experiments demonstrate that GlossGau achieves\ncompetitive or superior reconstruction on datasets with glossy surfaces.\nCompared with previous GS-based works that address the specular surface, our\noptimization time is considerably less.\n","authors":["Bang Du","Runfa Blark Li","Chen Du","Truong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.14129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14125v1","updated":"2025-02-19T22:00:20Z","published":"2025-02-19T22:00:20Z","title":"Modular Prompt Learning Improves Vision-Language Models","summary":"  Pre-trained vision-language models are able to interpret visual concepts and\nlanguage semantics. Prompt learning, a method of constructing prompts for text\nencoders or image encoders, elicits the potentials of pre-trained models and\nreadily adapts them to new scenarios. Compared to fine-tuning, prompt learning\nenables the model to achieve comparable or better performance using fewer\ntrainable parameters. Besides, prompt learning freezes the pre-trained model\nand avoids the catastrophic forgetting issue in the fine-tuning. Continuous\nprompts inserted into the input of every transformer layer (i.e. deep prompts)\ncan improve the performances of pre-trained models on downstream tasks. For\ni-th transformer layer, the inserted prompts replace previously inserted\nprompts in the $(i-1)$-th layer. Although the self-attention mechanism\ncontextualizes newly inserted prompts for the current layer and embeddings from\nthe previous layer's output, removing all inserted prompts from the previous\nlayer inevitably loses information contained in the continuous prompts. In this\nwork, we propose Modular Prompt Learning (MPL) that is designed to promote the\npreservation of information contained in the inserted prompts. We evaluate the\nproposed method on base-to-new generalization and cross-dataset tasks. On\naverage of 11 datasets, our method achieves 0.7% performance gain on the\nbase-to-new generalization task compared to the state-of-the-art method. The\nlargest improvement on the individual dataset is 10.7% (EuroSAT dataset).\n","authors":["Zhenhan Huang","Tejaswini Pedapati","Pin-Yu Chen","Jianxi Gao"],"pdf_url":"https://arxiv.org/pdf/2502.14125v1.pdf","comment":"2025 IEEE International Conference on Acoustics, Speech, and Signal\n  Processing"},{"id":"http://arxiv.org/abs/2404.03631v2","updated":"2025-02-19T21:38:33Z","published":"2024-04-04T17:52:13Z","title":"Robust Concept Erasure Using Task Vectors","summary":"  With the rapid growth of text-to-image models, a variety of techniques have\nbeen suggested to prevent undesirable image generations. Yet, these methods\noften only protect against specific user prompts and have been shown to allow\nunsafe generations with other inputs. Here we focus on unconditionally erasing\na concept from a text-to-image model rather than conditioning the erasure on\nthe user's prompt. We first show that compared to input-dependent erasure\nmethods, concept erasure that uses Task Vectors (TV) is more robust to\nunexpected user inputs, not seen during training. However, TV-based erasure can\nalso affect the core performance of the edited model, particularly when the\nrequired edit strength is unknown. To this end, we propose a method called\nDiverse Inversion, which we use to estimate the required strength of the TV\nedit. Diverse Inversion finds within the model input space a large set of word\nembeddings, each of which induces the generation of the target concept. We find\nthat encouraging diversity in the set makes our estimation more robust to\nunexpected prompts. Finally, we show that Diverse Inversion enables us to apply\na TV edit only to a subset of the model weights, enhancing the erasure\ncapabilities while better maintaining the core functionality of the model.\n","authors":["Minh Pham","Kelly O. Marshall","Chinmay Hegde","Niv Cohen"],"pdf_url":"https://arxiv.org/pdf/2404.03631v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14113v1","updated":"2025-02-19T21:30:51Z","published":"2025-02-19T21:30:51Z","title":"Object-centric Binding in Contrastive Language-Image Pretraining","summary":"  Recent advances in vision language models (VLM) have been driven by\ncontrastive models such as CLIP, which learn to associate visual information\nwith their corresponding text descriptions. However, these models have\nlimitations in understanding complex compositional scenes involving multiple\nobjects and their spatial relationships. To address these challenges, we\npropose a novel approach that diverges from commonly used strategies, which\nrely on the design of hard-negative augmentations. Instead, our work focuses on\nintegrating inductive biases into pre-trained CLIP-like models to improve their\ncompositional understanding without using any additional hard-negatives. To\nthat end, we introduce a binding module that connects a scene graph, derived\nfrom a text description, with a slot-structured image representation,\nfacilitating a structured similarity assessment between the two modalities. We\nalso leverage relationships as text-conditioned visual constraints, thereby\ncapturing the intricate interactions between objects and their contextual\nrelationships more effectively. Our resulting model not only enhances the\nperformance of CLIP-based models in multi-object compositional understanding\nbut also paves the way towards more accurate and sample-efficient image-text\nmatching of complex scenes.\n","authors":["Rim Assouel","Pietro Astolfi","Florian Bordes","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2502.14113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03685v2","updated":"2025-02-19T21:10:44Z","published":"2024-09-05T16:39:21Z","title":"View-Invariant Policy Learning via Zero-Shot Novel View Synthesis","summary":"  Large-scale visuomotor policy learning is a promising approach toward\ndeveloping generalizable manipulation systems. Yet, policies that can be\ndeployed on diverse embodiments, environments, and observational modalities\nremain elusive. In this work, we investigate how knowledge from large-scale\nvisual data of the world may be used to address one axis of variation for\ngeneralizable manipulation: observational viewpoint. Specifically, we study\nsingle-image novel view synthesis models, which learn 3D-aware scene-level\npriors by rendering images of the same scene from alternate camera viewpoints\ngiven a single input image. For practical application to diverse robotic data,\nthese models must operate zero-shot, performing view synthesis on unseen tasks\nand environments. We empirically analyze view synthesis models within a simple\ndata-augmentation scheme that we call View Synthesis Augmentation (VISTA) to\nunderstand their capabilities for learning viewpoint-invariant policies from\nsingle-viewpoint demonstration data. Upon evaluating the robustness of policies\ntrained with our method to out-of-distribution camera viewpoints, we find that\nthey outperform baselines in both simulated and real-world manipulation tasks.\nVideos and additional visualizations are available at\nhttps://s-tian.github.io/projects/vista.\n","authors":["Stephen Tian","Blake Wulfe","Kyle Sargent","Katherine Liu","Sergey Zakharov","Vitor Guizilini","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2409.03685v2.pdf","comment":"Accepted to CoRL 2024"},{"id":"http://arxiv.org/abs/2502.09873v2","updated":"2025-02-19T21:00:01Z","published":"2025-02-14T02:46:27Z","title":"Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal","summary":"  Diffusion models have demonstrated remarkable success in image restoration\ntasks. However, their multi-step denoising process introduces significant\ncomputational overhead, limiting their practical deployment. Furthermore,\nexisting methods struggle to effectively remove severe JPEG artifact,\nespecially in highly compressed images. To address these challenges, we propose\nCODiff, a compression-aware one-step diffusion model for JPEG artifact removal.\nThe core of CODiff is the compression-aware visual embedder (CaVE), which\nextracts and leverages JPEG compression priors to guide the diffusion model. We\npropose a dual learning strategy that combines explicit and implicit learning.\nSpecifically, explicit learning enforces a quality prediction objective to\ndifferentiate low-quality images with different compression levels. Implicit\nlearning employs a reconstruction objective that enhances the model's\ngeneralization. This dual learning allows for a deeper and more comprehensive\nunderstanding of JPEG compression. Experimental results demonstrate that CODiff\nsurpasses recent leading methods in both quantitative and visual quality\nmetrics. The code and models will be released at\nhttps://github.com/jp-guo/CODiff.\n","authors":["Jinpei Guo","Zheng Chen","Wenbo Li","Yong Guo","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14099v1","updated":"2025-02-19T20:58:53Z","published":"2025-02-19T20:58:53Z","title":"Point Cloud Geometry Scalable Coding Using a Resolution and\n  Quality-conditioned Latents Probability Estimator","summary":"  In the current age, users consume multimedia content in very heterogeneous\nscenarios in terms of network, hardware, and display capabilities. A naive\nsolution to this problem is to encode multiple independent streams, each\ncovering a different possible requirement for the clients, with an obvious\nnegative impact in both storage and computational requirements. These drawbacks\ncan be avoided by using codecs that enable scalability, i.e., the ability to\ngenerate a progressive bitstream, containing a base layer followed by multiple\nenhancement layers, that allow decoding the same bitstream serving multiple\nreconstructions and visualization specifications. While scalable coding is a\nwell-known and addressed feature in conventional image and video codecs, this\npaper focuses on a new and very different problem, notably the development of\nscalable coding solutions for deep learning-based Point Cloud (PC) coding. The\npeculiarities of this 3D representation make it hard to implement flexible\nsolutions that do not compromise the other functionalities of the codec. This\npaper proposes a joint quality and resolution scalability scheme, named\nScalable Resolution and Quality Hyperprior (SRQH), that, contrary to previous\nsolutions, can model the relationship between latents obtained with models\ntrained for different RD tradeoffs and/or at different resolutions.\nExperimental results obtained by integrating SRQH in the emerging JPEG Pleno\nlearning-based PC coding standard show that SRQH allows decoding the PC at\ndifferent qualities and resolutions with a single bitstream while incurring\nonly in a limited RD penalty and increment in complexity w.r.t. non-scalable\nJPEG PCC that would require one bitstream per coding configuration.\n","authors":["Daniele Mari","André F. R. Guarda","Nuno M. M. Rodrigues","Simone Milani","Fernando Pereira"],"pdf_url":"https://arxiv.org/pdf/2502.14099v1.pdf","comment":"Submitted to IEEE and currently under review"},{"id":"http://arxiv.org/abs/2409.15529v3","updated":"2025-02-19T20:55:41Z","published":"2024-09-23T20:27:10Z","title":"VaLID: Verification as Late Integration of Detections for LiDAR-Camera\n  Fusion","summary":"  Vehicle object detection benefits from both LiDAR and camera data, with LiDAR\noffering superior performance in many scenarios. Fusion of these modalities\nfurther enhances accuracy, but existing methods often introduce complexity or\ndataset-specific dependencies. In our study, we propose a model-adaptive\nlate-fusion method, VaLID, which validates whether each predicted bounding box\nis acceptable or not. Our method verifies the higher-performing, yet overly\noptimistic LiDAR model detections using camera detections that are obtained\nfrom either specially trained, general, or open-vocabulary models. VaLID uses a\nlightweight neural verification network trained with a high recall bias to\nreduce the false predictions made by the LiDAR detector, while still preserving\nthe true ones. Evaluating with multiple combinations of LiDAR and camera\ndetectors on the KITTI dataset, we reduce false positives by an average of\n63.9%, thus outperforming the individual detectors on 3D average precision\n(3DAP). Our approach is model-adaptive and demonstrates state-of-the-art\ncompetitive performance even when using generic camera detectors that were not\ntrained specifically for this dataset.\n","authors":["Vanshika Vats","Marzia Binta Nizam","James Davis"],"pdf_url":"https://arxiv.org/pdf/2409.15529v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14092v1","updated":"2025-02-19T20:35:41Z","published":"2025-02-19T20:35:41Z","title":"Hybrid Visual Servoing of Tendon-driven Continuum Robots","summary":"  This paper introduces a novel Hybrid Visual Servoing (HVS) approach for\ncontrolling tendon-driven continuum robots (TDCRs). The HVS system combines\nImage-Based Visual Servoing (IBVS) with Deep Learning-Based Visual Servoing\n(DLBVS) to overcome the limitations of each method and improve overall\nperformance. IBVS offers higher accuracy and faster convergence in feature-rich\nenvironments, while DLBVS enhances robustness against disturbances and offers a\nlarger workspace. By enabling smooth transitions between IBVS and DLBVS, the\nproposed HVS ensures effective control in dynamic, unstructured environments.\nThe effectiveness of this approach is validated through simulations and\nreal-world experiments, demonstrating that HVS achieves reduced iteration time,\nfaster convergence, lower final error, and smoother performance compared to\nDLBVS alone, while maintaining DLBVS's robustness in challenging conditions\nsuch as occlusions, lighting changes, actuator noise, and physical impacts.\n","authors":["Rana Danesh","Farrokh Janabi-Sharifi","Farhad Aghili"],"pdf_url":"https://arxiv.org/pdf/2502.14092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14090v1","updated":"2025-02-19T20:32:03Z","published":"2025-02-19T20:32:03Z","title":"MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge\n  Distillation","summary":"  Generative Artificial Intelligence (AI) has gained significant attention in\nrecent years, revolutionizing various applications across industries. Among\nthese, advanced vision models for image super-resolution are in high demand,\nparticularly for deployment on edge devices where real-time processing is\ncrucial. However, deploying such models on edge devices is challenging due to\nlimited computing power and memory. In this paper, we present MambaLiteSR, a\nnovel lightweight image Super-Resolution (SR) model that utilizes the\narchitecture of Vision Mamba. It integrates State Space Blocks and a\nreconstruction module for efficient feature extraction. To optimize efficiency\nwithout affecting performance, MambaLiteSR employs knowledge distillation to\ntransfer key insights from a larger Mamba-based teacher model to a smaller\nstudent model via hyperparameter tuning. Through mathematical analysis of model\nparameters and their impact on PSNR, we identify key factors and adjust them\naccordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms\nstate-of-the-art edge SR methods by reducing power consumption while\nmaintaining competitive PSNR and SSIM scores across benchmark datasets. It also\nreduces power usage during training via low-rank approximation. Moreover,\nMambaLiteSR reduces parameters with minimal performance loss, enabling\nefficient deployment of generative AI models on resource-constrained devices.\nDeployment on the embedded NVIDIA Jetson Orin Nano confirms the superior\nbalance of MambaLiteSR size, latency, and efficiency. Experiments show that\nMambaLiteSR achieves performance comparable to both the baseline and other edge\nmodels while using 15% fewer parameters. It also improves power consumption by\nup to 58% compared to state-of-the-art SR edge models, all while maintaining\nlow energy use during training.\n","authors":["Romina Aalishah","Mozhgan Navardi","Tinoosh Mohsenin"],"pdf_url":"https://arxiv.org/pdf/2502.14090v1.pdf","comment":"Special Session: Generative AI on Edge, 26th International Symposium\n  on Quality Electronic Design (ISQED'25)"},{"id":"http://arxiv.org/abs/2502.14088v1","updated":"2025-02-19T20:27:54Z","published":"2025-02-19T20:27:54Z","title":"Regression in EO: Are VLMs Up to the Challenge?","summary":"  Earth Observation (EO) data encompass a vast range of remotely sensed\ninformation, featuring multi-sensor and multi-temporal, playing an\nindispensable role in understanding our planet's dynamics. Recently, Vision\nLanguage Models (VLMs) have achieved remarkable success in perception and\nreasoning tasks, bringing new insights and opportunities to the EO field.\nHowever, the potential for EO applications, especially for scientific\nregression related applications remains largely unexplored. This paper bridges\nthat gap by systematically examining the challenges and opportunities of\nadapting VLMs for EO regression tasks. The discussion first contrasts the\ndistinctive properties of EO data with conventional computer vision datasets,\nthen identifies four core obstacles in applying VLMs to EO regression: 1) the\nabsence of dedicated benchmarks, 2) the discrete-versus-continuous\nrepresentation mismatch, 3) cumulative error accumulation, and 4) the\nsuboptimal nature of text-centric training objectives for numerical tasks.\nNext, a series of methodological insights and potential subtle pitfalls are\nexplored. Lastly, we offer some promising future directions for designing\nrobust, domain-aware solutions. Our findings highlight the promise of VLMs for\nscientific regression in EO, setting the stage for more precise and\ninterpretable modeling of critical environmental processes.\n","authors":["Xizhe Xue","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.14088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12138v2","updated":"2025-02-19T20:27:35Z","published":"2025-02-17T18:54:05Z","title":"FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views","summary":"  We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/\n","authors":["Shangzhan Zhang","Jianyuan Wang","Yinghao Xu","Nan Xue","Christian Rupprecht","Xiaowei Zhou","Yujun Shen","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2502.12138v2.pdf","comment":"8 pages. Website: https://zhanghe3z.github.io/FLARE/"},{"id":"http://arxiv.org/abs/2502.14070v1","updated":"2025-02-19T19:47:58Z","published":"2025-02-19T19:47:58Z","title":"DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image\n  Diffusion Models","summary":"  Fine-tuning text-to-image diffusion models to maximize rewards has proven\neffective for enhancing model performance. However, reward fine-tuning methods\noften suffer from slow convergence due to online sample generation. Therefore,\nobtaining diverse samples with strong reward signals is crucial for improving\nsample efficiency and overall performance. In this work, we introduce DiffExp,\na simple yet effective exploration strategy for reward fine-tuning of\ntext-to-image models. Our approach employs two key strategies: (a) dynamically\nadjusting the scale of classifier-free guidance to enhance sample diversity,\nand (b) randomly weighting phrases of the text prompt to exploit high-quality\nreward signals. We demonstrate that these strategies significantly enhance\nexploration during online sample generation, improving the sample efficiency of\nrecent reward fine-tuning methods, such as DDPO and AlignProp.\n","authors":["Daewon Chae","June Suk Choi","Jinkyu Kim","Kimin Lee"],"pdf_url":"https://arxiv.org/pdf/2502.14070v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2502.14068v1","updated":"2025-02-19T19:43:31Z","published":"2025-02-19T19:43:31Z","title":"A Racing Dataset and Baseline Model for Track Detection in Autonomous\n  Racing","summary":"  A significant challenge in racing-related research is the lack of publicly\navailable datasets containing raw images with corresponding annotations for the\ndownstream task. In this paper, we introduce RoRaTrack, a novel dataset that\ncontains annotated multi-camera image data from racing scenarios for track\ndetection. The data is collected on a Dallara AV-21 at a racing circuit in\nIndiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack\naddresses common problems such as blurriness due to high speed, color inversion\nfrom the camera, and absence of lane markings on the track. Consequently, we\npropose RaceGAN, a baseline model based on a Generative Adversarial Network\n(GAN) that effectively addresses these challenges. The proposed model\ndemonstrates superior performance compared to current state-of-the-art machine\nlearning models in track detection. The dataset and code for this work are\navailable at github.com/RaceGAN.\n","authors":["Shreya Ghosh","Yi-Huan Chen","Ching-Hsiang Huang","Abu Shafin Mohammad Mahdee Jameel","Chien Chou Ho","Aly El Gamal","Samuel Labi"],"pdf_url":"https://arxiv.org/pdf/2502.14068v1.pdf","comment":"Currently Under Review"},{"id":"http://arxiv.org/abs/2502.06997v2","updated":"2025-02-19T19:40:15Z","published":"2025-02-10T19:47:28Z","title":"Conditional diffusion model with spatial attention and latent embedding\n  for medical image segmentation","summary":"  Diffusion models have been used extensively for high quality image and video\ngeneration tasks. In this paper, we propose a novel conditional diffusion model\nwith spatial attention and latent embedding (cDAL) for medical image\nsegmentation. In cDAL, a convolutional neural network (CNN) based discriminator\nis used at every time-step of the diffusion process to distinguish between the\ngenerated labels and the real ones. A spatial attention map is computed based\non the features learned by the discriminator to help cDAL generate more\naccurate segmentation of discriminative regions in an input image.\nAdditionally, we incorporated a random latent embedding into each layer of our\nmodel to significantly reduce the number of training and sampling time-steps,\nthereby making it much faster than other diffusion models for image\nsegmentation. We applied cDAL on 3 publicly available medical image\nsegmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed\nsignificant qualitative and quantitative improvements with higher Dice scores\nand mIoU over the state-of-the-art algorithms. The source code is publicly\navailable at https://github.com/Hejrati/cDAL/.\n","authors":["Behzad Hejrati","Soumyanil Banerjee","Carri Glide-Hurst","Ming Dong"],"pdf_url":"https://arxiv.org/pdf/2502.06997v2.pdf","comment":"13 pages, 5 figures, 3 tables, Accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2502.14064v1","updated":"2025-02-19T19:31:52Z","published":"2025-02-19T19:31:52Z","title":"Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging","summary":"  Vision foundation models (VFMs) are pre-trained on extensive image datasets\nto learn general representations for diverse types of data. These models can\nsubsequently be fine-tuned for specific downstream tasks, significantly\nboosting performance across a broad range of applications. However, existing\nvision foundation models that claim to be applicable to various radiology tasks\nare mostly pre-trained on 3D computed tomography (CT), which benefits from the\navailability of extensive 3D CT databases. Significant differences between CT\nand magnetic resonance imaging (MRI) in imaging principles, signal\ncharacteristics, and data distribution may hinder their practical performance\nand versatility in MRI-specific applications. Here, we propose Triad, a vision\nfoundation model for 3D MRI. Triad adopts a widely used autoencoder\narchitecture to learn robust representations from 131,170 3D MRI volumes and\nuses organ-independent imaging descriptions to constrain the semantic\ndistribution of the visual modality. The above pre-training dataset is called\nTriad-131K, which is currently the largest 3D MRI pre-training dataset. We\nevaluate Triad across three tasks, namely, organ/tumor segmentation,\norgan/cancer classification, and medical image registration, in two data\nmodalities (within-domain and out-of-domain) settings using 25 downstream\ndatasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad\nimproves segmentation performance by 6.88% compared to nnUNet-Scratch across 17\ndatasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in\nclassification tasks across five datasets. SwinUNETR-Triad improves by 4.00%\ncompared to SwinUNETR-Scratch in registration tasks across two datasets. Our\nstudy demonstrates that pre-training can maximize performance when the data\nmodalities and organs of upstream and downstream tasks are consistent.\n","authors":["Shansong Wang","Mojtaba Safari","Qiang Li","Chih-Wei Chang","Richard LJ Qiu","Justin Roper","David S. Yu","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.14064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14063v1","updated":"2025-02-19T19:31:51Z","published":"2025-02-19T19:31:51Z","title":"PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian\n  Detection","summary":"  Pedestrian detection in intelligent transportation systems has made\nsignificant progress but faces two critical challenges: (1) insufficient fusion\nof complementary information between visible and infrared spectra, particularly\nin complex scenarios, and (2) sensitivity to illumination changes, such as\nlow-light or overexposed conditions, leading to degraded performance. To\naddress these issues, we propose PedDet, an adaptive spectral optimization\ncomplementarity framework specifically enhanced and optimized for multispectral\npedestrian detection. PedDet introduces the Multi-scale Spectral Feature\nPerception Module (MSFPM) to adaptively fuse visible and infrared features,\nenhancing robustness and flexibility in feature extraction. Additionally, the\nIllumination Robustness Feature Decoupling Module (IRFDM) improves detection\nstability under varying lighting by decoupling pedestrian and background\nfeatures. We further design a contrastive alignment to enhance intermodal\nfeature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that\nPedDet achieves state-of-the-art performance, improving the mAP by 6.6% with\nsuperior detection accuracy even in low-light conditions, marking a significant\nstep forward for road safety. Code will be available at\nhttps://github.com/AIGeeksGroup/PedDet.\n","authors":["Rui Zhao","Zeyu Zhang","Yi Xu","Yi Yao","Yan Huang","Wenxin Zhang","Zirui Song","Xiuying Chen","Yang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.14063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14061v1","updated":"2025-02-19T19:21:23Z","published":"2025-02-19T19:21:23Z","title":"EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation","summary":"  In industrial applications requiring real-time feedback, such as quality\ncontrol and robotic manipulation, the demand for high-speed and accurate pose\nestimation remains critical. Despite advances improving speed and accuracy in\npose estimation, finding a balance between computational efficiency and\naccuracy poses significant challenges in dynamic environments. Most current\nalgorithms lack scalability in estimation time, especially for diverse\ndatasets, and the state-of-the-art (SOTA) methods are often too slow. This\nstudy focuses on developing a fast and scalable set of pose estimators based on\nGDRNPP to meet or exceed current benchmarks in accuracy and robustness,\nparticularly addressing the efficiency-accuracy trade-off essential in\nreal-time scenarios. We propose the AMIS algorithm to tailor the utilized model\naccording to an application-specific trade-off between inference time and\naccuracy. We further show the effectiveness of the AMIS-based model choice on\nfour prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD).\n","authors":["Zixuan Fang","Thomas Pöllabauer","Tristan Wirth","Sarah Berkei","Volker Knauthe","Arjan Kuijper"],"pdf_url":"https://arxiv.org/pdf/2502.14061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14044v1","updated":"2025-02-19T19:05:45Z","published":"2025-02-19T19:05:45Z","title":"Enhancing Cognition and Explainability of Multimodal Foundation Models\n  with Self-Synthesized Data","summary":"  Large multimodal models (LMMs) have shown impressive capabilities in a wide\nrange of visual tasks. However, they often struggle with fine-grained visual\nreasoning, failing to identify domain-specific objectives and provide\njustifiable explanations for their predictions. To address this, we propose a\nnovel visual rejection sampling framework to improve the cognition and\nexplainability of LMMs using self-synthesized data. Specifically, visual\nfine-tuning requires images, queries, and target answers. Our approach begins\nby synthesizing interpretable answers that include human-verifiable visual\nfeatures. These features are based on expert-defined concepts, carefully\nselected based on their alignment with the image content. After each round of\nfine-tuning, we apply a reward model-free filtering mechanism to select the\nhighest-quality interpretable answers for the next round of tuning. This\niterative process of data synthesis and fine-tuning progressively improves the\nmodel's ability to generate accurate and reasonable explanations. Experimental\nresults demonstrate the effectiveness of our method in improving both the\naccuracy and explainability of specialized visual classification tasks.\n","authors":["Yucheng Shi","Quanzheng Li","Jin Sun","Xiang Li","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.14044v1.pdf","comment":"Accepted by ICLR 2025. Code: https://github.com/sycny/SelfSynthX"},{"id":"http://arxiv.org/abs/2309.12865v4","updated":"2025-02-19T18:59:01Z","published":"2023-09-22T13:39:24Z","title":"Bridging Sensor Gaps via Attention Gated Tuning for Hyperspectral Image\n  Classification","summary":"  Data-hungry HSI classification methods require high-quality labeled HSIs,\nwhich are often costly to obtain. This characteristic limits the performance\npotential of data-driven methods when dealing with limited annotated samples.\nBridging the domain gap between data acquired from different sensors allows us\nto utilize abundant labeled data across sensors to break this bottleneck. In\nthis paper, we propose a novel Attention-Gated Tuning (AGT) strategy and a\ntriplet-structured transformer model, Tri-Former, to address this issue. The\nAGT strategy serves as a bridge, allowing us to leverage existing labeled HSI\ndatasets, even RGB datasets to enhance the performance on new HSI datasets with\nlimited samples. Instead of inserting additional parameters inside the basic\nmodel, we train a lightweight auxiliary branch that takes intermediate features\nas input from the basic model and makes predictions. The proposed AGT resolves\nconflicts between heterogeneous and even cross-modal data by suppressing the\ndisturbing information and enhances the useful information through a soft gate.\nAdditionally, we introduce Tri-Former, a triplet-structured transformer with a\nspectral-spatial separation design that enhances parameter utilization and\ncomputational efficiency, enabling easier and flexible fine-tuning. Comparison\nexperiments conducted on three representative HSI datasets captured by\ndifferent sensors demonstrate the proposed Tri-Former achieves better\nperformance compared to several state-of-the-art methods. Homologous,\nheterologous and cross-modal tuning experiments verified the effectiveness of\nthe proposed AGT. Code has been released at:\n\\href{https://github.com/Cecilia-xue/AGT}{https://github.com/Cecilia-xue/AGT}.\n","authors":["Xizhe Xue","Haokui Zhang","Haizhao Jing","Lijie Tao","Zongwen Bai","Ying Li"],"pdf_url":"https://arxiv.org/pdf/2309.12865v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14023v1","updated":"2025-02-19T18:50:08Z","published":"2025-02-19T18:50:08Z","title":"Dynamic Activation with Knowledge Distillation for Energy-Efficient\n  Spiking NN Ensembles","summary":"  While foundation AI models excel at tasks like classification and\ndecision-making, their high energy consumption makes them unsuitable for\nenergy-constrained applications. Inspired by the brain's efficiency, spiking\nneural networks (SNNs) have emerged as a viable alternative due to their\nevent-driven nature and compatibility with neuromorphic chips. This work\nintroduces a novel system that combines knowledge distillation and ensemble\nlearning to bridge the performance gap between artificial neural networks\n(ANNs) and SNNs. A foundation AI model acts as a teacher network, guiding\nsmaller student SNNs organized into an ensemble, called Spiking Neural Ensemble\n(SNE). SNE enables the disentanglement of the teacher's knowledge, allowing\neach student to specialize in predicting a distinct aspect of it, while\nprocessing the same input. The core innovation of SNE is the adaptive\nactivation of a subset of SNN models of an ensemble, leveraging\nknowledge-distillation, enhanced with an informed-partitioning\n(disentanglement) of the teacher's feature space. By dynamically activating\nonly a subset of these student SNNs, the system balances accuracy and energy\nefficiency, achieving substantial energy savings with minimal accuracy loss.\nMoreover, SNE is significantly more efficient than the teacher network,\nreducing computational requirements by up to 20x with only a 2% drop in\naccuracy on the CIFAR-10 dataset. This disentanglement procedure achieves an\naccuracy improvement of up to 2.4% on the CIFAR-10 dataset compared to other\npartitioning schemes. Finally, we comparatively analyze SNE performance under\nnoisy conditions, demonstrating enhanced robustness compared to its ANN\nteacher. In summary, SNE offers a promising new direction for\nenergy-constrained applications.\n","authors":["Orestis Konstantaropoulos","Theodoris Mallios","Maria Papadopouli"],"pdf_url":"https://arxiv.org/pdf/2502.14023v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07160v2","updated":"2025-02-19T03:43:57Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.06616v2","updated":"2025-02-19T17:36:41Z","published":"2025-02-10T16:12:47Z","title":"From Code to Canvas","summary":"  The web-based dynamic geometry software CindyJS is a versatile tool to create\ninteractive applications for mathematics and other topics. In this workshop, we\nwill look at a code package that makes the creation of animations in CindyJS\neasier and more streamlined. Animations, which can then be embedded into\npresentations or be used in (lecture) videos. The focus lies on the creation of\nthe animations themselves and some of the technical and artistic fundamentals\nto do so.\n","authors":["Bernhard O. Werner"],"pdf_url":"https://arxiv.org/pdf/2502.06616v2.pdf","comment":"A workshop paper for the Bridges 2025 conference"},{"id":"http://arxiv.org/abs/2501.08514v2","updated":"2025-02-19T14:02:50Z","published":"2025-01-15T01:52:54Z","title":"Multimodal Fake News Video Explanation Generation: Dataset, Model, and\n  Evaluation","summary":"  Although existing methods have addressed fake news video detection as a\nclassification problem, it is not clear why certain news content is identified\nas fake. Without proper explanation, end users may not be able to understand\nthe potential meaning of fake news. Therefore, we propose a novel task, Fake\nNews Video Explanation (FNVE), to generate natural language explanations that\nreveal the falseness of news videos. To this end, we first developed ONVE and\nVTSE, two new datasets to explain fake news video posts. Then, we propose a\nMultimodal Relation Graph Transformer (MRGT) model to benchmark ONVE and VTSE.\nMRGT introduces a multimodal relation graph to comprehensively represent\nmultimodal relations and then introduces a BART-based decoder to explain\ngenerations. The experimental results show that the proposed MRGT outperforms\nthe strong baselines. In addition, the human evaluation on the annotated ONVE\nand VTSE also achieves high scores in terms of adequacy rating.\n","authors":["Lizhi Chen","Zhong Qian","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18552v3","updated":"2025-02-19T16:29:24Z","published":"2024-07-26T07:05:04Z","title":"Multimodal Emotion Recognition using Audio-Video Transformer Fusion with\n  Cross Attention","summary":"  Understanding emotions is a fundamental aspect of human communication.\nIntegrating audio and video signals offers a more comprehensive understanding\nof emotional states compared to traditional methods that rely on a single data\nsource, such as speech or facial expressions. Despite its potential, multimodal\nemotion recognition faces significant challenges, particularly in\nsynchronization, feature extraction, and fusion of diverse data sources. To\naddress these issues, this paper introduces a novel transformer-based model\nnamed Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA\nmodel employs a transformer fusion approach to effectively capture and\nsynchronize interlinked features from both audio and video inputs, thereby\nresolving synchronization problems. Additionally, the Cross Attention mechanism\nwithin AVT-CA selectively extracts and emphasizes critical features while\ndiscarding irrelevant ones from both modalities, addressing feature extraction\nand fusion challenges. Extensive experimental analysis conducted on the\nCMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the\nproposed model. The results underscore the importance of AVT-CA in developing\nprecise and reliable multimodal emotion recognition systems for practical\napplications.\n","authors":["Joe Dhanith P R","Shravan Venkatraman","Vigya Sharma","Santhosh Malarvannan","Modigari Narendra"],"pdf_url":"https://arxiv.org/pdf/2407.18552v3.pdf","comment":"38 Pages, 9 Tables, 12 Figures"},{"id":"http://arxiv.org/abs/2502.13637v1","updated":"2025-02-19T11:24:45Z","published":"2025-02-19T11:24:45Z","title":"Exploring Mutual Cross-Modal Attention for Context-Aware Human\n  Affordance Generation","summary":"  Human affordance learning investigates contextually relevant novel pose\nprediction such that the estimated pose represents a valid human action within\nthe scene. While the task is fundamental to machine perception and automated\ninteractive navigation agents, the exponentially large number of probable pose\nand action variations make the problem challenging and non-trivial. However,\nthe existing datasets and methods for human affordance prediction in 2D scenes\nare significantly limited in the literature. In this paper, we propose a novel\ncross-attention mechanism to encode the scene context for affordance prediction\nby mutually attending spatial feature maps from two different modalities. The\nproposed method is disentangled among individual subtasks to efficiently reduce\nthe problem complexity. First, we sample a probable location for a person\nwithin the scene using a variational autoencoder (VAE) conditioned on the\nglobal scene context encoding. Next, we predict a potential pose template from\na set of existing human pose candidates using a classifier on the local context\nencoding around the predicted location. In the subsequent steps, we use two\nVAEs to sample the scale and deformation parameters for the predicted pose\ntemplate by conditioning on the local context and template class. Our\nexperiments show significant improvements over the previous baseline of human\naffordance injection into complex 2D scenes.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2502.13637v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.17847v2","updated":"2025-02-19T03:05:56Z","published":"2024-12-19T01:30:19Z","title":"Bridging the Data Provenance Gap Across Text, Speech and Video","summary":"  Progress in AI is driven largely by the scale and quality of training data.\nDespite this, there is a deficit of empirical analysis examining the attributes\nof well-established datasets beyond text. In this work we conduct the largest\nand first-of-its-kind longitudinal audit across modalities--popular text,\nspeech, and video datasets--from their detailed sourcing trends and use\nrestrictions to their geographical and linguistic representation. Our manual\nanalysis covers nearly 4000 public datasets between 1990-2024, spanning 608\nlanguages, 798 sources, 659 organizations, and 67 countries. We find that\nmultimodal machine learning applications have overwhelmingly turned to\nweb-crawled, synthetic, and social media platforms, such as YouTube, for their\ntraining sets, eclipsing all other sources since 2019. Secondly, tracing the\nchain of dataset derivations we find that while less than 33% of datasets are\nrestrictively licensed, over 80% of the source content in widely-used text,\nspeech, and video datasets, carry non-commercial restrictions. Finally, counter\nto the rising number of languages and geographies represented in public AI\ntraining datasets, our audit demonstrates measures of relative geographical and\nmultilingual representation have failed to significantly improve their coverage\nsince 2013. We believe the breadth of our audit enables us to empirically\nexamine trends in data sourcing, restrictions, and Western-centricity at an\necosystem-level, and that visibility into these questions are essential to\nprogress in responsible AI. As a contribution to ongoing improvements in\ndataset transparency and responsible use, we release our entire multimodal\naudit, allowing practitioners to trace data provenance across text, speech, and\nvideo.\n","authors":["Shayne Longpre","Nikhil Singh","Manuel Cherep","Kushagra Tiwary","Joanna Materzynska","William Brannon","Robert Mahari","Naana Obeng-Marnu","Manan Dey","Mohammed Hamdy","Nayan Saxena","Ahmad Mustafa Anis","Emad A. Alghamdi","Vu Minh Chien","Da Yin","Kun Qian","Yizhi Li","Minnie Liang","An Dinh","Shrestha Mohanty","Deividas Mataciunas","Tobin South","Jianguo Zhang","Ariel N. Lee","Campbell S. Lund","Christopher Klamm","Damien Sileo","Diganta Misra","Enrico Shippole","Kevin Klyman","Lester JV Miranda","Niklas Muennighoff","Seonghyeon Ye","Seungone Kim","Vipul Gupta","Vivek Sharma","Xuhui Zhou","Caiming Xiong","Luis Villa","Stella Biderman","Alex Pentland","Sara Hooker","Jad Kabbara"],"pdf_url":"https://arxiv.org/pdf/2412.17847v2.pdf","comment":"ICLR 2025. 10 pages, 5 figures (main paper)"},{"id":"http://arxiv.org/abs/2502.13352v1","updated":"2025-02-19T01:19:52Z","published":"2025-02-19T01:19:52Z","title":"Integrated Sensing and Communication for 6G Holographic Digital Twins","summary":"  With the advent of 6G networks, offering ultra-high bandwidth and ultra-low\nlatency, coupled with the enhancement of terminal device resolutions,\nholographic communication is gradually becoming a reality. Holographic digital\ntwin (HDT) is considered one of key applications of holographic communication,\ncapable of creating virtual replicas for real-time mapping and prediction of\nphysical entity states, and performing three-dimensional reproduction of\nspatial information. In this context, integrated sensing and communication\n(ISAC) is expected to be a crucial pathway for providing data sources to HDT.\nThis paper proposes a four-layer architecture assisted by ISAC for HDT,\nintegrating emerging paradigms and key technologies to achieve low-cost,\nhigh-precision environmental data collection for constructing HDT.\nSpecifically, to enhance sensing resolution, we explore super-resolution\ntechniques from the perspectives of parameter estimation and point cloud\nconstruction. Additionally, we focus on multi-point collaborative sensing for\nconstructing HDT, and provide a comprehensive review of four key techniques:\nnode selection, multi-band collaboration, cooperative beamforming, and data\nfusion. Finally, we highlight several interesting research directions to guide\nand inspire future work.\n","authors":["Haijun Zhang","Ziyang Zhang","Xiangnan Liu","Wei Li","Haojin Li","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2502.13352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14007v1","updated":"2025-02-19T11:54:45Z","published":"2025-02-19T11:54:45Z","title":"d-Sketch: Improving Visual Fidelity of Sketch-to-Image Translation with\n  Pretrained Latent Diffusion Models without Retraining","summary":"  Structural guidance in an image-to-image translation allows intricate control\nover the shapes of synthesized images. Generating high-quality realistic images\nfrom user-specified rough hand-drawn sketches is one such task that aims to\nimpose a structural constraint on the conditional generation process. While the\npremise is intriguing for numerous use cases of content creation and academic\nresearch, the problem becomes fundamentally challenging due to substantial\nambiguities in freehand sketches. Furthermore, balancing the trade-off between\nshape consistency and realistic generation contributes to additional complexity\nin the process. Existing approaches based on Generative Adversarial Networks\n(GANs) generally utilize conditional GANs or GAN inversions, often requiring\napplication-specific data and optimization objectives. The recent introduction\nof Denoising Diffusion Probabilistic Models (DDPMs) achieves a generational\nleap for low-level visual attributes in general image synthesis. However,\ndirectly retraining a large-scale diffusion model on a domain-specific subtask\nis often extremely difficult due to demanding computation costs and\ninsufficient data. In this paper, we introduce a technique for sketch-to-image\ntranslation by exploiting the feature generalization capabilities of a\nlarge-scale diffusion model without retraining. In particular, we use a\nlearnable lightweight mapping network to achieve latent feature translation\nfrom source to target domain. Experimental results demonstrate that the\nproposed method outperforms the existing techniques in qualitative and\nquantitative benchmarks, allowing high-resolution realistic image synthesis\nfrom rough hand-drawn sketches.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2502.14007v1.pdf","comment":"Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.13912v1","updated":"2025-02-19T17:44:13Z","published":"2025-02-19T17:44:13Z","title":"Optimizing Research Portfolio For Semantic Impact","summary":"  Citation metrics are widely used to assess academic impact but suffer from\nsocial biases, including institutional prestige and journal visibility. Here we\nintroduce rXiv Semantic Impact (XSI), a novel framework that predicts research\nimpact by analyzing how scientific semantic graphs evolve in underlying fabric\nof science. Rather than counting citations, XSI tracks the evolution of\nresearch concepts in the academic knowledge graph (KG). Starting with a\nconstruction of a comprehensive KG from 324K biomedical publications\n(2003-2025), we demonstrate that XSI can predict a paper's future semantic\nimpact (SI) with remarkable accuracy ($R^2$ = 0.69) three years in advance. We\nleverage these predictions to develop an optimization framework for research\nportfolio selection that systematically outperforms random allocation. We\npropose SI as a complementary metric to citations and present XSI as a tool to\nguide funding and publishing decisions, enhancing research impact while\nmitigating risk.\n","authors":["Alexander V. Belikov"],"pdf_url":"https://arxiv.org/pdf/2502.13912v1.pdf","comment":"24 pages; 13 figures"},{"id":"http://arxiv.org/abs/2502.13909v1","updated":"2025-02-19T17:41:09Z","published":"2025-02-19T17:41:09Z","title":"Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?","summary":"  Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.\n","authors":["Sein Kim","Hongseok Kang","Kibum Kim","Jiwan Kim","Donghyun Kim","Minchul Yang","Kwangjin Oh","Julian McAuley","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2502.13909v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.13908v1","updated":"2025-02-19T17:40:32Z","published":"2025-02-19T17:40:32Z","title":"Judging the Judges: A Collection of LLM-Generated Relevance Judgements","summary":"  Using Large Language Models (LLMs) for relevance assessments offers promising\nopportunities to improve Information Retrieval (IR), Natural Language\nProcessing (NLP), and related fields. Indeed, LLMs hold the promise of allowing\nIR experimenters to build evaluation collections with a fraction of the manual\nhuman labor currently required. This could help with fresh topics on which\nthere is still limited knowledge and could mitigate the challenges of\nevaluating ranking systems in low-resource scenarios, where it is challenging\nto find human annotators. Given the fast-paced recent developments in the\ndomain, many questions concerning LLMs as assessors are yet to be answered.\nAmong the aspects that require further investigation, we can list the impact of\nvarious components in a relevance judgment generation pipeline, such as the\nprompt used or the LLM chosen.\n  This paper benchmarks and reports on the results of a large-scale automatic\nrelevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where\ndifferent relevance assessment approaches were proposed. In detail, we release\nand benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track\nrelevance judgments produced by eight international teams who participated in\nthe challenge. Given their diverse nature, these automatically generated\nrelevance judgments can help the community not only investigate systematic\nbiases caused by LLMs but also explore the effectiveness of ensemble models,\nanalyze the trade-offs between different models and human assessors, and\nadvance methodologies for improving automated evaluation techniques. The\nreleased resource is available at the following link:\nhttps://llm4eval.github.io/LLMJudge-benchmark/\n","authors":["Hossein A. Rahmani","Clemencia Siro","Mohammad Aliannejadi","Nick Craswell","Charles L. A. Clarke","Guglielmo Faggioli","Bhaskar Mitra","Paul Thomas","Emine Yilmaz"],"pdf_url":"https://arxiv.org/pdf/2502.13908v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2408.10604v2","updated":"2025-02-19T17:25:39Z","published":"2024-08-20T07:37:06Z","title":"Multilingual Non-Factoid Question Answering with Answer Paragraph\n  Selection","summary":"  Most existing Question Answering Datasets (QuADs) primarily focus on\nfactoid-based short-context Question Answering (QA) in high-resource languages.\nHowever, the scope of such datasets for low-resource languages remains limited,\nwith only a few works centered on factoid-based QuADs and none on non-factoid\nQuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with\nnon-factoid questions. It utilizes interrogative sub-headings from BBC news\narticles as questions and the corresponding paragraphs as silver answers. The\ndataset comprises over 578K QA pairs across 38 languages, encompassing several\nlow-resource languages, and stands as the largest multilingual QA dataset to\ndate. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden\nset), we observe that 98\\% of questions can be answered using their\ncorresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS)\nmodel outperforms the baselines. The APS model attained an accuracy of 80\\% and\n72\\%, as well as a macro F1 of 72\\% and 66\\%, on the MuNfQuAD testset and the\ngolden set, respectively. Furthermore, the APS model effectively generalizes a\ncertain language within the golden set, even after being fine-tuned on silver\nlabels. We also observe that the fine-tuned APS model is beneficial for\nreducing the context of a question. These findings suggest that this resource\nwould be a valuable contribution to the QA research community.\n","authors":["Ritwik Mishra","Sreeram Vennam","Rajiv Ratn Shah","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2408.10604v2.pdf","comment":"Shorter version accepted into DSFA, a special session in PAKDD 2025,\n  Sydney"},{"id":"http://arxiv.org/abs/2502.13881v1","updated":"2025-02-19T17:05:42Z","published":"2025-02-19T17:05:42Z","title":"PSCon: Toward Conversational Product Search","summary":"  Conversational Product Search (CPS) is confined to simulated conversations\ndue to the lack of real-world CPS datasets that reflect human-like language.\nAdditionally, current conversational datasets are limited to support\ncross-market and multi-lingual usage. In this paper, we introduce a new CPS\ndata collection protocol and present PSCon, a novel CPS dataset designed to\nassist product search via human-like conversations. The dataset is constructed\nusing a coached human-to-human data collection protocol and supports two\nlanguages and dual markets. Also, the dataset enables thorough exploration of\nsix subtasks of CPS: user intent detection, keyword extraction, system action\nprediction, question selection, item ranking, and response generation.\nFurthermore, we also offer an analysis of the dataset and propose a benchmark\nmodel on the proposed CPS dataset.\n","authors":["Jie Zou","Mohammad Aliannejadi","Evangelos Kanoulas","Shuxi Han","Heli Ma","Zheng Wang","Yang Yang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2502.13881v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2501.19241v4","updated":"2025-02-19T16:41:06Z","published":"2025-01-31T15:55:14Z","title":"Emancipatory Information Retrieval","summary":"  Our world today is facing a confluence of several mutually reinforcing crises\neach of which intersects with concerns of social justice and emancipation. This\npaper is a provocation for the role of computer-mediated information access in\nour emancipatory struggles. We define emancipatory information retrieval as the\nstudy and development of information access methods that challenge various\nforms of human oppression, and situates its activities within broader\ncollective emancipatory praxis. The term \"emancipatory\" here signifies the\nmoral concerns of universal humanization of all peoples and the elimination of\noppression to create the conditions under which we can collectively flourish.\nTo develop an emancipatory research agenda for information retrieval (IR), in\nthis paper we speculate about the practices that the community can adopt,\nenumerate some of the projects that the field should undertake, and discuss\nprovocations to spark new ideas and directions for research. We challenge the\nfield of IR research to embrace humanistic values and commit to universal\nemancipation and social justice. We also invite scholars from fields such as\nhuman-computer interaction, information sciences, media studies, design, social\nsciences, humanities, democratic theory, and critical theory, as well as legal\nand policy experts, civil rights and social justice activists, and artists to\njoin us in realizing this transformation. In this process, we must both imagine\npost-oppressive worlds, and reimagine the role of IR in that world and in the\njourney that leads us there.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2501.19241v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09713v3","updated":"2025-02-19T16:24:30Z","published":"2024-10-13T03:45:24Z","title":"Agentic Information Retrieval","summary":"  Since the 1970s, information retrieval (IR) has long been defined as the\nprocess of acquiring relevant information items from a pre-defined corpus to\nsatisfy user information needs. Traditional IR systems, while effective in\ndomains like web search, are constrained by their reliance on static,\npre-defined information items. To this end, this paper introduces agentic\ninformation retrieval (Agentic IR), a transformative next-generation paradigm\nfor IR driven by large language models (LLMs) and AI agents. The central shift\nin agentic IR is the evolving definition of ``information'' from static,\npre-defined information items to dynamic, context-dependent information states.\nInformation state refers to a particular information context that the user is\nright in within a dynamic environment, encompassing not only the acquired\ninformation items but also real-time user preferences, contextual factors, and\ndecision-making processes. In such a way, traditional information retrieval,\nfocused on acquiring relevant information items based on user queries, can be\nnaturally extended to achieving the target information state given the user\ninstruction, which thereby defines the agentic information retrieval. We\nsystematically discuss agentic IR from various aspects, i.e., task formulation,\narchitecture, evaluation, case studies, as well as challenges and future\nprospects. We believe that the concept of agentic IR introduced in this paper\nnot only broadens the scope of information retrieval research but also lays the\nfoundation for a more adaptive, interactive, and intelligent next-generation IR\nparadigm.\n","authors":["Weinan Zhang","Junwei Liao","Ning Li","Kounianhua Du","Jianghao Lin"],"pdf_url":"https://arxiv.org/pdf/2410.09713v3.pdf","comment":"11 pages, perspective paper"},{"id":"http://arxiv.org/abs/2502.13845v1","updated":"2025-02-19T16:08:17Z","published":"2025-02-19T16:08:17Z","title":"Enhancing LLM-Based Recommendations Through Personalized Reasoning","summary":"  Current recommendation systems powered by large language models (LLMs) often\nunderutilize their reasoning capabilities due to a lack of explicit logical\nstructuring. To address this limitation, we introduce CoT-Rec, a framework that\nintegrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by\nincorporating two crucial processes: user preference analysis and item\nperception evaluation. CoT-Rec operates in two key phases: (1) personalized\ndata extraction, where user preferences and item perceptions are identified,\nand (2) personalized data application, where this information is leveraged to\nrefine recommendations. Our experimental analysis demonstrates that CoT-Rec\nimproves recommendation accuracy by making better use of LLMs' reasoning\npotential. The implementation is publicly available at\nhttps://anonymous.4open.science/r/CoT-Rec.\n","authors":["Jiahao Liu","Xueshuo Yan","Dongsheng Li","Guangping Zhang","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13845v1.pdf","comment":"7 pages, under review"},{"id":"http://arxiv.org/abs/2502.13843v1","updated":"2025-02-19T16:02:59Z","published":"2025-02-19T16:02:59Z","title":"Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based\n  User Agents","summary":"  Large Language Model (LLM)-based user agents have emerged as a powerful tool\nfor improving recommender systems by simulating user interactions. However,\nexisting methods struggle with cross-domain scenarios due to inefficient memory\nstructures, leading to irrelevant information retention and failure to account\nfor social influence factors such as popularity. To address these limitations,\nwe introduce AgentCF++, a novel framework featuring a dual-layer memory\narchitecture and a two-step fusion mechanism to filter domain-specific\npreferences effectively. Additionally, we propose interest groups with shared\nmemory, allowing the model to capture the impact of popularity trends on users\nwith similar interests. Through extensive experiments on multiple cross-domain\ndatasets, AgentCF++ demonstrates superior performance over baseline models,\nhighlighting its effectiveness in refining user behavior simulation for\nrecommender systems. Our code is available at\nhttps://anonymous.4open.science/r/AgentCF-plus.\n","authors":["Jiahao Liu","Shengkang Gu","Dongsheng Li","Guangping Zhang","Mingzhe Han","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13843v1.pdf","comment":"6 pages, under review"},{"id":"http://arxiv.org/abs/2502.13840v1","updated":"2025-02-19T15:59:49Z","published":"2025-02-19T15:59:49Z","title":"Mitigating Popularity Bias in Collaborative Filtering through Fair\n  Sampling","summary":"  Recommender systems often suffer from popularity bias, where frequently\ninteracted items are overrepresented in recommendations. This bias stems from\npropensity factors influencing training data, leading to imbalanced exposure.\nIn this paper, we introduce a Fair Sampling (FS) approach to address this issue\nby ensuring that both users and items are selected with equal probability as\npositive and negative instances. Unlike traditional inverse propensity score\n(IPS) methods, FS does not require propensity estimation, eliminating errors\nassociated with inaccurate calculations. Our theoretical analysis demonstrates\nthat FS effectively neutralizes the influence of propensity factors, achieving\nunbiased learning. Experimental results validate that FS outperforms\nstate-of-the-art methods in both point-wise and pair-wise recommendation tasks,\nenhancing recommendation fairness without sacrificing accuracy. The\nimplementation is available at https://anonymous.4open.science/r/Fair-Sampling.\n","authors":["Jiahao Liu","Dongsheng Li","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13840v1.pdf","comment":"6 pages, under review"},{"id":"http://arxiv.org/abs/2502.13826v1","updated":"2025-02-19T15:41:08Z","published":"2025-02-19T15:41:08Z","title":"In-Place Updates of a Graph Index for Streaming Approximate Nearest\n  Neighbor Search","summary":"  Indices for approximate nearest neighbor search (ANNS) are a basic component\nfor information retrieval and widely used in database, search, recommendation\nand RAG systems. In these scenarios, documents or other objects are inserted\ninto and deleted from the working set at a high rate, requiring a stream of\nupdates to the vector index. Algorithms based on proximity graph indices are\nthe most efficient indices for ANNS, winning many benchmark competitions.\nHowever, it is challenging to update such graph index at a high rate, while\nsupporting stable recall after many updates. Since the graph is singly-linked,\ndeletions are hard because there is no fast way to find in-neighbors of a\ndeleted vertex. Therefore, to update the graph, state-of-the-art algorithms\nsuch as FreshDiskANN accumulate deletions in a batch and periodically\nconsolidate, removing edges to deleted vertices and modifying the graph to\nensure recall stability. In this paper, we present IP-DiskANN\n(InPlaceUpdate-DiskANN), the first algorithm to avoid batch consolidation by\nefficiently processing each insertion and deletion in-place. Our experiments\nusing standard benchmarks show that IP-DiskANN has stable recall over various\nlengthy update patterns in both high-recall and low-recall regimes. Further,\nits query throughput and update speed are better than using the batch\nconsolidation algorithm and HNSW.\n","authors":["Haike Xu","Magdalen Dobson Manohar","Philip A. Bernstein","Badrish Chandramouli","Richard Wen","Harsha Vardhan Simhadri"],"pdf_url":"https://arxiv.org/pdf/2502.13826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13783v1","updated":"2025-02-19T14:48:25Z","published":"2025-02-19T14:48:25Z","title":"Generative Large Recommendation Models: Emerging Trends in LLMs for\n  Recommendation","summary":"  In the era of information overload, recommendation systems play a pivotal\nrole in filtering data and delivering personalized content. Recent advancements\nin feature interaction and user behavior modeling have significantly enhanced\nthe recall and ranking processes of these systems. With the rise of large\nlanguage models (LLMs), new opportunities have emerged to further improve\nrecommendation systems. This tutorial explores two primary approaches for\nintegrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning\ncapabilities of general LLMs, and generative large recommendation models, which\nfocus on scaling and sophistication. While the former has been extensively\ncovered in existing literature, the latter remains underexplored. This tutorial\naims to fill this gap by providing a comprehensive overview of generative large\nrecommendation models, including their recent advancements, challenges, and\npotential research directions. Key topics include data quality, scaling laws,\nuser behavior mining, and efficiency in training and inference. By engaging\nwith this tutorial, participants will gain insights into the latest\ndevelopments and future opportunities in the field, aiding both academic\nresearch and practical applications. The timely nature of this exploration\nsupports the rapid evolution of recommendation systems, offering valuable\nguidance for researchers and practitioners alike.\n","authors":["Hao Wang","Wei Guo","Luankang Zhang","Jin Yao Chin","Yufei Ye","Huifeng Guo","Yong Liu","Defu Lian","Ruiming Tang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.13783v1.pdf","comment":"This paper has been accepted for the tutorial track at WWW 2025"},{"id":"http://arxiv.org/abs/2502.13763v1","updated":"2025-02-19T14:23:18Z","published":"2025-02-19T14:23:18Z","title":"Unsupervised Graph Embeddings for Session-based Recommendation with Item\n  Features","summary":"  In session-based recommender systems, predictions are based on the user's\npreceding behavior in the session. State-of-the-art sequential recommendation\nalgorithms either use graph neural networks to model sessions in a graph or\nleverage the similarity of sessions by exploiting item features. In this paper,\nwe combine these two approaches and propose a novel method, Graph Convolutional\nNetwork Extension (GCNext), which incorporates item features directly into the\ngraph representation via graph convolutional networks. GCNext creates a\nfeature-rich item co-occurrence graph and learns the corresponding item\nembeddings in an unsupervised manner. We show on three datasets that\nintegrating GCNext into sequential recommendation algorithms significantly\nboosts the performance of nearest-neighbor methods as well as neural network\nmodels. Our flexible extension is easy to incorporate in state-of-the-art\nmethods and increases the MRR@20 by up to 12.79%.\n","authors":["Andreas Peintner","Marta Moscati","Emilia Parada-Cabaleiro","Markus Schedl","Eva Zangerle"],"pdf_url":"https://arxiv.org/pdf/2502.13763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03365v3","updated":"2025-02-19T14:15:17Z","published":"2024-01-31T11:03:58Z","title":"Heterophily-Aware Fair Recommendation using Graph Convolutional Networks","summary":"  In recent years, graph neural networks (GNNs) have become a popular tool to\nimprove the accuracy and performance of recommender systems. Modern recommender\nsystems are not only designed to serve end users, but also to benefit other\nparticipants, such as items and item providers. These participants may have\ndifferent or conflicting goals and interests, which raises the need for\nfairness and popularity bias considerations. GNN-based recommendation methods\nalso face the challenges of unfairness and popularity bias, and their\nnormalization and aggregation processes suffer from these challenges. In this\npaper, we propose a fair GNN-based recommender system, called HetroFair, to\nimprove item-side fairness. HetroFair uses two separate components to generate\nfairness-aware embeddings: i) Fairness-aware attention, which incorporates the\ndot product in the normalization process of GNNs to decrease the effect of\nnodes' degrees. ii) Heterophily feature weighting, to assign distinct weights\nto different features during the aggregation process. To evaluate the\neffectiveness of HetroFair, we conduct extensive experiments over six\nreal-world datasets. Our experimental results reveal that HetroFair not only\nalleviates unfairness and popularity bias on the item side but also achieves\nsuperior accuracy on the user side. Our implementation is publicly available at\nhttps://github.com/NematGH/HetroFair.\n","authors":["Nemat Gholinejad","Mostafa Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2402.03365v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2502.13719v1","updated":"2025-02-19T13:45:27Z","published":"2025-02-19T13:45:27Z","title":"TrustRAG: An Information Assistant with Retrieval Augmented Generation","summary":"  \\Ac{RAG} has emerged as a crucial technique for enhancing large models with\nreal-time and domain-specific knowledge. While numerous improvements and\nopen-source tools have been proposed to refine the \\ac{RAG} framework for\naccuracy, relatively little attention has been given to improving the\ntrustworthiness of generated results. To address this gap, we introduce\nTrustRAG, a novel framework that enhances \\ac{RAG} from three perspectives:\nindexing, retrieval, and generation. Specifically, in the indexing stage, we\npropose a semantic-enhanced chunking strategy that incorporates hierarchical\nindexing to supplement each chunk with contextual information, ensuring\nsemantic completeness. In the retrieval stage, we introduce a utility-based\nfiltering mechanism to identify high-quality information, supporting answer\ngeneration while reducing input length. In the generation stage, we propose\nfine-grained citation enhancement, which detects opinion-bearing sentences in\nresponses and infers citation relationships at the sentence-level, thereby\nimproving citation accuracy. We open-source the TrustRAG framework and provide\na demonstration studio designed for excerpt-based question answering tasks\n\\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we\naim to help researchers: 1) systematically enhancing the trustworthiness of\n\\ac{RAG} systems and (2) developing their own \\ac{RAG} systems with more\nreliable outputs.\n","authors":["Yixing Fan","Qiang Yan","Wenshan Wang","Jiafeng Guo","Ruqing Zhang","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.13719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13668v1","updated":"2025-02-19T12:24:46Z","published":"2025-02-19T12:24:46Z","title":"PeerQA: A Scientific Question Answering Dataset from Peer Reviews","summary":"  We present PeerQA, a real-world, scientific, document-level Question\nAnswering (QA) dataset. PeerQA questions have been sourced from peer reviews,\nwhich contain questions that reviewers raised while thoroughly examining the\nscientific article. Answers have been annotated by the original authors of each\npaper. The dataset contains 579 QA pairs from 208 academic articles, with a\nmajority from ML and NLP, as well as a subset of other scientific communities\nlike Geoscience and Public Health. PeerQA supports three critical tasks for\ndeveloping practical QA systems: Evidence retrieval, unanswerable question\nclassification, and answer generation. We provide a detailed analysis of the\ncollected dataset and conduct experiments establishing baseline systems for all\nthree tasks. Our experiments and analyses reveal the need for\ndecontextualization in document-level retrieval, where we find that even simple\ndecontextualization approaches consistently improve retrieval performance\nacross architectures. On answer generation, PeerQA serves as a challenging\nbenchmark for long-context modeling, as the papers have an average size of 12k\ntokens. Our code and data is available at https://github.com/UKPLab/peerqa.\n","authors":["Tim Baumgärtner","Ted Briscoe","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2502.13668v1.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2404.16130v2","updated":"2025-02-19T10:49:41Z","published":"2024-04-24T18:38:11Z","title":"From Local to Global: A Graph RAG Approach to Query-Focused\n  Summarization","summary":"  The use of retrieval-augmented generation (RAG) to retrieve relevant\ninformation from an external knowledge source enables large language models\n(LLMs) to answer questions over private and/or previously unseen document\ncollections. However, RAG fails on global questions directed at an entire text\ncorpus, such as \"What are the main themes in the dataset?\", since this is\ninherently a query-focused summarization (QFS) task, rather than an explicit\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\ntext indexed by typical RAG systems. To combine the strengths of these\ncontrasting methods, we propose GraphRAG, a graph-based approach to question\nanswering over private text corpora that scales with both the generality of\nuser questions and the quantity of source text. Our approach uses an LLM to\nbuild a graph index in two stages: first, to derive an entity knowledge graph\nfrom the source documents, then to pregenerate community summaries for all\ngroups of closely related entities. Given a question, each community summary is\nused to generate a partial response, before all partial responses are again\nsummarized in a final response to the user. For a class of global sensemaking\nquestions over datasets in the 1 million token range, we show that GraphRAG\nleads to substantial improvements over a conventional RAG baseline for both the\ncomprehensiveness and diversity of generated answers.\n","authors":["Darren Edge","Ha Trinh","Newman Cheng","Joshua Bradley","Alex Chao","Apurva Mody","Steven Truitt","Dasha Metropolitansky","Robert Osazuwa Ness","Jonathan Larson"],"pdf_url":"https://arxiv.org/pdf/2404.16130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13595v1","updated":"2025-02-19T10:13:43Z","published":"2025-02-19T10:13:43Z","title":"MMTEB: Massive Multilingual Text Embedding Benchmark","summary":"  Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.\n","authors":["Kenneth Enevoldsen","Isaac Chung","Imene Kerboua","Márton Kardos","Ashwin Mathur","David Stap","Jay Gala","Wissam Siblini","Dominik Krzemiński","Genta Indra Winata","Saba Sturua","Saiteja Utpala","Mathieu Ciancone","Marion Schaeffer","Gabriel Sequeira","Diganta Misra","Shreeya Dhakal","Jonathan Rystrøm","Roman Solomatin","Ömer Çağatan","Akash Kundu","Martin Bernstorff","Shitao Xiao","Akshita Sukhlecha","Bhavish Pahwa","Rafał Poświata","Kranthi Kiran GV","Shawon Ashraf","Daniel Auras","Björn Plüster","Jan Philipp Harries","Loïc Magne","Isabelle Mohr","Mariya Hendriksen","Dawei Zhu","Hippolyte Gisserot-Boukhlef","Tom Aarsen","Jan Kostkan","Konrad Wojtasik","Taemin Lee","Marek Šuppa","Crystina Zhang","Roberta Rocca","Mohammed Hamdy","Andrianos Michail","John Yang","Manuel Faysse","Aleksei Vatolin","Nandan Thakur","Manan Dey","Dipam Vasani","Pranjal Chitale","Simone Tedeschi","Nguyen Tai","Artem Snegirev","Michael Günther","Mengzhou Xia","Weijia Shi","Xing Han Lù","Jordan Clive","Gayatri Krishnakumar","Anna Maksimova","Silvan Wehrli","Maria Tikhonova","Henil Panchal","Aleksandr Abramov","Malte Ostendorff","Zheng Liu","Simon Clematide","Lester James Miranda","Alena Fenogenova","Guangyu Song","Ruqiya Bin Safi","Wen-Ding Li","Alessia Borghini","Federico Cassano","Hongjin Su","Jimmy Lin","Howard Yen","Lasse Hansen","Sara Hooker","Chenghao Xiao","Vaibhav Adlakha","Orion Weller","Siva Reddy","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2502.13595v1.pdf","comment":"Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV"},{"id":"http://arxiv.org/abs/2502.13581v1","updated":"2025-02-19T09:45:29Z","published":"2025-02-19T09:45:29Z","title":"ActionPiece: Contextually Tokenizing Action Sequences for Generative\n  Recommendation","summary":"  Generative recommendation (GR) is an emerging paradigm where user actions are\ntokenized into discrete token patterns and autoregressively generated as\npredictions. However, existing GR models tokenize each action independently,\nassigning the same fixed tokens to identical actions across all sequences\nwithout considering contextual relationships. This lack of context-awareness\ncan lead to suboptimal performance, as the same action may hold different\nmeanings depending on its surrounding context. To address this issue, we\npropose ActionPiece to explicitly incorporate context when tokenizing action\nsequences. In ActionPiece, each action is represented as a set of item\nfeatures, which serve as the initial tokens. Given the action sequence corpora,\nwe construct the vocabulary by merging feature patterns as new tokens, based on\ntheir co-occurrence frequency both within individual sets and across adjacent\nsets. Considering the unordered nature of feature sets, we further introduce\nset permutation regularization, which produces multiple segmentations of action\nsequences with the same semantics. Experiments on public datasets demonstrate\nthat ActionPiece consistently outperforms existing action tokenization methods,\nimproving NDCG@$10$ by $6.00\\%$ to $12.82\\%$.\n","authors":["Yupeng Hou","Jianmo Ni","Zhankui He","Noveen Sachdeva","Wang-Cheng Kang","Ed H. Chi","Julian McAuley","Derek Zhiyuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.13581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00430v6","updated":"2025-02-19T09:23:03Z","published":"2024-11-30T10:56:30Z","title":"Optimizing Sequential Recommendation Models with Scaling Laws and\n  Approximate Entropy","summary":"  Scaling Laws have emerged as a powerful framework for understanding how model\nperformance evolves as they increase in size, providing valuable insights for\noptimizing computational resources. In the realm of Sequential Recommendation\n(SR), which is pivotal for predicting users' sequential preferences, these laws\noffer a lens through which to address the challenges posed by the scalability\nof SR models. However, the presence of structural and collaborative issues in\nrecommender systems prevents the direct application of the Scaling Law (SL) in\nthese systems. In response, we introduce the Performance Law for SR models,\nwhich aims to theoretically investigate and model the relationship between\nmodel performance and data quality. Specifically, we first fit the HR and NDCG\nmetrics to transformer-based SR models. Subsequently, we propose Approximate\nEntropy (ApEn) to assess data quality, presenting a more nuanced approach\ncompared to traditional data quantity metrics. Our method enables accurate\npredictions across various dataset scales and model sizes, demonstrating a\nstrong correlation in large SR models and offering insights into achieving\noptimal performance for any given model configuration.\n","authors":["Tingjia Shen","Hao Wang","Chuhan Wu","Jin Yao Chin","Wei Guo","Yong Liu","Huifeng Guo","Defu Lian","Ruiming Tang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.00430v6.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.13539v1","updated":"2025-02-19T08:47:42Z","published":"2025-02-19T08:47:42Z","title":"Bursting Filter Bubble: Enhancing Serendipity Recommendations with\n  Aligned Large Language Models","summary":"  Recommender systems (RSs) often suffer from the feedback loop phenomenon,\ne.g., RSs are trained on data biased by their recommendations. This leads to\nthe filter bubble effect that reinforces homogeneous content and reduces user\nsatisfaction. To this end, serendipity recommendations, which offer unexpected\nyet relevant items, are proposed. Recently, large language models (LLMs) have\nshown potential in serendipity prediction due to their extensive world\nknowledge and reasoning capabilities. However, they still face challenges in\naligning serendipity judgments with human assessments, handling long user\nbehavior sequences, and meeting the latency requirements of industrial RSs. To\naddress these issues, we propose SERAL (Serendipity Recommendations with\nAligned Large Language Models), a framework comprising three stages: (1)\nCognition Profile Generation to compress user behavior into multi-level\nprofiles; (2) SerenGPT Alignment to align serendipity judgments with human\npreferences using enriched training data; and (3) Nearline Adaptation to\nintegrate SerenGPT into industrial RSs pipelines efficiently. Online\nexperiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and\ntransactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user\nexperience without much impact on overall revenue. Now, it has been fully\ndeployed in the \"Guess What You Like\" of the Taobao App homepage.\n","authors":["Yunjia Xi","Muyan Weng","Wen Chen","Chao Yi","Dian Chen","Gaoyang Guo","Mao Zhang","Jian Wu","Yuning Jiang","Qingwen Liu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13539v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.13530v1","updated":"2025-02-19T08:35:28Z","published":"2025-02-19T08:35:28Z","title":"Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential\n  Recommendation","summary":"  Traditional sequential recommendation (SR) methods heavily rely on explicit\nitem IDs to capture user preferences over time. This reliance introduces\ncritical limitations in cold-start scenarios and domain transfer tasks, where\nunseen items and new contexts often lack established ID mappings. To overcome\nthese limitations, recent studies have shifted towards leveraging text-only\ninformation for recommendation, thereby improving model generalization and\nadaptability across domains. Although promising, text-based SR faces unique\ndifficulties: items' text descriptions often share semantic similarities that\nlead to clustered item representations, compromising their uniformity, a\nproperty essential for promoting diversity and enhancing generalization in\nrecommendation systems. In this paper, we explore a novel framework to improve\nthe uniformity of item representations in text-based SR. Our analysis reveals\nthat items within a sequence exhibit marked semantic similarity, meaning they\nare closer in representation than items overall, and that this effect is more\npronounced for less popular items, which form tighter clusters compared to\ntheir more popular counterparts. Based on these findings, we propose UniT, a\nframework that employs three pairwise item sampling strategies: Unified General\nSampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven\nSampling Strategy. Each strategy applies varying degrees of repulsion to\nselectively adjust the distances between item pairs, thereby refining\nrepresentation uniformity while considering both sequence context and item\npopularity. Extensive experiments on multiple real-world datasets demonstrate\nthat our proposed approach outperforms state-of-the-art models, validating the\neffectiveness of UniT in enhancing both representation uniformity and\nrecommendation accuracy.The source code is available at\nhttps://github.com/ccwwhhh/Model-Rec.\n","authors":["Wuhan Chen","Zongwei Wang","Min Gao","Xin Xia","Feng Jiang","Junhao Wen"],"pdf_url":"https://arxiv.org/pdf/2502.13530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06610v2","updated":"2025-02-19T08:20:01Z","published":"2023-05-11T07:13:24Z","title":"Planted vertex cover problem on regular random graphs and nonmonotonic\n  temperature-dependence in the supercooled region","summary":"  We introduce a planted vertex cover problem on regular random graphs and\nstudy it by the cavity method of statistical mechanics. Different from\nconventional Ising models, the equilibrium ferromagnetic phase transition of\nthis binary-spin two-body interaction system is discontinuous, as the\nparamagnetic phase is separated from the ferromagnetic phase by an extensive\nfree energy barrier. The free energy landscape can be distinguished into three\ndifferent types depending on the two degree parameters of the planted graph.\nThe critical inverse temperatures at which the paramagnetic phase becomes\nlocally unstable towards the ferromagnetic phase ($\\beta_{\\textrm{pf}}$) and\ntowards spin glass phases ($\\beta_{\\textrm{pg}}$) satisfy $\\beta_{\\textrm{pf}}\n> \\beta_{\\textrm{pg}}$, $\\beta_{\\textrm{pf}} < \\beta_{\\textrm{pg}}$ and\n$\\beta_{\\textrm{pf}} = \\beta_{\\textrm{pg}}$, respectively, in these three\nlandscapes. A locally stable anti-ferromagnetic phase emerges in the free\nenergy landscape if $\\beta_{\\textrm{pf}} < \\beta_{\\textrm{pg}}$. When exploring\nthe free energy landscape by stochastic local search dynamics, we find that in\nagreement with our theoretical prediction, the first-passage time from the\nparamagnetic phase to the ferromagnetic phase is nonmonotonic with the inverse\ntemperature. The potential relevance of the planted vertex cover model to\nsupercooled glass-forming liquids is briefly discussed.\n","authors":["Xin-Yi Fan","Hai-Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.06610v2.pdf","comment":"Extensively revised and expanded. Changed title. A mistake in\n  numerical simulation corrected. Accepted for publication in PRE as a regular\n  article"},{"id":"http://arxiv.org/abs/2502.13506v1","updated":"2025-02-19T07:50:59Z","published":"2025-02-19T07:50:59Z","title":"Reproducing NevIR: Negation in Neural Information Retrieval","summary":"  Negation is a fundamental aspect of human communication, yet it remains a\nchallenge for Language Models (LMs) in Information Retrieval (IR). Despite the\nheavy reliance of modern neural IR systems on LMs, little attention has been\ngiven to their handling of negation. In this study, we reproduce and extend the\nfindings of NevIR, a benchmark study that revealed most IR models perform at or\nbelow the level of random ranking when dealing with negation. We replicate\nNevIR's original experiments and evaluate newly developed state-of-the-art IR\nmodels. Our findings show that a recently emerging category - listwise Large\nLanguage Model (LLM) rerankers - outperforms other models but still\nunderperforms human performance. Additionally, we leverage ExcluIR, a benchmark\ndataset designed for exclusionary queries with extensive negation, to assess\nthe generalizability of negation understanding. Our findings suggest that\nfine-tuning on one dataset does not reliably improve performance on the other,\nindicating notable differences in their data distributions. Furthermore, we\nobserve that only cross-encoders and listwise LLM rerankers achieve reasonable\nperformance across both negation tasks.\n","authors":["Coen van Elsen","Francien Barkhof","Thijmen Nijdam","Simon Lupart","Mohammad Alliannejadi"],"pdf_url":"https://arxiv.org/pdf/2502.13506v1.pdf","comment":"9 pages, 5 figures, under review at SIGIR 2025"},{"id":"http://arxiv.org/abs/2502.13481v1","updated":"2025-02-19T07:10:23Z","published":"2025-02-19T07:10:23Z","title":"LLM4Tag: Automatic Tagging System for Information Retrieval via Large\n  Language Models","summary":"  Tagging systems play an essential role in various information retrieval\napplications such as search engines and recommender systems. Recently, Large\nLanguage Models (LLMs) have been applied in tagging systems due to their\nextensive world knowledge, semantic understanding, and reasoning capabilities.\nDespite achieving remarkable performance, existing methods still have\nlimitations, including difficulties in retrieving relevant candidate tags\ncomprehensively, challenges in adapting to emerging domain-specific knowledge,\nand the lack of reliable tag confidence quantification. To address these three\nlimitations above, we propose an automatic tagging system LLM4Tag. First, a\ngraph-based tag recall module is designed to effectively and comprehensively\nconstruct a small-scale highly relevant candidate tag set. Subsequently, a\nknowledge-enhanced tag generation module is employed to generate accurate tags\nwith long-term and short-term knowledge injection. Finally, a tag confidence\ncalibration module is introduced to generate reliable tag confidence scores.\nExtensive experiments over three large-scale industrial datasets show that\nLLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag\nhas been deployed online for content tagging to serve hundreds of millions of\nusers.\n","authors":["Ruiming Tang","Chenxu Zhu","Bo Chen","Weipeng Zhang","Menghui Zhu","Xinyi Dai","Huifeng Guo"],"pdf_url":"https://arxiv.org/pdf/2502.13481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13465v1","updated":"2025-02-19T06:33:39Z","published":"2025-02-19T06:33:39Z","title":"HawkBench: Investigating Resilience of RAG Methods on Stratified\n  Information-Seeking Tasks","summary":"  In real-world information-seeking scenarios, users have dynamic and diverse\nneeds, requiring RAG systems to demonstrate adaptable resilience. To\ncomprehensively evaluate the resilience of current RAG methods, we introduce\nHawkBench, a human-labeled, multi-domain benchmark designed to rigorously\nassess RAG performance across categorized task types. By stratifying tasks\nbased on information-seeking behaviors, HawkBench provides a systematic\nevaluation of how well RAG systems adapt to diverse user needs.\n  Unlike existing benchmarks, which focus primarily on specific task types\n(mostly factoid queries) and rely on varying knowledge bases, HawkBench offers:\n(1) systematic task stratification to cover a broad range of query types,\nincluding both factoid and rationale queries, (2) integration of multi-domain\ncorpora across all task types to mitigate corpus bias, and (3) rigorous\nannotation for high-quality evaluation.\n  HawkBench includes 1,600 high-quality test samples, evenly distributed across\ndomains and task types. Using this benchmark, we evaluate representative RAG\nmethods, analyzing their performance in terms of answer quality and response\nlatency. Our findings highlight the need for dynamic task strategies that\nintegrate decision-making, query interpretation, and global knowledge\nunderstanding to improve RAG generalizability. We believe HawkBench serves as a\npivotal benchmark for advancing the resilience of RAG methods and their ability\nto achieve general-purpose information seeking.\n","authors":["Hongjin Qian","Zheng Liu","Chao Gao","Yankai Wang","Defu Lian","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2502.13465v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2501.10548v2","updated":"2025-02-19T05:22:32Z","published":"2025-01-17T20:43:47Z","title":"Diffusion Models in Recommendation Systems: A Survey","summary":"  Recommender systems remain an essential topic due to its wide application in\nvarious domains and the business potential behind them. With the rise of deep\nlearning, common solutions have leveraged neural networks to facilitate\ncollaborative filtering, and some have turned to generative adversarial\nnetworks to augment the dataset and tackle the data sparsity issue. However,\nthey are limited in learning the complex user and item distribution and still\nsuffer from model collapse. Given the great generation capability exhibited by\ndiffusion models in computer vision recently, many recommender systems have\nadopted diffusion models and found improvements in performance for various\ntasks. Diffusion models in recommender systems excel in managing complex user\nand item distributions and do not suffer from mode collapse. With these\nadvantages, the amount of research in this domain have been growing rapidly and\ncalling for a systematic survey. In this survey paper, we present and propose a\ntaxonomy on past research papers in recommender systems that utilize diffusion\nmodels. Distinct from a prior survey paper that categorizes based on the role\nof the diffusion model, we categorize based on the recommendation task at hand.\nThe decision originates from the rationale that after all, the adoption of\ndiffusion models is to enhance the recommendation performance, not vice versa:\nadapting the recommendation task to enable diffusion models. Nonetheless, we\noffer a unique perspective for diffusion models in recommender systems\ncomplementary to existing surveys. We present the foundation algorithms in\ndiffusion models and their applications in recommender systems to summarize the\nrapid development in this field. Finally, we discuss open research directions\nto prepare and encourage further efforts to advance the field. We compile the\nrelevant papers in a public GitHub repository.\n","authors":["Ting-Ruen Wei","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2501.10548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14304v2","updated":"2025-02-19T02:43:51Z","published":"2023-10-22T13:56:14Z","title":"One Model for All: Large Language Models are Domain-Agnostic\n  Recommendation Systems","summary":"  Sequential recommendation systems aim to predict users' next likely\ninteraction based on their history. However, these systems face data sparsity\nand cold-start problems. Utilizing data from other domains, known as\nmulti-domain methods, is useful for alleviating these problems. However,\ntraditional multi-domain methods rely on meaningless ID-based item\nrepresentation, which makes it difficult to align items with similar meanings\nfrom different domains, yielding sup-optimal knowledge transfer. This paper\nintroduces LLM-Rec, a framework that utilizes pre-trained large language models\n(LLMs) for domain-agnostic recommendation. Specifically, we mix user's\nbehaviors from multiple domains and concatenate item titles into a sentence,\nthen use LLMs for generating user and item representations. By mixing behaviors\nacross different domains, we can exploit the knowledge encoded in LLMs to\nbridge the semantic across over multi-domain behaviors, thus obtaining\nsemantically rich representations and improving performance in all domains.\nFurthermore, we explore the underlying reasons why LLMs are effective and\ninvestigate whether LLMs can understand the semantic correlations as the\nrecommendation model, and if advanced techniques like scaling laws in NLP also\nwork in recommendations. We conduct extensive experiments with LLMs ranging\nfrom 40M to 6.7B to answer the above questions and to verify the effectiveness\nof LLM-Rec in multi-domain recommendation.\n","authors":["Zuoli Tang","Zhaoxin Huan","Zihao Li","Xiaolu Zhang","Jun Hu","Chilin Fu","Jun Zhou","Lixin Zou","Chenliang Li"],"pdf_url":"https://arxiv.org/pdf/2310.14304v2.pdf","comment":"27 pages, 15 figures, 7 tables, Accepted by TOIS"},{"id":"http://arxiv.org/abs/2502.10916v2","updated":"2025-02-19T19:36:25Z","published":"2025-02-15T22:08:53Z","title":"An Open-Source Web-Based Tool for Evaluating Open-Source Large Language\n  Models Leveraging Information Retrieval from Custom Documents","summary":"  In our work, we present the first-of-its-kind open-source web-based tool\nwhich is able to demonstrate the impacts of a user's speech act during\ndiscourse with conversational agents, which leverages open-source large\nlanguage models. With this software resource, it is possible for researchers\nand experts to evaluate the performance of various dialogues, visualize the\nuser's communicative intents, and utilise uploaded specific documents for the\nchat agent to use for its information retrieval to respond to the user query.\nThe context gathered by these models is obtained from a set of linguistic\nfeatures extracted, which forms the context embeddings of the models.\nRegardless of these models showing good context understanding based on these\nfeatures, there still remains a gap in including deeper pragmatic features to\nimprove the model's comprehension of the query, hence the efforts to develop\nthis web resource, which is able to extract and then inject this overlooked\nfeature in the encoder-decoder pipeline of the conversational agent. To\ndemonstrate the effect and impact of the resource, we carried out an experiment\nwhich evaluated the system using 2 knowledge files for information retrieval,\nwith two user queries each, across 5 open-source large language models using 10\nstandard metrics. Our results showed that larger open-source models,\ndemonstrated an improved alignment when the user speech act was included with\ntheir query. The smaller models in contrast showed an increased perplexity and\nmixed performance, which explicitly indicated struggles in processing queries\nthat explicitly included speech acts. The results from the analysis using the\ndeveloped web resource highlight the potential of speech acts towards enhancing\nconversational depths while underscoring the need for model-specific\noptimizations to address increased computational costs and response times.\n","authors":["Godfrey I"],"pdf_url":"https://arxiv.org/pdf/2502.10916v2.pdf","comment":"19 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2410.16458v2","updated":"2025-02-19T23:34:29Z","published":"2024-10-21T19:34:40Z","title":"STAR: A Simple Training-free Approach for Recommendations using Large\n  Language Models","summary":"  Recent progress in large language models (LLMs) offers promising new\napproaches for recommendation system tasks. While the current state-of-the-art\nmethods rely on fine-tuning LLMs to achieve optimal results, this process is\ncostly and introduces significant engineering complexities. Conversely, methods\nthat directly use LLMs without additional fine-tuning result in a large drop in\nrecommendation quality, often due to the inability to capture collaborative\ninformation. In this paper, we propose a Simple Training-free Approach for\nRecommendation (STAR), a framework that utilizes LLMs and can be applied to\nvarious recommendation tasks without the need for fine-tuning, while\nmaintaining high quality recommendation performance. Our approach involves a\nretrieval stage that uses semantic embeddings from LLMs combined with\ncollaborative user information to retrieve candidate items. We then apply an\nLLM for pairwise ranking to enhance next-item prediction. Experimental results\non the Amazon Review dataset show competitive performance for next item\nprediction, even with our retrieval stage alone. Our full method achieves\nHits@10 performance of +23.8% on Beauty, +37.5% on Toys & Games, and -1.8% on\nSports & Outdoors relative to the best supervised models. This framework offers\nan effective alternative to traditional supervised models, highlighting the\npotential of LLMs in recommendation systems without extensive training or\ncustom architectures.\n","authors":["Dong-Ho Lee","Adam Kraft","Long Jin","Nikhil Mehta","Taibai Xu","Lichan Hong","Ed H. Chi","Xinyang Yi"],"pdf_url":"https://arxiv.org/pdf/2410.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14137v1","updated":"2025-02-19T22:47:40Z","published":"2025-02-19T22:47:40Z","title":"Collaborative Retrieval for Large Language Model-based Conversational\n  Recommender Systems","summary":"  Conversational recommender systems (CRS) aim to provide personalized\nrecommendations via interactive dialogues with users. While large language\nmodels (LLMs) enhance CRS with their superior understanding of context-aware\nuser preferences, they typically struggle to leverage behavioral data, which\nhave proven to be important for classical collaborative filtering (CF)-based\napproaches. For this reason, we propose CRAG, Collaborative Retrieval Augmented\nGeneration for LLM-based CRS. To the best of our knowledge, CRAG is the first\napproach that combines state-of-the-art LLMs with CF for conversational\nrecommendations. Our experiments on two publicly available movie conversational\nrecommendation datasets, i.e., a refined Reddit dataset (which we name\nReddit-v2) as well as the Redial dataset, demonstrate the superior item\ncoverage and recommendation performance of CRAG, compared to several CRS\nbaselines. Moreover, we observe that the improvements are mainly due to better\nrecommendation accuracy on recently released movies. The code and data are\navailable at https://github.com/yaochenzhu/CRAG.\n","authors":["Yaochen Zhu","Chao Wan","Harald Steck","Dawen Liang","Yesu Feng","Nathan Kallus","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2502.14137v1.pdf","comment":"Accepted by WWW'2025"},{"id":"http://arxiv.org/abs/2502.02464v3","updated":"2025-02-19T22:46:25Z","published":"2025-02-04T16:33:25Z","title":"Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and\n  Retrieval-Augmented Generation","summary":"  Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical\ncomponents of modern applications in information retrieval, question answering,\nor knowledge-based text generation. However, existing solutions are often\nfragmented, lacking a unified framework that easily integrates these essential\nprocesses. The absence of a standardized implementation, coupled with the\ncomplexity of retrieval and re-ranking workflows, makes it challenging for\nresearchers to compare and evaluate different approaches in a consistent\nenvironment. While existing toolkits such as Rerankers and RankLLM provide\ngeneral-purpose reranking pipelines, they often lack the flexibility required\nfor fine-grained experimentation and benchmarking. In response to these\nchallenges, we introduce Rankify, a powerful and modular open-source toolkit\ndesigned to unify retrieval, re-ranking, and RAG within a cohesive framework.\nRankify supports a wide range of retrieval techniques, including dense and\nsparse retrievers, while incorporating state-of-the-art re-ranking models to\nenhance retrieval quality. Additionally, Rankify includes a collection of\npre-retrieved datasets to facilitate benchmarking, available at Huggingface\n(https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To\nencourage adoption and ease of integration, we provide comprehensive\ndocumentation (http://rankify.readthedocs.io/), an open-source implementation\non GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for\neasy installation (https://pypi.org/project/rankify/). As a unified and\nlightweight framework, Rankify allows researchers and practitioners to advance\nretrieval and re-ranking methodologies while ensuring consistency, scalability,\nand ease of use.\n","authors":["Abdelrahman Abdallah","Bhawna Piryani","Jamshid Mozafari","Mohammed Ali","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.02464v3.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2406.14043v3","updated":"2025-02-19T22:05:46Z","published":"2024-06-20T07:06:58Z","title":"Taxonomy-Guided Zero-Shot Recommendations with LLMs","summary":"  With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.\n","authors":["Yueqing Liang","Liangwei Yang","Chen Wang","Xiongxiao Xu","Philip S. Yu","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2406.14043v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14100v1","updated":"2025-02-19T20:59:35Z","published":"2025-02-19T20:59:35Z","title":"Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach","summary":"  Large Language Models (LLMs) enhanced with external contexts, such as through\nretrieval-augmented generation (RAG), often face challenges in handling\nimperfect evidence. They tend to over-rely on external knowledge, making them\nvulnerable to misleading and unhelpful contexts. To address this, we propose\nthe concept of context-robust LLMs, which can effectively balance internal\nknowledge with external context, similar to human cognitive processes.\nSpecifically, context-robust LLMs should rely on external context only when\nlacking internal knowledge, identify contradictions between internal and\nexternal knowledge, and disregard unhelpful contexts. To achieve this goal, we\nintroduce Grft, a lightweight and plug-and-play gated representation\nfine-tuning approach. Grft consists of two key components: a gating mechanism\nto detect and filter problematic inputs, and low-rank representation adapters\nto adjust hidden representations. By training a lightweight intervention\nfunction with only 0.0004\\% of model size on fewer than 200 examples, Grft can\neffectively adapt LLMs towards context-robust behaviors.\n","authors":["Shenglai Zeng","Pengfei He","Kai Guo","Tianqi Zheng","Hanqing Lu","Yue Xing","Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2502.14100v1.pdf","comment":null}]},"2025-02-20T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.07172v3","updated":"2025-02-20T01:17:01Z","published":"2025-02-11T01:39:11Z","title":"SemiHMER: Semi-supervised Handwritten Mathematical Expression\n  Recognition using pseudo-labels","summary":"  In this paper, we study semi-supervised Handwritten Mathematical Expression\nRecognition (HMER) via exploring both labeled data and extra unlabeled data. We\npropose a novel consistency regularization framework, termed SemiHMER, which\nintroduces dual-branch semi-supervised learning. Specifically, we enforce\nconsistency between the two networks for the same input image. The\npseudo-label, generated by one perturbed recognition network, is utilized to\nsupervise the other network using the standard cross-entropy loss. The SemiHMER\nconsistency encourages high similarity between the predictions of the two\nperturbed networks for the same input image and expands the training data by\nleveraging unlabeled data with pseudo-labels. We further introduce a\nweak-to-strong strategy by applying different levels of augmentation to each\nbranch, effectively expanding the training data and enhancing the quality of\nnetwork training. Additionally, we propose a novel module, the Global Dynamic\nCounting Module (GDCM), to enhance the performance of the HMER decoder by\nalleviating recognition inaccuracies in long-distance formula recognition and\nreducing the occurrence of repeated characters. The experimental results\ndemonstrate that our work achieves significant performance improvements, with\nan average accuracy increase of 5.47% on CROHME14, 4.87% on CROHME16, and 5.25%\non CROHME19, compared to our baselines.\n","authors":["Kehua Chen","Haoyang Shen"],"pdf_url":"https://arxiv.org/pdf/2502.07172v3.pdf","comment":"17 pages,3 figures"},{"id":"http://arxiv.org/abs/2502.14865v1","updated":"2025-02-20T18:59:51Z","published":"2025-02-20T18:59:51Z","title":"Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical\n  and Cultural Artifacts","summary":"  Understanding historical and cultural artifacts demands human expertise and\nadvanced computational techniques, yet the process remains complex and\ntime-intensive. While large multimodal models offer promising support, their\nevaluation and improvement require a standardized benchmark. To address this,\nwe introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\n266 distinct cultures across 10 major historical regions. Designed for\nAI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\ndiscoveries, TimeTravel provides a structured dataset and robust evaluation\nframework to assess AI models' capabilities in classification, interpretation,\nand historical comprehension. By integrating AI with historical research,\nTimeTravel fosters AI-powered tools for historians, archaeologists,\nresearchers, and cultural tourists to extract valuable insights while ensuring\ntechnology contributes meaningfully to historical discovery and cultural\nheritage preservation. We evaluate contemporary AI models on TimeTravel,\nhighlighting their strengths and identifying areas for improvement. Our goal is\nto establish AI as a reliable partner in preserving cultural heritage, ensuring\nthat technological advancements contribute meaningfully to historical\ndiscovery. Our code is available at:\n\\url{https://github.com/mbzuai-oryx/TimeTravel}.\n","authors":["Sara Ghaboura","Ketan More","Ritesh Thawkar","Wafa Alghallabi","Omkar Thawakar","Fahad Shahbaz Khan","Hisham Cholakkal","Salman Khan","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2502.14865v1.pdf","comment":"4 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.14864v1","updated":"2025-02-20T18:59:42Z","published":"2025-02-20T18:59:42Z","title":"Benchmarking Multimodal RAG through a Chart-based Document\n  Question-Answering Generation Framework","summary":"  Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning\ncapabilities by integrating external knowledge. However, existing benchmarks\nprimarily focus on simple image-text interactions, overlooking complex visual\nformats like charts that are prevalent in real-world applications. In this\nwork, we introduce a novel task, Chart-based MRAG, to address this limitation.\nTo semi-automatically generate high-quality evaluation samples, we propose\nCHARt-based document question-answering GEneration (CHARGE), a framework that\nproduces evaluation data through structured keypoint extraction, crossmodal\nverification, and keypoint-based generation. By combining CHARGE with expert\nvalidation, we construct Chart-MRAG Bench, a comprehensive benchmark for\nchart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8\ndomains from real-world documents. Our evaluation reveals three critical\nlimitations in current approaches: (1) unified multimodal embedding retrieval\nmethods struggles in chart-based scenarios, (2) even with ground-truth\nretrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%\nCoverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality\nbias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are\nreleased at https://github.com/Nomothings/CHARGE.git.\n","authors":["Yuming Yang","Jiang Zhong","Li Jin","Jingwang Huang","Jingpeng Gao","Qing Liu","Yang Bai","Jingyuan Zhang","Rui Jiang","Kaiwen Wei"],"pdf_url":"https://arxiv.org/pdf/2502.14864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06020v2","updated":"2025-02-20T18:56:25Z","published":"2023-04-12T17:57:15Z","title":"VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs","summary":"  We propose $\\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled\n$\\textbf{Vid}$eo representation based upon $\\textbf{Style}$GAN and\nNeural-$\\textbf{ODE}$s. Effective traversal of the latent space learned by\nGenerative Adversarial Networks (GANs) has been the basis for recent\nbreakthroughs in image editing. However, the applicability of such advancements\nto the video domain has been hindered by the difficulty of representing and\ncontrolling videos in the latent space of GANs. In particular, videos are\ncomposed of content (i.e., appearance) and complex motion components that\nrequire a special mechanism to disentangle and control. To achieve this,\nVidStyleODE encodes the video content in a pre-trained StyleGAN $\\mathcal{W}_+$\nspace and benefits from a latent ODE component to summarize the spatiotemporal\ndynamics of the input video. Our novel continuous video generation process then\ncombines the two to generate high-quality and temporally consistent videos with\nvarying frame rates. We show that our proposed method enables a variety of\napplications on real videos: text-guided appearance manipulation, motion\nmanipulation, image animation, and video interpolation and extrapolation.\nProject website: https://cyberiada.github.io/VidStyleODE\n","authors":["Moayed Haji Ali","Andrew Bond","Tolga Birdal","Duygu Ceylan","Levent Karacan","Erkut Erdem","Aykut Erdem"],"pdf_url":"https://arxiv.org/pdf/2304.06020v2.pdf","comment":"Project website: https://cyberiada.github.io/VidStyleODE"},{"id":"http://arxiv.org/abs/2502.14846v1","updated":"2025-02-20T18:55:30Z","published":"2025-02-20T18:55:30Z","title":"Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation","summary":"  Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.\n","authors":["Yue Yang","Ajay Patel","Matt Deitke","Tanmay Gupta","Luca Weihs","Andrew Head","Mark Yatskar","Chris Callison-Burch","Ranjay Krishna","Aniruddha Kembhavi","Christopher Clark"],"pdf_url":"https://arxiv.org/pdf/2502.14846v1.pdf","comment":"20 pages, 19 figures, 9 tables, website:\n  https://yueyang1996.github.io/cosyn/"},{"id":"http://arxiv.org/abs/2502.14844v1","updated":"2025-02-20T18:53:39Z","published":"2025-02-20T18:53:39Z","title":"Dynamic Concepts Personalization from Single Videos","summary":"  Personalizing generative text-to-image models has seen remarkable progress,\nbut extending this personalization to text-to-video models presents unique\nchallenges. Unlike static concepts, personalizing text-to-video models has the\npotential to capture dynamic concepts, i.e., entities defined not only by their\nappearance but also by their motion. In this paper, we introduce\nSet-and-Sequence, a novel framework for personalizing Diffusion Transformers\n(DiTs)-based generative video models with dynamic concepts. Our approach\nimposes a spatio-temporal weight space within an architecture that does not\nexplicitly separate spatial and temporal features. This is achieved in two key\nstages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an\nunordered set of frames from the video to learn an identity LoRA basis that\nrepresents the appearance, free from temporal interference. In the second\nstage, with the identity LoRAs frozen, we augment their coefficients with\nMotion Residuals and fine-tune them on the full video sequence, capturing\nmotion dynamics. Our Set-and-Sequence framework results in a spatio-temporal\nweight space that effectively embeds dynamic concepts into the video model's\noutput domain, enabling unprecedented editability and compositionality while\nsetting a new benchmark for personalizing dynamic concepts.\n","authors":["Rameen Abdal","Or Patashnik","Ivan Skorokhodov","Willi Menapace","Aliaksandr Siarohin","Sergey Tulyakov","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2502.14844v1.pdf","comment":"Webpage: https://snap-research.github.io/dynamic_concepts/"},{"id":"http://arxiv.org/abs/2502.14834v1","updated":"2025-02-20T18:47:36Z","published":"2025-02-20T18:47:36Z","title":"LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V\n","authors":["Shangqing Tu","Yucheng Wang","Daniel Zhang-Li","Yushi Bai","Jifan Yu","Yuhao Wu","Lei Hou","Huiqin Liu","Zhiyuan Liu","Bin Xu","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2502.14834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14831v1","updated":"2025-02-20T18:45:44Z","published":"2025-02-20T18:45:44Z","title":"Improving the Diffusability of Autoencoders","summary":"  Latent diffusion models have emerged as the leading approach for generating\nhigh-quality images and videos, utilizing compressed latent representations to\nreduce the computational burden of the diffusion process. While recent\nadvancements have primarily focused on scaling diffusion backbones and\nimproving autoencoder reconstruction quality, the interaction between these\ncomponents has received comparatively less attention. In this work, we perform\na spectral analysis of modern autoencoders and identify inordinate\nhigh-frequency components in their latent spaces, which are especially\npronounced in the autoencoders with a large bottleneck channel size. We\nhypothesize that this high-frequency component interferes with the\ncoarse-to-fine nature of the diffusion synthesis process and hinders the\ngeneration quality. To mitigate the issue, we propose scale equivariance: a\nsimple regularization strategy that aligns latent and RGB spaces across\nfrequencies by enforcing scale equivariance in the decoder. It requires minimal\ncode changes and only up to 20K autoencoder fine-tuning steps, yet\nsignificantly improves generation quality, reducing FID by 19% for image\ngeneration on ImageNet-1K 256x256 and FVD by at least 44% for video generation\non Kinetics-700 17x256x256.\n","authors":["Ivan Skorokhodov","Sharath Girish","Benran Hu","Willi Menapace","Yanyu Li","Rameen Abdal","Sergey Tulyakov","Aliaksandr Siarohin"],"pdf_url":"https://arxiv.org/pdf/2502.14831v1.pdf","comment":"26 pages, 22 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.14827v1","updated":"2025-02-20T18:45:00Z","published":"2025-02-20T18:45:00Z","title":"Exploring Advanced Techniques for Visual Question Answering: A\n  Comprehensive Comparison","summary":"  Visual Question Answering (VQA) has emerged as a pivotal task in the\nintersection of computer vision and natural language processing, requiring\nmodels to understand and reason about visual content in response to natural\nlanguage questions. Analyzing VQA datasets is essential for developing robust\nmodels that can handle the complexities of multimodal reasoning. Several\napproaches have been developed to examine these datasets, each offering\ndistinct perspectives on question diversity, answer distribution, and\nvisual-textual correlations. Despite significant progress, existing VQA models\nface challenges related to dataset bias, limited model complexity, commonsense\nreasoning gaps, rigid evaluation methods, and generalization to real world\nscenarios. This paper presents a comprehensive comparative study of five\nadvanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling,\nBLIP-2, and OFA, each employing distinct methodologies to address these\nchallenges.\n","authors":["Aiswarya Baby","Tintu Thankom Koshy"],"pdf_url":"https://arxiv.org/pdf/2502.14827v1.pdf","comment":"8 pages, No figures"},{"id":"http://arxiv.org/abs/2502.14807v1","updated":"2025-02-20T18:30:34Z","published":"2025-02-20T18:30:34Z","title":"FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image\n  Analysis","summary":"  Foundation models are becoming increasingly effective in the medical domain,\noffering pre-trained models on large datasets that can be readily adapted for\ndownstream tasks. Despite progress, fetal ultrasound images remain a\nchallenging domain for foundation models due to their inherent complexity,\noften requiring substantial additional training and facing limitations due to\nthe scarcity of paired multimodal data. To overcome these challenges, here we\nintroduce FetalCLIP, a vision-language foundation model capable of generating\nuniversal representation of fetal ultrasound images. FetalCLIP was pre-trained\nusing a multimodal learning approach on a diverse dataset of 210,035 fetal\nultrasound images paired with text. This represents the largest paired dataset\nof its kind used for foundation model development to date. This unique training\napproach allows FetalCLIP to effectively learn the intricate anatomical\nfeatures present in fetal ultrasound images, resulting in robust\nrepresentations that can be used for a variety of downstream applications. In\nextensive benchmarking across a range of key fetal ultrasound applications,\nincluding classification, gestational age estimation, congenital heart defect\n(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all\nbaselines while demonstrating remarkable generalizability and strong\nperformance even with limited labeled data. We plan to release the FetalCLIP\nmodel publicly for the benefit of the broader scientific community.\n","authors":["Fadillah Maani","Numan Saeed","Tausifa Saleem","Zaid Farooq","Hussain Alasmawi","Werner Diehl","Ameera Mohammad","Gareth Waring","Saudabi Valappi","Leanne Bricker","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2502.14807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14801v1","updated":"2025-02-20T18:22:44Z","published":"2025-02-20T18:22:44Z","title":"AVD2: Accident Video Diffusion for Accident Video Description","summary":"  Traffic accidents present complex challenges for autonomous driving, often\nfeaturing unpredictable scenarios that hinder accurate system interpretation\nand responses.Nonetheless, prevailing methodologies fall short in elucidating\nthe causes of accidents and proposing preventive measures due to the paucity of\ntraining data specific to accident scenarios.In this work, we introduce AVD2\n(Accident Video Diffusion for Accident Video Description), a novel framework\nthat enhances accident scene understanding by generating accident videos that\naligned with detailed natural language descriptions and reasoning, resulting in\nthe contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding)\ndataset. Empirical results reveal that the integration of the EMM-AU dataset\nestablishes state-of-the-art performance across both automated metrics and\nhuman evaluations, markedly advancing the domains of accident analysis and\nprevention. Project resources are available at https://an-answer-tree.github.io\n","authors":["Cheng Li","Keyuan Zhou","Tong Liu","Yu Wang","Mingqiao Zhuang","Huan-ang Gao","Bu Jin","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.14801v1.pdf","comment":"ICRA 2025, Project Page: https://an-answer-tree.github.io/"},{"id":"http://arxiv.org/abs/2502.14799v1","updated":"2025-02-20T18:19:57Z","published":"2025-02-20T18:19:57Z","title":"A Survey on Text-Driven 360-Degree Panorama Generation","summary":"  The advent of text-driven 360-degree panorama generation, enabling the\nsynthesis of 360-degree panoramic images directly from textual descriptions,\nmarks a transformative advancement in immersive visual content creation. This\ninnovation significantly simplifies the traditionally complex process of\nproducing such content. Recent progress in text-to-image diffusion models has\naccelerated the rapid development in this emerging field. This survey presents\na comprehensive review of text-driven 360-degree panorama generation, offering\nan in-depth analysis of state-of-the-art algorithms and their expanding\napplications in 360-degree 3D scene generation. Furthermore, we critically\nexamine current limitations and propose promising directions for future\nresearch. A curated project page with relevant resources and research papers is\navailable at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.\n","authors":["Hai Wang","Xiaoyu Xiang","Weihao Xia","Jing-Hao Xue"],"pdf_url":"https://arxiv.org/pdf/2502.14799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14795v1","updated":"2025-02-20T18:17:11Z","published":"2025-02-20T18:17:11Z","title":"Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration","summary":"  This paper addresses the limitations of current humanoid robot control\nframeworks, which primarily rely on reactive mechanisms and lack autonomous\ninteraction capabilities due to data scarcity. We propose Humanoid-VLA, a novel\nframework that integrates language understanding, egocentric scene perception,\nand motion control, enabling universal humanoid control. Humanoid-VLA begins\nwith language-motion pre-alignment using non-egocentric human motion datasets\npaired with textual descriptions, allowing the model to learn universal motion\npatterns and action semantics. We then incorporate egocentric visual context\nthrough a parameter efficient video-conditioned fine-tuning, enabling\ncontext-aware motion generation. Furthermore, we introduce a self-supervised\ndata augmentation strategy that automatically generates pseudoannotations\ndirectly derived from motion data. This process converts raw motion sequences\ninto informative question-answer pairs, facilitating the effective use of\nlarge-scale unlabeled video data. Built upon whole-body control architectures,\nextensive experiments show that Humanoid-VLA achieves object interaction and\nenvironment exploration tasks with enhanced contextual awareness, demonstrating\na more human-like capacity for adaptive and intelligent engagement.\n","authors":["Pengxiang Ding","Jianfei Ma","Xinyang Tong","Binghong Zou","Xinxin Luo","Yiguo Fan","Ting Wang","Hongchao Lu","Panzhong Mo","Jinxin Liu","Yuefan Wang","Huaicheng Zhou","Wenshuo Feng","Jiacheng Liu","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14792v1","updated":"2025-02-20T18:11:44Z","published":"2025-02-20T18:11:44Z","title":"RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye\n  View Segmentation","summary":"  Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention\nas a useful representation of the environment to tackle assisted and autonomous\ndriving tasks. However, most of the existing work focuses on the fully\nsupervised setting, training networks on large annotated datasets. In this\nwork, we present RendBEV, a new method for the self-supervised training of BEV\nsemantic segmentation networks, leveraging differentiable volumetric rendering\nto receive supervision from semantic perspective views computed by a 2D\nsemantic segmentation model. Our method enables zero-shot BEV semantic\nsegmentation, and already delivers competitive results in this challenging\nsetting. When used as pretraining to then fine-tune on labeled BEV\nground-truth, our method significantly boosts performance in low-annotation\nregimes, and sets a new state of the art when fine-tuning on all available\nlabels.\n","authors":["Henrique Piñeiro Monteagudo","Leonardo Taccari","Aurel Pjetri","Francesco Sambo","Samuele Salti"],"pdf_url":"https://arxiv.org/pdf/2502.14792v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2502.14789v1","updated":"2025-02-20T18:09:27Z","published":"2025-02-20T18:09:27Z","title":"Structurally Disentangled Feature Fields Distillation for 3D\n  Understanding and Editing","summary":"  Recent work has demonstrated the ability to leverage or distill pre-trained\n2D features obtained using large pre-trained 2D models into 3D features,\nenabling impressive 3D editing and understanding capabilities using only 2D\nsupervision. Although impressive, models assume that 3D features are captured\nusing a single feature field and often make a simplifying assumption that\nfeatures are view-independent. In this work, we propose instead to capture 3D\nfeatures using multiple disentangled feature fields that capture different\nstructural components of 3D features involving view-dependent and\nview-independent components, which can be learned from 2D feature supervision\nonly. Subsequently, each element can be controlled in isolation, enabling\nsemantic and structural understanding and editing capabilities. For instance,\nusing a user click, one can segment 3D features corresponding to a given object\nand then segment, edit, or remove their view-dependent (reflective) properties.\nWe evaluate our approach on the task of 3D segmentation and demonstrate a set\nof novel understanding and editing tasks.\n","authors":["Yoel Levy","David Shavin","Itai Lang","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.14789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14786v1","updated":"2025-02-20T18:08:29Z","published":"2025-02-20T18:08:29Z","title":"SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features","summary":"  We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).\n","authors":["Michael Tschannen","Alexey Gritsenko","Xiao Wang","Muhammad Ferjad Naeem","Ibrahim Alabdulmohsin","Nikhil Parthasarathy","Talfan Evans","Lucas Beyer","Ye Xia","Basil Mustafa","Olivier Hénaff","Jeremiah Harmsen","Andreas Steiner","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.14786v1.pdf","comment":"Model checkpoints are available at\n  https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md"},{"id":"http://arxiv.org/abs/2410.00255v2","updated":"2025-02-20T18:06:19Z","published":"2024-09-30T21:55:38Z","title":"Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning","summary":"  Recent advancements in 3D Large Language Models (3DLLMs) have highlighted\ntheir potential in building general-purpose agents in the 3D real world, yet\nchallenges remain due to the lack of high-quality robust instruction-following\ndata, leading to limited discriminative power and generalization of 3DLLMs. In\nthis paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale\ninstruction-following data generated by our novel data engine, Robust\nInstruction Generation (RIG) engine. RIG generates two key instruction data: 1)\nthe Adversarial Instruction-following data, which features mixed negative and\npositive samples to enhance the model's discriminative understanding. 2) the\nDiverse Instruction-following data, which contains various instruction styles\nto enhance model's generalization. As a result, we construct 1 million\ninstruction-following data, consisting of 344K Adversarial samples, 508K\nDiverse samples, and 165K benchmark training set samples. To better handle\nthese complex instructions, Robin3D first incorporates Relation-Augmented\nProjector to enhance spatial understanding, and then strengthens the object\nreferring and grounding ability through ID-Feature Bonding. Robin3D\nconsistently outperforms previous methods across five widely-used 3D multimodal\nlearning benchmarks, without the need for task-specific fine-tuning. Notably,\nwe achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\%\nimprovement in the captioning task (Scan2Cap).\n","authors":["Weitai Kang","Haifeng Huang","Yuzhang Shang","Mubarak Shah","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2410.00255v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.14780v1","updated":"2025-02-20T18:01:41Z","published":"2025-02-20T18:01:41Z","title":"ReVision: A Dataset and Baseline VLM for Privacy-Preserving\n  Task-Oriented Visual Instruction Rewriting","summary":"  Efficient and privacy-preserving multimodal interaction is essential as AR,\nVR, and modern smartphones with powerful cameras become primary interfaces for\nhuman-computer communication. Existing powerful large vision-language models\n(VLMs) enabling multimodal interaction often rely on cloud-based processing,\nraising significant concerns about (1) visual privacy by transmitting sensitive\nvision data to servers, and (2) their limited real-time, on-device usability.\nThis paper explores Visual Instruction Rewriting, a novel approach that\ntransforms multimodal instructions into text-only commands, allowing seamless\nintegration of lightweight on-device instruction rewriter VLMs (250M\nparameters) with existing conversational AI systems, enhancing vision data\nprivacy. To achieve this, we present a dataset of over 39,000 examples across\n14 domains and develop a compact VLM, pretrained on image captioning datasets\nand fine-tuned for instruction rewriting. Experimental results, evaluated\nthrough NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic\nparsing analysis, demonstrate that even a quantized version of the model\n(<500MB storage footprint) can achieve effective instruction rewriting, thus\nenabling privacy-focused, multimodal AI applications.\n","authors":["Abhijit Mishra","Richard Noh","Hsiang Fu","Mingda Li","Minji Kim"],"pdf_url":"https://arxiv.org/pdf/2502.14780v1.pdf","comment":"12 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.14779v1","updated":"2025-02-20T18:01:02Z","published":"2025-02-20T18:01:02Z","title":"DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image\n  Generation with Diffusion Models","summary":"  In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and\nprecisely controllable framework for multi-condition image generation. The core\nidea behind DC-ControlNet is to decouple control conditions, transforming\nglobal control into a hierarchical system that integrates distinct elements,\ncontents, and layouts. This enables users to mix these individual conditions\nwith greater flexibility, leading to more efficient and accurate image\ngeneration control. Previous ControlNet-based models rely solely on global\nconditions, which affect the entire image and lack the ability of element- or\nregion-specific control. This limitation reduces flexibility and can cause\ncondition misunderstandings in multi-conditional image generation. To address\nthese challenges, we propose both intra-element and Inter-element Controllers\nin DC-ControlNet. The Intra-Element Controller handles different types of\ncontrol signals within individual elements, accurately describing the content\nand layout characteristics of the object. For interactions between elements, we\nintroduce the Inter-Element Controller, which accurately handles multi-element\ninteractions and occlusion based on user-defined relationships. Extensive\nevaluations show that DC-ControlNet significantly outperforms existing\nControlNet models and Layout-to-Image generative models in terms of control\nflexibility and precision in multi-condition control.\n","authors":["Hongji Yang","Wencheng Han","Yucheng Zhou","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2502.14779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14778v1","updated":"2025-02-20T17:59:59Z","published":"2025-02-20T17:59:59Z","title":"Harnessing PDF Data for Improving Japanese Large Multimodal Models","summary":"  Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs. We plan to make the source code and data publicly\navailable upon acceptance.\n","authors":["Jeonghun Baek","Akiko Aizawa","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2502.14778v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.14762v1","updated":"2025-02-20T17:37:08Z","published":"2025-02-20T17:37:08Z","title":"Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental\n  Learning","summary":"  Class-incremental learning requires models to continually acquire knowledge\nof new classes without forgetting old ones. Although pre-trained models have\ndemonstrated strong performance in class-incremental learning, they remain\nsusceptible to catastrophic forgetting when learning new concepts. Excessive\nplasticity in the models breaks generalizability and causes forgetting, while\nstrong stability results in insufficient adaptation to new classes. This\nnecessitates effective adaptation with minimal modifications to preserve the\ngeneral knowledge of pre-trained models. To address this challenge, we first\nintroduce a new parameter-efficient fine-tuning module 'Learn and Calibrate',\nor LuCA, designed to acquire knowledge through an adapter-calibrator couple,\nenabling effective adaptation with well-refined feature representations.\nSecond, for each learning session, we deploy a sparse LuCA module on top of the\nlast token just before the classifier, which we refer to as 'Token-level Sparse\nCalibration and Adaptation', or TOSCA. This strategic design improves the\northogonality between the modules and significantly reduces both training and\ninference complexity. By leaving the generalization capabilities of the\npre-trained models intact and adapting exclusively via the last token, our\napproach achieves a harmonious balance between stability and plasticity.\nExtensive experiments demonstrate TOSCA's state-of-the-art performance while\nintroducing ~8 times fewer parameters compared to prior methods.\n","authors":["Murat Onur Yildirim","Elif Ceren Gok Yildirim","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2502.14762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14753v1","updated":"2025-02-20T17:24:06Z","published":"2025-02-20T17:24:06Z","title":"MedVAE: Efficient Automated Interpretation of Medical Images with\n  Large-Scale Generalizable Autoencoders","summary":"  Medical images are acquired at high resolutions with large fields of view in\norder to capture fine-grained features necessary for clinical decision-making.\nConsequently, training deep learning models on medical images can incur large\ncomputational costs. In this work, we address the challenge of downsizing\nmedical images in order to improve downstream computational efficiency while\npreserving clinically-relevant features. We introduce MedVAE, a family of six\nlarge-scale 2D and 3D autoencoders capable of encoding medical images as\ndownsized latent representations and decoding latent representations back to\nhigh-resolution images. We train MedVAE autoencoders using a novel two-stage\ntraining approach with 1,052,730 medical images. Across diverse tasks obtained\nfrom 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent\nrepresentations in place of high-resolution images when training downstream\nmodels can lead to efficiency benefits (up to 70x improvement in throughput)\nwhile simultaneously preserving clinically-relevant features and (2) MedVAE can\ndecode latent representations back to high-resolution images with high\nfidelity. Our work demonstrates that large-scale, generalizable autoencoders\ncan help address critical efficiency challenges in the medical domain. Our code\nis available at https://github.com/StanfordMIMI/MedVAE.\n","authors":["Maya Varma","Ashwin Kumar","Rogier van der Sluijs","Sophie Ostmeier","Louis Blankemeier","Pierre Chambon","Christian Bluethgen","Jip Prince","Curtis Langlotz","Akshay Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2502.14753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09408v3","updated":"2025-02-20T17:16:03Z","published":"2024-06-13T17:59:44Z","title":"Data Attribution for Text-to-Image Models by Unlearning Synthesized\n  Images","summary":"  The goal of data attribution for text-to-image models is to identify the\ntraining images that most influence the generation of a new image. Influence is\ndefined such that, for a given output, if a model is retrained from scratch\nwithout the most influential images, the model would fail to reproduce the same\noutput. Unfortunately, directly searching for these influential images is\ncomputationally infeasible, since it would require repeatedly retraining models\nfrom scratch. In our work, we propose an efficient data attribution method by\nsimulating unlearning the synthesized image. We achieve this by increasing the\ntraining loss on the output image, without catastrophic forgetting of other,\nunrelated concepts. We then identify training images with significant loss\ndeviations after the unlearning process and label these as influential. We\nevaluate our method with a computationally intensive but \"gold-standard\"\nretraining from scratch and demonstrate our method's advantages over previous\nmethods.\n","authors":["Sheng-Yu Wang","Aaron Hertzmann","Alexei A. Efros","Jun-Yan Zhu","Richard Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.09408v3.pdf","comment":"NeurIPS 2024 camera ready version. Project page:\n  https://peterwang512.github.io/AttributeByUnlearning Code:\n  https://github.com/PeterWang512/AttributeByUnlearning"},{"id":"http://arxiv.org/abs/2502.14740v1","updated":"2025-02-20T17:08:43Z","published":"2025-02-20T17:08:43Z","title":"YOLOv12: A Breakdown of the Key Architectural Features","summary":"  This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters.\n","authors":["Mujadded Al Rabbani Alif","Muhammad Hussain"],"pdf_url":"https://arxiv.org/pdf/2502.14740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14721v1","updated":"2025-02-20T16:48:14Z","published":"2025-02-20T16:48:14Z","title":"Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes","summary":"  The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.\n","authors":["Lukas Rauch","Thomas Braml"],"pdf_url":"https://arxiv.org/pdf/2502.14721v1.pdf","comment":"18 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2309.16850v2","updated":"2025-02-20T16:34:06Z","published":"2023-09-28T21:02:04Z","title":"Sketch2CAD: 3D CAD Model Reconstruction from 2D Sketch using Visual\n  Transformer","summary":"  Current 3D reconstruction methods typically generate outputs in the form of\nvoxels, point clouds, or meshes. However, each of these formats has inherent\nlimitations, such as rough surfaces and distorted structures. Additionally,\nthese data types are not ideal for further manual editing and post-processing.\nIn this paper, we present a novel 3D reconstruction method designed to overcome\nthese disadvantages by reconstructing CAD-compatible models. We trained a\nvisual transformer to predict a \"scene descriptor\" from a single 2D wire-frame\nimage. This descriptor includes essential information, such as object types and\nparameters like position, rotation, and size. Using the predicted parameters, a\n3D scene can be reconstructed with 3D modeling software that has programmable\ninterfaces, such as Rhino Grasshopper, to build highly editable 3D models in\nthe form of B-rep. To evaluate our proposed model, we created two datasets: one\nconsisting of simple scenes and another with more complex scenes. The test\nresults indicate the model's capability to accurately reconstruct simple scenes\nwhile highlighting its difficulties with more complex ones.\n","authors":["Hong-Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2309.16850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14684v1","updated":"2025-02-20T16:12:13Z","published":"2025-02-20T16:12:13Z","title":"CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has shown significant advantages in novel view\nsynthesis (NVS), particularly in achieving high rendering speeds and\nhigh-quality results. However, its geometric accuracy in 3D reconstruction\nremains limited due to the lack of explicit geometric constraints during\noptimization. This paper introduces CDGS, a confidence-aware depth\nregularization approach developed to enhance 3DGS. We leverage multi-cue\nconfidence maps of monocular depth estimation and sparse Structure-from-Motion\ndepth to adaptively adjust depth supervision during the optimization process.\nOur method demonstrates improved geometric detail preservation in early\ntraining stages and achieves competitive performance in both NVS quality and\ngeometric accuracy. Experiments on the publicly available Tanks and Temples\nbenchmark dataset show that our method achieves more stable convergence\nbehavior and more accurate geometric reconstruction results, with improvements\nof up to 2.31 dB in PSNR for NVS and consistently lower geometric errors in\nM3C2 distance metrics. Notably, our method reaches comparable F-scores to the\noriginal 3DGS with only 50% of the training iterations. We expect this work\nwill facilitate the development of efficient and accurate 3D reconstruction\nsystems for real-world applications such as digital twin creation, heritage\npreservation, or forestry applications.\n","authors":["Qilin Zhang","Olaf Wysocki","Steffen Urban","Boris Jutzi"],"pdf_url":"https://arxiv.org/pdf/2502.14684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14676v1","updated":"2025-02-20T16:09:21Z","published":"2025-02-20T16:09:21Z","title":"BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution\n  Network for Pedestrian and Heterogeneous Trajectory Prediction","summary":"  Trajectory prediction allows better decision-making in applications of\nautonomous vehicles or surveillance by predicting the short-term future\nmovement of traffic agents. It is classified into pedestrian or heterogeneous\ntrajectory prediction. The former exploits the relatively consistent behavior\nof pedestrians, but is limited in real-world scenarios with heterogeneous\ntraffic agents such as cyclists and vehicles. The latter typically relies on\nextra class label information to distinguish the heterogeneous agents, but such\nlabels are costly to annotate and cannot be generalized to represent different\nbehaviors within the same class of agents. In this work, we introduce the\nbehavioral pseudo-labels that effectively capture the behavior distributions of\npedestrians and heterogeneous agents solely based on their motion features,\nsignificantly improving the accuracy of trajectory prediction. To implement the\nframework, we propose the Behavioral Pseudo-Label Informed Sparse Graph\nConvolution Network (BP-SGCN) that learns pseudo-labels and informs to a\ntrajectory predictor. For optimization, we propose a cascaded training scheme,\nin which we first learn the pseudo-labels in an unsupervised manner, and then\nperform end-to-end fine-tuning on the labels in the direction of increasing the\ntrajectory prediction accuracy. Experiments show that our pseudo-labels\neffectively model different behavior clusters and improve trajectory\nprediction. Our proposed BP-SGCN outperforms existing methods using both\npedestrian (ETH/UCY, pedestrian-only SDD) and heterogeneous agent datasets\n(SDD, Argoverse 1).\n","authors":["Ruochen Li","Stamos Katsigiannis","Tae-Kyun Kim","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2502.14676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14659v1","updated":"2025-02-20T15:54:37Z","published":"2025-02-20T15:54:37Z","title":"MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only\n  VIBE MRI","summary":"  Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images\nsuitable for water and fat signal composition estimation. While the two-point\nVIBE provides water-fat-separated images, the six-point VIBE allows estimation\nof the effective transversal relaxation rate R2* and the proton density fat\nfraction (PDFF), which are imaging markers for health and disease. Ambiguity\nduring signal reconstruction can lead to water-fat swaps. This shortcoming\nchallenges the application of VIBE-MRI for automated PDFF analyses of\nlarge-scale clinical data and of population studies. This study develops an\nautomated pipeline to detect and correct water-fat swaps in\nnon-contrast-enhanced VIBE images. Our three-step pipeline begins with training\na segmentation network to classify volumes as \"fat-like\" or \"water-like,\" using\nsynthetic water-fat swaps generated by merging fat and water volumes with\nPerlin noise. Next, a denoising diffusion image-to-image network predicts water\nvolumes as signal priors for correction. Finally, we integrate this prior into\na physics-constrained model to recover accurate water and fat signals. Our\napproach achieves a < 1% error rate in water-fat swap detection for a 6-point\nVIBE. Notably, swaps disproportionately affect individuals in the Underweight\nand Class 3 Obesity BMI categories. Our correction algorithm ensures accurate\nsolution selection in chemical phase MRIs, enabling reliable PDFF estimation.\nThis forms a solid technical foundation for automated large-scale population\nimaging analysis.\n","authors":["Robert Graf","Hendrik Möller","Sophie Starck","Matan Atad","Philipp Braun","Jonathan Stelter","Annette Peters","Lilian Krist","Stefan N. Willich","Henry Völzke","Robin Bülow","Klaus Berger","Tobias Pischon","Thoralf Niendorf","Johannes Paetzold","Dimitrios Karampinos","Daniel Rueckert","Jan Kirschke"],"pdf_url":"https://arxiv.org/pdf/2502.14659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14638v1","updated":"2025-02-20T15:21:35Z","published":"2025-02-20T15:21:35Z","title":"NAVIG: Natural Language-guided Analysis with Vision Language Models for\n  Image Geo-localization","summary":"  Image geo-localization is the task of predicting the specific location of an\nimage and requires complex reasoning across visual, geographical, and cultural\ncontexts. While prior Vision Language Models (VLMs) have the best accuracy at\nthis task, there is a dearth of high-quality datasets and models for analytical\nreasoning. We first create NaviClues, a high-quality dataset derived from\nGeoGuessr, a popular geography game, to supply examples of expert reasoning\nfrom language. Using this dataset, we present Navig, a comprehensive image\ngeo-localization framework integrating global and fine-grained image\ninformation. By reasoning with language, Navig reduces the average distance\nerror by 14% compared to previous state-of-the-art models while requiring fewer\nthan 1000 training samples. Our dataset and code are available at\nhttps://github.com/SparrowZheyuan18/Navig/.\n","authors":["Zheyuan Zhang","Runze Li","Tasnim Kabir","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2502.14638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14715v3","updated":"2025-02-20T15:02:44Z","published":"2024-03-19T06:46:24Z","title":"Towards Understanding Why Label Smoothing Degrades Selective\n  Classification and How to Fix It","summary":"  Label smoothing (LS) is a popular regularisation method for training neural\nnetworks as it is effective in improving test accuracy and is simple to\nimplement. ``Hard'' one-hot labels are ``smoothed'' by uniformly distributing\nprobability mass to other classes, reducing overfitting. Prior work has\nsuggested that in some cases LS can degrade selective classification (SC) --\nwhere the aim is to reject misclassifications using a model's uncertainty. In\nthis work, we first demonstrate empirically across an extended range of\nlarge-scale tasks and architectures that LS consistently degrades SC. We then\naddress a gap in existing knowledge, providing an explanation for this\nbehaviour by analysing logit-level gradients: LS degrades the uncertainty rank\nordering of correct vs incorrect predictions by suppressing the max logit more\nwhen a prediction is likely to be correct, and less when it is likely to be\nwrong. This elucidates previously reported experimental results where strong\nclassifiers underperform in SC. We then demonstrate the empirical effectiveness\nof post-hoc logit normalisation for recovering lost SC performance caused by\nLS. Furthermore, linking back to our gradient analysis, we again provide an\nexplanation for why such normalisation is effective.\n","authors":["Guoxuan Xia","Olivier Laurent","Gianni Franchi","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2403.14715v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.14616v1","updated":"2025-02-20T14:57:01Z","published":"2025-02-20T14:57:01Z","title":"Monocular Depth Estimation and Segmentation for Transparent Object with\n  Iterative Semantic and Geometric Fusion","summary":"  Transparent object perception is indispensable for numerous robotic tasks.\nHowever, accurately segmenting and estimating the depth of transparent objects\nremain challenging due to complex optical properties. Existing methods\nprimarily delve into only one task using extra inputs or specialized sensors,\nneglecting the valuable interactions among tasks and the subsequent refinement\nprocess, leading to suboptimal and blurry predictions. To address these issues,\nwe propose a monocular framework, which is the first to excel in both\nsegmentation and depth estimation of transparent objects, with only a\nsingle-image input. Specifically, we devise a novel semantic and geometric\nfusion module, effectively integrating the multi-scale information between\ntasks. In addition, drawing inspiration from human perception of objects, we\nfurther incorporate an iterative strategy, which progressively refines initial\nfeatures for clearer results. Experiments on two challenging synthetic and\nreal-world datasets demonstrate that our model surpasses state-of-the-art\nmonocular, stereo, and multi-view methods by a large margin of about\n38.8%-46.2% with only a single RGB input. Codes and models are publicly\navailable at https://github.com/L-J-Yuan/MODEST.\n","authors":["Jiangyuan Liu","Hongxuan Ma","Yuxin Guo","Yuhao Zhao","Chi Zhang","Wei Sui","Wei Zou"],"pdf_url":"https://arxiv.org/pdf/2502.14616v1.pdf","comment":"Accepted by ICRA(2025). The code is accessible through:\n  https://github.com/L-J-Yuan/MODEST"},{"id":"http://arxiv.org/abs/2308.05480v2","updated":"2025-02-20T14:53:38Z","published":"2023-08-10T10:12:27Z","title":"YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time\n  Object Detection","summary":"  We aim at providing the object detection community with an efficient and\nperformant object detector, termed YOLO-MS. The core design is based on a\nseries of investigations on how multi-branch features of the basic block and\nconvolutions with different kernel sizes affect the detection performance of\nobjects at different scales. The outcome is a new strategy that can\nsignificantly enhance multi-scale feature representations of real-time object\ndetectors. To verify the effectiveness of our work, we train our YOLO-MS on the\nMS COCO dataset from scratch without relying on any other large-scale datasets,\nlike ImageNet or pre-trained weights. Without bells and whistles, our YOLO-MS\noutperforms the recent state-of-the-art real-time object detectors, including\nYOLO-v7, RTMDet, and YOLO-v8. Taking the XS version of YOLO-MS as an example,\nit can achieve an AP score of 42+% on MS COCO, which is about 2% higher than\nRTMDet with the same model size. Furthermore, our work can also serve as a\nplug-and-play module for other YOLO models. Typically, our method significantly\nadvances the APs, APl, and AP of YOLOv8-N from 18%+, 52%+, and 37%+ to 20%+,\n55%+, and 40%+, respectively, with even fewer parameters and MACs. Code and\ntrained models are publicly available at\nhttps://github.com/FishAndWasabi/YOLO-MS. We also provide the Jittor version at\nhttps://github.com/NK-JittorCV/nk-yolo.\n","authors":["Yuming Chen","Xinbin Yuan","Jiabao Wang","Ruiqi Wu","Xiang Li","Qibin Hou","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2308.05480v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.14584v1","updated":"2025-02-20T14:13:46Z","published":"2025-02-20T14:13:46Z","title":"Vision Foundation Models in Medical Image Analysis: Advances and\n  Challenges","summary":"  The rapid development of Vision Foundation Models (VFMs), particularly Vision\nTransformers (ViT) and Segment Anything Model (SAM), has sparked significant\nadvances in the field of medical image analysis. These models have demonstrated\nexceptional capabilities in capturing long-range dependencies and achieving\nhigh generalization in segmentation tasks. However, adapting these large models\nto medical image analysis presents several challenges, including domain\ndifferences between medical and natural images, the need for efficient model\nadaptation strategies, and the limitations of small-scale medical datasets.\nThis paper reviews the state-of-the-art research on the adaptation of VFMs to\nmedical image segmentation, focusing on the challenges of domain adaptation,\nmodel compression, and federated learning. We discuss the latest developments\nin adapter-based improvements, knowledge distillation techniques, and\nmulti-scale contextual feature modeling, and propose future directions to\novercome these bottlenecks. Our analysis highlights the potential of VFMs,\nalong with emerging methodologies such as federated learning and model\ncompression, to revolutionize medical image analysis and enhance clinical\napplications. The goal of this work is to provide a comprehensive overview of\ncurrent approaches and suggest key areas for future research that can drive the\nnext wave of innovation in medical image segmentation.\n","authors":["Pengchen Liang","Bin Pu","Haishan Huang","Yiwei Li","Hualiang Wang","Weibo Ma","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2502.14584v1.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.16340v4","updated":"2025-02-20T14:12:01Z","published":"2024-08-29T08:23:57Z","title":"Learned Image Transmission with Hierarchical Variational Autoencoder","summary":"  In this paper, we introduce an innovative hierarchical joint source-channel\ncoding (HJSCC) framework for image transmission, utilizing a hierarchical\nvariational autoencoder (VAE). Our approach leverages a combination of\nbottom-up and top-down paths at the transmitter to autoregressively generate\nmultiple hierarchical representations of the original image. These\nrepresentations are then directly mapped to channel symbols for transmission by\nthe JSCC encoder. We extend this framework to scenarios with a feedback link,\nmodeling transmission over a noisy channel as a probabilistic sampling process\nand deriving a novel generative formulation for JSCC with feedback. Compared\nwith existing approaches, our proposed HJSCC provides enhanced adaptability by\ndynamically adjusting transmission bandwidth, encoding these representations\ninto varying amounts of channel symbols. Extensive experiments on images of\nvarying resolutions demonstrate that our proposed model outperforms existing\nbaselines in rate-distortion performance and maintains robustness against\nchannel noise. The source code will be made available upon acceptance.\n","authors":["Guangyi Zhang","Hanlei Li","Yunlong Cai","Qiyu Hu","Guanding Yu","Runmin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.16340v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14573v1","updated":"2025-02-20T13:59:40Z","published":"2025-02-20T13:59:40Z","title":"Self-supervised Monocular Depth Estimation Robust to Reflective Surface\n  Leveraged by Triplet Mining","summary":"  Self-supervised monocular depth estimation (SSMDE) aims to predict the dense\ndepth map of a monocular image, by learning depth from RGB image sequences,\neliminating the need for ground-truth depth labels. Although this approach\nsimplifies data acquisition compared to supervised methods, it struggles with\nreflective surfaces, as they violate the assumptions of Lambertian reflectance,\nleading to inaccurate training on such surfaces. To tackle this problem, we\npropose a novel training strategy for an SSMDE by leveraging triplet mining to\npinpoint reflective regions at the pixel level, guided by the camera geometry\nbetween different viewpoints. The proposed reflection-aware triplet mining loss\nspecifically penalizes the inappropriate photometric error minimization on the\nlocalized reflective regions while preserving depth accuracy in non-reflective\nareas. We also incorporate a reflection-aware knowledge distillation method\nthat enables a student model to selectively learn the pixel-level knowledge\nfrom reflective and non-reflective regions. This results in robust depth\nestimation across areas. Evaluation results on multiple datasets demonstrate\nthat our method effectively enhances depth quality on reflective surfaces and\noutperforms state-of-the-art SSMDE baselines.\n","authors":["Wonhyeok Choi","Kyumin Hwang","Wei Peng","Minwoo Choi","Sunghoon Im"],"pdf_url":"https://arxiv.org/pdf/2502.14573v1.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2408.08822v3","updated":"2025-02-20T13:21:01Z","published":"2024-08-16T16:12:44Z","title":"PFDiff: Training-Free Acceleration of Diffusion Models Combining Past\n  and Future Scores","summary":"  Diffusion Probabilistic Models (DPMs) have shown remarkable potential in\nimage generation, but their sampling efficiency is hindered by the need for\nnumerous denoising steps. Most existing solutions accelerate the sampling\nprocess by proposing fast ODE solvers. However, the inevitable discretization\nerrors of the ODE solvers are significantly magnified when the number of\nfunction evaluations (NFE) is fewer. In this work, we propose PFDiff, a novel\ntraining-free and orthogonal timestep-skipping strategy, which enables existing\nfast ODE solvers to operate with fewer NFE. Specifically, PFDiff initially\nutilizes score replacement from past time steps to predict a ``springboard\".\nSubsequently, it employs this ``springboard\" along with foresight updates\ninspired by Nesterov momentum to rapidly update current intermediate states.\nThis approach effectively reduces unnecessary NFE while correcting for\ndiscretization errors inherent in first-order ODE solvers. Experimental results\ndemonstrate that PFDiff exhibits flexible applicability across various\npre-trained DPMs, particularly excelling in conditional DPMs and surpassing\nprevious state-of-the-art training-free methods. For instance, using DDIM as a\nbaseline, we achieved 16.46 FID (4 NFE) compared to 138.81 FID with DDIM on\nImageNet 64x64 with classifier guidance, and 13.06 FID (10 NFE) on Stable\nDiffusion with 7.5 guidance scale. Code is available at\n\\url{https://github.com/onefly123/PFDiff}.\n","authors":["Guangyi Wang","Yuren Cai","Lijiang Li","Wei Peng","Songzhi Su"],"pdf_url":"https://arxiv.org/pdf/2408.08822v3.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.03293v4","updated":"2025-02-20T13:17:01Z","published":"2024-06-05T14:02:31Z","title":"Text-to-Image Rectified Flow as Plug-and-Play Priors","summary":"  Large-scale diffusion models have achieved remarkable performance in\ngenerative tasks. Beyond their initial training applications, these models have\nproven their ability to function as versatile plug-and-play priors. For\ninstance, 2D diffusion models can serve as loss functions to optimize 3D\nimplicit models. Rectified flow, a novel class of generative models, enforces a\nlinear progression from the source to the target distribution and has\ndemonstrated superior performance across various domains. Compared to\ndiffusion-based methods, rectified flow approaches surpass in terms of\ngeneration quality and efficiency, requiring fewer inference steps. In this\nwork, we present theoretical and experimental evidence demonstrating that\nrectified flow based methods offer similar functionalities to diffusion models\n- they can also serve as effective priors. Besides the generative capabilities\nof diffusion priors, motivated by the unique time-symmetry properties of\nrectified flow models, a variant of our method can additionally perform image\ninversion. Experimentally, our rectified flow-based priors outperform their\ndiffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our\nmethod also displays competitive performance in image inversion and editing.\n","authors":["Xiaofeng Yang","Cheng Chen","Xulei Yang","Fayao Liu","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2406.03293v4.pdf","comment":"ICLR 2025 Camera Ready. Code:\n  https://github.com/yangxiaofeng/rectified_flow_prior"},{"id":"http://arxiv.org/abs/2311.11782v2","updated":"2025-02-20T13:12:27Z","published":"2023-11-20T14:07:38Z","title":"Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural\n  Networks","summary":"  Segmenting the boundary between tumor and healthy tissue during surgical\ncancer resection poses a significant challenge. In recent years, Hyperspectral\nImaging (HSI) combined with Machine Learning (ML) has emerged as a promising\nsolution. However, due to the extensive information contained within the\nspectral domain, most ML approaches primarily classify individual HSI\n(super-)pixels, or tiles, without taking into account their spatial context. In\nthis paper, we propose an improved methodology that leverages the spatial\ncontext of tiles for more robust and smoother segmentation. To address the\nirregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate\ncontext information across neighboring regions. The features for each tile\nwithin the graph are extracted using a Convolutional Neural Network (CNN),\nwhich is trained simultaneously with the subsequent GNN. Moreover, we\nincorporate local image quality metrics into the loss function to enhance the\ntraining procedure's robustness against low-quality regions in the training\nimages. We demonstrate the superiority of our proposed method using a clinical\nex vivo dataset consisting of 51 HSI images from 30 patients. Despite the\nlimited dataset, the GNN-based model significantly outperforms context-agnostic\napproaches, accurately distinguishing between healthy and tumor tissues, even\nin images from previously unseen patients. Furthermore, we show that our\ncarefully designed loss function, accounting for local image quality, results\nin additional improvements. Our findings demonstrate that context-aware GNN\nalgorithms can robustly find tumor demarcations on HSI images, ultimately\ncontributing to better surgery success and patient outcome.\n","authors":["Mayar Lotfy Mostafa","Anna Alperovich","Tommaso Giannantonio","Bjorn Barz","Xiaohan Zhang","Felix Holm","Nassir Navab","Felix Boehm","Carolin Schwamborn","Thomas K. Hoffmann","Patrick J. Schuler"],"pdf_url":"https://arxiv.org/pdf/2311.11782v2.pdf","comment":"18 pages, 5 figures, The German Conference on Pattern Recognition\n  (GCPR) 2024"},{"id":"http://arxiv.org/abs/2502.14520v1","updated":"2025-02-20T12:52:36Z","published":"2025-02-20T12:52:36Z","title":"Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance","summary":"  3D Semantic Scene Completion (SSC) provides comprehensive scene geometry and\nsemantics for autonomous driving perception, which is crucial for enabling\naccurate and reliable decision-making. However, existing SSC methods are\nlimited to capturing sparse information from the current frame or naively\nstacking multi-frame temporal features, thereby failing to acquire effective\nscene context. These approaches ignore critical motion dynamics and struggle to\nachieve temporal consistency. To address the above challenges, we propose a\nnovel temporal SSC method FlowScene: Learning Temporal 3D Semantic Scene\nCompletion via Optical Flow Guidance. By leveraging optical flow, FlowScene can\nintegrate motion, different viewpoints, occlusions, and other contextual cues,\nthereby significantly improving the accuracy of 3D scene completion.\nSpecifically, our framework introduces two key components: (1) a Flow-Guided\nTemporal Aggregation module that aligns and aggregates temporal features using\noptical flow, capturing motion-aware context and deformable structures; and (2)\nan Occlusion-Guided Voxel Refinement module that injects occlusion masks and\ntemporally aggregated features into 3D voxel space, adaptively refining voxel\nrepresentations for explicit geometric modeling. Experimental results\ndemonstrate that FlowScene achieves state-of-the-art performance on the\nSemanticKITTI and SSCBench-KITTI-360 benchmarks.\n","authors":["Meng Wang","Fan Wu","Ruihui Li","Yunchuan Qin","Zhuo Tang","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2502.14520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14514v1","updated":"2025-02-20T12:39:08Z","published":"2025-02-20T12:39:08Z","title":"A Mobile Robotic Approach to Autonomous Surface Scanning in Legal\n  Medicine","summary":"  Purpose: Comprehensive legal medicine documentation includes both an internal\nbut also an external examination of the corpse. Typically, this documentation\nis conducted manually during conventional autopsy. A systematic digital\ndocumentation would be desirable, especially for the external examination of\nwounds, which is becoming more relevant for legal medicine analysis. For this\npurpose, RGB surface scanning has been introduced. While a manual full surface\nscan using a handheld camera is timeconsuming and operator dependent, floor or\nceiling mounted robotic systems require substantial space and a dedicated room.\nHence, we consider whether a mobile robotic system can be used for external\ndocumentation. Methods: We develop a mobile robotic system that enables\nfull-body RGB-D surface scanning. Our work includes a detailed configuration\nspace analysis to identify the environmental parameters that need to be\nconsidered to successfully perform a surface scan. We validate our findings\nthrough an experimental study in the lab and demonstrate the system's\napplication in a legal medicine environment. Results: Our configuration space\nanalysis shows that a good trade-off between coverage and time is reached with\nthree robot base positions, leading to a coverage of 94.96 %. Experiments\nvalidate the effectiveness of the system in accurately capturing body surface\ngeometry with an average surface coverage of 96.90 +- 3.16 % and 92.45 +- 1.43\n% for a body phantom and actual corpses, respectively. Conclusion: This work\ndemonstrates the potential of a mobile robotic system to automate RGB-D surface\nscanning in legal medicine, complementing the use of post-mortem CT scans for\ninner documentation. Our results indicate that the proposed system can\ncontribute to more efficient and autonomous legal medicine documentation,\nreducing the need for manual intervention.\n","authors":["Sarah Grube","Sarah Latus","Martin Fischer","Vidas Raudonis","Axel Heinemann","Benjamin Ondruschka","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2502.14514v1.pdf","comment":"Submitted and accepted for presentation at CARS 2025. This preprint\n  has not undergone peer review or post-submission revisions. The final version\n  of this work will appear in the official CARS 2025 proceedings"},{"id":"http://arxiv.org/abs/2502.14504v1","updated":"2025-02-20T12:31:31Z","published":"2025-02-20T12:31:31Z","title":"PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.\n","authors":["Yu Meng","Kaiyuan Li","Chenran Huang","Chen Gao","Xinlei Chen","Yong Li","Xiaoping Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14504v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.14503v1","updated":"2025-02-20T12:31:24Z","published":"2025-02-20T12:31:24Z","title":"LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of\n  4D Radar and Camera","summary":"  As the previous state-of-the-art 4D radar-camera fusion-based 3D object\ndetection method, LXL utilizes the predicted image depth distribution maps and\nradar 3D occupancy grids to assist the sampling-based image view\ntransformation. However, the depth prediction lacks accuracy and consistency,\nand the concatenation-based fusion in LXL impedes the model robustness. In this\nwork, we propose LXLv2, where modifications are made to overcome the\nlimitations and improve the performance. Specifically, considering the position\nerror in radar measurements, we devise a one-to-many depth supervision strategy\nvia radar points, where the radar cross section (RCS) value is further\nexploited to adjust the supervision area for object-level depth consistency.\nAdditionally, a channel and spatial attention-based fusion module named\nCSAFusion is introduced to improve feature adaptiveness. Experimental results\non the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can\noutperform LXL in detection accuracy, inference speed and robustness,\ndemonstrating the effectiveness of the model.\n","authors":["Weiyi Xiong","Zean Zou","Qiuchi Zhao","Fengchun He","Bing Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.14503v1.pdf","comment":"Accepted by IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2502.14495v1","updated":"2025-02-20T12:25:30Z","published":"2025-02-20T12:25:30Z","title":"Nearshore Underwater Target Detection Meets UAV-borne Hyperspectral\n  Remote Sensing: A Novel Hybrid-level Contrastive Learning Framework and\n  Benchmark Dataset","summary":"  UAV-borne hyperspectral remote sensing has emerged as a promising approach\nfor underwater target detection (UTD). However, its effectiveness is hindered\nby spectral distortions in nearshore environments, which compromise the\naccuracy of traditional hyperspectral UTD (HUTD) methods that rely on\nbathymetric model. These distortions lead to significant uncertainty in target\nand background spectra, challenging the detection process. To address this, we\npropose the Hyperspectral Underwater Contrastive Learning Network (HUCLNet), a\nnovel framework that integrates contrastive learning with a self-paced learning\nparadigm for robust HUTD in nearshore regions. HUCLNet extracts discriminative\nfeatures from distorted hyperspectral data through contrastive learning, while\nthe self-paced learning strategy selectively prioritizes the most informative\nsamples. Additionally, a reliability-guided clustering strategy enhances the\nrobustness of learned representations.To evaluate the method effectiveness, we\nconduct a novel nearshore HUTD benchmark dataset, ATR2-HUTD, covering three\ndiverse scenarios with varying water types and turbidity, and target types.\nExtensive experiments demonstrate that HUCLNet significantly outperforms\nstate-of-the-art methods. The dataset and code will be publicly available at:\nhttps://github.com/qjh1996/HUTD\n","authors":["Jiahao Qi","Chuanhong Zhou","Xingyue Liu","Chen Chen","Dehui Zhu","Kangcheng Bin","Ping Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.14495v1.pdf","comment":"18pages,13figures"},{"id":"http://arxiv.org/abs/2502.14493v1","updated":"2025-02-20T12:19:30Z","published":"2025-02-20T12:19:30Z","title":"CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor\n  Top-K Vision Alignment and Beyond","summary":"  Infrared and visible image fusion (IVIF) is increasingly applied in critical\nfields such as video surveillance and autonomous driving systems. Significant\nprogress has been made in deep learning-based fusion methods. However, these\nmodels frequently encounter out-of-distribution (OOD) scenes in real-world\napplications, which severely impact their performance and reliability.\nTherefore, addressing the challenge of OOD data is crucial for the safe\ndeployment of these models in open-world environments. Unlike existing\nresearch, our focus is on the challenges posed by OOD data in real-world\napplications and on enhancing the robustness and generalization of models. In\nthis paper, we propose an infrared-visible fusion framework based on Multi-View\nAugmentation. For external data augmentation, Top-k Selective Vision Alignment\nis employed to mitigate distribution shifts between datasets by performing\nRGB-wise transformations on visible images. This strategy effectively\nintroduces augmented samples, enhancing the adaptability of the model to\ncomplex real-world scenarios. Additionally, for internal data augmentation,\nself-supervised learning is established using Weak-Aggressive Augmentation.\nThis enables the model to learn more robust and general feature representations\nduring the fusion process, thereby improving robustness and generalization.\nExtensive experiments demonstrate that the proposed method exhibits superior\nperformance and robustness across various conditions and environments. Our\napproach significantly enhances the reliability and stability of IVIF tasks in\npractical applications.\n","authors":["Yukai Shi","Cidan Shi","Zhipeng Weng","Yin Tian","Xiaoyu Xian","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.14493v1.pdf","comment":"IEEE T-CSVT. We mainly discuss the out-of-distribution challenges in\n  infrared and visible image fusion"},{"id":"http://arxiv.org/abs/2410.00486v3","updated":"2025-02-20T12:14:13Z","published":"2024-10-01T08:18:12Z","title":"CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with\nphotorealistic scene reconstruction emerging as a key challenge. To address\nthis, we introduce Computational Alignment for Real-Time Gaussian Splatting\nSLAM (CaRtGS), a novel method enhancing the efficiency and quality of\nphotorealistic scene reconstruction in real-time environments. Leveraging 3D\nGaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and\nprocessing speed, which is crucial for scene photorealistic reconstruction. Our\napproach tackles computational misalignment in Gaussian Splatting SLAM\n(GS-SLAM) through an adaptive strategy that enhances optimization iterations,\naddresses long-tail optimization, and refines densification. Experiments on\nReplica, TUM-RGBD, and VECtor datasets demonstrate CaRtGS's effectiveness in\nachieving high-fidelity rendering with fewer Gaussian primitives. This work\npropels SLAM towards real-time, photorealistic dense rendering, significantly\nadvancing photorealistic scene representation. For the benefit of the research\ncommunity, we release the code and accompanying videos on our project website:\nhttps://dapengfeng.github.io/cartgs.\n","authors":["Dapeng Feng","Zhiqiang Chen","Yizhen Yin","Shipeng Zhong","Yuhua Qi","Hongbo Chen"],"pdf_url":"https://arxiv.org/pdf/2410.00486v3.pdf","comment":"Accepted by IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2502.14487v1","updated":"2025-02-20T12:09:30Z","published":"2025-02-20T12:09:30Z","title":"Temporal Misalignment and Probabilistic Neurons","summary":"  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to\nArtificial Neural Networks (ANNs) by mimicking biological neural principles,\nestablishing them as a promising approach to mitigate the increasing energy\ndemands of large-scale neural models. However, fully harnessing the\ncapabilities of SNNs remains challenging due to their discrete signal\nprocessing and temporal dynamics. ANN-SNN conversion has emerged as a practical\napproach, enabling SNNs to achieve competitive performance on complex machine\nlearning tasks. In this work, we identify a phenomenon in the ANN-SNN\nconversion framework, termed temporal misalignment, in which random spike\nrearrangement across SNN layers leads to performance improvements. Based on\nthis observation, we introduce biologically plausible two-phase probabilistic\n(TPP) spiking neurons, further enhancing the conversion process. We demonstrate\nthe advantages of our proposed method both theoretically and empirically\nthrough comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet\nacross a variety of architectures, achieving state-of-the-art results.\n","authors":["Velibor Bojković","Xiaofeng Wu","Bin Gu"],"pdf_url":"https://arxiv.org/pdf/2502.14487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12788v3","updated":"2025-02-20T12:02:11Z","published":"2024-02-20T07:56:02Z","title":"RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse\n  Attention","summary":"  Remote photoplethysmography (rPPG) is a non-contact method for detecting\nphysiological signals based on facial videos, holding high potential in various\napplications. Due to the periodicity nature of rPPG signals, the long-range\ndependency capturing capacity of the transformer was assumed to be advantageous\nfor such signals. However, existing methods have not conclusively demonstrated\nthe superior performance of transformers over traditional convolutional neural\nnetworks. This may be attributed to the quadratic scaling exhibited by\ntransformer with sequence length, resulting in coarse-grained feature\nextraction, which in turn affects robustness and generalization. To address\nthat, this paper proposes a periodic sparse attention mechanism based on\ntemporal attention sparsity induced by periodicity. A pre-attention stage is\nintroduced before the conventional attention mechanism. This stage learns\nperiodic patterns to filter out a large number of irrelevant attention\ncomputations, thus enabling fine-grained feature extraction. Moreover, to\naddress the issue of fine-grained features being more susceptible to noise\ninterference, a fusion stem is proposed to effectively guide self-attention\ntowards rPPG features. It can be easily integrated into existing methods to\nenhance their performance. Extensive experiments show that the proposed method\nachieves state-of-the-art performance in both intra-dataset and cross-dataset\nevaluations. The codes are available at\nhttps://github.com/zizheng-guo/RhythmFormer.\n","authors":["Bochao Zou","Zizheng Guo","Jiansheng Chen","Junbao Zhuo","Weiran Huang","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2402.12788v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14471v1","updated":"2025-02-20T11:49:50Z","published":"2025-02-20T11:49:50Z","title":"Integrating Extra Modality Helps Segmentor Find Camouflaged Objects Well","summary":"  Camouflaged Object Segmentation (COS) remains a challenging problem due to\nthe subtle visual differences between camouflaged objects and backgrounds.\nOwing to the exceedingly limited visual cues available from visible spectrum,\nprevious RGB single-modality approaches often struggle to achieve satisfactory\nresults, prompting the exploration of multimodal data to enhance detection\naccuracy. In this work, we present UniCOS, a novel framework that effectively\nleverages diverse data modalities to improve segmentation performance. UniCOS\ncomprises two key components: a multimodal segmentor, UniSEG, and a cross-modal\nknowledge learning module, UniLearner. UniSEG employs a state space fusion\nmechanism to integrate cross-modal features within a unified state space,\nenhancing contextual understanding and improving robustness to integration of\nheterogeneous data. Additionally, it includes a fusion-feedback mechanism that\nfacilitate feature extraction. UniLearner exploits multimodal data unrelated to\nthe COS task to improve the segmentation ability of the COS models by\ngenerating pseudo-modal content and cross-modal semantic associations.\nExtensive experiments demonstrate that UniSEG outperforms existing Multimodal\nCOS (MCOS) segmentors, regardless of whether real or pseudo-multimodal COS data\nis available. Moreover, in scenarios where multimodal COS data is unavailable\nbut multimodal non-COS data is accessible, UniLearner effectively exploits\nthese data to enhance segmentation performance. Our code will be made publicly\navailable on \\href{https://github.com/cnyvfang/UniCOS}{GitHub}.\n","authors":["Chengyu Fang","Chunming He","Longxiang Tang","Yuelin Zhang","Chenyang Zhu","Yuqi Shen","Chubin Chen","Guoxia Xu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2502.14471v1.pdf","comment":"12 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.14462v1","updated":"2025-02-20T11:33:17Z","published":"2025-02-20T11:33:17Z","title":"Single-image Reflectance and Transmittance Estimation from Any Flatbed\n  Scanner","summary":"  Flatbed scanners have emerged as promising devices for high-resolution,\nsingle-image material capture. However, existing approaches assume very\nspecific conditions, such as uniform diffuse illumination, which are only\navailable in certain high-end devices, hindering their scalability and cost. In\ncontrast, in this work, we introduce a method inspired by intrinsic image\ndecomposition, which accurately removes both shading and specularity,\neffectively allowing captures with any flatbed scanner. Further, we extend\nprevious work on single-image material reflectance capture with the estimation\nof opacity and transmittance, critical components of full material appearance\n(SVBSDF), improving the results for any material captured with a flatbed\nscanner, at a very high resolution and accuracy\n","authors":["Carlos Rodriguez-Pardo","David Pascual-Hernandez","Javier Rodriguez-Vazquez","Jorge Lopez-Moreno","Elena Garces"],"pdf_url":"https://arxiv.org/pdf/2502.14462v1.pdf","comment":"Accepted to Computers & Graphics"},{"id":"http://arxiv.org/abs/2406.02506v3","updated":"2025-02-20T11:23:21Z","published":"2024-06-04T17:24:19Z","title":"An Open-Source Tool for Mapping War Destruction at Scale in Ukraine\n  using Sentinel-1 Time Series","summary":"  Access to detailed war impact assessments is crucial for humanitarian\norganizations to assist affected populations effectively. However, maintaining\na comprehensive understanding of the situation on the ground is challenging,\nespecially in widespread and prolonged conflicts. Here we present a scalable\nmethod for estimating building damage resulting from armed conflicts. By\ntraining a machine learning model on Synthetic Aperture Radar image time\nseries, we generate probabilistic damage estimates at the building level,\nleveraging existing damage assessments and open building footprints. To allow\nlarge-scale inference and ensure accessibility, we tie our method to run on\nGoogle Earth Engine. Users can adjust confidence intervals to suit their needs,\nenabling rapid and flexible assessments of war-related damage across large\nareas. We provide two publicly accessible dashboards: a Ukraine Damage Explorer\nto dynamically view our precomputed estimates, and a Rapid Damage Mapping Tool\nto run our method and generate custom maps.\n","authors":["Olivier Dietrich","Torben Peters","Vivien Sainte Fare Garnot","Valerie Sticher","Thao Ton-That Whelan","Konrad Schindler","Jan Dirk Wegner"],"pdf_url":"https://arxiv.org/pdf/2406.02506v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14454v1","updated":"2025-02-20T11:11:18Z","published":"2025-02-20T11:11:18Z","title":"Exploiting Deblurring Networks for Radiance Fields","summary":"  In this paper, we propose DeepDeblurRF, a novel radiance field deblurring\napproach that can synthesize high-quality novel views from blurred training\nviews with significantly reduced training time. DeepDeblurRF leverages deep\nneural network (DNN)-based deblurring modules to enjoy their deblurring\nperformance and computational efficiency. To effectively combine DNN-based\ndeblurring and radiance field construction, we propose a novel radiance field\n(RF)-guided deblurring and an iterative framework that performs RF-guided\ndeblurring and radiance field construction in an alternating manner. Moreover,\nDeepDeblurRF is compatible with various scene representations, such as voxel\ngrids and 3D Gaussians, expanding its applicability. We also present\nBlurRF-Synth, the first large-scale synthetic dataset for training radiance\nfield deblurring frameworks. We conduct extensive experiments on both camera\nmotion blur and defocus blur, demonstrating that DeepDeblurRF achieves\nstate-of-the-art novel-view synthesis quality with significantly reduced\ntraining time.\n","authors":["Haeyun Choi","Heemin Yang","Janghyeok Han","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2502.14454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07175v2","updated":"2025-02-20T10:59:40Z","published":"2024-12-10T04:17:22Z","title":"Robust Feature Engineering Techniques for Designing Efficient Motor\n  Imagery-Based BCI-Systems","summary":"  A multitude of individuals across the globe grapple with motor disabilities.\nNeural prosthetics utilizing Brain-Computer Interface (BCI) technology exhibit\npromise for improving motor rehabilitation outcomes. The intricate nature of\nEEG data poses a significant hurdle for current BCI systems. Recently, a\nqualitative repository of EEG signals tied to both upper and lower limb\nexecution of motor and motor imagery tasks has been unveiled. Despite this, the\nproductivity of the Machine Learning (ML) Models that were trained on this\ndataset was alarmingly deficient, and the evaluation framework seemed\ninsufficient. To enhance outcomes, robust feature engineering (signal\nprocessing) methodologies are implemented. A collection of time domain,\nfrequency domain, and wavelet-derived features was obtained from 16-channel EEG\nsignals, and the Maximum Relevance Minimum Redundancy (MRMR) approach was\nemployed to identify the four most significant features. For classification K\nNearest Neighbors (KNN), Support Vector Machine (SVM), Decision Tree (DT), and\nNa\\\"ive Bayes (NB) models were implemented with these selected features,\nevaluating their effectiveness through metrics such as testing accuracy,\nprecision, recall, and F1 Score. By leveraging SVM with a Gaussian Kernel, a\nremarkable maximum testing accuracy of 92.50% for motor activities and 95.48%\nfor imagery activities is achieved. These results are notably more dependable\nand gratifying compared to the previous study, where the peak accuracy was\nrecorded at 74.36%. This research work provides an in-depth analysis of the MI\nLimb EEG dataset and it will help in designing and developing simple,\ncost-effective and reliable BCI systems for neuro-rehabilitation.\n","authors":["Syed Saim Gardezi","Soyiba Jawed","Mahnoor Khan","Muneeba Bukhari","Rizwan Ahmed Khan"],"pdf_url":"https://arxiv.org/pdf/2412.07175v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2502.14442v1","updated":"2025-02-20T10:48:49Z","published":"2025-02-20T10:48:49Z","title":"Stochastic Resonance Improves the Detection of Low Contrast Images in\n  Deep Learning Models","summary":"  Stochastic resonance describes the utility of noise in improving the\ndetectability of weak signals in certain types of systems. It has been observed\nwidely in natural and engineered settings, but its utility in image\nclassification with rate-based neural networks has not been studied\nextensively. In this analysis a simple LSTM recurrent neural network is trained\nfor digit recognition and classification. During the test phase, image contrast\nis reduced to a point where the model fails to recognize the presence of a\nstimulus. Controlled noise is added to partially recover classification\nperformance. The results indicate the presence of stochastic resonance in\nrate-based recurrent neural networks.\n","authors":["Siegfried Ludwig"],"pdf_url":"https://arxiv.org/pdf/2502.14442v1.pdf","comment":"MSc Course Project"},{"id":"http://arxiv.org/abs/2409.06490v4","updated":"2025-02-20T10:35:34Z","published":"2024-09-09T13:27:53Z","title":"UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection","summary":"  The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance,\nsecurity, and airspace management has created an urgent demand for precise,\nscalable, and efficient UAV detection. However, existing datasets often suffer\nfrom limited scale diversity and inaccurate annotations, hindering robust model\ndevelopment. This paper introduces UAVDB, a high-resolution UAV detection\ndataset constructed using Patch Intensity Convergence (PIC). This novel\ntechnique automatically generates high-fidelity bounding box annotations from\nUAV trajectory data~\\cite{li2020reconstruction}, eliminating the need for\nmanual labeling. UAVDB features single-class annotations with a fixed-camera\nsetup and consists of RGB frames capturing UAVs across various scales, from\nlarge-scale UAVs to near-single-pixel representations, along with challenging\nbackgrounds that pose difficulties for modern detectors. We first validate the\naccuracy and efficiency of PIC-generated bounding boxes by comparing\nIntersection over Union (IoU) performance and runtime against alternative\nannotation methods, demonstrating that PIC achieves higher annotation accuracy\nwhile being more efficient. Subsequently, we benchmark UAVDB using\nstate-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable\nresource for advancing long-range and high-resolution UAV detection.\n","authors":["Yu-Hsi Chen"],"pdf_url":"https://arxiv.org/pdf/2409.06490v4.pdf","comment":"9 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.14433v1","updated":"2025-02-20T10:35:01Z","published":"2025-02-20T10:35:01Z","title":"Daily Land Surface Temperature Reconstruction in Landsat Cross-Track\n  Areas Using Deep Ensemble Learning With Uncertainty Quantification","summary":"  Many real-world applications rely on land surface temperature (LST) data at\nhigh spatiotemporal resolution. In complex urban areas, LST exhibits\nsignificant variations, fluctuating dramatically within and across city blocks.\nLandsat provides high spatial resolution data at 100 meters but is limited by\nlong revisit time, with cloud cover further disrupting data collection. Here,\nwe propose DELAG, a deep ensemble learning method that integrates annual\ntemperature cycles and Gaussian processes, to reconstruct Landsat LST in\ncomplex urban areas. Leveraging the cross-track characteristics and\ndual-satellite operation of Landsat since 2021, we further enhance data\navailability to 4 scenes every 16 days. We select New York City, London and\nHong Kong from three different continents as study areas. Experiments show that\nDELAG successfully reconstructed LST in the three cities under clear-sky (RMSE\n= 0.73-0.96 K) and heavily-cloudy (RMSE = 0.84-1.62 K) situations, superior to\nexisting methods. Additionally, DELAG can quantify uncertainty that enhances\nLST reconstruction reliability. We further tested the reconstructed LST to\nestimate near-surface air temperature, achieving results (RMSE = 1.48-2.11 K)\ncomparable to those derived from clear-sky LST (RMSE = 1.63-2.02 K). The\nresults demonstrate the successful reconstruction through DELAG and highlight\nthe broader applications of LST reconstruction for estimating accurate air\ntemperature. Our study thus provides a novel and practical method for Landsat\nLST reconstruction, particularly suited for complex urban areas within Landsat\ncross-track areas, taking one step toward addressing complex climate events at\nhigh spatiotemporal resolution.\n","authors":["Shengjie Liu","Siqin Wang","Lu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14420v1","updated":"2025-02-20T10:16:18Z","published":"2025-02-20T10:16:18Z","title":"ChatVLA: Unified Multimodal Understanding and Robot Control with\n  Vision-Language-Action Model","summary":"  Humans possess a unified cognitive ability to perceive, comprehend, and\ninteract with the physical world. Why can't large language models replicate\nthis holistic understanding? Through a systematic analysis of existing training\nparadigms in vision-language-action models (VLA), we identify two key\nchallenges: spurious forgetting, where robot training overwrites crucial\nvisual-text alignments, and task interference, where competing control and\nunderstanding tasks degrade performance when trained jointly. To overcome these\nlimitations, we propose ChatVLA, a novel framework featuring Phased Alignment\nTraining, which incrementally integrates multimodal data after initial control\nmastery, and a Mixture-of-Experts architecture to minimize task interference.\nChatVLA demonstrates competitive performance on visual question-answering\ndatasets and significantly surpasses state-of-the-art vision-language-action\n(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a\nsix times higher performance on MMMU and scores 47.2% on MMStar with a more\nparameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates\nsuperior performance on 25 real-world robot manipulation tasks compared to\nexisting VLA methods like OpenVLA. Our findings highlight the potential of our\nunified framework for achieving both robust multimodal understanding and\neffective robot control.\n","authors":["Zhongyi Zhou","Yichen Zhu","Minjie Zhu","Junjie Wen","Ning Liu","Zhiyuan Xu","Weibin Meng","Ran Cheng","Yaxin Peng","Chaomin Shen","Feifei Feng"],"pdf_url":"https://arxiv.org/pdf/2502.14420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14418v1","updated":"2025-02-20T10:15:43Z","published":"2025-02-20T10:15:43Z","title":"Role of the Pretraining and the Adaptation data sizes for low-resource\n  real-time MRI video segmentation","summary":"  Real-time Magnetic Resonance Imaging (rtMRI) is frequently used in speech\nproduction studies as it provides a complete view of the vocal tract during\narticulation. This study investigates the effectiveness of rtMRI in analyzing\nvocal tract movements by employing the SegNet and UNet models for Air-Tissue\nBoundary (ATB)segmentation tasks. We conducted pretraining of a few base models\nusing increasing numbers of subjects and videos, to assess performance on two\ndatasets. First, consisting of unseen subjects with unseen videos from the same\ndata source, achieving 0.33% and 0.91% (Pixel-wise Classification Accuracy\n(PCA) and Dice Coefficient respectively) better than its matched condition.\nSecond, comprising unseen videos from a new data source, where we obtained an\naccuracy of 99.63% and 98.09% (PCA and Dice Coefficient respectively) of its\nmatched condition performance. Here, matched condition performance refers to\nthe performance of a model trained only on the test subjects which was set as a\nbenchmark for the other models. Our findings highlight the significance of\nfine-tuning and adapting models with limited data. Notably, we demonstrated\nthat effective model adaptation can be achieved with as few as 15 rtMRI frames\nfrom any new dataset.\n","authors":["Masoud Thajudeen Tholan","Vinayaka Hegde","Chetan Sharma","Prasanta Kumar Ghosh"],"pdf_url":"https://arxiv.org/pdf/2502.14418v1.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2407.01230v3","updated":"2025-02-20T10:15:23Z","published":"2024-07-01T12:22:16Z","title":"DaBiT: Depth and Blur informed Transformer for Video Focal Deblurring","summary":"  In many real-world scenarios, recorded videos suffer from accidental focus\nblur, and while video deblurring methods exist, most specifically target motion\nblur or spatial-invariant blur. This paper introduces a framework optimized for\nthe as yet unattempted task of video focal deblurring (refocusing). The\nproposed method employs novel map-guided transformers, in addition to image\npropagation, to effectively leverage the continuous spatial variance of focal\nblur and restore the footage. We also introduce a flow re-focusing module\ndesigned to efficiently align relevant features between blurry and sharp\ndomains. Additionally, we propose a novel technique for generating synthetic\nfocal blur data, broadening the model's learning capabilities and robustness to\ninclude a wider array of content. We have made a new benchmark dataset,\nDAVIS-Blur, available. This dataset, a modified extension of the popular DAVIS\nvideo segmentation set, provides realistic focal blur degradations as well as\nthe corresponding blur maps. Comprehensive experiments demonstrate the\nsuperiority of our approach. We achieve state-of-the-art results with an\naverage PSNR performance over 1.9dB greater than comparable existing video\nrestoration methods. Our source code and the developed databases will be made\navailable at https://github.com/crispianm/DaBiT\n","authors":["Crispian Morris","Nantheera Anantrasirichai","Fan Zhang","David Bull"],"pdf_url":"https://arxiv.org/pdf/2407.01230v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14412v1","updated":"2025-02-20T09:59:28Z","published":"2025-02-20T09:59:28Z","title":"Evaluating Precise Geolocation Inference Capabilities of Vision Language\n  Models","summary":"  The prevalence of Vision-Language Models (VLMs) raises important questions\nabout privacy in an era where visual information is increasingly available.\nWhile foundation VLMs demonstrate broad knowledge and learned capabilities, we\nspecifically investigate their ability to infer geographic location from\npreviously unseen image data. This paper introduces a benchmark dataset\ncollected from Google Street View that represents its global distribution of\ncoverage. Foundation models are evaluated on single-image geolocation\ninference, with many achieving median distance errors of <300 km. We further\nevaluate VLM \"agents\" with access to supplemental tools, observing up to a\n30.6% decrease in distance error. Our findings establish that modern foundation\nVLMs can act as powerful image geolocation tools, without being specifically\ntrained for this task. When coupled with increasing accessibility of these\nmodels, our findings have greater implications for online privacy. We discuss\nthese risks, as well as future work in this area.\n","authors":["Neel Jay","Hieu Minh Nguyen","Trung Dung Hoang","Jacob Haimes"],"pdf_url":"https://arxiv.org/pdf/2502.14412v1.pdf","comment":"AAAI 2025 Workshop DATASAFE"},{"id":"http://arxiv.org/abs/2502.14401v1","updated":"2025-02-20T09:38:13Z","published":"2025-02-20T09:38:13Z","title":"MedFuncta: Modality-Agnostic Representations Based on Efficient Neural\n  Fields","summary":"  Recent research in medical image analysis with deep learning almost\nexclusively focuses on grid- or voxel-based data representations. We challenge\nthis common choice by introducing MedFuncta, a modality-agnostic continuous\ndata representation based on neural fields. We demonstrate how to scale neural\nfields from single instances to large datasets by exploiting redundancy in\nmedical signals and by applying an efficient meta-learning approach with a\ncontext reduction scheme. We further address the spectral bias in commonly used\nSIREN activations, by introducing an $\\omega_0$-schedule, improving\nreconstruction quality and convergence speed. We validate our proposed approach\non a large variety of medical signals of different dimensions and modalities\n(1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, Colon\nHistopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfully\ndemonstrate that we can solve relevant downstream tasks on these\nrepresentations. We additionally release a large-scale dataset of > 550k\nannotated neural fields to promote research in this direction.\n","authors":["Paul Friedrich","Florentin Bieder","Phlippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2502.14401v1.pdf","comment":"Code and Dataset: https://github.com/pfriedri/medfuncta"},{"id":"http://arxiv.org/abs/2502.14397v1","updated":"2025-02-20T09:35:38Z","published":"2025-02-20T09:35:38Z","title":"PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data","summary":"  We introduce PhotoDoodle, a novel image editing framework designed to\nfacilitate photo doodling by enabling artists to overlay decorative elements\nonto photographs. Photo doodling is challenging because the inserted elements\nmust appear seamlessly integrated with the background, requiring realistic\nblending, perspective alignment, and contextual coherence. Additionally, the\nbackground must be preserved without distortion, and the artist's unique style\nmust be captured efficiently from limited training data. These requirements are\nnot addressed by previous methods that primarily focus on global style transfer\nor regional inpainting. The proposed method, PhotoDoodle, employs a two-stage\ntraining strategy. Initially, we train a general-purpose image editing model,\nOmniEditor, using large-scale data. Subsequently, we fine-tune this model with\nEditLoRA using a small, artist-curated dataset of before-and-after image pairs\nto capture distinct editing styles and techniques. To enhance consistency in\nthe generated results, we introduce a positional encoding reuse mechanism.\nAdditionally, we release a PhotoDoodle dataset featuring six high-quality\nstyles. Extensive experiments demonstrate the advanced performance and\nrobustness of our method in customized image editing, opening new possibilities\nfor artistic creation.\n","authors":["Shijie Huang","Yiren Song","Yuxuan Zhang","Hailong Guo","Xueyin Wang","Mike Zheng Shou","Jiaming Liu"],"pdf_url":"https://arxiv.org/pdf/2502.14397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14377v1","updated":"2025-02-20T09:10:05Z","published":"2025-02-20T09:10:05Z","title":"RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers","summary":"  The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta. More examples are available\nat https://relactrl.github.io/RelaCtrl/.\n","authors":["Ke Cao","Jing Wang","Ao Ma","Jiasong Feng","Zhanjie Zhang","Xuanhua He","Shanyuan Liu","Bo Cheng","Dawei Leng","Yuhui Yin","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14377v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.14376v1","updated":"2025-02-20T09:06:44Z","published":"2025-02-20T09:06:44Z","title":"A Similarity Paradigm Through Textual Regularization Without Forgetting","summary":"  Prompt learning has emerged as a promising method for adapting pre-trained\nvisual-language models (VLMs) to a range of downstream tasks. While optimizing\nthe context can be effective for improving performance on specific tasks, it\ncan often lead to poor generalization performance on unseen classes or datasets\nsampled from different distributions. It may be attributed to the fact that\ntextual prompts tend to overfit downstream data distributions, leading to the\nforgetting of generalized knowledge derived from hand-crafted prompts. In this\npaper, we propose a novel method called Similarity Paradigm with Textual\nRegularization (SPTR) for prompt learning without forgetting. SPTR is a\ntwo-pronged design based on hand-crafted prompts that is an inseparable\nframework. 1) To avoid forgetting general textual knowledge, we introduce the\noptimal transport as a textual regularization to finely ensure approximation\nwith hand-crafted features and tuning textual features. 2) In order to\ncontinuously unleash the general ability of multiple hand-crafted prompts, we\npropose a similarity paradigm for natural alignment score and adversarial\nalignment score to improve model robustness for generalization. Both modules\nshare a common objective in addressing generalization issues, aiming to\nmaximize the generalization capability derived from multiple hand-crafted\nprompts. Four representative tasks (i.e., non-generalization few-shot learning,\nbase-to-novel generalization, cross-dataset generalization, domain\ngeneralization) across 11 datasets demonstrate that SPTR outperforms existing\nprompt learning methods.\n","authors":["Fangming Cui","Jan Fong","Rongfei Zeng","Xinmei Tian","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2502.14376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14373v1","updated":"2025-02-20T09:05:35Z","published":"2025-02-20T09:05:35Z","title":"CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual\n  Try-on guided by Tri-zone Priors","summary":"  Despite remarkable progress in image-based virtual try-on systems, generating\nrealistic and robust fitting images for cross-category virtual try-on remains a\nchallenging task. The primary difficulty arises from the absence of human-like\nreasoning, which involves addressing size mismatches between garments and\nmodels while recognizing and leveraging the distinct functionalities of various\nregions within the model images. To address this issue, we draw inspiration\nfrom human cognitive processes and disentangle the complex reasoning required\nfor cross-category try-on into a structured framework. This framework\nsystematically decomposes the model image into three distinct regions: try-on,\nreconstruction, and imagination zones. Each zone plays a specific role in\naccommodating the garment and facilitating realistic synthesis. To endow the\nmodel with robust reasoning capabilities for cross-category scenarios, we\npropose an iterative data constructor. This constructor encompasses diverse\nscenarios, including intra-category try-on, any-to-dress transformations\n(replacing any garment category with a dress), and dress-to-any transformations\n(replacing a dress with another garment category). Utilizing the generated\ndataset, we introduce a tri-zone priors generator that intelligently predicts\nthe try-on, reconstruction, and imagination zones by analyzing how the input\ngarment is expected to align with the model image. Guided by these tri-zone\npriors, our proposed method, CrossVTON, achieves state-of-the-art performance,\nsurpassing existing baselines in both qualitative and quantitative evaluations.\nNotably, it demonstrates superior capability in handling cross-category virtual\ntry-on, meeting the complex demands of real-world applications.\n","authors":["Donghao Luo","Yujie Liang","Xu Peng","Xiaobin Hu","Boyuan Jiang","Chengming Xu","Taisong Jin","Chengjie Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.14373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00341v2","updated":"2025-02-20T09:02:23Z","published":"2024-06-01T07:35:21Z","title":"DSCA: A Digital Subtraction Angiography Sequence Dataset and\n  Spatio-Temporal Model for Cerebral Artery Segmentation","summary":"  Cerebrovascular diseases (CVDs) remain a leading cause of global disability\nand mortality. Digital Subtraction Angiography (DSA) sequences, recognized as\nthe gold standard for diagnosing CVDs, can clearly visualize the dynamic flow\nand reveal pathological conditions within the cerebrovasculature. Therefore,\nprecise segmentation of cerebral arteries (CAs) and classification between\ntheir main trunks and branches are crucial for physicians to accurately\nquantify diseases. However, achieving accurate CA segmentation in DSA sequences\nremains a challenging task due to small vessels with low contrast, and\nambiguity between vessels and residual skull structures. Moreover, the lack of\npublicly available datasets limits exploration in the field. In this paper, we\nintroduce a DSA Sequence-based Cerebral Artery segmentation dataset (DSCA), the\npublicly accessible dataset designed specifically for pixel-level semantic\nsegmentation of CAs. Additionally, we propose DSANet, a spatio-temporal network\nfor CA segmentation in DSA sequences. Unlike existing DSA segmentation methods\nthat focus only on a single frame, the proposed DSANet introduces a separate\ntemporal encoding branch to capture dynamic vessel details across multiple\nframes. To enhance small vessel segmentation and improve vessel connectivity,\nwe design a novel TemporalFormer module to capture global context and\ncorrelations among sequential frames. Furthermore, we develop a Spatio-Temporal\nFusion (STF) module to effectively integrate spatial and temporal features from\nthe encoder. Extensive experiments demonstrate that DSANet outperforms other\nstate-of-the-art methods in CA segmentation, achieving a Dice of 0.9033.\n","authors":["Jiong Zhang","Qihang Xie","Lei Mou","Dan Zhang","Da Chen","Caifeng Shan","Yitian Zhao","Ruisheng Su","Mengguo Guo"],"pdf_url":"https://arxiv.org/pdf/2406.00341v2.pdf","comment":"Published by TMI"},{"id":"http://arxiv.org/abs/2502.01401v3","updated":"2025-02-20T08:59:27Z","published":"2025-02-03T14:32:36Z","title":"Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection","summary":"  3D visual grounding (3DVG) is challenging because of the requirement of\nunderstanding on visual information, language and spatial relationships. While\nsupervised approaches have achieved superior performance, they are constrained\nby the scarcity and high cost of 3D vision-language datasets. On the other\nhand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for\ntraining data. However, these methods incur prohibitive time and token costs\nduring inference. To address the challenges, we introduce a novel training-free\nsymbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual\nGrounder, that offers significantly reduced inference costs compared to\nprevious agent-based methods while maintaining comparable performance. EaSe\nuses LLM generated codes to compute on spatial relationships. EaSe also\nimplements an automatic pipeline to evaluate and optimize the quality of these\ncodes and integrate VLMs to assist in the grounding process. Experimental\nresults demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%\nAcc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,\nit substantially reduces the inference time and cost, offering a balanced\ntrade-off between performance and efficiency. Codes are available at\nhttps://github.com/OpenRobotLab/EaSe.\n","authors":["Boyu Mi","Hanqing Wang","Tai Wang","Yilun Chen","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.01401v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14370v1","updated":"2025-02-20T08:57:45Z","published":"2025-02-20T08:57:45Z","title":"PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy\n  Optimization","summary":"  Model inversion attacks pose a significant privacy risk by attempting to\nreconstruct private training data from trained models. Most of the existing\nmethods either depend on gradient estimation or require white-box access to\nmodel parameters, which limits their applicability in practical scenarios. In\nthis paper, we propose PPO-MI, a novel reinforcement learning-based framework\nfor black-box model inversion attacks. Our approach formulates the inversion\ntask as a Markov Decision Process, where an agent navigates the latent space of\na generative model to reconstruct private training samples using only model\npredictions. By employing Proximal Policy Optimization (PPO) with a\nmomentum-based state transition mechanism, along with a reward function\nbalancing prediction accuracy and exploration, PPO-MI ensures efficient latent\nspace exploration and high query efficiency. We conduct extensive experiments\nillustrates that PPO-MI outperforms the existing methods while require less\nattack knowledge, and it is robust across various model architectures and\ndatasets. These results underline its effectiveness and generalizability in\npractical black-box scenarios, raising important considerations for the privacy\nvulnerabilities of deployed machine learning models.\n","authors":["Xinpeng Shou"],"pdf_url":"https://arxiv.org/pdf/2502.14370v1.pdf","comment":"6 pages, submitting to ICML 2025"},{"id":"http://arxiv.org/abs/2311.08816v2","updated":"2025-02-20T08:51:25Z","published":"2023-11-15T09:35:07Z","title":"Texture and Noise Dual Adaptation for Infrared Image Super-Resolution","summary":"  Recent efforts have explored leveraging visible light images to enrich\ntexture details in infrared (IR) super-resolution. However, this direct\nadaptation approach often becomes a double-edged sword, as it improves texture\nat the cost of introducing noise and blurring artifacts. To address these\nchallenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN),\nan innovative framework specifically engineered for robust IR super-resolution\nmodel adaptation. DASRGAN operates on the synergy of two key components: 1)\nTexture-Oriented Adaptation (TOA) to refine texture details meticulously, and\n2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer.\nSpecifically, TOA uniquely integrates a specialized discriminator,\nincorporating a prior extraction branch, and employs a Sobel-guided adversarial\nloss to align texture distributions effectively. Concurrently, NOA utilizes a\nnoise adversarial loss to distinctly separate the generative and Gaussian noise\npattern distributions during adversarial training. Our extensive experiments\nconfirm DASRGAN's superiority. Comparative analyses against leading methods\nacross multiple benchmarks and upsampling factors reveal that DASRGAN sets new\nstate-of-the-art performance standards. Code are available at\n\\url{https://github.com/yongsongH/DASRGAN}.\n","authors":["Yongsong Huang","Tomo Miyazaki","Xiaofeng Liu","Yafei Dong","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2311.08816v2.pdf","comment":"Accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2502.14363v1","updated":"2025-02-20T08:40:41Z","published":"2025-02-20T08:40:41Z","title":"Topology-Aware Wavelet Mamba for Airway Structure Segmentation in\n  Postoperative Recurrent Nasopharyngeal Carcinoma CT Scans","summary":"  Nasopharyngeal carcinoma (NPC) patients often undergo radiotherapy and\nchemotherapy, which can lead to postoperative complications such as limited\nmouth opening and joint stiffness, particularly in recurrent cases that require\nre-surgery. These complications can affect airway function, making accurate\npostoperative airway risk assessment essential for managing patient care.\nAccurate segmentation of airway-related structures in postoperative CT scans is\ncrucial for assessing these risks. This study introduces TopoWMamba\n(Topology-aware Wavelet Mamba), a novel segmentation model specifically\ndesigned to address the challenges of postoperative airway risk evaluation in\nrecurrent NPC patients. TopoWMamba combines wavelet-based multi-scale feature\nextraction, state-space sequence modeling, and topology-aware modules to\nsegment airway-related structures in CT scans robustly. By leveraging the\nWavelet-based Mamba Block (WMB) for hierarchical frequency decomposition and\nthe Snake Conv VSS (SCVSS) module to preserve anatomical continuity, TopoWMamba\neffectively captures both fine-grained boundaries and global structural\ncontext, crucial for accurate segmentation in complex postoperative scenarios.\nThrough extensive testing on the NPCSegCT dataset, TopoWMamba achieves an\naverage Dice score of 88.02%, outperforming existing models such as UNet,\nAttention UNet, and SwinUNet. Additionally, TopoWMamba is tested on the SegRap\n2023 Challenge dataset, where it shows a significant improvement in trachea\nsegmentation with a Dice score of 95.26%. The proposed model provides a strong\nfoundation for automated segmentation, enabling more accurate postoperative\nairway risk evaluation.\n","authors":["Haishan Huang","Pengchen Liang","Naier Lin","Luxi Wang","Bin Pu","Jianguo Chen","Qing Chang","Xia Shen","Guo Ran"],"pdf_url":"https://arxiv.org/pdf/2502.14363v1.pdf","comment":"20 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.14360v1","updated":"2025-02-20T08:37:23Z","published":"2025-02-20T08:37:23Z","title":"Weed Detection using Convolutional Neural Network","summary":"  In this paper we use convolutional neural networks (CNNs) for weed detection\nin agricultural land. We specifically investigate the application of two CNN\nlayer types, Conv2d and dilated Conv2d, for weed detection in crop fields. The\nsuggested method extracts features from the input photos using pre-trained\nmodels, which are subsequently adjusted for weed detection. The findings of the\nexperiment, which used a sizable collection of dataset consisting of 15336\nsegments, being 3249 of soil, 7376 of soybean, 3520 grass and 1191 of broadleaf\nweeds. show that the suggested approach can accurately and successfully detect\nweeds at an accuracy of 94%. This study has significant ramifications for\nlowering the usage of toxic herbicides and increasing the effectiveness of weed\nmanagement in agriculture.\n","authors":["Santosh Kumar Tripathi","Shivendra Pratap Singh","Devansh Sharma","Harshavardhan U Patekar"],"pdf_url":"https://arxiv.org/pdf/2502.14360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14355v1","updated":"2025-02-20T08:28:01Z","published":"2025-02-20T08:28:01Z","title":"Triply Laplacian Scale Mixture Modeling for Seismic Data Noise\n  Suppression","summary":"  Sparsity-based tensor recovery methods have shown great potential in\nsuppressing seismic data noise. These methods exploit tensor sparsity measures\ncapturing the low-dimensional structures inherent in seismic data tensors to\nremove noise by applying sparsity constraints through soft-thresholding or\nhard-thresholding operators. However, in these methods, considering that real\nseismic data are non-stationary and affected by noise, the variances of tensor\ncoefficients are unknown and may be difficult to accurately estimate from the\ndegraded seismic data, leading to undesirable noise suppression performance. In\nthis paper, we propose a novel triply Laplacian scale mixture (TLSM) approach\nfor seismic data noise suppression, which significantly improves the estimation\naccuracy of both the sparse tensor coefficients and hidden scalar parameters.\nTo make the optimization problem manageable, an alternating direction method of\nmultipliers (ADMM) algorithm is employed to solve the proposed TLSM-based\nseismic data noise suppression problem. Extensive experimental results on\nsynthetic and field seismic data demonstrate that the proposed TLSM algorithm\noutperforms many state-of-the-art seismic data noise suppression methods in\nboth quantitative and qualitative evaluations while providing exceptional\ncomputational efficiency.\n","authors":["Sirui Pan","Zhiyuan Zha","Shigang Wang","Yue Li","Zipei Fan","Gang Yan","Binh T. Nguyen","Bihan Wen","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.14355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12322v4","updated":"2025-02-20T08:23:07Z","published":"2022-12-22T08:33:32Z","title":"Infrared Image Super-Resolution: Systematic Review, and Future Trends","summary":"  Image Super-Resolution (SR) is essential for a wide range of computer vision\nand image processing tasks. Investigating infrared (IR) image (or thermal\nimages) super-resolution is a continuing concern within the development of deep\nlearning. This survey aims to provide a comprehensive perspective of IR image\nsuper-resolution, including its applications, hardware imaging system dilemmas,\nand taxonomy of image processing methodologies. In addition, the datasets and\nevaluation metrics in IR image super-resolution tasks are also discussed.\nFurthermore, the deficiencies in current technologies and possible promising\ndirections for the community to explore are highlighted. To cope with the rapid\ndevelopment in this field, we intend to regularly update the relevant excellent\nwork at \\url{https://github.com/yongsongH/Infrared_Image_SR_Survey\n","authors":["Yongsong Huang","Tomo Miyazaki","Xiaofeng Liu","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2212.12322v4.pdf","comment":"This work has been submitted to the Pattern Recognition for possible\n  publication"},{"id":"http://arxiv.org/abs/2502.14351v1","updated":"2025-02-20T08:17:13Z","published":"2025-02-20T08:17:13Z","title":"SegAnyPET: Universal Promptable Segmentation from Positron Emission\n  Tomography Images","summary":"  Positron Emission Tomography (PET) imaging plays a crucial role in modern\nmedical diagnostics by revealing the metabolic processes within a patient's\nbody, which is essential for quantification of therapy response and monitoring\ntreatment progress. However, the segmentation of PET images presents unique\nchallenges due to their lower contrast and less distinct boundaries compared to\nother structural medical modalities. Recent developments in segmentation\nfoundation models have shown superior versatility across diverse natural image\nsegmentation tasks. Despite the efforts of medical adaptations, these works\nprimarily focus on structural medical images with detailed physiological\nstructural information and exhibit poor generalization ability when adapted to\nmolecular PET imaging. In this paper, we collect and construct PETS-5k, the\nlargest PET segmentation dataset to date, comprising 5,731 three-dimensional\nwhole-body PET images and encompassing over 1.3M 2D images. Based on the\nestablished dataset, we develop SegAnyPET, a modality-specific 3D foundation\nmodel for universal promptable segmentation from PET images. To issue the\nchallenge of discrepant annotation quality of PET images, we adopt a cross\nprompting confident learning (CPCL) strategy with an uncertainty-guided\nself-rectification process to robustly learn segmentation from high-quality\nlabeled data and low-quality noisy labeled data. Experimental results\ndemonstrate that SegAnyPET can correctly segment seen and unseen targets using\nonly one or a few prompt points, outperforming state-of-the-art foundation\nmodels and task-specific fully supervised models with higher accuracy and\nstrong generalization ability for universal segmentation. As the first\nfoundation model for PET images, we believe that SegAnyPET will advance the\napplications to various downstream tasks for molecular imaging.\n","authors":["Yichi Zhang","Le Xue","Wenbo Zhang","Lanlan Li","Yuchen Liu","Chen Jiang","Yuan Cheng","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2502.14351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12448v3","updated":"2025-02-20T08:14:34Z","published":"2024-09-19T03:58:32Z","title":"Infrared Small Target Detection in Satellite Videos: A New Dataset and A\n  Novel Recurrent Feature Refinement Framework","summary":"  Multi-frame infrared small target (MIRST) detection in satellite videos is a\nlong-standing, fundamental yet challenging task for decades, and the challenges\ncan be summarized as: First, extremely small target size, highly complex\nclutters & noises, various satellite motions result in limited feature\nrepresentation, high false alarms, and difficult motion analyses. Second, the\nlack of large-scale public available MIRST dataset in satellite videos greatly\nhinders the algorithm development. To address the aforementioned challenges, in\nthis paper, we first build a large-scale dataset for MIRST detection in\nsatellite videos (namely IRSatVideo-LEO), and then develop a recurrent feature\nrefinement (RFR) framework as the baseline method. Specifically, IRSatVideo-LEO\nis a semi-simulated dataset with synthesized satellite motion, target\nappearance, trajectory and intensity, which can provide a standard toolbox for\nsatellite video generation and a reliable evaluation platform to facilitate the\nalgorithm development. For baseline method, RFR is proposed to be equipped with\nexisting powerful CNN-based methods for long-term temporal dependency\nexploitation and integrated motion compensation & MIRST detection.\nSpecifically, a pyramid deformable alignment (PDA) module and a\ntemporal-spatial-frequency modulation (TSFM) module are proposed to achieve\neffective and efficient feature alignment, propagation, aggregation and\nrefinement. Extensive experiments have been conducted to demonstrate the\neffectiveness and superiority of our scheme. The comparative results show that\nResUNet equipped with RFR outperforms the state-of-the-art MIRST detection\nmethods. Dataset and code are released at https://github.com/XinyiYing/RFR.\n","authors":["Xinyi Ying","Li Liu","Zaipin Lin","Yangsi Shi","Yingqian Wang","Ruojing Li","Xu Cao","Boyang Li","Shilin Zhou","Wei An"],"pdf_url":"https://arxiv.org/pdf/2409.12448v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18362v2","updated":"2025-02-20T08:02:22Z","published":"2025-01-30T14:07:56Z","title":"MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding","summary":"  We introduce MedXpertQA, a highly challenging and comprehensive benchmark to\nevaluate expert-level medical knowledge and advanced reasoning. MedXpertQA\nincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text for text evaluation and MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and rich clinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodal benchmarks with\nsimple QA pairs generated from image captions. MedXpertQA applies rigorous\nfiltering and augmentation to address the insufficient difficulty of existing\nbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We perform data synthesis to mitigate\ndata leakage risk and conduct multiple rounds of expert reviews to ensure\naccuracy and reliability. We evaluate 16 leading models on MedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop a reasoning-oriented subset to\nfacilitate the assessment of o1-like models.\n","authors":["Yuxin Zuo","Shang Qu","Yifei Li","Zhangren Chen","Xuekai Zhu","Ermo Hua","Kaiyan Zhang","Ning Ding","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.18362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14344v1","updated":"2025-02-20T07:59:08Z","published":"2025-02-20T07:59:08Z","title":"Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive\n  Gradient Modulation Mechanism","summary":"  Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of\nSNNs, while also adopting the reduced storage burden of binarization\ntechniques. These distinct advantages grant BSNNs lightweight and\nenergy-efficient characteristics, rendering them ideal for deployment on\nresource-constrained edge devices. However, due to the binary synaptic weights\nand non-differentiable spike function, effectively training BSNNs remains an\nopen question. In this paper, we conduct an in-depth analysis of the challenge\nfor BSNN learning, namely the frequent weight sign flipping problem. To\nmitigate this issue, we propose an Adaptive Gradient Modulation Mechanism\n(AGMM), which is designed to reduce the frequency of weight sign flipping by\nadaptively adjusting the gradients during the learning process. The proposed\nAGMM can enable BSNNs to achieve faster convergence speed and higher accuracy,\neffectively narrowing the gap between BSNNs and their full-precision\nequivalents. We validate AGMM on both static and neuromorphic datasets, and\nresults indicate that it achieves state-of-the-art results among BSNNs. This\nwork substantially reduces storage demands and enhances SNNs' inherent energy\nefficiency, making them highly feasible for resource-constrained environments.\n","authors":["Yu Liang","Wenjie Wei","Ammar Belatreche","Honglin Cao","Zijian Zhou","Shuai Wang","Malu Zhang","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2502.14344v1.pdf","comment":"9 pages, 8 figures, AAAI conference"},{"id":"http://arxiv.org/abs/2406.14482v2","updated":"2025-02-20T07:53:16Z","published":"2024-06-20T16:43:58Z","title":"Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines","summary":"  Small object detection (SOD) has been a longstanding yet challenging task for\ndecades, with numerous datasets and algorithms being developed. However, they\nmainly focus on either visible or thermal modality, while visible-thermal\n(RGBT) bimodality is rarely explored. Although some RGBT datasets have been\ndeveloped recently, the insufficient quantity, limited category, misaligned\nimages and large target size cannot provide an impartial benchmark to evaluate\nmulti-category visible-thermal small object detection (RGBT SOD) algorithms. In\nthis paper, we build the first large-scale benchmark with high diversity for\nRGBT SOD (namely RGBT-Tiny), including 115 paired sequences, 93K frames and\n1.2M manual annotations. RGBT-Tiny contains abundant targets (7 categories) and\nhigh-diversity scenes (8 types that cover different illumination and density\nvariations). Note that, over 81% of targets are smaller than 16x16, and we\nprovide paired bounding box annotations with tracking ID to offer an extremely\nchallenging benchmark with wide-range applications, such as RGBT fusion,\ndetection and tracking. In addition, we propose a scale adaptive fitness\n(SAFit) measure that exhibits high robustness on both small and large targets.\nThe proposed SAFit can provide reasonable performance evaluation and promote\ndetection performance. Based on the proposed RGBT-Tiny dataset and SAFit\nmeasure, extensive evaluations have been conducted, including 23 recent\nstate-of-the-art algorithms that cover four different types (i.e., visible\ngeneric detection, visible SOD, thermal SOD and RGBT object detection). Project\nis available at https://github.com/XinyiYing/RGBT-Tiny.\n","authors":["Xinyi Ying","Chao Xiao","Ruojing Li","Xu He","Boyang Li","Xu Cao","Zhaoxu Li","Yingqian Wang","Mingyuan Hu","Qingyu Xu","Zaiping Lin","Miao Li","Shilin Zhou","Wei An","Weidong Sheng","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2406.14482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15740v3","updated":"2025-02-20T07:50:16Z","published":"2024-08-28T12:06:11Z","title":"MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with\n  Attention Mamba Mechanisms","summary":"  Vision Language Place Recognition (VLVPR) enhances robot localization\nperformance by incorporating natural language descriptions from images. By\nutilizing language information, VLVPR directs robot place matching, overcoming\nthe constraint of solely depending on vision. The essence of multimodal fusion\nlies in mining the complementary information between different modalities.\nHowever, general fusion methods rely on traditional neural architectures and\nare not well equipped to capture the dynamics of cross modal interactions,\nespecially in the presence of complex intra modal and inter modal correlations.\nTo this end, this paper proposes a novel coarse to fine and end to end\nconnected cross modal place recognition framework, called MambaPlace. In the\ncoarse localization stage, the text description and 3D point cloud are encoded\nby the pretrained T5 and instance encoder, respectively. They are then\nprocessed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for\ndata enhancement and alignment. In the subsequent fine localization stage, the\nfeatures of the text description and 3D point cloud are cross modally fused and\nfurther enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we\npredict the positional offset from the fused text point cloud features,\nachieving the most accurate localization. Extensive experiments show that\nMambaPlace achieves improved localization accuracy on the KITTI360Pose dataset\ncompared to the state of the art methods.\n","authors":["Tianyi Shang","Zhenyu Li","Pengjie Xu","Jinwei Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.15740v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.11859v2","updated":"2025-02-20T07:42:46Z","published":"2025-02-17T14:50:53Z","title":"Defining and Evaluating Visual Language Models' Basic Spatial Abilities:\n  A Perspective from Psychometrics","summary":"  The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.\n","authors":["Wenrui Xu","Dalin Lyu","Weihang Wang","Jie Feng","Chen Gao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.11859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14679v5","updated":"2025-02-20T07:37:41Z","published":"2025-01-24T17:57:06Z","title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation","summary":"  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n","authors":["Rongzhao He","Weihao Zheng","Leilei Zhao","Ying Wang","Dalin Zhu","Dan Wu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2501.14679v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03051v2","updated":"2025-02-20T07:36:12Z","published":"2024-10-04T00:13:54Z","title":"AuroraCap: Efficient, Performant Video Detailed Captioning and a New\n  Benchmark","summary":"  Video detailed captioning is a key task which aims to generate comprehensive\nand coherent textual descriptions of video content, benefiting both video\nunderstanding and generation. In this paper, we propose AuroraCap, a video\ncaptioner based on a large multimodal model. We follow the simplest\narchitecture design without additional parameters for temporal modeling. To\naddress the overhead caused by lengthy video sequences, we implement the token\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\nfound that this strategy results in little performance loss. AuroraCap shows\nsuperior performance on various video and image captioning benchmarks, for\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\nsimple descriptions, consisting of a few dozen words, which limits research in\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\nwith over one thousand carefully annotated structured captions. In addition, we\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\nadopts a divide-and-conquer strategy to transform long caption evaluation into\nmultiple short question-answer pairs. With the help of human Elo ranking, our\nexperiments show that this benchmark better correlates with human judgments of\nvideo detailed captioning quality.\n","authors":["Wenhao Chai","Enxin Song","Yilun Du","Chenlin Meng","Vashisht Madhavan","Omer Bar-Tal","Jenq-Neng Hwang","Saining Xie","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2410.03051v2.pdf","comment":"Accepted to ICLR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://rese1f.github.io/aurora-web/"},{"id":"http://arxiv.org/abs/2502.14332v1","updated":"2025-02-20T07:30:43Z","published":"2025-02-20T07:30:43Z","title":"A Collaborative Jade Recognition System for Mobile Devices Based on\n  Lightweight and Large Models","summary":"  With the widespread adoption and development of mobile devices, vision-based\nrecognition applications have become a hot topic in research. Jade, as an\nimportant cultural heritage and artistic item, has significant applications in\nfields such as jewelry identification and cultural relic preservation. However,\nexisting jade recognition systems still face challenges in mobile\nimplementation, such as limited computing resources, real-time requirements,\nand accuracy issues. To address these challenges, this paper proposes a jade\nrecognition system based on size model collaboration, aiming to achieve\nefficient and accurate jade identification using mobile devices such as\nsmartphones.First, we design a size model based on multi-scale image\nprocessing, extracting key visual information by analyzing jade's dimensions,\nshapes, and surface textures. Then, a collaborative multi-model classification\nframework is built by combining deep learning and traditional computer vision\nalgorithms. This framework can effectively select and adjust models based on\ndifferent jade characteristics, providing high accuracy results across various\nenvironments and devices.Experimental results show that the proposed system can\nprovide high recognition accuracy and fast processing time on mobile devices,\nwhile consuming relatively low computational resources. The system not only\nholds great application potential but also provides new ideas and technical\nsupport for the intelligent development of jade identification.\n","authors":["Zhenyu Wang","Wenjia Li","Pengyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.14332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05195v2","updated":"2025-02-20T07:22:53Z","published":"2024-11-07T21:39:51Z","title":"Exploring How Generative MLLMs Perceive More Than CLIP with the Same\n  Vision Encoder","summary":"  Recent research has shown that CLIP models struggle with visual reasoning\ntasks that require grounding compositionality, understanding spatial\nrelationships, or capturing fine-grained details. One natural hypothesis is\nthat the CLIP vision encoder does not embed essential information for these\ntasks. However, we find that this is not always the case: The encoder gathers\nquery-relevant visual information, while CLIP fails to extract it. In\nparticular, we show that another branch of Vision-Language Models (VLMs),\nGenerative Multimodal Large Language Models (MLLMs), achieve significantly\nhigher accuracy than CLIP in many of these tasks using the same vision encoder\nand weights, indicating that these Generative MLLMs perceive more -- as they\nextract and utilize visual information more effectively. We conduct a series of\ncontrolled experiments and reveal that their success is attributed to multiple\nkey design choices, including patch tokens, position embeddings, and\nprompt-based weighting. On the other hand, enhancing the training data alone or\napplying a stronger text encoder does not suffice to solve the task, and\nadditional text tokens offer little benefit. Interestingly, we find that\nfine-grained visual reasoning is not exclusive to generative models trained by\nan autoregressive loss: When converted into CLIP-like encoders by contrastive\nfinetuning, these MLLMs still outperform CLIP under the same cosine\nsimilarity-based evaluation protocol. Our study highlights the importance of\nVLM architectural choices and suggests directions for improving the performance\nof CLIP-like contrastive VLMs.\n","authors":["Siting Li","Pang Wei Koh","Simon Shaolei Du"],"pdf_url":"https://arxiv.org/pdf/2411.05195v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.07742v3","updated":"2025-02-20T07:08:30Z","published":"2024-11-12T12:07:27Z","title":"Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial\n  Pruning","summary":"  This paper studies point cloud perception within outdoor environments.\nExisting methods face limitations in recognizing objects located at a distance\nor occluded, due to the sparse nature of outdoor point clouds. In this work, we\nobserve a significant mitigation of this problem by accumulating multiple\ntemporally consecutive point cloud sweeps, resulting in a remarkable\nimprovement in perception accuracy. However, the computation cost also\nincreases, hindering previous approaches from utilizing a large number of point\ncloud sweeps. To tackle this challenge, we find that a considerable portion of\npoints in the accumulated point cloud is redundant, and discarding these points\nhas minimal impact on perception accuracy. We introduce a simple yet effective\nGumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a\nlearned end-to-end sampling. The GSP layer is decoupled from other network\ncomponents and thus can be seamlessly integrated into existing point cloud\nnetwork architectures. Without incurring additional computational overhead, we\nincrease the number of point cloud sweeps from 10, a common practice, to as\nmany as 40. Consequently, there is a significant enhancement in perception\nperformance. For instance, in nuScenes 3D object detection and BEV map\nsegmentation tasks, our pruning strategy improves several 3D perception\nbaseline methods.\n","authors":["Tianyu Sun","Jianhao Li","Xueqian Zhang","Zhongdao Wang","Bailan Feng","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.07742v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13344v2","updated":"2025-02-20T07:06:03Z","published":"2023-10-20T08:15:13Z","title":"DeepFracture: A Generative Approach for Predicting Brittle Fractures\n  with Neural Discrete Representation Learning","summary":"  In the field of brittle fracture animation, generating realistic destruction\nanimations using physics-based simulation methods is computationally expensive.\nWhile techniques based on Voronoi diagrams or pre-fractured patterns are\neffective for real-time applications, they fail to incorporate collision\nconditions when determining fractured shapes during runtime. This paper\nintroduces a novel learning-based approach for predicting fractured shapes\nbased on collision dynamics at runtime. Our approach seamlessly integrates\nrealistic brittle fracture animations with rigid body simulations, utilising\nboundary element method (BEM) brittle fracture simulations to generate training\ndata. To integrate collision scenarios and fractured shapes into a deep\nlearning framework, we introduce generative geometric segmentation, distinct\nfrom both instance and semantic segmentation, to represent 3D fragment shapes.\nWe propose an eight-dimensional latent code to address the challenge of\noptimising multiple discrete fracture pattern targets that share similar\ncontinuous collision latent codes. This code will follow a discrete normal\ndistribution corresponding to a specific fracture pattern within our latent\nimpulse representation design. This adaptation enables the prediction of\nfractured shapes using neural discrete representation learning. Our\nexperimental results show that our approach generates considerably more\ndetailed brittle fractures than existing techniques, while the computational\ntime is typically reduced compared to traditional simulation methods at\ncomparable resolutions.\n","authors":["Yuhang Huang","Takashi Kanai"],"pdf_url":"https://arxiv.org/pdf/2310.13344v2.pdf","comment":"This is a preprint of an article published in the Computer Graphics\n  Forum. The final authenticated version is available at\n  (https://doi.org/10.1111/cgf.70002). Please also check the project page:\n  https://nikoloside.github.io/deepfracture/"},{"id":"http://arxiv.org/abs/2502.14316v1","updated":"2025-02-20T07:02:22Z","published":"2025-02-20T07:02:22Z","title":"Textured 3D Regenerative Morphing with 3D Diffusion Prior","summary":"  Textured 3D morphing creates smooth and plausible interpolation sequences\nbetween two 3D objects, focusing on transitions in both shape and texture. This\nis important for creative applications like visual effects in filmmaking.\nPrevious methods rely on establishing point-to-point correspondences and\ndetermining smooth deformation trajectories, which inherently restrict them to\nshape-only morphing on untextured, topologically aligned datasets. This\nrestriction leads to labor-intensive preprocessing and poor generalization. To\novercome these challenges, we propose a method for 3D regenerative morphing\nusing a 3D diffusion prior. Unlike previous methods that depend on explicit\ncorrespondences and deformations, our method eliminates the additional need for\nobtaining correspondence and uses the 3D diffusion prior to generate morphing.\nSpecifically, we introduce a 3D diffusion model and interpolate the source and\ntarget information at three levels: initial noise, model parameters, and\ncondition features. We then explore an Attention Fusion strategy to generate\nmore smooth morphing sequences. To further improve the plausibility of semantic\ninterpolation and the generated 3D surfaces, we propose two strategies: (a)\nToken Reordering, where we match approximate tokens based on semantic analysis\nto guide implicit correspondences in the denoising process of the diffusion\nmodel, and (b) Low-Frequency Enhancement, where we enhance low-frequency\nsignals in the tokens to improve the quality of generated surfaces.\nExperimental results show that our method achieves superior smoothness and\nplausibility in 3D morphing across diverse cross-category object pairs,\noffering a novel regenerative method for 3D morphing with textured\nrepresentations.\n","authors":["Songlin Yang","Yushi Lan","Honghua Chen","Xingang Pan"],"pdf_url":"https://arxiv.org/pdf/2502.14316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14314v1","updated":"2025-02-20T06:57:58Z","published":"2025-02-20T06:57:58Z","title":"ODVerse33: Is the New YOLO Version Always Better? A Multi Domain\n  benchmark from YOLO v5 to v11","summary":"  You Look Only Once (YOLO) models have been widely used for building real-time\nobject detectors across various domains. With the increasing frequency of new\nYOLO versions being released, key questions arise. Are the newer versions\nalways better than their previous versions? What are the core innovations in\neach YOLO version and how do these changes translate into real-world\nperformance gains? In this paper, we summarize the key innovations from YOLOv1\nto YOLOv11, introduce a comprehensive benchmark called ODverse33, which\nincludes 33 datasets spanning 11 diverse domains (Autonomous driving,\nAgricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife,\nRetail, Microscopic, and Security), and explore the practical impact of model\nimprovements in real-world, multi-domain applications through extensive\nexperimental results. We hope this study can provide some guidance to the\nextensive users of object detection models and give some references for future\nreal-time object detector development.\n","authors":["Tianyou Jiang","Yang Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.14314v1.pdf","comment":"18 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2310.02664v2","updated":"2025-02-20T06:17:25Z","published":"2023-10-04T09:04:20Z","title":"On Memorization in Diffusion Models","summary":"  Due to their capacity to generate novel and high-quality samples, diffusion\nmodels have attracted significant research interest in recent years. Notably,\nthe typical training objective of diffusion models, i.e., denoising score\nmatching, has a closed-form optimal solution that can only generate training\ndata replicating samples. This indicates that a memorization behavior is\ntheoretically expected, which contradicts the common generalization ability of\nstate-of-the-art diffusion models, and thus calls for a deeper understanding.\nLooking into this, we first observe that memorization behaviors tend to occur\non smaller-sized datasets, which motivates our definition of effective model\nmemorization (EMM), a metric measuring the maximum size of training data at\nwhich a learned diffusion model approximates its theoretical optimum. Then, we\nquantify the impact of the influential factors on these memorization behaviors\nin terms of EMM, focusing primarily on data distribution, model configuration,\nand training procedure. Besides comprehensive empirical results identifying the\ninfluential factors, we surprisingly find that conditioning training data on\nuninformative random labels can significantly trigger the memorization in\ndiffusion models. Our study holds practical significance for diffusion model\nusers and offers clues to theoretical research in deep generative models. Code\nis available at https://github.com/sail-sg/DiffMemorize.\n","authors":["Xiangming Gu","Chao Du","Tianyu Pang","Chongxuan Li","Min Lin","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02664v2.pdf","comment":"TMLR 2025"},{"id":"http://arxiv.org/abs/2405.16406v4","updated":"2025-02-20T06:07:00Z","published":"2024-05-26T02:15:49Z","title":"SpinQuant: LLM quantization with learned rotations","summary":"  Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.\n","authors":["Zechun Liu","Changsheng Zhao","Igor Fedorov","Bilge Soran","Dhruv Choudhary","Raghuraman Krishnamoorthi","Vikas Chandra","Yuandong Tian","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2405.16406v4.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2312.04398v5","updated":"2025-02-20T05:58:40Z","published":"2023-12-07T16:10:10Z","title":"Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning","summary":"  The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.\n","authors":["Yongqi Dong","Xingmin Lu","Ruohan Li","Wei Song","Bart van Arem","Haneen Farah"],"pdf_url":"https://arxiv.org/pdf/2312.04398v5.pdf","comment":"26 pages, 7 figures, accepted by the 103rd Transportation Research\n  Board (TRB) Annual Meeting, under review by Transportation Research Record:\n  Journal of the Transportation Research Board"},{"id":"http://arxiv.org/abs/2407.04903v3","updated":"2025-02-20T05:57:34Z","published":"2024-07-06T00:40:53Z","title":"MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal\n  Scientific Understanding","summary":"  Scientific figure interpretation is a crucial capability for AI-driven\nscientific assistants built on advanced Large Vision Language Models. However,\ncurrent datasets and benchmarks primarily focus on simple charts or other\nrelatively straightforward figures from limited science domains. To address\nthis gap, we present a comprehensive dataset compiled from peer-reviewed Nature\nCommunications articles covering 72 scientific fields, encompassing complex\nvisualizations such as schematic diagrams, microscopic images, and experimental\ndata which require graduate-level expertise to interpret. We evaluated 19\nproprietary and open-source models on two benchmark tasks, figure captioning\nand multiple-choice, and conducted human expert annotation. Our analysis\nrevealed significant task challenges and performance gaps among models. Beyond\nserving as a benchmark, this dataset serves as a valuable resource for\nlarge-scale training. Fine-tuning Qwen2-VL-7B with our task-specific data\nachieved better performance than GPT-4o and even human experts in\nmultiple-choice evaluations. Furthermore, continuous pre-training on our\ninterleaved article and figure data substantially enhanced the model's\ndownstream task performance in materials science. We have released our dataset\nto support further research.\n","authors":["Zekun Li","Xianjun Yang","Kyuri Choi","Wanrong Zhu","Ryan Hsieh","HyeonJung Kim","Jin Hyuk Lim","Sungyoung Ji","Byungju Lee","Xifeng Yan","Linda Ruth Petzold","Stephen D. Wilson","Woosang Lim","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.04903v3.pdf","comment":"Code and data are available at https://github.com/Leezekun/MMSci"},{"id":"http://arxiv.org/abs/2502.14282v1","updated":"2025-02-20T05:41:55Z","published":"2025-02-20T05:41:55Z","title":"PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex\n  Task Automation on PC","summary":"  In the field of MLLM-based GUI agents, compared to smartphones, the PC\nscenario not only features a more complex interactive environment, but also\ninvolves more intricate intra- and inter-app workflows. To address these\nissues, we propose a hierarchical agent framework named PC-Agent. Specifically,\nfrom the perception perspective, we devise an Active Perception Module (APM) to\novercome the inadequate abilities of current MLLMs in perceiving screenshot\ncontent. From the decision-making perspective, to handle complex user\ninstructions and interdependent subtasks more effectively, we propose a\nhierarchical multi-agent collaboration architecture that decomposes\ndecision-making processes into Instruction-Subtask-Action levels. Within this\narchitecture, three agents (i.e., Manager, Progress and Decision) are set up\nfor instruction decomposition, progress tracking and step-by-step\ndecision-making respectively. Additionally, a Reflection agent is adopted to\nenable timely bottom-up error feedback and adjustment. We also introduce a new\nbenchmark PC-Eval with 25 real-world complex instructions. Empirical results on\nPC-Eval show that our PC-Agent achieves a 32% absolute improvement of task\nsuccess rate over previous state-of-the-art methods. The code will be publicly\navailable.\n","authors":["Haowei Liu","Xi Zhang","Haiyang Xu","Yuyang Wanyan","Junyang Wang","Ming Yan","Ji Zhang","Chunfeng Yuan","Changsheng Xu","Weiming Hu","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2502.14282v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.14279v1","updated":"2025-02-20T05:40:56Z","published":"2025-02-20T05:40:56Z","title":"OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from\n  Monocular Camera Images","summary":"  Monocular depth estimation is a rudimentary task in robotic perception.\nRecently, with the development of more accurate and robust neural network\nmodels and different types of datasets, monocular depth estimation has\nsignificantly improved performance and efficiency. However, most of the\nresearch in this area focuses on very concentrated domains. In particular, most\nof the benchmarks in outdoor scenarios belong to urban environments for the\nimprovement of autonomous driving devices, and these benchmarks have a massive\ndisparity with the orchard/vineyard environment, which is hardly helpful for\nresearch in the primary industry. Therefore, we propose OrchardDepth, which\nfills the gap in the estimation of the metric depth of the monocular camera in\nthe orchard/vineyard environment. In addition, we present a new retraining\nmethod to improve the training result by monitoring the consistent\nregularization between dense depth maps and sparse points. Our method improves\nthe RMSE of depth estimation in the orchard environment from 1.5337 to 0.6738,\nproving our method's validation.\n","authors":["Zhichao Zheng","Henry Williams","Bruce A MacDonald"],"pdf_url":"https://arxiv.org/pdf/2502.14279v1.pdf","comment":"10 pages, 5 figures, Australasian Conference on Robotics and\n  Automation, ACRA, 2024"},{"id":"http://arxiv.org/abs/2502.14273v1","updated":"2025-02-20T05:18:36Z","published":"2025-02-20T05:18:36Z","title":"LLM-EvRep: Learning an LLM-Compatible Event Representation Using a\n  Self-Supervised Framework","summary":"  Recent advancements in event-based recognition have demonstrated significant\npromise, yet most existing approaches rely on extensive training, limiting\ntheir adaptability for efficient processing of event-driven visual content.\nMeanwhile, large language models (LLMs) have exhibited remarkable zero-shot\ncapabilities across diverse domains, but their application to event-based\nvisual recognition remains largely unexplored. To bridge this gap, we propose\n\\textbf{LLM-EvGen}, an event representation generator that produces\nLLM-compatible event representations \\textbf{LLM-EvRep}, thereby enhancing the\nperformance of LLMs on event recognition tasks. The generator is trained using\na self-supervised framework, aligning the generated representations with\nsemantic consistency and structural fidelity. Comprehensive experiments were\nconducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results\ndemonstrate that our method, \\textbf{LLM-EvRep}, outperforms the event-to-video\nmethod, E2VID, by 15.93\\%, 0.82\\%, and 50.21\\%, respectively, in recognition\ntasks when evaluated using GPT-4o.\n","authors":["Zongyou Yu","Qiang Qu","Qian Zhang","Nan Zhang","Xiaoming Chen"],"pdf_url":"https://arxiv.org/pdf/2502.14273v1.pdf","comment":"6 pages, 2 figures,Companion Proceedings of the ACM Web Conference\n  2025 (WWW Companion '25)"},{"id":"http://arxiv.org/abs/2502.14267v1","updated":"2025-02-20T05:07:46Z","published":"2025-02-20T05:07:46Z","title":"Money Recognition for the Visually Impaired: A Case Study on Sri Lankan\n  Banknotes","summary":"  Currency note recognition is a critical accessibility need for blind\nindividuals, as identifying banknotes accurately can impact their independence\nand security in financial transactions. Several traditional and technological\ninitiatives have been taken to date. Nevertheless, these approaches are less\nuser-friendly and have made it more challenging for blind people to identify\nbanknotes. This research proposes a user-friendly stand-alone system for the\nidentification of Sri Lankan currency notes. A custom-created dataset of images\nof Sri Lankan currency notes was used to fine-tune an EfficientDet model. The\ncurrency note recognition model achieved 0.9847 AP on the validation dataset\nand performs exceptionally well in real-world scenarios. The high accuracy and\nthe intuitive interface have enabled blind individuals to quickly and\naccurately identify currency denominations, ultimately encouraging\naccessibility and independence.\n","authors":["Akshaan Bandara"],"pdf_url":"https://arxiv.org/pdf/2502.14267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14260v1","updated":"2025-02-20T04:56:03Z","published":"2025-02-20T04:56:03Z","title":"EyeBench: A Call for More Rigorous Evaluation of Retinal Image\n  Enhancement","summary":"  Over the past decade, generative models have achieved significant success in\nenhancement fundus images.However, the evaluation of these models still\npresents a considerable challenge. A comprehensive evaluation benchmark for\nfundus image enhancement is indispensable for three main reasons: 1) The\nexisting denoising metrics (e.g., PSNR, SSIM) are hardly to extend to\ndownstream real-world clinical research (e.g., Vessel morphology consistency).\n2) There is a lack of comprehensive evaluation for both paired and unpaired\nenhancement methods, along with the need for expert protocols to accurately\nassess clinical value. 3) An ideal evaluation system should provide insights to\ninform future developments of fundus image enhancement. To this end, we propose\na novel comprehensive benchmark, EyeBench, to provide insights that align\nenhancement models with clinical needs, offering a foundation for future work\nto improve the clinical relevance and applicability of generative models for\nfundus image enhancement. EyeBench has three appealing properties: 1)\nmulti-dimensional clinical alignment downstream evaluation: In addition to\nevaluating the enhancement task, we provide several clinically significant\ndownstream tasks for fundus images, including vessel segmentation, DR grading,\ndenoising generalization, and lesion segmentation. 2) Medical expert-guided\nevaluation design: We introduce a novel dataset that promote comprehensive and\nfair comparisons between paired and unpaired methods and includes a manual\nevaluation protocol by medical experts. 3) Valuable insights: Our benchmark\nstudy provides a comprehensive and rigorous evaluation of existing methods\nacross different downstream tasks, assisting medical experts in making informed\nchoices. Additionally, we offer further analysis of the challenges faced by\nexisting methods. The code is available at\n\\url{https://github.com/Retinal-Research/EyeBench}\n","authors":["Wenhui Zhu","Xuanzhao Dong","Xin Li","Yujian Xiong","Xiwen Chen","Peijie Qiu","Vamsi Krishna Vasa","Zhangsihao Yang","Yi Su","Oana Dumitrascu","Yalin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17719v3","updated":"2025-02-20T04:28:19Z","published":"2024-05-28T00:27:29Z","title":"Do Egocentric Video-Language Models Truly Understand Hand-Object\n  Interactions?","summary":"  Egocentric video-language pretraining is a crucial step in advancing the\nunderstanding of hand-object interactions in first-person scenarios. Despite\nsuccesses on existing testbeds, we find that current EgoVLMs can be easily\nmisled by simple modifications, such as changing the verbs or nouns in\ninteraction descriptions, with models struggling to distinguish between these\nchanges. This raises the question: Do EgoVLMs truly understand hand-object\ninteractions? To address this question, we introduce a benchmark called\nEgoHOIBench, revealing the performance limitation of current egocentric models\nwhen confronted with such challenges. We attribute this performance gap to\ninsufficient fine-grained supervision and the greater difficulty EgoVLMs\nexperience in recognizing verbs compared to nouns. To tackle these issues, we\npropose a novel asymmetric contrastive objective named EgoNCE++. For the\nvideo-to-text objective, we enhance text supervision by generating negative\ncaptions using large language models or leveraging pretrained vocabulary for\nHOI-related word substitutions. For the text-to-video objective, we focus on\npreserving an object-centric feature space that clusters video representations\nbased on shared nouns. Extensive experiments demonstrate that EgoNCE++\nsignificantly enhances EgoHOI understanding, leading to improved performance\nacross various EgoVLMs in tasks such as multi-instance retrieval, action\nrecognition, and temporal understanding. Our code is available at\nhttps://github.com/xuboshen/EgoNCEpp.\n","authors":["Boshen Xu","Ziheng Wang","Yang Du","Zhinan Song","Sipeng Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2405.17719v3.pdf","comment":"Accepted by ICLR 2025. Code: https://github.com/xuboshen/EgoNCEpp"},{"id":"http://arxiv.org/abs/2502.14247v1","updated":"2025-02-20T04:22:30Z","published":"2025-02-20T04:22:30Z","title":"Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and\n  Texture Generation","summary":"  This report presents a comprehensive framework for generating high-quality 3D\nshapes and textures from diverse input prompts, including single images,\nmulti-view images, and text descriptions. The framework consists of 3D shape\ngeneration and texture generation. (1). The 3D shape generation pipeline\nemploys a Variational Autoencoder (VAE) to encode implicit 3D geometries into a\nlatent space and a diffusion network to generate latents conditioned on input\nprompts, with modifications to enhance model capacity. An alternative\nArtist-Created Mesh (AM) generation approach is also explored, yielding\npromising results for simpler geometries. (2). Texture generation involves a\nmulti-stage process starting with frontal images generation followed by\nmulti-view images generation, RGB-to-PBR texture conversion, and\nhigh-resolution multi-view texture refinement. A consistency scheduler is\nplugged into every stage, to enforce pixel-wise consistency among multi-view\ntextures during inference, ensuring seamless integration.\n  The pipeline demonstrates effective handling of diverse input formats,\nleveraging advanced neural architectures and novel methodologies to produce\nhigh-quality 3D content. This report details the system architecture,\nexperimental results, and potential future directions to improve and expand the\nframework. The source code and pretrained weights are released at:\n\\url{https://github.com/Tencent/Tencent-XR-3DGen}.\n","authors":["Jiayu Yang","Taizhang Shang","Weixuan Sun","Xibin Song","Ziang Chen","Senbo Wang","Shenzhou Chen","Weizhe Liu","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2502.14247v1.pdf","comment":"Tencent XR 3D Gen"},{"id":"http://arxiv.org/abs/2410.03858v2","updated":"2025-02-20T04:13:04Z","published":"2024-10-04T18:43:27Z","title":"Pose Prior Learner: Unsupervised Categorical Prior Learning for Pose\n  Estimation","summary":"  A prior represents a set of beliefs or assumptions about a system, aiding\ninference and decision-making. In this paper, we introduce the challenge of\nunsupervised categorical prior learning in pose estimation, where AI models\nlearn a general pose prior for an object category from images in a\nself-supervised manner. Although priors are effective in estimating pose,\nacquiring them can be difficult. We propose a novel method, named Pose Prior\nLearner (PPL), to learn a general pose prior for any object category. PPL uses\na hierarchical memory to store compositional parts of prototypical poses, from\nwhich we distill a general pose prior. This prior improves pose estimation\naccuracy through template transformation and image reconstruction. PPL learns\nmeaningful pose priors without any additional human annotations or\ninterventions, outperforming competitive baselines on both human and animal\npose estimation datasets. Notably, our experimental results reveal the\neffectiveness of PPL using learned prototypical poses for pose estimation on\noccluded images. Through iterative inference, PPL leverages the pose prior to\nrefine estimated poses, regressing them to any prototypical poses stored in\nmemory. Our code, model, and data will be publicly available.\n","authors":["Ziyu Wang","Shuangpeng Han","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14235v1","updated":"2025-02-20T04:00:47Z","published":"2025-02-20T04:00:47Z","title":"OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving","summary":"  Accurate and realistic 3D scene reconstruction enables the lifelike creation\nof autonomous driving simulation environments. With advancements in 3D Gaussian\nSplatting (3DGS), previous studies have applied it to reconstruct complex\ndynamic driving scenes. These methods typically require expensive LiDAR sensors\nand pre-annotated datasets of dynamic objects. To address these challenges, we\npropose OG-Gaussian, a novel approach that replaces LiDAR point clouds with\nOccupancy Grids (OGs) generated from surround-view camera images using\nOccupancy Prediction Network (ONet). Our method leverages the semantic\ninformation in OGs to separate dynamic vehicles from static street background,\nconverting these grids into two distinct sets of initial point clouds for\nreconstructing both static and dynamic objects. Additionally, we estimate the\ntrajectories and poses of dynamic objects through a learning-based approach,\neliminating the need for complex manual annotations. Experiments on Waymo Open\ndataset demonstrate that OG-Gaussian is on par with the current\nstate-of-the-art in terms of reconstruction quality and rendering speed,\nachieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while\nsignificantly reducing computational costs and economic overhead.\n","authors":["Yedong Shen","Xinran Zhang","Yifan Duan","Shiqi Zhang","Heng Li","Yilong Wu","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14226v1","updated":"2025-02-20T03:40:02Z","published":"2025-02-20T03:40:02Z","title":"Designing Parameter and Compute Efficient Diffusion Transformers using\n  Distillation","summary":"  Diffusion Transformers (DiTs) with billions of model parameters form the\nbackbone of popular image and video generation models like DALL.E,\nStable-Diffusion and SORA. Though these models are necessary in many\nlow-latency applications like Augmented/Virtual Reality, they cannot be\ndeployed on resource-constrained Edge devices (like Apple Vision Pro or Meta\nRay-Ban glasses) due to their huge computational complexity. To overcome this,\nwe turn to knowledge distillation and perform a thorough design-space\nexploration to achieve the best DiT for a given parameter size. In particular,\nwe provide principles for how to choose design knobs such as depth, width,\nattention heads and distillation setup for a DiT. During the process, a\nthree-way trade-off emerges between model performance, size and speed that is\ncrucial for Edge implementation of diffusion. We also propose two distillation\napproaches - Teaching Assistant (TA) method and Multi-In-One (MI1) method - to\nperform feature distillation in the DiT context. Unlike existing solutions, we\ndemonstrate and benchmark the efficacy of our approaches on practical Edge\ndevices such as NVIDIA Jetson Orin Nano.\n","authors":["Vignesh Sundaresha"],"pdf_url":"https://arxiv.org/pdf/2502.14226v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2502.14221v1","updated":"2025-02-20T03:36:12Z","published":"2025-02-20T03:36:12Z","title":"H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical\n  Imaging","summary":"  3D landmark detection is a critical task in medical image analysis, and\naccurately detecting anatomical landmarks is essential for subsequent medical\nimaging tasks. However, mainstream deep learning methods in this field struggle\nto simultaneously capture fine-grained local features and model global spatial\nrelationships, while maintaining a balance between accuracy and computational\nefficiency. Local feature extraction requires capturing fine-grained anatomical\ndetails, while global modeling requires understanding the spatial relationships\nwithin complex anatomical structures. The high-dimensional nature of 3D volume\nfurther exacerbates these challenges, as landmarks are sparsely distributed,\nleading to significant computational costs. Therefore, achieving efficient and\nprecise 3D landmark detection remains a pressing challenge in medical image\nanalysis.\n  In this work, We propose a \\textbf{H}ybrid \\textbf{3}D \\textbf{DE}tection\n\\textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature\nextraction with a lightweight attention mechanism designed to efficiently\ncapture global dependencies in 3D volumetric data. This mechanism employs a\nhierarchical routing strategy to reduce computational cost while maintaining\nglobal context modeling. To our knowledge, H3DE-Net is the first 3D landmark\ndetection model that integrates such a lightweight attention mechanism with\nCNNs. Additionally, integrating multi-scale feature fusion further enhances\ndetection accuracy and robustness. Experimental results on a public CT dataset\ndemonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance,\nsignificantly improving accuracy and robustness, particularly in scenarios with\nmissing landmarks or complex anatomical variations. We aready open-source our\nproject, including code, data and model weights.\n","authors":["Zhen Huang","Ronghao Xu","Xiaoqian Zhou","Yangbo Wei","Suhua Wang","Xiaoxin Sun","Han Li","Qingsong Yao"],"pdf_url":"https://arxiv.org/pdf/2502.14221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14214v1","updated":"2025-02-20T02:58:45Z","published":"2025-02-20T02:58:45Z","title":"Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation","summary":"  Source-free unsupervised domain adaptation (SFUDA) has gained significant\nattention as an alternative to traditional unsupervised domain adaptation\n(UDA), which relies on the constant availability of labeled source data.\nHowever, SFUDA approaches come with inherent limitations that are frequently\noverlooked. These challenges include performance degradation when the unlabeled\ntarget data fails to meet critical assumptions, such as having a closed-set\nlabel distribution identical to that of the source domain, or when sufficient\nunlabeled target data is unavailable-a common situation in real-world\napplications. To address these issues, we propose an asymmetric co-training\n(ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a\nmore practical alternative to SFUDA, as gathering a few labeled target\ninstances is more feasible than acquiring large volumes of unlabeled target\ndata in many real-world contexts. Our ACT method begins by employing a\nweak-strong augmentation to enhance data diversity. Then we use a two-step\noptimization process to train the target model. In the first step, we optimize\nthe label smoothing cross-entropy loss, the entropy of the class-conditional\ndistribution, and the reverse-entropy loss to bolster the model's\ndiscriminative ability while mitigating overfitting. The second step focuses on\nreducing redundancy in the output space by minimizing classifier determinacy\ndisparity. Extensive experiments across four benchmarks demonstrate the\nsuperiority of our ACT approach, which outperforms state-of-the-art SFUDA\nmethods and transfer learning techniques. Our findings suggest that adapting a\nsource pre-trained model using only a small amount of labeled target data\noffers a practical and dependable solution. The code is available at\nhttps://github.com/gengxuli/ACT.\n","authors":["Gengxu Li","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2502.14214v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.06527v2","updated":"2025-02-20T02:55:52Z","published":"2025-02-10T14:50:32Z","title":"CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for\n  Zero-Shot Customized Video Diffusion Transformers","summary":"  Customized generation has achieved significant progress in image synthesis,\nyet personalized video generation remains challenging due to temporal\ninconsistencies and quality degradation. In this paper, we introduce\nCustomVideoX, an innovative framework leveraging the video diffusion\ntransformer for personalized video generation from a reference image.\nCustomVideoX capitalizes on pre-trained video networks by exclusively training\nthe LoRA parameters to extract reference features, ensuring both efficiency and\nadaptability. To facilitate seamless interaction between the reference image\nand video content, we propose 3D Reference Attention, which enables direct and\nsimultaneous engagement of reference image features with all video frames\nacross spatial and temporal dimensions. To mitigate the excessive influence of\nreference image features and textual guidance on generated video content during\ninference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,\ndynamically modulating reference bias over different time steps. Additionally,\nwe introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly\nactivated regions of key entity tokens with reference feature injection by\nadjusting attention bias. To thoroughly evaluate personalized video generation,\nwe establish a new benchmark, VideoBench, comprising over 50 objects and 100\nprompts for extensive assessment. Experimental results show that CustomVideoX\nsignificantly outperforms existing methods in terms of video consistency and\nquality.\n","authors":["D. She","Mushui Liu","Jingxuan Pang","Jin Wang","Zhen Yang","Wanggui He","Guanghao Zhang","Yi Wang","Qihan Huang","Haobin Tang","Yunlong Yu","Siming Fu"],"pdf_url":"https://arxiv.org/pdf/2502.06527v2.pdf","comment":"Section 4 in CustomVideoX Entity Region-Aware Enhancement has\n  description errors. The compared methods data of Table I lacks other metrics"},{"id":"http://arxiv.org/abs/2502.14209v1","updated":"2025-02-20T02:43:55Z","published":"2025-02-20T02:43:55Z","title":"Spatial and Frequency Domain Adaptive Fusion Network for Image\n  Deblurring","summary":"  Image deblurring aims to reconstruct a latent sharp image from its\ncorresponding blurred one. Although existing methods have achieved good\nperformance, most of them operate exclusively in either the spatial domain or\nthe frequency domain, rarely exploring solutions that fuse both domains. In\nthis paper, we propose a spatial-frequency domain adaptive fusion network\n(SFAFNet) to address this limitation. Specifically, we design a gated\nspatial-frequency domain feature fusion block (GSFFBlock), which consists of\nthree key components: a spatial domain information module, a frequency domain\ninformation dynamic generation module (FDGM), and a gated fusion module (GFM).\nThe spatial domain information module employs the NAFBlock to integrate local\ninformation. Meanwhile, in the FDGM, we design a learnable low-pass filter that\ndynamically decomposes features into separate frequency subbands, capturing the\nimage-wide receptive field and enabling the adaptive exploration of global\ncontextual information. Additionally, to facilitate information flow and the\nlearning of complementary representations. In the GFM, we present a gating\nmechanism (GATE) to re-weight spatial and frequency domain features, which are\nthen fused through the cross-attention mechanism (CAM). Experimental results\ndemonstrate that our SFAFNet performs favorably compared to state-of-the-art\napproaches on commonly used benchmarks.\n","authors":["Hu Gao","Depeng Dang"],"pdf_url":"https://arxiv.org/pdf/2502.14209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18974v2","updated":"2025-02-20T02:42:30Z","published":"2024-10-24T17:59:30Z","title":"3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D\n  Generation","summary":"  Multi-view image diffusion models have significantly advanced open-domain 3D\nobject generation. However, most existing models rely on 2D network\narchitectures that lack inherent 3D biases, resulting in compromised geometric\nconsistency. To address this challenge, we introduce 3D-Adapter, a plug-in\nmodule designed to infuse 3D geometry awareness into pretrained image diffusion\nmodels. Central to our approach is the idea of 3D feedback augmentation: for\neach denoising step in the sampling loop, 3D-Adapter decodes intermediate\nmulti-view features into a coherent 3D representation, then re-encodes the\nrendered RGBD views to augment the pretrained base model through feature\naddition. We study two variants of 3D-Adapter: a fast feed-forward version\nbased on Gaussian splatting and a versatile training-free version utilizing\nneural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter\nnot only greatly enhances the geometry quality of text-to-multi-view models\nsuch as Instant3D and Zero123++, but also enables high-quality 3D generation\nusing the plain text-to-image Stable Diffusion. Furthermore, we showcase the\nbroad application potential of 3D-Adapter by presenting high quality results in\ntext-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.\n","authors":["Hansheng Chen","Bokui Shen","Yulin Liu","Ruoxi Shi","Linqi Zhou","Connor Z. Lin","Jiayuan Gu","Hao Su","Gordon Wetzstein","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2410.18974v2.pdf","comment":"Project page: https://lakonik.github.io/3d-adapter/"},{"id":"http://arxiv.org/abs/2404.13425v3","updated":"2025-02-20T02:24:55Z","published":"2024-04-20T17:19:54Z","title":"Enhancing Adversarial Robustness of Vision-Language Models through\n  Low-Rank Adaptation","summary":"  Vision-Language Models (VLMs) play a crucial role in the advancement of\nArtificial General Intelligence (AGI). As AGI rapidly evolves, addressing\nsecurity concerns has emerged as one of the most significant challenges for\nVLMs. In this paper, we present extensive experiments that expose the\nvulnerabilities of conventional adaptation methods for VLMs, highlighting\nsignificant security risks. Moreover, as VLMs grow in size, the application of\ntraditional adversarial adaptation techniques incurs substantial computational\ncosts. To address these issues, we propose a parameter-efficient adversarial\nadaptation method called \\textbf{\\textit{AdvLoRA}} based on Low-Rank\nAdaptation. We investigate and reveal the inherent low-rank properties involved\nin adversarial adaptation for VLMs. Different from LoRA, we enhance the\nefficiency and robustness of adversarial adaptation by introducing a novel\nreparameterization method that leverages parameter clustering and alignment.\nAdditionally, we propose an adaptive parameter update strategy to further\nbolster robustness. These innovations enable our AdvLoRA to mitigate issues\nrelated to model security and resource wastage. Extensive experiments confirm\nthe effectiveness and efficiency of AdvLoRA.\n","authors":["Yuheng Ji","Yue Liu","Zhicheng Zhang","Zhao Zhang","Yuting Zhao","Xiaoshuai Hao","Gang Zhou","Xingwei Zhang","Xiaolong Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.13425v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08449v3","updated":"2025-02-20T02:16:20Z","published":"2024-04-12T13:00:06Z","title":"OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering","summary":"  Rendering dynamic 3D human from monocular videos is crucial for various\napplications such as virtual reality and digital entertainment. Most methods\nassume the people is in an unobstructed scene, while various objects may cause\nthe occlusion of body parts in real-life scenarios. Previous method utilizing\nNeRF for surface rendering to recover the occluded areas, but it requiring more\nthan one day to train and several seconds to render, failing to meet the\nrequirements of real-time interactive applications. To address these issues, we\npropose OccGaussian based on 3D Gaussian Splatting, which can be trained within\n6 minutes and produces high-quality human renderings up to 160 FPS with\noccluded input. OccGaussian initializes 3D Gaussian distributions in the\ncanonical space, and we perform occlusion feature query at occluded regions,\nthe aggregated pixel-align feature is extracted to compensate for the missing\ninformation. Then we use Gaussian Feature MLP to further process the feature\nalong with the occlusion-aware loss functions to better perceive the occluded\narea. Extensive experiments both in simulated and real-world occlusions,\ndemonstrate that our method achieves comparable or even superior performance\ncompared to the state-of-the-art method. And we improving training and\ninference speeds by 250x and 800x, respectively. Our code will be available for\nresearch purposes.\n","authors":["Jingrui Ye","Zongkai Zhang","Yujiao Jiang","Qingmin Liao","Wenming Yang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2404.08449v3.pdf","comment":"We have decided to withdraw this paper because the results require\n  further verification or additional experimental data. We plan to resubmit an\n  updated version once the necessary work is completed"},{"id":"http://arxiv.org/abs/2502.14195v1","updated":"2025-02-20T02:00:02Z","published":"2025-02-20T02:00:02Z","title":"Bridging Text and Vision: A Multi-View Text-Vision Registration Approach\n  for Cross-Modal Place Recognition","summary":"  Mobile robots necessitate advanced natural language understanding\ncapabilities to accurately identify locations and perform tasks such as package\ndelivery. However, traditional visual place recognition (VPR) methods rely\nsolely on single-view visual information and cannot interpret human language\ndescriptions. To overcome this challenge, we bridge text and vision by\nproposing a multiview (360{\\deg} views of the surroundings) text-vision\nregistration approach called Text4VPR for place recognition task, which is the\nfirst method that exclusively utilizes textual descriptions to match a database\nof images. Text4VPR employs the frozen T5 language model to extract global\ntextual embeddings. Additionally, it utilizes the Sinkhorn algorithm with\ntemperature coefficient to assign local tokens to their respective clusters,\nthereby aggregating visual descriptors from images. During the training stage,\nText4VPR emphasizes the alignment between individual text-image pairs for\nprecise textual description. In the inference stage, Text4VPR uses the Cascaded\nCross-Attention Cosine Alignment (CCCA) to address the internal mismatch\nbetween text and image groups. Subsequently, Text4VPR performs precisely place\nmatch based on the descriptions of text-image groups. On Street360Loc, the\nfirst text to image VPR dataset we created, Text4VPR builds a robust baseline,\nachieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92%\nwithin a 5-meter radius on the test set, which indicates that localization from\ntextual descriptions to images is not only feasible but also holds significant\npotential for further advancement, as shown in Figure 1.\n","authors":["Tianyi Shang","Zhenyu Li","Pengjie Xu","Jinwei Qiao","Gang Chen","Zihan Ruan","Weijun Hu"],"pdf_url":"https://arxiv.org/pdf/2502.14195v1.pdf","comment":"8 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2502.14191v1","updated":"2025-02-20T01:48:13Z","published":"2025-02-20T01:48:13Z","title":"Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision\n  Language Models","summary":"  Reward models play an essential role in training vision-language models\n(VLMs) by assessing output quality to enable aligning with human preferences.\nDespite their importance, the research community lacks comprehensive open\nbenchmarks for evaluating multimodal reward models in VLMs. To address this\ngap, we introduce Multimodal RewardBench, an expert-annotated benchmark\ncovering six domains: general correctness, preference, knowledge, reasoning,\nsafety, and visual question-answering. Our dataset comprises 5,211 annotated\n(prompt, chosen response, rejected response) triplets collected from various\nVLMs. In evaluating a range of VLM judges, we find that even the top-performing\nmodels, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall\naccuracy. Notably, most models struggle in the reasoning and safety domains.\nThese findings suggest that Multimodal RewardBench offers a challenging testbed\nfor advancing reward model development across multiple domains. We release the\nbenchmark at https://github.com/facebookresearch/multimodal_rewardbench.\n","authors":["Michihiro Yasunaga","Luke Zettlemoyer","Marjan Ghazvininejad"],"pdf_url":"https://arxiv.org/pdf/2502.14191v1.pdf","comment":"Dataset available at\n  https://github.com/facebookresearch/multimodal_rewardbench"},{"id":"http://arxiv.org/abs/2502.14190v1","updated":"2025-02-20T01:46:17Z","published":"2025-02-20T01:46:17Z","title":"Stereo Image Coding for Machines with Joint Visual Feature Compression","summary":"  2D image coding for machines (ICM) has achieved great success in coding\nefficiency, while less effort has been devoted to stereo image fields. To\npromote the efficiency of stereo image compression (SIC) and intelligent\nanalysis, the stereo image coding for machines (SICM) is formulated and\nexplored in this paper. More specifically, a machine vision-oriented stereo\nfeature compression network (MVSFC-Net) is proposed for SICM, where the stereo\nvisual features are effectively extracted, compressed, and transmitted for 3D\nvisual task. To efficiently compress stereo visual features in MVSFC-Net, a\nstereo multi-scale feature compression (SMFC) module is designed to gradually\ntransform sparse stereo multi-scale features into compact joint visual\nrepresentations by removing spatial, inter-view, and cross-scale redundancies\nsimultaneously. Experimental results show that the proposed MVSFC-Net obtains\nsuperior compression efficiency as well as 3D visual task performance, when\ncompared with the existing ICM anchors recommended by MPEG and the\nstate-of-the-art SIC method.\n","authors":["Dengchao Jin","Jianjun Lei","Bo Peng","Zhaoqing Pan","Nam Ling","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2502.14190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14184v1","updated":"2025-02-20T01:26:05Z","published":"2025-02-20T01:26:05Z","title":"Bayesian SegNet for Semantic Segmentation with Improved Interpretation\n  of Microstructural Evolution During Irradiation of Materials","summary":"  Understanding the relationship between the evolution of microstructures of\nirradiated LiAlO2 pellets and tritium diffusion, retention and release could\nimprove predictions of tritium-producing burnable absorber rod performance.\nGiven expert-labeled segmented images of irradiated and unirradiated pellets,\nwe trained Deep Convolutional Neural Networks to segment images into defect,\ngrain, and boundary classes. Qualitative microstructural information was\ncalculated from these segmented images to facilitate the comparison of\nunirradiated and irradiated pellets. We tested modifications to improve the\nsensitivity of the model, including incorporating meta-data into the model and\nutilizing uncertainty quantification. The predicted segmentation was similar to\nthe expert-labeled segmentation for most methods of microstructural\nqualification, including pixel proportion, defect area, and defect density.\nOverall, the high performance metrics for the best models for both irradiated\nand unirradiated images shows that utilizing neural network models is a viable\nalternative to expert-labeled images.\n","authors":["Marjolein Oostrom","Alex Hagen","Nicole LaHaye","Karl Pazdernik"],"pdf_url":"https://arxiv.org/pdf/2502.14184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14178v1","updated":"2025-02-20T01:16:11Z","published":"2025-02-20T01:16:11Z","title":"NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio\n  Disentanglement for Talking Head Synthesis","summary":"  Talking head synthesis is to synthesize a lip-synchronized talking head video\nusing audio. Recently, the capability of NeRF to enhance the realism and\ntexture details of synthesized talking heads has attracted the attention of\nresearchers. However, most current NeRF methods based on audio are exclusively\nconcerned with the rendering of frontal faces. These methods are unable to\ngenerate clear talking heads in novel views. Another prevalent challenge in\ncurrent 3D talking head synthesis is the difficulty in aligning acoustic and\nvisual spaces, which often results in suboptimal lip-syncing of the generated\ntalking heads. To address these issues, we propose Neural Radiance Field with\n3D Prior Aided Audio Disentanglement for Talking Head Synthesis\n(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information\nto synthesize clear talking heads with free views. Additionally, we propose a\n3D Prior Aided Audio Disentanglement module, which is designed to disentangle\nthe audio into two distinct categories: features related to 3D awarded speech\nmovements and features related to speaking style. Moreover, to reposition the\ngenerated frames that are distant from the speaker's motion space in the real\nspace, we have devised a local-global Standardized Space. This method\nnormalizes the irregular positions in the generated frames from both global and\nlocal semantic perspectives. Through comprehensive qualitative and quantitative\nexperiments, it has been demonstrated that our NeRF-3DTalker outperforms\nstate-of-the-art in synthesizing realistic talking head videos, exhibiting\nsuperior image quality and lip synchronization. Project page:\nhttps://nerf-3dtalker.github.io/NeRF-3Dtalker.\n","authors":["Xiaoxing Liu","Zhilei Liu","Chongke Bi"],"pdf_url":"https://arxiv.org/pdf/2502.14178v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.14168v1","updated":"2025-02-20T00:35:14Z","published":"2025-02-20T00:35:14Z","title":"Deep learning based infrared small object segmentation: Challenges and\n  future directions","summary":"  Infrared sensing is a core method for supporting unmanned systems, such as\nautonomous vehicles and drones. Recently, infrared sensors have been widely\ndeployed on mobile and stationary platforms for detection and classification of\nobjects from long distances and in wide field of views. Given its success in\nthe vision image analysis domain, deep learning has also been applied for\nobject recognition in infrared images. However, techniques that have proven\nsuccessful in visible light perception face new challenges in the infrared\ndomain. These challenges include extremely low signal-to-noise ratios in\ninfrared images, very small and blurred objects of interest, and limited\navailability of labeled/unlabeled training data due to the specialized nature\nof infrared sensors. Numerous methods have been proposed in the literature for\nthe detection and classification of small objects in infrared images achieving\nvaried levels of success. There is a need for a survey paper that critically\nanalyzes existing techniques in this domain, identifies unsolved challenges and\nprovides future research directions. This paper fills the gap and offers a\nconcise and insightful review of deep learning-based methods. It also\nidentifies the challenges faced by existing infrared object segmentation\nmethods and provides a structured review of existing infrared perception\nmethods from the perspective of these challenges and highlights the motivations\nbehind the various approaches. Finally, this review suggests promising future\ndirections based on recent advancements within this domain.\n","authors":["Zhengeng Yang","Hongshan Yu","Jianjun Zhang","Qiang Tang","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2502.14168v1.pdf","comment":"This is a submitted version of a paper accepted by Information\n  Fusion. If you want a better reading experience, please refer to the final\n  published version of Information Fusion"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.11589v4","updated":"2025-02-20T03:41:23Z","published":"2024-06-17T14:34:14Z","title":"CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents","summary":"  Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 96.4%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.\n","authors":["Jing Gong","Yanghui Wu","Linxi Liang","Yanlin Wang","Jiachi Chen","Mingwei Liu","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.11589v4.pdf","comment":"15 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2502.13713v2","updated":"2025-02-20T02:43:15Z","published":"2025-02-19T13:28:20Z","title":"TALKPLAY: Multimodal Music Recommendation with Large Language Models","summary":"  We present TalkPlay, a multimodal music recommendation system that\nreformulates the recommendation task as large language model token generation.\nTalkPlay represents music through an expanded token vocabulary that encodes\nmultiple modalities - audio, lyrics, metadata, semantic tags, and playlist\nco-occurrence. Using these rich representations, the model learns to generate\nrecommendations through next-token prediction on music recommendation\nconversations, that requires learning the associations natural language query\nand response, as well as music items. In other words, the formulation\ntransforms music recommendation into a natural language understanding task,\nwhere the model's ability to predict conversation tokens directly optimizes\nquery-item relevance. Our approach eliminates traditional\nrecommendation-dialogue pipeline complexity, enabling end-to-end learning of\nquery-aware music recommendations. In the experiment, TalkPlay is successfully\ntrained and outperforms baseline methods in various aspects, demonstrating\nstrong context understanding as a conversational music recommender.\n","authors":["Seungheon Doh","Keunwoo Choi","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2502.13713v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14862v1","updated":"2025-02-20T18:59:34Z","published":"2025-02-20T18:59:34Z","title":"Interpretable Text Embeddings and Text Similarity Explanation: A Primer","summary":"  Text embeddings and text embedding models are a backbone of many AI and NLP\nsystems, particularly those involving search. However, interpretability\nchallenges persist, especially in explaining obtained similarity scores, which\nis crucial for applications requiring transparency. In this paper, we give a\nstructured overview of interpretability methods specializing in explaining\nthose similarity scores, an emerging research area. We study the methods'\nindividual ideas and techniques, evaluating their potential for improving\ninterpretability of text embeddings and explaining predicted similarities.\n","authors":["Juri Opitz","Lucas Möller","Andrianos Michail","Simon Clematide"],"pdf_url":"https://arxiv.org/pdf/2502.14862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14822v1","updated":"2025-02-20T18:42:58Z","published":"2025-02-20T18:42:58Z","title":"A Survey of Model Architectures in Information Retrieval","summary":"  This survey examines the evolution of model architectures in information\nretrieval (IR), focusing on two key aspects: backbone models for feature\nextraction and end-to-end system architectures for relevance estimation. The\nreview intentionally separates architectural considerations from training\nmethodologies to provide a focused analysis of structural innovations in IR\nsystems.We trace the development from traditional term-based methods to modern\nneural approaches, particularly highlighting the impact of transformer-based\nmodels and subsequent large language models (LLMs). We conclude by discussing\nemerging challenges and future directions, including architectural\noptimizations for performance and scalability, handling of multimodal,\nmultilingual data, and adaptation to novel application domains beyond\ntraditional search paradigms.\n","authors":["Zhichao Xu","Fengran Mo","Zhiqi Huang","Crystina Zhang","Puxuan Yu","Bei Wang","Jimmy Lin","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2502.14822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14796v1","updated":"2025-02-20T18:17:26Z","published":"2025-02-20T18:17:26Z","title":"A Multi-Agent Perspective on Modern Information Retrieval","summary":"  The rise of large language models (LLMs) has introduced a new era in\ninformation retrieval (IR), where queries and documents that were once assumed\nto be generated exclusively by humans can now also be created by automated\nagents. These agents can formulate queries, generate documents, and perform\nranking. This shift challenges some long-standing IR paradigms and calls for a\nreassessment of both theoretical frameworks and practical methodologies. We\nadvocate for a multi-agent perspective to better capture the complex\ninteractions between query agents, document agents, and ranker agents. Through\nempirical exploration of various multi-agent retrieval settings, we reveal the\nsignificant impact of these interactions on system performance. Our findings\nunderscore the need to revisit classical IR paradigms and develop new\nframeworks for more effective modeling and evaluation of modern retrieval\nsystems.\n","authors":["Haya Nachimovsky","Moshe Tennenholtz","Oren Kurland"],"pdf_url":"https://arxiv.org/pdf/2502.14796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14735v1","updated":"2025-02-20T17:01:57Z","published":"2025-02-20T17:01:57Z","title":"EAGER-LLM: Enhancing Large Language Models as Recommenders through\n  Exogenous Behavior-Semantic Integration","summary":"  Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks.\n","authors":["Minjie Hong","Yan Xia","Zehan Wang","Jieming Zhu","Ye Wang","Sihang Cai","Xiaoda Yang","Quanyu Dai","Zhenhua Dong","Zhimeng Zhang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.14735v1.pdf","comment":"9 pages, 6 figures, accpeted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.14714v1","updated":"2025-02-20T16:39:57Z","published":"2025-02-20T16:39:57Z","title":"From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT","summary":"  The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%).\n","authors":["Ahmed Abdeen Hamed","Byung Suk Lee"],"pdf_url":"https://arxiv.org/pdf/2502.14714v1.pdf","comment":"26 pages, 6 figures, In Review with a Cell Press Journal"},{"id":"http://arxiv.org/abs/2502.14662v1","updated":"2025-02-20T15:58:25Z","published":"2025-02-20T15:58:25Z","title":"InstructAgent: Building User Controllable Recommender via LLM Agent","summary":"  Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure. To this end, we first construct four recommendation\ndatasets, denoted as $\\dataset$, along with user instructions for each record.\n","authors":["Wujiang Xu","Yunxiao Shi","Zujie Liang","Xuying Ning","Kai Mei","Kun Wang","Xi Zhu","Min Xu","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14662v1.pdf","comment":"WWW2025@HCRS"},{"id":"http://arxiv.org/abs/2501.09751v2","updated":"2025-02-20T15:05:18Z","published":"2025-01-16T18:58:06Z","title":"OmniThink: Expanding Knowledge Boundaries in Machine Writing through\n  Thinking","summary":"  Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles.\n","authors":["Zekun Xi","Wenbiao Yin","Jizhan Fang","Jialong Wu","Runnan Fang","Ningyu Zhang","Jiang Yong","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09751v2.pdf","comment":"Code is available at https://github.com/zjunlp/OmniThink"},{"id":"http://arxiv.org/abs/2502.14625v1","updated":"2025-02-20T15:05:00Z","published":"2025-02-20T15:05:00Z","title":"Multi-Record Web Page Information Extraction From News Websites","summary":"  In this paper, we focused on the problem of extracting information from web\npages containing many records, a task of growing importance in the era of\nmassive web data. Recently, the development of neural network methods has\nimproved the quality of information extraction from web pages. Nevertheless,\nmost of the research and datasets are aimed at studying detailed pages. This\nhas left multi-record \"list pages\" relatively understudied, despite their\nwidespread presence and practical significance.\n  To address this gap, we created a large-scale, open-access dataset\nspecifically designed for list pages. This is the first dataset for this task\nin the Russian language. Our dataset contains 13,120 web pages with news lists,\nsignificantly exceeding existing datasets in both scale and complexity. Our\ndataset contains attributes of various types, including optional and\nmulti-valued, providing a realistic representation of real-world list pages.\nThese features make our dataset a valuable resource for studying information\nextraction from pages containing many records.\n  Furthermore, we proposed our own multi-stage information extraction methods.\nIn this work, we explore and demonstrate several strategies for applying\nMarkupLM to the specific challenges of multi-record web pages. Our experiments\nvalidate the advantages of our methods.\n  By releasing our dataset to the public, we aim to advance the field of\ninformation extraction from multi-record pages.\n","authors":["Alexander Kustenkov","Maksim Varlamov","Alexander Yatskov"],"pdf_url":"https://arxiv.org/pdf/2502.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05806v2","updated":"2025-02-20T14:58:39Z","published":"2024-09-09T17:11:51Z","title":"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs","summary":"  Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Jizhan Fang","Tianhe Lu","Yunzhi Yao","Ziyan Jiang","Xin Xu","Ningyu Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05806v2.pdf","comment":"Ongoing work; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2408.08073v2","updated":"2025-02-20T14:33:47Z","published":"2024-08-15T10:54:55Z","title":"Extracting Sentence Embeddings from Pretrained Transformer Models","summary":"  Pre-trained transformer models shine in many natural language processing\ntasks and therefore are expected to bear the representation of the input\nsentence or text meaning. These sentence-level embeddings are also important in\nretrieval-augmented generation. But do commonly used plain averaging or prompt\ntemplates sufficiently capture and represent the underlying meaning? After\nproviding a comprehensive review of existing sentence embedding extraction and\nrefinement methods, we thoroughly test different combinations and our original\nextensions of the most promising ones on pretrained models. Namely, given 110 M\nparameters, BERT's hidden representations from multiple layers, and many\ntokens, we try diverse ways to extract optimal sentence embeddings. We test\nvarious token aggregation and representation post-processing techniques. We\nalso test multiple ways of using a general Wikitext dataset to complement\nBERT's sentence embeddings. All methods are tested on eight Semantic Textual\nSimilarity (STS), six short text clustering, and twelve classification tasks.\nWe also evaluate our representation-shaping techniques on other static models,\nincluding random token representations. Proposed representation extraction\nmethods improve the performance on STS and clustering tasks for all models\nconsidered. Very high improvements for static token-based models, especially\nrandom embeddings for STS tasks, almost reach the performance of BERT-derived\nrepresentations. Our work shows that the representation-shaping techniques\nsignificantly improve sentence embeddings extracted from BERT-based and simple\nbaseline models.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2408.08073v2.pdf","comment":"Postprint update"},{"id":"http://arxiv.org/abs/2502.14409v1","updated":"2025-02-20T09:57:42Z","published":"2025-02-20T09:57:42Z","title":"Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization","summary":"  Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query. Extracting and properly citing\nevidence spans could help improve the transparency and reliability of these\nsummaries. At the same time, LLMs suffer from positional biases in terms of\nwhich information they understand and attend to, which could affect evidence\ncitation. Whereas previous work has focused on evidence citation with\npredefined levels of granularity (e.g. sentence, paragraph, document, etc.), we\npropose the task of long-context query focused summarization with unstructured\nevidence citation. We show how existing systems struggle to generate and\nproperly cite unstructured evidence from their context, and that evidence tends\nto be \"lost-in-the-middle\". To help mitigate this, we create the Summaries with\nUnstructured Evidence Text dataset (SUnsET), a synthetic dataset generated\nusing a novel domain-agnostic pipeline which can be used as supervision to\nadapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4\ndatasets with varying document types and lengths that LLMs adapted with SUnsET\ndata generate more relevant and factually consistent evidence than their base\nmodels, extract evidence from more diverse locations in their context, and can\ngenerate more relevant and consistent summaries.\n","authors":["Dustin Wright","Zain Muhammad Mujahid","Lu Wang","Isabelle Augenstein","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.14409v1.pdf","comment":"24 pages; 21 figures; 5 tables"},{"id":"http://arxiv.org/abs/2502.14361v1","updated":"2025-02-20T08:40:09Z","published":"2025-02-20T08:40:09Z","title":"Retrieval-Augmented Process Reward Model for Generalizable Mathematical\n  Reasoning","summary":"  While large language models (LLMs) have significantly advanced mathematical\nreasoning, Process Reward Models (PRMs) have been developed to evaluate the\nlogical validity of reasoning steps. However, PRMs still struggle with\nout-of-distribution (OOD) challenges. This paper identifies key OOD issues,\nincluding step OOD, caused by differences in reasoning patterns across model\ntypes and sizes, and question OOD, which arises from dataset shifts between\ntraining data and real-world problems. To address these issues, we introduce\nRetrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework\ndesigned to tackle these OOD issues. By utilizing a two-stage\nretrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar\nquestions and steps as a warmup, enhancing PRM's ability to evaluate target\nsteps and improving generalization and reasoning consistency across different\nmodels and problem types. Our extensive experiments demonstrate that\nRetrievalPRM outperforms existing baselines across multiple real-world\ndatasets. Our open-source contributions include a retrieval-enhanced dataset, a\ntuning framework for PRM training, and the RetrievalPRM model, establishing a\nnew standard for PRM performance.\n","authors":["Jiachen Zhu","Congmin Zheng","Jianghao Lin","Kounianhua Du","Ying Wen","Yong Yu","Jun Wang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14332v1","updated":"2025-02-20T07:30:43Z","published":"2025-02-20T07:30:43Z","title":"A Collaborative Jade Recognition System for Mobile Devices Based on\n  Lightweight and Large Models","summary":"  With the widespread adoption and development of mobile devices, vision-based\nrecognition applications have become a hot topic in research. Jade, as an\nimportant cultural heritage and artistic item, has significant applications in\nfields such as jewelry identification and cultural relic preservation. However,\nexisting jade recognition systems still face challenges in mobile\nimplementation, such as limited computing resources, real-time requirements,\nand accuracy issues. To address these challenges, this paper proposes a jade\nrecognition system based on size model collaboration, aiming to achieve\nefficient and accurate jade identification using mobile devices such as\nsmartphones.First, we design a size model based on multi-scale image\nprocessing, extracting key visual information by analyzing jade's dimensions,\nshapes, and surface textures. Then, a collaborative multi-model classification\nframework is built by combining deep learning and traditional computer vision\nalgorithms. This framework can effectively select and adjust models based on\ndifferent jade characteristics, providing high accuracy results across various\nenvironments and devices.Experimental results show that the proposed system can\nprovide high recognition accuracy and fast processing time on mobile devices,\nwhile consuming relatively low computational resources. The system not only\nholds great application potential but also provides new ideas and technical\nsupport for the intelligent development of jade identification.\n","authors":["Zhenyu Wang","Wenjia Li","Pengyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.14332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14305v1","updated":"2025-02-20T06:40:12Z","published":"2025-02-20T06:40:12Z","title":"Efficient AI in Practice: Training and Deployment of Efficient LLMs for\n  Industry Applications","summary":"  Large language models (LLMs) have demonstrated remarkable performance across\na wide range of industrial applications, from search and recommendations to\ngenerative tasks. Although scaling laws indicate that larger models generally\nyield better generalization and performance, their substantial computational\nrequirements often render them impractical for many real-world scenarios at\nscale. In this paper, we present methods and insights for training small\nlanguage models (SLMs) that deliver high performance and efficiency in\ndeployment. We focus on two key techniques: (1) knowledge distillation and (2)\nmodel compression via quantization and pruning. These approaches enable SLMs to\nretain much of the quality of their larger counterparts while significantly\nreducing training, serving costs, and latency. We detail the impact of these\ntechniques on a variety of use cases at a large professional social network\nplatform and share deployment lessons - including hardware optimization\nstrategies that enhance speed and throughput for both predictive and\nreasoning-based applications.\n","authors":["Kayhan Behdin","Yun Dai","Ata Fatahibaarzi","Aman Gupta","Qingquan Song","Shao Tang","Hejian Sang","Gregory Dexter","Sirou Zhu","Siyu Zhu","Tejas Dharamsi","Maziar Sanjabi","Vignesh Kothapalli","Hamed Firooz","Zhoutong Fu","Yihan Cao","Pin-Lun Hsu","Fedor Borisyuk","Zhipeng Wang","Rahul Mazumder","Natesh Pillai","Luke Simon"],"pdf_url":"https://arxiv.org/pdf/2502.14305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14297v1","updated":"2025-02-20T06:22:03Z","published":"2025-02-20T06:22:03Z","title":"An Evaluation of Sakana's AI Scientist for Autonomous Research: Wishful\n  Thinking or an Emerging Reality Towards 'Artificial General Research\n  Intelligence' (AGRI)?","summary":"  A major step toward Artificial General Intelligence (AGI) and Super\nIntelligence is AI's ability to autonomously conduct research - what we term\nArtificial General Research Intelligence (AGRI). If machines could generate\nhypotheses, conduct experiments, and write research papers without human\nintervention, it would transform science. Recently, Sakana.ai introduced the AI\nScientist, a system claiming to automate the research lifecycle, generating\nboth excitement and skepticism.\n  We evaluated the AI Scientist and found it a milestone in AI-driven research.\nWhile it streamlines some aspects, it falls short of expectations. Literature\nreviews are weak, nearly half the experiments failed, and manuscripts sometimes\ncontain hallucinated results. Most notably, users must provide an experimental\npipeline, limiting the AI Scientist's autonomy in research design and\nexecution.\n  Despite its limitations, the AI Scientist advances research automation. Many\nreviewers or instructors who assess work superficially may not recognize its\noutput as AI-generated. The system produces research papers with minimal human\neffort and low cost. Our analysis suggests a paper costs a few USD with a few\nhours of human involvement, making it significantly faster than human\nresearchers. Compared to AI capabilities from a few years ago, this marks\nprogress toward AGRI.\n  The rise of AI-driven research systems requires urgent discussion within\nInformation Retrieval (IR) and broader scientific communities. Enhancing\nliterature retrieval, citation validation, and evaluation benchmarks could\nimprove AI-generated research reliability. We propose concrete steps, including\nAGRI-specific benchmarks, refined peer review, and standardized attribution\nframeworks. Whether AGRI becomes a stepping stone to AGI depends on how the\nacademic and AI communities shape its development.\n","authors":["Joeran Beel","Min-Yen Kan","Moritz Baumgart"],"pdf_url":"https://arxiv.org/pdf/2502.14297v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2402.08976v3","updated":"2025-02-20T04:50:17Z","published":"2024-02-14T06:43:02Z","title":"Confidence-aware Fine-tuning of Sequential Recommendation Systems via\n  Conformal Prediction","summary":"  In Sequential Recommendation Systems (SRecsys), traditional training\napproaches that rely on Cross-Entropy (CE) loss often prioritize accuracy but\nfail to align well with user satisfaction metrics. CE loss focuses on\nmaximizing the confidence of the ground truth item, which is challenging to\nachieve universally across all users and sessions. It also overlooks the\npractical acceptability of ranking the ground truth item within the top-$K$\npositions, a common metric in SRecsys. To address this limitation, we propose\n\\textbf{CPFT}, a novel fine-tuning framework that integrates Conformal\nPrediction (CP)-based losses with CE loss to optimize accuracy alongside\nconfidence that better aligns with widely used top-$K$ metrics. CPFT embeds CP\nprinciples into the training loop using differentiable proxy losses and\ncomputationally efficient calibration strategies, enabling the generation of\nhigh-confidence prediction sets. These sets focus on items with high relevance\nwhile maintaining robust coverage guarantees. Extensive experiments on five\nreal-world datasets and four distinct sequential models demonstrate that CPFT\nimproves precision metrics and confidence calibration. Our results highlight\nthe importance of confidence-aware fine-tuning in delivering accurate,\ntrustworthy recommendations that enhance user satisfaction.\n","authors":["Chen Wang","Fangxin Wang","Ruocheng Guo","Yueqing Liang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2402.08976v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14212v1","updated":"2025-02-20T02:47:09Z","published":"2025-02-20T02:47:09Z","title":"Less is More: On the Importance of Data Quality for Unit Test Generation","summary":"  Unit testing is crucial for software development and maintenance. Effective\nunit testing ensures and improves software quality, but writing unit tests is\ntime-consuming and labor-intensive. Recent studies have proposed deep learning\n(DL) techniques or large language models (LLMs) to automate unit test\ngeneration. These models are usually trained or fine-tuned on large-scale\ndatasets. Despite growing awareness of the importance of data quality, there\nhas been limited research on the quality of datasets used for test generation.\nTo bridge this gap, we systematically examine the impact of noise on the\nperformance of learning-based test generation models. We first apply the open\ncard sorting method to analyze the most popular and largest test generation\ndataset, Methods2Test, to categorize eight distinct types of noise. Further, we\nconduct detailed interviews with 17 domain experts to validate and assess the\nimportance, reasonableness, and correctness of the noise taxonomy. Then, we\npropose CleanTest, an automated noise-cleaning framework designed to improve\nthe quality of test generation datasets. CleanTest comprises three filters: a\nrule-based syntax filter, a rule-based relevance filter, and a model-based\ncoverage filter. To evaluate its effectiveness, we apply CleanTest on two\nwidely-used test generation datasets, i.e., Methods2Test and Atlas. Our\nfindings indicate that 43.52% and 29.65% of datasets contain noise,\nhighlighting its prevalence. Finally, we conduct comparative experiments using\nfour LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess\nthe impact of noise on test generation performance. The results show that\nfiltering noise positively influences the test generation ability of the\nmodels.\n","authors":["Junwei Zhang","Xing Hu","Shan Gao","Xin Xia","David Lo","Shanping Li"],"pdf_url":"https://arxiv.org/pdf/2502.14212v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.06616v3","updated":"2025-02-20T14:27:15Z","published":"2025-02-10T16:12:47Z","title":"From Code to Canvas","summary":"  The web-based dynamic geometry software CindyJS is a versatile tool to create\ninteractive applications for mathematics and other topics. In this workshop, we\nwill look at a code package that makes the creation of animations in CindyJS\neasier and more streamlined. Animations, which can then be embedded into\npresentations or be used in (lecture) videos. The focus lies on the creation of\nthe animations themselves and some of the technical and artistic fundamentals\nto do so.\n","authors":["Bernhard O. Werner"],"pdf_url":"https://arxiv.org/pdf/2502.06616v3.pdf","comment":"A workshop paper for the Bridges 2025 conference"},{"id":"http://arxiv.org/abs/2502.14439v1","updated":"2025-02-20T10:42:29Z","published":"2025-02-20T10:42:29Z","title":"Visual and Auditory Aesthetic Preferences Across Cultures","summary":"  Research on how humans perceive aesthetics in shapes, colours, and music has\npredominantly focused on Western populations, limiting our understanding of how\ncultural environments shape aesthetic preferences. We present a large-scale\ncross-cultural study examining aesthetic preferences across five distinct\nmodalities extensively explored in the literature: shape, curvature, colour,\nmusical harmony and melody. Our investigation gathers 401,403 preference\njudgements from 4,835 participants across 10 countries, systematically sampling\ntwo-dimensional parameter spaces for each modality. The findings reveal both\nuniversal patterns and cultural variations. Preferences for shape and curvature\ncross-culturally demonstrate a consistent preference for symmetrical forms.\nWhile colour preferences are categorically consistent, relational preferences\nvary across cultures. Musical harmony shows strong agreement in interval\nrelationships despite differing regions of preference within the broad\nfrequency spectrum, while melody shows the highest cross-cultural variation.\nThese results suggest that aesthetic preferences emerge from an interplay\nbetween shared perceptual mechanisms and cultural learning.\n","authors":["Harin Lee","Eline Van Geert","Elif Celen","Raja Marjieh","Pol van Rijn","Minsu Park","Nori Jacoby"],"pdf_url":"https://arxiv.org/pdf/2502.14439v1.pdf","comment":"Submission to CogSci 2025"},{"id":"http://arxiv.org/abs/2502.14273v1","updated":"2025-02-20T05:18:36Z","published":"2025-02-20T05:18:36Z","title":"LLM-EvRep: Learning an LLM-Compatible Event Representation Using a\n  Self-Supervised Framework","summary":"  Recent advancements in event-based recognition have demonstrated significant\npromise, yet most existing approaches rely on extensive training, limiting\ntheir adaptability for efficient processing of event-driven visual content.\nMeanwhile, large language models (LLMs) have exhibited remarkable zero-shot\ncapabilities across diverse domains, but their application to event-based\nvisual recognition remains largely unexplored. To bridge this gap, we propose\n\\textbf{LLM-EvGen}, an event representation generator that produces\nLLM-compatible event representations \\textbf{LLM-EvRep}, thereby enhancing the\nperformance of LLMs on event recognition tasks. The generator is trained using\na self-supervised framework, aligning the generated representations with\nsemantic consistency and structural fidelity. Comprehensive experiments were\nconducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results\ndemonstrate that our method, \\textbf{LLM-EvRep}, outperforms the event-to-video\nmethod, E2VID, by 15.93\\%, 0.82\\%, and 50.21\\%, respectively, in recognition\ntasks when evaluated using GPT-4o.\n","authors":["Zongyou Yu","Qiang Qu","Qian Zhang","Nan Zhang","Xiaoming Chen"],"pdf_url":"https://arxiv.org/pdf/2502.14273v1.pdf","comment":"6 pages, 2 figures,Companion Proceedings of the ACM Web Conference\n  2025 (WWW Companion '25)"},{"id":"http://arxiv.org/abs/2502.14178v1","updated":"2025-02-20T01:16:11Z","published":"2025-02-20T01:16:11Z","title":"NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio\n  Disentanglement for Talking Head Synthesis","summary":"  Talking head synthesis is to synthesize a lip-synchronized talking head video\nusing audio. Recently, the capability of NeRF to enhance the realism and\ntexture details of synthesized talking heads has attracted the attention of\nresearchers. However, most current NeRF methods based on audio are exclusively\nconcerned with the rendering of frontal faces. These methods are unable to\ngenerate clear talking heads in novel views. Another prevalent challenge in\ncurrent 3D talking head synthesis is the difficulty in aligning acoustic and\nvisual spaces, which often results in suboptimal lip-syncing of the generated\ntalking heads. To address these issues, we propose Neural Radiance Field with\n3D Prior Aided Audio Disentanglement for Talking Head Synthesis\n(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information\nto synthesize clear talking heads with free views. Additionally, we propose a\n3D Prior Aided Audio Disentanglement module, which is designed to disentangle\nthe audio into two distinct categories: features related to 3D awarded speech\nmovements and features related to speaking style. Moreover, to reposition the\ngenerated frames that are distant from the speaker's motion space in the real\nspace, we have devised a local-global Standardized Space. This method\nnormalizes the irregular positions in the generated frames from both global and\nlocal semantic perspectives. Through comprehensive qualitative and quantitative\nexperiments, it has been demonstrated that our NeRF-3DTalker outperforms\nstate-of-the-art in synthesizing realistic talking head videos, exhibiting\nsuperior image quality and lip synchronization. Project page:\nhttps://nerf-3dtalker.github.io/NeRF-3Dtalker.\n","authors":["Xiaoxing Liu","Zhilei Liu","Chongke Bi"],"pdf_url":"https://arxiv.org/pdf/2502.14178v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.00358v2","updated":"2025-02-20T22:37:12Z","published":"2025-02-01T07:40:29Z","title":"Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?","summary":"  Unlike traditional visual segmentation, audio-visual segmentation (AVS)\nrequires the model not only to identify and segment objects but also to\ndetermine whether they are sound sources. Recent AVS approaches, leveraging\ntransformer architectures and powerful foundation models like SAM, have\nachieved impressive performance on standard benchmarks. Yet, an important\nquestion remains: Do these models genuinely integrate audio-visual cues to\nsegment sounding objects? In this paper, we systematically investigate this\nissue in the context of robust AVS. Our study reveals a fundamental bias in\ncurrent methods: they tend to generate segmentation masks based predominantly\non visual salience, irrespective of the audio context. This bias results in\nunreliable predictions when sounds are absent or irrelevant. To address this\nchallenge, we introduce AVSBench-Robust, a comprehensive benchmark\nincorporating diverse negative audio scenarios including silence, ambient\nnoise, and off-screen sounds. We also propose a simple yet effective approach\ncombining balanced training with negative samples and classifier-guided\nsimilarity learning. Our extensive experiments show that state-of-theart AVS\nmethods consistently fail under negative audio conditions, demonstrating the\nprevalence of visual bias. In contrast, our approach achieves remarkable\nimprovements in both standard metrics and robustness measures, maintaining\nnear-perfect false positive rates while preserving highquality segmentation\nperformance.\n","authors":["Jia Li","Wenjie Zhao","Ziru Huang","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2502.00358v2.pdf","comment":null}]},"2025-02-21T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.14801v2","updated":"2025-02-21T05:33:06Z","published":"2025-02-20T18:22:44Z","title":"AVD2: Accident Video Diffusion for Accident Video Description","summary":"  Traffic accidents present complex challenges for autonomous driving, often\nfeaturing unpredictable scenarios that hinder accurate system interpretation\nand responses. Nonetheless, prevailing methodologies fall short in elucidating\nthe causes of accidents and proposing preventive measures due to the paucity of\ntraining data specific to accident scenarios. In this work, we introduce AVD2\n(Accident Video Diffusion for Accident Video Description), a novel framework\nthat enhances accident scene understanding by generating accident videos that\naligned with detailed natural language descriptions and reasoning, resulting in\nthe contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding)\ndataset. Empirical results reveal that the integration of the EMM-AU dataset\nestablishes state-of-the-art performance across both automated metrics and\nhuman evaluations, markedly advancing the domains of accident analysis and\nprevention. Project resources are available at https://an-answer-tree.github.io\n","authors":["Cheng Li","Keyuan Zhou","Tong Liu","Yu Wang","Mingqiao Zhuang","Huan-ang Gao","Bu Jin","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.14801v2.pdf","comment":"ICRA 2025, Project Page: https://an-answer-tree.github.io/"},{"id":"http://arxiv.org/abs/2502.14795v2","updated":"2025-02-21T08:09:14Z","published":"2025-02-20T18:17:11Z","title":"Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration","summary":"  This paper addresses the limitations of current humanoid robot control\nframeworks, which primarily rely on reactive mechanisms and lack autonomous\ninteraction capabilities due to data scarcity. We propose Humanoid-VLA, a novel\nframework that integrates language understanding, egocentric scene perception,\nand motion control, enabling universal humanoid control. Humanoid-VLA begins\nwith language-motion pre-alignment using non-egocentric human motion datasets\npaired with textual descriptions, allowing the model to learn universal motion\npatterns and action semantics. We then incorporate egocentric visual context\nthrough a parameter efficient video-conditioned fine-tuning, enabling\ncontext-aware motion generation. Furthermore, we introduce a self-supervised\ndata augmentation strategy that automatically generates pseudoannotations\ndirectly derived from motion data. This process converts raw motion sequences\ninto informative question-answer pairs, facilitating the effective use of\nlarge-scale unlabeled video data. Built upon whole-body control architectures,\nextensive experiments show that Humanoid-VLA achieves object interaction and\nenvironment exploration tasks with enhanced contextual awareness, demonstrating\na more human-like capacity for adaptive and intelligent engagement.\n","authors":["Pengxiang Ding","Jianfei Ma","Xinyang Tong","Binghong Zou","Xinxin Luo","Yiguo Fan","Ting Wang","Hongchao Lu","Panzhong Mo","Jinxin Liu","Yuefan Wang","Huaicheng Zhou","Wenshuo Feng","Jiacheng Liu","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14584v2","updated":"2025-02-21T03:42:34Z","published":"2025-02-20T14:13:46Z","title":"Vision Foundation Models in Medical Image Analysis: Advances and\n  Challenges","summary":"  The rapid development of Vision Foundation Models (VFMs), particularly Vision\nTransformers (ViT) and Segment Anything Model (SAM), has sparked significant\nadvances in the field of medical image analysis. These models have demonstrated\nexceptional capabilities in capturing long-range dependencies and achieving\nhigh generalization in segmentation tasks. However, adapting these large models\nto medical image analysis presents several challenges, including domain\ndifferences between medical and natural images, the need for efficient model\nadaptation strategies, and the limitations of small-scale medical datasets.\nThis paper reviews the state-of-the-art research on the adaptation of VFMs to\nmedical image segmentation, focusing on the challenges of domain adaptation,\nmodel compression, and federated learning. We discuss the latest developments\nin adapter-based improvements, knowledge distillation techniques, and\nmulti-scale contextual feature modeling, and propose future directions to\novercome these bottlenecks. Our analysis highlights the potential of VFMs,\nalong with emerging methodologies such as federated learning and model\ncompression, to revolutionize medical image analysis and enhance clinical\napplications. The goal of this work is to provide a comprehensive overview of\ncurrent approaches and suggest key areas for future research that can drive the\nnext wave of innovation in medical image segmentation.\n","authors":["Pengchen Liang","Bin Pu","Haishan Huang","Yiwei Li","Hualiang Wang","Weibo Ma","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2502.14584v2.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.14487v2","updated":"2025-02-21T09:05:35Z","published":"2025-02-20T12:09:30Z","title":"Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via\n  Probabilistic Spiking Neurons","summary":"  Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to\nArtificial Neural Networks (ANNs) by mimicking biological neural principles,\nestablishing them as a promising approach to mitigate the increasing energy\ndemands of large-scale neural models. However, fully harnessing the\ncapabilities of SNNs remains challenging due to their discrete signal\nprocessing and temporal dynamics. ANN-SNN conversion has emerged as a practical\napproach, enabling SNNs to achieve competitive performance on complex machine\nlearning tasks. In this work, we identify a phenomenon in the ANN-SNN\nconversion framework, termed temporal misalignment, in which random spike\nrearrangement across SNN layers leads to performance improvements. Based on\nthis observation, we introduce biologically plausible two-phase probabilistic\n(TPP) spiking neurons, further enhancing the conversion process. We demonstrate\nthe advantages of our proposed method both theoretically and empirically\nthrough comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet\nacross a variety of architectures, achieving state-of-the-art results.\n","authors":["Velibor Bojković","Xiaofeng Wu","Bin Gu"],"pdf_url":"https://arxiv.org/pdf/2502.14487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14420v2","updated":"2025-02-21T07:28:36Z","published":"2025-02-20T10:16:18Z","title":"ChatVLA: Unified Multimodal Understanding and Robot Control with\n  Vision-Language-Action Model","summary":"  Humans possess a unified cognitive ability to perceive, comprehend, and\ninteract with the physical world. Why can't large language models replicate\nthis holistic understanding? Through a systematic analysis of existing training\nparadigms in vision-language-action models (VLA), we identify two key\nchallenges: spurious forgetting, where robot training overwrites crucial\nvisual-text alignments, and task interference, where competing control and\nunderstanding tasks degrade performance when trained jointly. To overcome these\nlimitations, we propose ChatVLA, a novel framework featuring Phased Alignment\nTraining, which incrementally integrates multimodal data after initial control\nmastery, and a Mixture-of-Experts architecture to minimize task interference.\nChatVLA demonstrates competitive performance on visual question-answering\ndatasets and significantly surpasses state-of-the-art vision-language-action\n(VLA) methods on multimodal understanding benchmarks. Notably, it achieves a\nsix times higher performance on MMMU and scores 47.2% on MMStar with a more\nparameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates\nsuperior performance on 25 real-world robot manipulation tasks compared to\nexisting VLA methods like OpenVLA. Our findings highlight the potential of our\nunified framework for achieving both robust multimodal understanding and\neffective robot control.\n","authors":["Zhongyi Zhou","Yichen Zhu","Minjie Zhu","Junjie Wen","Ning Liu","Zhiyuan Xu","Weibin Meng","Ran Cheng","Yaxin Peng","Chaomin Shen","Feifei Feng"],"pdf_url":"https://arxiv.org/pdf/2502.14420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14377v2","updated":"2025-02-21T10:02:02Z","published":"2025-02-20T09:10:05Z","title":"RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers","summary":"  The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta.\n","authors":["Ke Cao","Jing Wang","Ao Ma","Jiasong Feng","Zhanjie Zhang","Xuanhua He","Shanyuan Liu","Bo Cheng","Dawei Leng","Yuhui Yin","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14377v2.pdf","comment":"Homepage: https://360cvgroup.github.io/RelaCtrl/ Github:\n  https://github.com/360CVGroup/RelaCtrl"},{"id":"http://arxiv.org/abs/2502.14282v2","updated":"2025-02-21T02:54:09Z","published":"2025-02-20T05:41:55Z","title":"PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex\n  Task Automation on PC","summary":"  In the field of MLLM-based GUI agents, compared to smartphones, the PC\nscenario not only features a more complex interactive environment, but also\ninvolves more intricate intra- and inter-app workflows. To address these\nissues, we propose a hierarchical agent framework named PC-Agent. Specifically,\nfrom the perception perspective, we devise an Active Perception Module (APM) to\novercome the inadequate abilities of current MLLMs in perceiving screenshot\ncontent. From the decision-making perspective, to handle complex user\ninstructions and interdependent subtasks more effectively, we propose a\nhierarchical multi-agent collaboration architecture that decomposes\ndecision-making processes into Instruction-Subtask-Action levels. Within this\narchitecture, three agents (i.e., Manager, Progress and Decision) are set up\nfor instruction decomposition, progress tracking and step-by-step\ndecision-making respectively. Additionally, a Reflection agent is adopted to\nenable timely bottom-up error feedback and adjustment. We also introduce a new\nbenchmark PC-Eval with 25 real-world complex instructions. Empirical results on\nPC-Eval show that our PC-Agent achieves a 32% absolute improvement of task\nsuccess rate over previous state-of-the-art methods. The code is available at\nhttps://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent.\n","authors":["Haowei Liu","Xi Zhang","Haiyang Xu","Yuyang Wanyan","Junyang Wang","Ming Yan","Ji Zhang","Chunfeng Yuan","Changsheng Xu","Weiming Hu","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2502.14282v2.pdf","comment":"14 pages, 7 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.13909v2","updated":"2025-02-21T03:12:46Z","published":"2025-02-19T17:41:09Z","title":"Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?","summary":"  Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.\n","authors":["Sein Kim","Hongseok Kang","Kibum Kim","Jiwan Kim","Donghyun Kim","Minchul Yang","Kwangjin Oh","Julian McAuley","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2502.13909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13506v2","updated":"2025-02-21T10:18:26Z","published":"2025-02-19T07:50:59Z","title":"Reproducing NevIR: Negation in Neural Information Retrieval","summary":"  Negation is a fundamental aspect of human communication, yet it remains a\nchallenge for Language Models (LMs) in Information Retrieval (IR). Despite the\nheavy reliance of modern neural IR systems on LMs, little attention has been\ngiven to their handling of negation. In this study, we reproduce and extend the\nfindings of NevIR, a benchmark study that revealed most IR models perform at or\nbelow the level of random ranking when dealing with negation. We replicate\nNevIR's original experiments and evaluate newly developed state-of-the-art IR\nmodels. Our findings show that a recently emerging category - listwise Large\nLanguage Model (LLM) rerankers - outperforms other models but still\nunderperforms human performance. Additionally, we leverage ExcluIR, a benchmark\ndataset designed for exclusionary queries with extensive negation, to assess\nthe generalizability of negation understanding. Our findings suggest that\nfine-tuning on one dataset does not reliably improve performance on the other,\nindicating notable differences in their data distributions. Furthermore, we\nobserve that only cross-encoders and listwise LLM rerankers achieve reasonable\nperformance across both negation tasks.\n","authors":["Coen van den Elsen","Francien Barkhof","Thijmen Nijdam","Simon Lupart","Mohammad Alliannejadi"],"pdf_url":"https://arxiv.org/pdf/2502.13506v2.pdf","comment":"9 pages, 5 figures, under review at SIGIR 2025"},{"id":"http://arxiv.org/abs/2502.13245v2","updated":"2025-02-21T15:20:30Z","published":"2025-02-18T19:18:01Z","title":"Range Retrieval with Graph-Based Indices","summary":"  Retrieving points based on proximity in a high-dimensional vector space is a\ncrucial step in information retrieval applications. The approximate nearest\nneighbor search (ANNS) problem, which identifies the $k$ nearest neighbors for\na query (approximately, since exactly is hard), has been extensively studied in\nrecent years. However, comparatively little attention has been paid to the\nrelated problem of finding all points within a given distance of a query, the\nrange retrieval problem, despite its applications in areas such as duplicate\ndetection, plagiarism checking, and facial recognition. In this paper, we\npresent a set of algorithms for range retrieval on graph-based vector indices,\nwhich are known to achieve excellent performance on ANNS queries. Since a range\nquery may have anywhere from no matching results to thousands of matching\nresults in the database, we introduce a set of range retrieval algorithms based\non modifications of the standard graph search that adapt to terminate quickly\non queries in the former group, and to put more resources into finding results\nfor the latter group. Due to the lack of existing benchmarks for range\nretrieval, we also undertake a comprehensive study of range characteristics of\nexisting embedding datasets, and select a suitable range retrieval radius for\neight existing datasets with up to 100 million points in addition to the one\nexisting benchmark. We test our algorithms on these datasets, and find up to\n100x improvement in query throughput over a naive baseline approach, with 5-10x\nimprovement on average, and strong performance up to 100 million data points.\n","authors":["Magdalen Dobson Manohar","Taekseung Kim","Guy E. Blelloch"],"pdf_url":"https://arxiv.org/pdf/2502.13245v2.pdf","comment":null}]}}